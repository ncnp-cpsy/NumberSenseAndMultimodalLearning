working directory:  /home/taka/MMVAE/NumberSenseAndMultimodalLearning
omp thread num:  2
ncpus:  2
cuda visible devices:  GPU-7bbdfddb-899d-ec05-00f1-27e63c9f71b9
2023/12/05 PM 08:51:56
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'pretrained_path': '',
 'print_freq': 0,
 'run_id': 'test_classifier-cmnist',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 test_classifier-cmnist
Run Directory:
 ./rslt/test-pbs-2/Classifier_CMNIST/test_classifier-cmnist
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'output_dir': './rslt/test-pbs-2/Classifier_CMNIST/test_classifier-cmnist/train',
 'pretrained_path': '',
 'print_freq': 0,
 'run_dir': './rslt/test-pbs-2/Classifier_CMNIST/test_classifier-cmnist',
 'run_id': 'test_classifier-cmnist',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross
====> Epoch: 001 Train loss: 0.0026  took : 2.982353687286377
====> Test loss: -0.0015, Test accuracy: 0.9450
====> Epoch: 002 Train loss: 0.0009  took : 2.828847646713257
====> Test loss: -0.0010, Test accuracy: 0.9673
====> Epoch: 003 Train loss: 0.0005  took : 2.814796209335327
====> Test loss: -0.0009, Test accuracy: 0.9710
====> Epoch: 004 Train loss: 0.0003  took : 2.846580743789673
====> Test loss: -0.0010, Test accuracy: 0.9735
====> Epoch: 005 Train loss: 0.0001  took : 2.827582836151123
====> Test loss: -0.0010, Test accuracy: 0.9740
====> Epoch: 006 Train loss: 0.0001  took : 2.8477532863616943
====> Test loss: -0.0010, Test accuracy: 0.9762
====> Epoch: 007 Train loss: 0.0000  took : 2.8366334438323975
====> Test loss: -0.0011, Test accuracy: 0.9750
====> Epoch: 008 Train loss: 0.0000  took : 2.95149302482605
====> Test loss: -0.0012, Test accuracy: 0.9760
====> Epoch: 009 Train loss: 0.0000  took : 2.9788060188293457
====> Test loss: -0.0011, Test accuracy: 0.9768
====> Epoch: 010 Train loss: 0.0000  took : 2.933053731918335
====> Test loss: -0.0011, Test accuracy: 0.9758
====> Epoch: 011 Train loss: 0.0000  took : 2.9425313472747803
====> Test loss: -0.0012, Test accuracy: 0.9760
====> Epoch: 012 Train loss: 0.0000  took : 2.9432199001312256
====> Test loss: -0.0012, Test accuracy: 0.9768
====> Epoch: 013 Train loss: 0.0000  took : 2.939281463623047
====> Test loss: -0.0012, Test accuracy: 0.9762
====> Epoch: 014 Train loss: 0.0000  took : 2.95145583152771
====> Test loss: -0.0013, Test accuracy: 0.9760
====> Epoch: 015 Train loss: 0.0000  took : 2.962752342224121
====> Test loss: -0.0013, Test accuracy: 0.9758
====> Epoch: 016 Train loss: 0.0000  took : 2.9577009677886963
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 017 Train loss: 0.0000  took : 2.932910442352295
====> Test loss: -0.0013, Test accuracy: 0.9762
====> Epoch: 018 Train loss: 0.0000  took : 2.951418161392212
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 019 Train loss: 0.0000  took : 2.947558879852295
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 020 Train loss: 0.0000  took : 2.9432168006896973
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 021 Train loss: 0.0000  took : 2.9443066120147705
====> Test loss: -0.0014, Test accuracy: 0.9760
====> Epoch: 022 Train loss: 0.0000  took : 2.971709966659546
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 023 Train loss: 0.0000  took : 2.9584739208221436
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 024 Train loss: 0.0000  took : 2.971372365951538
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 025 Train loss: 0.0000  took : 2.9466958045959473
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 026 Train loss: 0.0000  took : 2.996229648590088
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 027 Train loss: 0.0000  took : 2.9324750900268555
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 028 Train loss: 0.0000  took : 2.9365103244781494
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 029 Train loss: 0.0000  took : 2.920069932937622
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 030 Train loss: 0.0000  took : 2.94033145904541
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 031 Train loss: 0.0000  took : 2.9334826469421387
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 032 Train loss: 0.0000  took : 2.9233920574188232
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 033 Train loss: 0.0000  took : 2.915949583053589
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 034 Train loss: 0.0000  took : 2.955630302429199
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 035 Train loss: 0.0000  took : 2.9358088970184326
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 036 Train loss: 0.0000  took : 2.932849168777466
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 037 Train loss: 0.0000  took : 2.967625379562378
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 038 Train loss: 0.0000  took : 2.9307363033294678
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 039 Train loss: 0.0000  took : 2.924182891845703
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 040 Train loss: 0.0000  took : 2.9386391639709473
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 041 Train loss: 0.0000  took : 2.945402145385742
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 042 Train loss: 0.0000  took : 2.9532198905944824
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 043 Train loss: 0.0000  took : 2.996912956237793
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 044 Train loss: 0.0000  took : 2.971658706665039
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 045 Train loss: 0.0000  took : 2.9321017265319824
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 046 Train loss: 0.0000  took : 2.9269957542419434
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 047 Train loss: 0.0000  took : 2.9348652362823486
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 048 Train loss: 0.0000  took : 2.9425156116485596
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 049 Train loss: 0.0000  took : 2.9498374462127686
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 050 Train loss: 0.0000  took : 2.952615261077881
====> Test loss: -0.0015, Test accuracy: 0.9765
====> [MM-VAE] Time: 176.621s or 00:02:56
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'pretrained_path': '',
 'print_freq': 0,
 'run_id': 'test-classifier-oscn',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 test-classifier-oscn
Run Directory:
 ./rslt/test-pbs-2/Classifier_OSCN/test-classifier-oscn
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'output_dir': './rslt/test-pbs-2/Classifier_OSCN/test-classifier-oscn/train',
 'pretrained_path': '',
 'print_freq': 0,
 'run_dir': './rslt/test-pbs-2/Classifier_OSCN/test-classifier-oscn',
 'run_id': 'test-classifier-oscn',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross
====> Epoch: 001 Train loss: 0.0015  took : 4.033600807189941
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 002 Train loss: 0.0000  took : 3.847630500793457
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 003 Train loss: 0.0000  took : 3.873579502105713
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 004 Train loss: 0.0000  took : 3.9122350215911865
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 005 Train loss: 0.0000  took : 3.8710596561431885
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 006 Train loss: 0.0000  took : 3.9280269145965576
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 007 Train loss: 0.0000  took : 3.8812928199768066
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 008 Train loss: 0.0000  took : 3.832775831222534
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 009 Train loss: 0.0000  took : 4.4013824462890625
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 010 Train loss: 0.0000  took : 3.9679245948791504
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 011 Train loss: 0.0000  took : 3.811641216278076
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 012 Train loss: 0.0000  took : 3.839779853820801
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 013 Train loss: 0.0000  took : 3.8290390968322754
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 014 Train loss: 0.0000  took : 4.0102598667144775
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 015 Train loss: 0.0000  took : 4.274298429489136
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 016 Train loss: 0.0000  took : 4.38140869140625
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 017 Train loss: 0.0000  took : 3.8671255111694336
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 018 Train loss: 0.0000  took : 4.062257766723633
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 019 Train loss: 0.0000  took : 3.8429465293884277
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 020 Train loss: 0.0000  took : 3.8543272018432617
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 021 Train loss: 0.0000  took : 3.9866654872894287
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 022 Train loss: 0.0000  took : 4.018359184265137
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 023 Train loss: 0.0000  took : 3.8071041107177734
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 024 Train loss: 0.0000  took : 4.185268402099609
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 025 Train loss: 0.0000  took : 3.831845760345459
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 026 Train loss: 0.0000  took : 3.8288660049438477
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 027 Train loss: 0.0000  took : 3.865785837173462
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 028 Train loss: 0.0000  took : 3.916619300842285
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 029 Train loss: 0.0000  took : 3.9157097339630127
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 030 Train loss: 0.0000  took : 3.8255703449249268
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 031 Train loss: 0.0000  took : 3.839803457260132
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 032 Train loss: 0.0000  took : 4.067456007003784
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 033 Train loss: 0.0000  took : 3.916908025741577
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 034 Train loss: 0.0000  took : 3.9229063987731934
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 035 Train loss: 0.0000  took : 3.8935885429382324
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 036 Train loss: 0.0000  took : 3.8083856105804443
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 037 Train loss: 0.0000  took : 3.8627638816833496
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 038 Train loss: 0.0000  took : 3.82769775390625
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 039 Train loss: 0.0000  took : 3.955613613128662
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 040 Train loss: 0.0000  took : 4.067643165588379
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 041 Train loss: 0.0000  took : 4.215612411499023
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 042 Train loss: 0.0000  took : 3.9728035926818848
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 043 Train loss: 0.0000  took : 4.114436149597168
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 044 Train loss: 0.0000  took : 3.855334520339966
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 045 Train loss: 0.0000  took : 3.843183755874634
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 046 Train loss: 0.0000  took : 3.8863532543182373
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 047 Train loss: 0.0000  took : 3.8596112728118896
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 048 Train loss: 0.0000  took : 3.7990264892578125
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 049 Train loss: 0.0000  took : 3.8358852863311768
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 050 Train loss: 0.0000  took : 3.8195958137512207
====> Test loss: -0.0000, Test accuracy: 1.0000
====> [MM-VAE] Time: 212.865s or 00:03:32
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'print_freq': 100,
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_0
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1993.552
iteration 0100: loss: 1572.296
iteration 0200: loss: 1556.896
iteration 0300: loss: 1551.120
iteration 0400: loss: 1541.943
iteration 0500: loss: 1541.464
iteration 0600: loss: 1542.875
iteration 0700: loss: 1538.344
iteration 0800: loss: 1536.366
iteration 0900: loss: 1541.367
====> Epoch: 001 Train loss: 1551.8095  took : 5.864028453826904
====> Test loss: 1531.4405
iteration 0000: loss: 1535.404
iteration 0100: loss: 1531.760
iteration 0200: loss: 1536.481
iteration 0300: loss: 1535.013
iteration 0400: loss: 1538.104
iteration 0500: loss: 1535.611
iteration 0600: loss: 1533.547
iteration 0700: loss: 1538.750
iteration 0800: loss: 1532.781
iteration 0900: loss: 1531.346
====> Epoch: 002 Train loss: 1534.0265  took : 5.935276746749878
====> Test loss: 1527.1358
iteration 0000: loss: 1533.235
iteration 0100: loss: 1530.160
iteration 0200: loss: 1530.821
iteration 0300: loss: 1532.710
iteration 0400: loss: 1527.171
iteration 0500: loss: 1527.048
iteration 0600: loss: 1533.412
iteration 0700: loss: 1526.254
iteration 0800: loss: 1534.011
iteration 0900: loss: 1531.451
====> Epoch: 003 Train loss: 1531.3003  took : 5.887913227081299
====> Test loss: 1525.5146
iteration 0000: loss: 1530.399
iteration 0100: loss: 1526.265
iteration 0200: loss: 1530.819
iteration 0300: loss: 1528.401
iteration 0400: loss: 1530.388
iteration 0500: loss: 1529.413
iteration 0600: loss: 1529.705
iteration 0700: loss: 1532.364
iteration 0800: loss: 1527.393
iteration 0900: loss: 1528.432
====> Epoch: 004 Train loss: 1530.1085  took : 5.865945339202881
====> Test loss: 1524.7097
iteration 0000: loss: 1528.126
iteration 0100: loss: 1528.626
iteration 0200: loss: 1528.685
iteration 0300: loss: 1532.624
iteration 0400: loss: 1528.808
iteration 0500: loss: 1530.698
iteration 0600: loss: 1529.271
iteration 0700: loss: 1528.573
iteration 0800: loss: 1526.950
iteration 0900: loss: 1528.520
====> Epoch: 005 Train loss: 1529.3308  took : 5.917011022567749
====> Test loss: 1524.1514
iteration 0000: loss: 1527.825
iteration 0100: loss: 1526.419
iteration 0200: loss: 1524.677
iteration 0300: loss: 1527.880
iteration 0400: loss: 1529.652
iteration 0500: loss: 1525.282
iteration 0600: loss: 1531.178
iteration 0700: loss: 1527.467
iteration 0800: loss: 1529.472
iteration 0900: loss: 1528.282
====> Epoch: 006 Train loss: 1528.7638  took : 5.884819507598877
====> Test loss: 1523.6905
iteration 0000: loss: 1529.046
iteration 0100: loss: 1530.664
iteration 0200: loss: 1526.075
iteration 0300: loss: 1529.714
iteration 0400: loss: 1527.443
iteration 0500: loss: 1528.655
iteration 0600: loss: 1530.573
iteration 0700: loss: 1531.424
iteration 0800: loss: 1522.908
iteration 0900: loss: 1527.313
====> Epoch: 007 Train loss: 1528.3374  took : 5.864850997924805
====> Test loss: 1523.2171
iteration 0000: loss: 1530.499
iteration 0100: loss: 1527.552
iteration 0200: loss: 1525.858
iteration 0300: loss: 1528.065
iteration 0400: loss: 1529.690
iteration 0500: loss: 1529.367
iteration 0600: loss: 1527.506
iteration 0700: loss: 1526.438
iteration 0800: loss: 1528.852
iteration 0900: loss: 1527.857
====> Epoch: 008 Train loss: 1527.9861  took : 5.898035049438477
====> Test loss: 1522.9885
iteration 0000: loss: 1527.423
iteration 0100: loss: 1524.745
iteration 0200: loss: 1525.573
iteration 0300: loss: 1525.878
iteration 0400: loss: 1530.625
iteration 0500: loss: 1523.914
iteration 0600: loss: 1525.400
iteration 0700: loss: 1529.799
iteration 0800: loss: 1526.840
iteration 0900: loss: 1529.279
====> Epoch: 009 Train loss: 1527.6945  took : 5.912106990814209
====> Test loss: 1522.8372
iteration 0000: loss: 1528.085
iteration 0100: loss: 1526.448
iteration 0200: loss: 1525.850
iteration 0300: loss: 1525.681
iteration 0400: loss: 1527.986
iteration 0500: loss: 1528.667
iteration 0600: loss: 1526.921
iteration 0700: loss: 1527.729
iteration 0800: loss: 1527.628
iteration 0900: loss: 1531.221
====> Epoch: 010 Train loss: 1527.4252  took : 5.883217096328735
====> Test loss: 1522.6721
iteration 0000: loss: 1525.967
iteration 0100: loss: 1526.067
iteration 0200: loss: 1527.014
iteration 0300: loss: 1529.842
iteration 0400: loss: 1524.806
iteration 0500: loss: 1524.903
iteration 0600: loss: 1527.998
iteration 0700: loss: 1527.418
iteration 0800: loss: 1528.547
iteration 0900: loss: 1528.298
====> Epoch: 011 Train loss: 1527.1920  took : 5.877269744873047
====> Test loss: 1522.4114
iteration 0000: loss: 1526.372
iteration 0100: loss: 1526.399
iteration 0200: loss: 1526.380
iteration 0300: loss: 1528.115
iteration 0400: loss: 1528.812
iteration 0500: loss: 1527.389
iteration 0600: loss: 1528.478
iteration 0700: loss: 1527.583
iteration 0800: loss: 1530.561
iteration 0900: loss: 1528.179
====> Epoch: 012 Train loss: 1526.9717  took : 5.916065692901611
====> Test loss: 1522.1585
iteration 0000: loss: 1527.620
iteration 0100: loss: 1525.695
iteration 0200: loss: 1527.734
iteration 0300: loss: 1527.627
iteration 0400: loss: 1527.075
iteration 0500: loss: 1528.122
iteration 0600: loss: 1528.393
iteration 0700: loss: 1529.814
iteration 0800: loss: 1525.309
iteration 0900: loss: 1526.514
====> Epoch: 013 Train loss: 1526.7716  took : 5.879583120346069
====> Test loss: 1522.0073
iteration 0000: loss: 1525.840
iteration 0100: loss: 1524.164
iteration 0200: loss: 1526.395
iteration 0300: loss: 1528.344
iteration 0400: loss: 1525.880
iteration 0500: loss: 1526.135
iteration 0600: loss: 1526.123
iteration 0700: loss: 1525.687
iteration 0800: loss: 1526.962
iteration 0900: loss: 1524.313
====> Epoch: 014 Train loss: 1526.5985  took : 5.966478109359741
====> Test loss: 1521.9148
iteration 0000: loss: 1525.970
iteration 0100: loss: 1526.759
iteration 0200: loss: 1526.902
iteration 0300: loss: 1525.722
iteration 0400: loss: 1527.481
iteration 0500: loss: 1528.096
iteration 0600: loss: 1525.814
iteration 0700: loss: 1528.620
iteration 0800: loss: 1524.576
iteration 0900: loss: 1528.375
====> Epoch: 015 Train loss: 1526.4729  took : 5.921621561050415
====> Test loss: 1521.9079
iteration 0000: loss: 1526.579
iteration 0100: loss: 1523.211
iteration 0200: loss: 1524.640
iteration 0300: loss: 1526.297
iteration 0400: loss: 1526.271
iteration 0500: loss: 1523.369
iteration 0600: loss: 1526.021
iteration 0700: loss: 1529.389
iteration 0800: loss: 1524.354
iteration 0900: loss: 1525.631
====> Epoch: 016 Train loss: 1526.3276  took : 5.901143550872803
====> Test loss: 1521.5109
iteration 0000: loss: 1524.877
iteration 0100: loss: 1524.371
iteration 0200: loss: 1527.488
iteration 0300: loss: 1524.069
iteration 0400: loss: 1524.042
iteration 0500: loss: 1526.159
iteration 0600: loss: 1527.608
iteration 0700: loss: 1524.737
iteration 0800: loss: 1523.150
iteration 0900: loss: 1525.395
====> Epoch: 017 Train loss: 1526.2217  took : 5.924368619918823
====> Test loss: 1521.5284
iteration 0000: loss: 1526.298
iteration 0100: loss: 1524.151
iteration 0200: loss: 1526.770
iteration 0300: loss: 1527.175
iteration 0400: loss: 1525.093
iteration 0500: loss: 1524.105
iteration 0600: loss: 1527.430
iteration 0700: loss: 1523.782
iteration 0800: loss: 1526.835
iteration 0900: loss: 1527.781
====> Epoch: 018 Train loss: 1526.0967  took : 5.9230029582977295
====> Test loss: 1521.2980
iteration 0000: loss: 1524.492
iteration 0100: loss: 1525.726
iteration 0200: loss: 1526.877
iteration 0300: loss: 1526.005
iteration 0400: loss: 1527.068
iteration 0500: loss: 1525.119
iteration 0600: loss: 1522.792
iteration 0700: loss: 1522.974
iteration 0800: loss: 1524.846
iteration 0900: loss: 1528.976
====> Epoch: 019 Train loss: 1525.9829  took : 5.881334066390991
====> Test loss: 1521.3875
iteration 0000: loss: 1527.396
iteration 0100: loss: 1528.377
iteration 0200: loss: 1527.305
iteration 0300: loss: 1527.597
iteration 0400: loss: 1526.298
iteration 0500: loss: 1526.312
iteration 0600: loss: 1525.801
iteration 0700: loss: 1523.107
iteration 0800: loss: 1525.300
iteration 0900: loss: 1525.309
====> Epoch: 020 Train loss: 1525.9039  took : 5.876672029495239
====> Test loss: 1521.3186
iteration 0000: loss: 1524.610
iteration 0100: loss: 1525.937
iteration 0200: loss: 1527.730
iteration 0300: loss: 1524.487
iteration 0400: loss: 1524.260
iteration 0500: loss: 1530.269
iteration 0600: loss: 1527.553
iteration 0700: loss: 1527.074
iteration 0800: loss: 1524.863
iteration 0900: loss: 1524.205
====> Epoch: 021 Train loss: 1525.8287  took : 5.93926215171814
====> Test loss: 1521.1903
iteration 0000: loss: 1525.215
iteration 0100: loss: 1527.146
iteration 0200: loss: 1527.812
iteration 0300: loss: 1527.519
iteration 0400: loss: 1525.460
iteration 0500: loss: 1523.468
iteration 0600: loss: 1527.427
iteration 0700: loss: 1527.395
iteration 0800: loss: 1523.590
iteration 0900: loss: 1524.438
====> Epoch: 022 Train loss: 1525.7392  took : 5.884166717529297
====> Test loss: 1521.2208
iteration 0000: loss: 1526.768
iteration 0100: loss: 1527.820
iteration 0200: loss: 1523.119
iteration 0300: loss: 1524.295
iteration 0400: loss: 1522.508
iteration 0500: loss: 1526.570
iteration 0600: loss: 1525.361
iteration 0700: loss: 1525.693
iteration 0800: loss: 1524.054
iteration 0900: loss: 1523.761
====> Epoch: 023 Train loss: 1525.6782  took : 5.892877101898193
====> Test loss: 1521.1451
iteration 0000: loss: 1526.626
iteration 0100: loss: 1525.591
iteration 0200: loss: 1526.723
iteration 0300: loss: 1524.232
iteration 0400: loss: 1527.312
iteration 0500: loss: 1526.514
iteration 0600: loss: 1527.165
iteration 0700: loss: 1527.571
iteration 0800: loss: 1526.804
iteration 0900: loss: 1522.302
====> Epoch: 024 Train loss: 1525.6041  took : 5.908569097518921
====> Test loss: 1520.9999
iteration 0000: loss: 1529.751
iteration 0100: loss: 1526.640
iteration 0200: loss: 1524.586
iteration 0300: loss: 1529.794
iteration 0400: loss: 1522.973
iteration 0500: loss: 1525.010
iteration 0600: loss: 1524.490
iteration 0700: loss: 1523.633
iteration 0800: loss: 1526.107
iteration 0900: loss: 1525.326
====> Epoch: 025 Train loss: 1525.5230  took : 5.9246885776519775
====> Test loss: 1520.9470
iteration 0000: loss: 1527.809
iteration 0100: loss: 1527.685
iteration 0200: loss: 1523.372
iteration 0300: loss: 1525.018
iteration 0400: loss: 1522.983
iteration 0500: loss: 1525.966
iteration 0600: loss: 1523.717
iteration 0700: loss: 1525.396
iteration 0800: loss: 1526.286
iteration 0900: loss: 1529.029
====> Epoch: 026 Train loss: 1525.4906  took : 5.9093544483184814
====> Test loss: 1520.9471
iteration 0000: loss: 1524.396
iteration 0100: loss: 1522.375
iteration 0200: loss: 1525.019
iteration 0300: loss: 1528.094
iteration 0400: loss: 1525.152
iteration 0500: loss: 1523.954
iteration 0600: loss: 1527.103
iteration 0700: loss: 1526.347
iteration 0800: loss: 1526.072
iteration 0900: loss: 1525.675
====> Epoch: 027 Train loss: 1525.4366  took : 5.912041664123535
====> Test loss: 1520.8545
iteration 0000: loss: 1524.857
iteration 0100: loss: 1526.035
iteration 0200: loss: 1524.331
iteration 0300: loss: 1525.351
iteration 0400: loss: 1525.203
iteration 0500: loss: 1525.218
iteration 0600: loss: 1528.328
iteration 0700: loss: 1526.615
iteration 0800: loss: 1524.719
iteration 0900: loss: 1526.214
====> Epoch: 028 Train loss: 1525.4046  took : 5.901066780090332
====> Test loss: 1520.9483
iteration 0000: loss: 1523.774
iteration 0100: loss: 1523.564
iteration 0200: loss: 1527.695
iteration 0300: loss: 1526.191
iteration 0400: loss: 1523.876
iteration 0500: loss: 1526.268
iteration 0600: loss: 1526.073
iteration 0700: loss: 1520.578
iteration 0800: loss: 1525.902
iteration 0900: loss: 1525.028
====> Epoch: 029 Train loss: 1525.3269  took : 5.961560249328613
====> Test loss: 1520.7340
iteration 0000: loss: 1525.597
iteration 0100: loss: 1526.599
iteration 0200: loss: 1528.157
iteration 0300: loss: 1523.892
iteration 0400: loss: 1525.467
iteration 0500: loss: 1525.251
iteration 0600: loss: 1525.893
iteration 0700: loss: 1524.735
iteration 0800: loss: 1524.619
iteration 0900: loss: 1524.572
====> Epoch: 030 Train loss: 1525.2555  took : 5.875223159790039
====> Test loss: 1520.7609
iteration 0000: loss: 1522.615
iteration 0100: loss: 1523.765
iteration 0200: loss: 1525.032
iteration 0300: loss: 1526.629
iteration 0400: loss: 1523.941
iteration 0500: loss: 1524.915
iteration 0600: loss: 1527.003
iteration 0700: loss: 1522.722
iteration 0800: loss: 1526.955
iteration 0900: loss: 1525.548
====> Epoch: 031 Train loss: 1525.2525  took : 5.916271209716797
====> Test loss: 1520.7515
iteration 0000: loss: 1525.773
iteration 0100: loss: 1526.980
iteration 0200: loss: 1523.915
iteration 0300: loss: 1526.554
iteration 0400: loss: 1521.797
iteration 0500: loss: 1526.139
iteration 0600: loss: 1523.042
iteration 0700: loss: 1524.661
iteration 0800: loss: 1524.837
iteration 0900: loss: 1523.992
====> Epoch: 032 Train loss: 1525.1991  took : 5.908276081085205
====> Test loss: 1520.6232
iteration 0000: loss: 1525.702
iteration 0100: loss: 1525.922
iteration 0200: loss: 1525.397
iteration 0300: loss: 1528.408
iteration 0400: loss: 1522.611
iteration 0500: loss: 1523.516
iteration 0600: loss: 1525.545
iteration 0700: loss: 1525.445
iteration 0800: loss: 1523.872
iteration 0900: loss: 1523.911
====> Epoch: 033 Train loss: 1525.1561  took : 5.886809587478638
====> Test loss: 1520.7207
iteration 0000: loss: 1524.241
iteration 0100: loss: 1522.719
iteration 0200: loss: 1525.387
iteration 0300: loss: 1521.225
iteration 0400: loss: 1527.114
iteration 0500: loss: 1525.691
iteration 0600: loss: 1526.742
iteration 0700: loss: 1523.566
iteration 0800: loss: 1527.067
iteration 0900: loss: 1526.633
====> Epoch: 034 Train loss: 1525.1307  took : 5.895784378051758
====> Test loss: 1520.6854
iteration 0000: loss: 1522.913
iteration 0100: loss: 1523.476
iteration 0200: loss: 1525.806
iteration 0300: loss: 1523.563
iteration 0400: loss: 1525.756
iteration 0500: loss: 1526.255
iteration 0600: loss: 1524.339
iteration 0700: loss: 1522.018
iteration 0800: loss: 1524.991
iteration 0900: loss: 1524.875
====> Epoch: 035 Train loss: 1525.0977  took : 5.87969183921814
====> Test loss: 1520.5090
iteration 0000: loss: 1524.397
iteration 0100: loss: 1524.010
iteration 0200: loss: 1525.296
iteration 0300: loss: 1524.910
iteration 0400: loss: 1525.459
iteration 0500: loss: 1526.090
iteration 0600: loss: 1523.079
iteration 0700: loss: 1527.163
iteration 0800: loss: 1524.087
iteration 0900: loss: 1527.176
====> Epoch: 036 Train loss: 1525.0326  took : 5.909809827804565
====> Test loss: 1520.6365
iteration 0000: loss: 1524.522
iteration 0100: loss: 1521.533
iteration 0200: loss: 1527.490
iteration 0300: loss: 1524.784
iteration 0400: loss: 1521.630
iteration 0500: loss: 1523.398
iteration 0600: loss: 1527.024
iteration 0700: loss: 1527.246
iteration 0800: loss: 1524.440
iteration 0900: loss: 1527.149
====> Epoch: 037 Train loss: 1525.0368  took : 5.903303623199463
====> Test loss: 1520.4686
iteration 0000: loss: 1524.931
iteration 0100: loss: 1522.811
iteration 0200: loss: 1526.288
iteration 0300: loss: 1524.981
iteration 0400: loss: 1524.022
iteration 0500: loss: 1521.317
iteration 0600: loss: 1525.539
iteration 0700: loss: 1528.881
iteration 0800: loss: 1521.175
iteration 0900: loss: 1528.597
====> Epoch: 038 Train loss: 1525.0190  took : 7.722476959228516
====> Test loss: 1520.5118
iteration 0000: loss: 1523.500
iteration 0100: loss: 1519.627
iteration 0200: loss: 1528.520
iteration 0300: loss: 1524.693
iteration 0400: loss: 1524.912
iteration 0500: loss: 1524.475
iteration 0600: loss: 1526.398
iteration 0700: loss: 1523.954
iteration 0800: loss: 1526.132
iteration 0900: loss: 1527.739
====> Epoch: 039 Train loss: 1524.9710  took : 5.885540246963501
====> Test loss: 1520.4523
iteration 0000: loss: 1525.002
iteration 0100: loss: 1524.818
iteration 0200: loss: 1526.656
iteration 0300: loss: 1527.027
iteration 0400: loss: 1523.331
iteration 0500: loss: 1525.431
iteration 0600: loss: 1525.205
iteration 0700: loss: 1525.027
iteration 0800: loss: 1523.869
iteration 0900: loss: 1522.380
====> Epoch: 040 Train loss: 1524.8842  took : 7.625213146209717
====> Test loss: 1520.4577
iteration 0000: loss: 1529.093
iteration 0100: loss: 1526.708
iteration 0200: loss: 1522.917
iteration 0300: loss: 1528.429
iteration 0400: loss: 1525.558
iteration 0500: loss: 1523.257
iteration 0600: loss: 1524.153
iteration 0700: loss: 1521.829
iteration 0800: loss: 1525.428
iteration 0900: loss: 1525.640
====> Epoch: 041 Train loss: 1524.8759  took : 5.9374871253967285
====> Test loss: 1520.4461
iteration 0000: loss: 1523.944
iteration 0100: loss: 1524.722
iteration 0200: loss: 1524.893
iteration 0300: loss: 1522.319
iteration 0400: loss: 1526.679
iteration 0500: loss: 1525.039
iteration 0600: loss: 1524.751
iteration 0700: loss: 1524.398
iteration 0800: loss: 1523.328
iteration 0900: loss: 1527.613
====> Epoch: 042 Train loss: 1524.8901  took : 5.891319274902344
====> Test loss: 1520.3439
iteration 0000: loss: 1528.687
iteration 0100: loss: 1526.702
iteration 0200: loss: 1521.849
iteration 0300: loss: 1524.742
iteration 0400: loss: 1523.969
iteration 0500: loss: 1526.013
iteration 0600: loss: 1525.366
iteration 0700: loss: 1521.873
iteration 0800: loss: 1526.703
iteration 0900: loss: 1523.628
====> Epoch: 043 Train loss: 1524.8392  took : 5.904074430465698
====> Test loss: 1520.4421
iteration 0000: loss: 1522.559
iteration 0100: loss: 1525.732
iteration 0200: loss: 1525.562
iteration 0300: loss: 1524.555
iteration 0400: loss: 1525.664
iteration 0500: loss: 1523.744
iteration 0600: loss: 1523.303
iteration 0700: loss: 1529.418
iteration 0800: loss: 1525.149
iteration 0900: loss: 1522.455
====> Epoch: 044 Train loss: 1524.8178  took : 5.917107820510864
====> Test loss: 1520.3258
iteration 0000: loss: 1525.671
iteration 0100: loss: 1528.741
iteration 0200: loss: 1525.627
iteration 0300: loss: 1524.890
iteration 0400: loss: 1524.585
iteration 0500: loss: 1521.897
iteration 0600: loss: 1524.891
iteration 0700: loss: 1526.329
iteration 0800: loss: 1523.871
iteration 0900: loss: 1525.242
====> Epoch: 045 Train loss: 1524.7721  took : 5.906111240386963
====> Test loss: 1520.3092
iteration 0000: loss: 1525.341
iteration 0100: loss: 1526.206
iteration 0200: loss: 1523.644
iteration 0300: loss: 1525.459
iteration 0400: loss: 1524.276
iteration 0500: loss: 1526.856
iteration 0600: loss: 1525.298
iteration 0700: loss: 1524.098
iteration 0800: loss: 1524.052
iteration 0900: loss: 1524.921
====> Epoch: 046 Train loss: 1524.7312  took : 5.941468954086304
====> Test loss: 1520.3624
iteration 0000: loss: 1523.252
iteration 0100: loss: 1524.015
iteration 0200: loss: 1527.802
iteration 0300: loss: 1526.586
iteration 0400: loss: 1527.038
iteration 0500: loss: 1524.029
iteration 0600: loss: 1523.471
iteration 0700: loss: 1524.869
iteration 0800: loss: 1523.115
iteration 0900: loss: 1526.018
====> Epoch: 047 Train loss: 1524.7134  took : 5.899978160858154
====> Test loss: 1520.3767
iteration 0000: loss: 1525.152
iteration 0100: loss: 1525.130
iteration 0200: loss: 1524.778
iteration 0300: loss: 1525.280
iteration 0400: loss: 1526.105
iteration 0500: loss: 1523.441
iteration 0600: loss: 1526.473
iteration 0700: loss: 1525.031
iteration 0800: loss: 1523.222
iteration 0900: loss: 1523.554
====> Epoch: 048 Train loss: 1524.7019  took : 5.904340028762817
====> Test loss: 1520.2884
iteration 0000: loss: 1523.532
iteration 0100: loss: 1522.947
iteration 0200: loss: 1524.249
iteration 0300: loss: 1526.588
iteration 0400: loss: 1522.478
iteration 0500: loss: 1525.037
iteration 0600: loss: 1526.722
iteration 0700: loss: 1526.127
iteration 0800: loss: 1525.424
iteration 0900: loss: 1524.853
====> Epoch: 049 Train loss: 1524.6737  took : 5.906793117523193
====> Test loss: 1520.2963
iteration 0000: loss: 1524.859
iteration 0100: loss: 1526.712
iteration 0200: loss: 1521.742
iteration 0300: loss: 1525.621
iteration 0400: loss: 1523.581
iteration 0500: loss: 1524.825
iteration 0600: loss: 1524.879
iteration 0700: loss: 1525.473
iteration 0800: loss: 1527.757
iteration 0900: loss: 1520.930
====> Epoch: 050 Train loss: 1524.6719  took : 5.896642208099365
====> Test loss: 1520.2717
====> [MM-VAE] Time: 454.488s or 00:07:34
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'print_freq': 100,
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2678.374
iteration 0100: loss: 2101.544
iteration 0200: loss: 2117.628
iteration 0300: loss: 2079.493
iteration 0400: loss: 2068.920
iteration 0500: loss: 2052.820
iteration 0600: loss: 2025.775
iteration 0700: loss: 1999.685
iteration 0800: loss: 1985.120
iteration 0900: loss: 1968.249
====> Epoch: 001 Train loss: 2056.7710  took : 8.81485390663147
====> Test loss: 1961.1663
iteration 0000: loss: 1967.452
iteration 0100: loss: 1964.024
iteration 0200: loss: 1962.591
iteration 0300: loss: 1964.301
iteration 0400: loss: 1961.992
iteration 0500: loss: 1960.607
iteration 0600: loss: 1960.862
iteration 0700: loss: 1960.706
iteration 0800: loss: 1960.401
iteration 0900: loss: 1960.052
====> Epoch: 002 Train loss: 1962.1352  took : 8.675546884536743
====> Test loss: 1954.6772
iteration 0000: loss: 1960.693
iteration 0100: loss: 1960.146
iteration 0200: loss: 1960.547
iteration 0300: loss: 1958.714
iteration 0400: loss: 1960.717
iteration 0500: loss: 1959.744
iteration 0600: loss: 1959.520
iteration 0700: loss: 1959.727
iteration 0800: loss: 1958.940
iteration 0900: loss: 1958.440
====> Epoch: 003 Train loss: 1959.6011  took : 8.682983636856079
====> Test loss: 1953.5156
iteration 0000: loss: 1957.961
iteration 0100: loss: 1959.056
iteration 0200: loss: 1958.352
iteration 0300: loss: 1959.342
iteration 0400: loss: 1958.307
iteration 0500: loss: 1959.985
iteration 0600: loss: 1958.847
iteration 0700: loss: 1958.329
iteration 0800: loss: 1959.505
iteration 0900: loss: 1958.312
====> Epoch: 004 Train loss: 1958.8328  took : 8.61085319519043
====> Test loss: 1953.4396
iteration 0000: loss: 1958.196
iteration 0100: loss: 1957.843
iteration 0200: loss: 1958.591
iteration 0300: loss: 1958.403
iteration 0400: loss: 1957.243
iteration 0500: loss: 1959.028
iteration 0600: loss: 1958.133
iteration 0700: loss: 1958.033
iteration 0800: loss: 1957.815
iteration 0900: loss: 1957.430
====> Epoch: 005 Train loss: 1958.3678  took : 8.61966323852539
====> Test loss: 1953.0131
iteration 0000: loss: 1958.480
iteration 0100: loss: 1957.644
iteration 0200: loss: 1959.592
iteration 0300: loss: 1958.377
iteration 0400: loss: 1958.079
iteration 0500: loss: 1958.541
iteration 0600: loss: 1957.823
iteration 0700: loss: 1958.162
iteration 0800: loss: 1957.587
iteration 0900: loss: 1957.220
====> Epoch: 006 Train loss: 1958.0435  took : 8.627235412597656
====> Test loss: 1952.9487
iteration 0000: loss: 1957.905
iteration 0100: loss: 1959.063
iteration 0200: loss: 1957.822
iteration 0300: loss: 1957.608
iteration 0400: loss: 1958.504
iteration 0500: loss: 1957.834
iteration 0600: loss: 1958.256
iteration 0700: loss: 1958.043
iteration 0800: loss: 1957.868
iteration 0900: loss: 1957.355
====> Epoch: 007 Train loss: 1957.8700  took : 8.638777017593384
====> Test loss: 1952.5200
iteration 0000: loss: 1957.048
iteration 0100: loss: 1956.524
iteration 0200: loss: 1957.415
iteration 0300: loss: 1957.687
iteration 0400: loss: 1956.665
iteration 0500: loss: 1957.098
iteration 0600: loss: 1958.313
iteration 0700: loss: 1958.288
iteration 0800: loss: 1957.829
iteration 0900: loss: 1958.747
====> Epoch: 008 Train loss: 1957.7425  took : 8.845715999603271
====> Test loss: 1952.4478
iteration 0000: loss: 1956.592
iteration 0100: loss: 1958.694
iteration 0200: loss: 1958.143
iteration 0300: loss: 1957.606
iteration 0400: loss: 1957.718
iteration 0500: loss: 1955.987
iteration 0600: loss: 1956.700
iteration 0700: loss: 1958.909
iteration 0800: loss: 1957.861
iteration 0900: loss: 1957.176
====> Epoch: 009 Train loss: 1957.6041  took : 8.646102666854858
====> Test loss: 1952.3374
iteration 0000: loss: 1957.962
iteration 0100: loss: 1958.419
iteration 0200: loss: 1957.337
iteration 0300: loss: 1958.867
iteration 0400: loss: 1957.327
iteration 0500: loss: 1957.855
iteration 0600: loss: 1957.480
iteration 0700: loss: 1957.410
iteration 0800: loss: 1956.474
iteration 0900: loss: 1955.889
====> Epoch: 010 Train loss: 1957.5395  took : 8.657785177230835
====> Test loss: 1952.2659
iteration 0000: loss: 1957.779
iteration 0100: loss: 1956.502
iteration 0200: loss: 1958.487
iteration 0300: loss: 1956.215
iteration 0400: loss: 1958.224
iteration 0500: loss: 1959.188
iteration 0600: loss: 1958.250
iteration 0700: loss: 1957.907
iteration 0800: loss: 1956.223
iteration 0900: loss: 1957.177
====> Epoch: 011 Train loss: 1957.5006  took : 8.685444355010986
====> Test loss: 1952.3520
iteration 0000: loss: 1958.433
iteration 0100: loss: 1957.805
iteration 0200: loss: 1956.507
iteration 0300: loss: 1957.554
iteration 0400: loss: 1957.215
iteration 0500: loss: 1956.537
iteration 0600: loss: 1957.081
iteration 0700: loss: 1956.727
iteration 0800: loss: 1958.139
iteration 0900: loss: 1957.326
====> Epoch: 012 Train loss: 1957.3454  took : 8.860222578048706
====> Test loss: 1952.0326
iteration 0000: loss: 1956.615
iteration 0100: loss: 1957.436
iteration 0200: loss: 1955.435
iteration 0300: loss: 1956.061
iteration 0400: loss: 1958.587
iteration 0500: loss: 1957.171
iteration 0600: loss: 1956.989
iteration 0700: loss: 1957.484
iteration 0800: loss: 1957.552
iteration 0900: loss: 1957.651
====> Epoch: 013 Train loss: 1957.3695  took : 8.638461112976074
====> Test loss: 1952.0879
iteration 0000: loss: 1958.001
iteration 0100: loss: 1960.076
iteration 0200: loss: 1957.712
iteration 0300: loss: 1958.910
iteration 0400: loss: 1958.133
iteration 0500: loss: 1956.618
iteration 0600: loss: 1956.951
iteration 0700: loss: 1956.010
iteration 0800: loss: 1956.187
iteration 0900: loss: 1957.657
====> Epoch: 014 Train loss: 1957.2667  took : 8.650506734848022
====> Test loss: 1951.9157
iteration 0000: loss: 1955.953
iteration 0100: loss: 1957.518
iteration 0200: loss: 1956.764
iteration 0300: loss: 1956.961
iteration 0400: loss: 1960.108
iteration 0500: loss: 1957.708
iteration 0600: loss: 1956.716
iteration 0700: loss: 1958.927
iteration 0800: loss: 1958.909
iteration 0900: loss: 1958.223
====> Epoch: 015 Train loss: 1957.2979  took : 8.846696138381958
====> Test loss: 1951.9687
iteration 0000: loss: 1956.673
iteration 0100: loss: 1956.244
iteration 0200: loss: 1957.701
iteration 0300: loss: 1957.262
iteration 0400: loss: 1958.006
iteration 0500: loss: 1956.845
iteration 0600: loss: 1957.807
iteration 0700: loss: 1958.089
iteration 0800: loss: 1957.639
iteration 0900: loss: 1957.718
====> Epoch: 016 Train loss: 1957.1715  took : 8.645786046981812
====> Test loss: 1952.0578
iteration 0000: loss: 1956.942
iteration 0100: loss: 1957.413
iteration 0200: loss: 1956.324
iteration 0300: loss: 1956.981
iteration 0400: loss: 1956.185
iteration 0500: loss: 1957.298
iteration 0600: loss: 1957.341
iteration 0700: loss: 1955.739
iteration 0800: loss: 1957.086
iteration 0900: loss: 1956.875
====> Epoch: 017 Train loss: 1957.1198  took : 8.85345458984375
====> Test loss: 1951.8888
iteration 0000: loss: 1957.687
iteration 0100: loss: 1957.508
iteration 0200: loss: 1957.784
iteration 0300: loss: 1957.099
iteration 0400: loss: 1959.050
iteration 0500: loss: 1956.711
iteration 0600: loss: 1956.173
iteration 0700: loss: 1957.765
iteration 0800: loss: 1957.960
iteration 0900: loss: 1957.062
====> Epoch: 018 Train loss: 1957.0755  took : 8.63440990447998
====> Test loss: 1951.9953
iteration 0000: loss: 1956.823
iteration 0100: loss: 1956.704
iteration 0200: loss: 1957.566
iteration 0300: loss: 1957.670
iteration 0400: loss: 1957.334
iteration 0500: loss: 1956.305
iteration 0600: loss: 1955.325
iteration 0700: loss: 1958.312
iteration 0800: loss: 1956.029
iteration 0900: loss: 1956.527
====> Epoch: 019 Train loss: 1957.0550  took : 8.658164024353027
====> Test loss: 1952.4047
iteration 0000: loss: 1956.723
iteration 0100: loss: 1957.509
iteration 0200: loss: 1955.779
iteration 0300: loss: 1956.784
iteration 0400: loss: 1957.069
iteration 0500: loss: 1956.906
iteration 0600: loss: 1954.778
iteration 0700: loss: 1956.450
iteration 0800: loss: 1957.386
iteration 0900: loss: 1958.122
====> Epoch: 020 Train loss: 1957.0182  took : 8.6299467086792
====> Test loss: 1951.9666
iteration 0000: loss: 1956.706
iteration 0100: loss: 1956.035
iteration 0200: loss: 1956.622
iteration 0300: loss: 1956.643
iteration 0400: loss: 1956.527
iteration 0500: loss: 1956.901
iteration 0600: loss: 1955.789
iteration 0700: loss: 1956.114
iteration 0800: loss: 1957.111
iteration 0900: loss: 1958.066
====> Epoch: 021 Train loss: 1956.9393  took : 8.622554302215576
====> Test loss: 1951.7971
iteration 0000: loss: 1956.515
iteration 0100: loss: 1957.058
iteration 0200: loss: 1956.558
iteration 0300: loss: 1958.458
iteration 0400: loss: 1957.598
iteration 0500: loss: 1955.971
iteration 0600: loss: 1956.644
iteration 0700: loss: 1956.005
iteration 0800: loss: 1956.362
iteration 0900: loss: 1956.048
====> Epoch: 022 Train loss: 1956.9967  took : 8.746910810470581
====> Test loss: 1952.2974
iteration 0000: loss: 1956.741
iteration 0100: loss: 1956.550
iteration 0200: loss: 1957.898
iteration 0300: loss: 1956.607
iteration 0400: loss: 1956.806
iteration 0500: loss: 1956.795
iteration 0600: loss: 1955.203
iteration 0700: loss: 1957.439
iteration 0800: loss: 1955.660
iteration 0900: loss: 1956.813
====> Epoch: 023 Train loss: 1956.9453  took : 8.612593412399292
====> Test loss: 1951.7197
iteration 0000: loss: 1957.838
iteration 0100: loss: 1957.203
iteration 0200: loss: 1955.910
iteration 0300: loss: 1955.851
iteration 0400: loss: 1955.609
iteration 0500: loss: 1956.342
iteration 0600: loss: 1957.040
iteration 0700: loss: 1957.220
iteration 0800: loss: 1956.375
iteration 0900: loss: 1956.385
====> Epoch: 024 Train loss: 1956.8902  took : 8.618785619735718
====> Test loss: 1951.6534
iteration 0000: loss: 1955.794
iteration 0100: loss: 1957.152
iteration 0200: loss: 1957.983
iteration 0300: loss: 1955.575
iteration 0400: loss: 1957.190
iteration 0500: loss: 1956.858
iteration 0600: loss: 1956.909
iteration 0700: loss: 1955.621
iteration 0800: loss: 1956.657
iteration 0900: loss: 1956.866
====> Epoch: 025 Train loss: 1956.8815  took : 8.632110118865967
====> Test loss: 1951.7231
iteration 0000: loss: 1956.438
iteration 0100: loss: 1956.296
iteration 0200: loss: 1956.144
iteration 0300: loss: 1956.209
iteration 0400: loss: 1955.984
iteration 0500: loss: 1955.689
iteration 0600: loss: 1957.697
iteration 0700: loss: 1956.814
iteration 0800: loss: 1956.133
iteration 0900: loss: 1958.676
====> Epoch: 026 Train loss: 1956.8915  took : 8.610867500305176
====> Test loss: 1951.7666
iteration 0000: loss: 1957.493
iteration 0100: loss: 1956.319
iteration 0200: loss: 1957.022
iteration 0300: loss: 1956.234
iteration 0400: loss: 1958.028
iteration 0500: loss: 1957.034
iteration 0600: loss: 1956.838
iteration 0700: loss: 1956.220
iteration 0800: loss: 1957.301
iteration 0900: loss: 1956.607
====> Epoch: 027 Train loss: 1956.7695  took : 8.66483759880066
====> Test loss: 1951.5166
iteration 0000: loss: 1957.608
iteration 0100: loss: 1956.707
iteration 0200: loss: 1957.619
iteration 0300: loss: 1956.899
iteration 0400: loss: 1957.246
iteration 0500: loss: 1956.450
iteration 0600: loss: 1955.830
iteration 0700: loss: 1956.193
iteration 0800: loss: 1957.629
iteration 0900: loss: 1957.131
====> Epoch: 028 Train loss: 1956.7813  took : 8.637580633163452
====> Test loss: 1951.8304
iteration 0000: loss: 1956.040
iteration 0100: loss: 1956.654
iteration 0200: loss: 1957.021
iteration 0300: loss: 1956.528
iteration 0400: loss: 1956.828
iteration 0500: loss: 1957.086
iteration 0600: loss: 1956.596
iteration 0700: loss: 1955.494
iteration 0800: loss: 1955.963
iteration 0900: loss: 1956.363
====> Epoch: 029 Train loss: 1956.7671  took : 8.64619255065918
====> Test loss: 1951.6484
iteration 0000: loss: 1957.913
iteration 0100: loss: 1956.231
iteration 0200: loss: 1956.430
iteration 0300: loss: 1957.108
iteration 0400: loss: 1956.613
iteration 0500: loss: 1956.586
iteration 0600: loss: 1960.509
iteration 0700: loss: 1956.432
iteration 0800: loss: 1956.407
iteration 0900: loss: 1957.796
====> Epoch: 030 Train loss: 1956.7463  took : 8.610130310058594
====> Test loss: 1951.8263
iteration 0000: loss: 1957.264
iteration 0100: loss: 1957.097
iteration 0200: loss: 1956.380
iteration 0300: loss: 1955.785
iteration 0400: loss: 1956.948
iteration 0500: loss: 1956.002
iteration 0600: loss: 1958.104
iteration 0700: loss: 1956.569
iteration 0800: loss: 1956.360
iteration 0900: loss: 1956.309
====> Epoch: 031 Train loss: 1956.7546  took : 8.633767366409302
====> Test loss: 1951.8014
iteration 0000: loss: 1956.814
iteration 0100: loss: 1956.240
iteration 0200: loss: 1956.909
iteration 0300: loss: 1956.579
iteration 0400: loss: 1955.636
iteration 0500: loss: 1956.847
iteration 0600: loss: 1957.069
iteration 0700: loss: 1956.982
iteration 0800: loss: 1956.114
iteration 0900: loss: 1956.110
====> Epoch: 032 Train loss: 1956.7400  took : 8.627406597137451
====> Test loss: 1951.6969
iteration 0000: loss: 1956.515
iteration 0100: loss: 1957.294
iteration 0200: loss: 1956.011
iteration 0300: loss: 1956.257
iteration 0400: loss: 1957.097
iteration 0500: loss: 1956.208
iteration 0600: loss: 1956.483
iteration 0700: loss: 1957.403
iteration 0800: loss: 1956.384
iteration 0900: loss: 1957.219
====> Epoch: 033 Train loss: 1956.7272  took : 8.733309268951416
====> Test loss: 1951.6787
iteration 0000: loss: 1956.679
iteration 0100: loss: 1957.746
iteration 0200: loss: 1955.831
iteration 0300: loss: 1955.377
iteration 0400: loss: 1956.678
iteration 0500: loss: 1956.751
iteration 0600: loss: 1956.614
iteration 0700: loss: 1957.217
iteration 0800: loss: 1957.395
iteration 0900: loss: 1957.472
====> Epoch: 034 Train loss: 1956.6348  took : 8.893539667129517
====> Test loss: 1951.6921
iteration 0000: loss: 1956.599
iteration 0100: loss: 1957.039
iteration 0200: loss: 1956.676
iteration 0300: loss: 1955.770
iteration 0400: loss: 1956.520
iteration 0500: loss: 1955.777
iteration 0600: loss: 1956.547
iteration 0700: loss: 1957.495
iteration 0800: loss: 1957.949
iteration 0900: loss: 1957.005
====> Epoch: 035 Train loss: 1956.6383  took : 8.61060380935669
====> Test loss: 1951.8211
iteration 0000: loss: 1956.555
iteration 0100: loss: 1957.419
iteration 0200: loss: 1956.836
iteration 0300: loss: 1960.738
iteration 0400: loss: 1955.238
iteration 0500: loss: 1956.380
iteration 0600: loss: 1959.511
iteration 0700: loss: 1954.883
iteration 0800: loss: 1955.674
iteration 0900: loss: 1955.443
====> Epoch: 036 Train loss: 1956.6246  took : 8.64603590965271
====> Test loss: 1951.5658
iteration 0000: loss: 1956.282
iteration 0100: loss: 1955.406
iteration 0200: loss: 1957.941
iteration 0300: loss: 1955.688
iteration 0400: loss: 1955.249
iteration 0500: loss: 1956.736
iteration 0600: loss: 1956.284
iteration 0700: loss: 1958.219
iteration 0800: loss: 1958.590
iteration 0900: loss: 1956.635
====> Epoch: 037 Train loss: 1956.6545  took : 8.804532289505005
====> Test loss: 1951.3510
iteration 0000: loss: 1955.911
iteration 0100: loss: 1956.402
iteration 0200: loss: 1958.416
iteration 0300: loss: 1956.376
iteration 0400: loss: 1957.136
iteration 0500: loss: 1956.391
iteration 0600: loss: 1957.992
iteration 0700: loss: 1955.356
iteration 0800: loss: 1956.606
iteration 0900: loss: 1956.376
====> Epoch: 038 Train loss: 1956.5931  took : 8.765998601913452
====> Test loss: 1951.7215
iteration 0000: loss: 1956.470
iteration 0100: loss: 1956.256
iteration 0200: loss: 1956.894
iteration 0300: loss: 1958.862
iteration 0400: loss: 1955.927
iteration 0500: loss: 1956.343
iteration 0600: loss: 1956.468
iteration 0700: loss: 1955.973
iteration 0800: loss: 1955.703
iteration 0900: loss: 1956.336
====> Epoch: 039 Train loss: 1956.6082  took : 8.785316467285156
====> Test loss: 1952.2032
iteration 0000: loss: 1955.716
iteration 0100: loss: 1956.232
iteration 0200: loss: 1955.390
iteration 0300: loss: 1955.991
iteration 0400: loss: 1955.806
iteration 0500: loss: 1956.186
iteration 0600: loss: 1956.940
iteration 0700: loss: 1956.830
iteration 0800: loss: 1957.467
iteration 0900: loss: 1956.826
====> Epoch: 040 Train loss: 1956.6110  took : 8.944922924041748
====> Test loss: 1951.4160
iteration 0000: loss: 1955.788
iteration 0100: loss: 1956.116
iteration 0200: loss: 1955.943
iteration 0300: loss: 1956.839
iteration 0400: loss: 1958.539
iteration 0500: loss: 1956.744
iteration 0600: loss: 1956.586
iteration 0700: loss: 1957.394
iteration 0800: loss: 1957.449
iteration 0900: loss: 1956.955
====> Epoch: 041 Train loss: 1956.5582  took : 8.700883865356445
====> Test loss: 1951.6159
iteration 0000: loss: 1956.529
iteration 0100: loss: 1956.274
iteration 0200: loss: 1956.751
iteration 0300: loss: 1956.035
iteration 0400: loss: 1956.817
iteration 0500: loss: 1955.849
iteration 0600: loss: 1957.423
iteration 0700: loss: 1955.396
iteration 0800: loss: 1960.436
iteration 0900: loss: 1955.925
====> Epoch: 042 Train loss: 1956.5051  took : 8.750481128692627
====> Test loss: 1951.7168
iteration 0000: loss: 1956.468
iteration 0100: loss: 1957.317
iteration 0200: loss: 1955.916
iteration 0300: loss: 1956.076
iteration 0400: loss: 1956.215
iteration 0500: loss: 1956.000
iteration 0600: loss: 1955.475
iteration 0700: loss: 1955.464
iteration 0800: loss: 1955.320
iteration 0900: loss: 1955.816
====> Epoch: 043 Train loss: 1956.5465  took : 8.719691753387451
====> Test loss: 1951.7320
iteration 0000: loss: 1956.359
iteration 0100: loss: 1955.774
iteration 0200: loss: 1956.264
iteration 0300: loss: 1955.868
iteration 0400: loss: 1956.545
iteration 0500: loss: 1956.451
iteration 0600: loss: 1955.684
iteration 0700: loss: 1955.804
iteration 0800: loss: 1956.405
iteration 0900: loss: 1957.132
====> Epoch: 044 Train loss: 1956.5266  took : 8.644353866577148
====> Test loss: 1951.4342
iteration 0000: loss: 1955.574
iteration 0100: loss: 1956.581
iteration 0200: loss: 1956.747
iteration 0300: loss: 1956.329
iteration 0400: loss: 1956.122
iteration 0500: loss: 1957.326
iteration 0600: loss: 1957.713
iteration 0700: loss: 1956.625
iteration 0800: loss: 1958.571
iteration 0900: loss: 1958.758
====> Epoch: 045 Train loss: 1956.5505  took : 8.646207809448242
====> Test loss: 1951.4858
iteration 0000: loss: 1956.074
iteration 0100: loss: 1955.679
iteration 0200: loss: 1956.196
iteration 0300: loss: 1957.550
iteration 0400: loss: 1956.042
iteration 0500: loss: 1957.044
iteration 0600: loss: 1957.302
iteration 0700: loss: 1956.971
iteration 0800: loss: 1956.022
iteration 0900: loss: 1956.026
====> Epoch: 046 Train loss: 1956.5009  took : 8.954837083816528
====> Test loss: 1951.4026
iteration 0000: loss: 1959.609
iteration 0100: loss: 1957.443
iteration 0200: loss: 1957.700
iteration 0300: loss: 1955.795
iteration 0400: loss: 1954.996
iteration 0500: loss: 1958.384
iteration 0600: loss: 1955.917
iteration 0700: loss: 1955.411
iteration 0800: loss: 1957.181
iteration 0900: loss: 1955.718
====> Epoch: 047 Train loss: 1956.4313  took : 8.758388996124268
====> Test loss: 1951.2984
iteration 0000: loss: 1955.868
iteration 0100: loss: 1957.860
iteration 0200: loss: 1957.107
iteration 0300: loss: 1956.150
iteration 0400: loss: 1956.414
iteration 0500: loss: 1956.239
iteration 0600: loss: 1956.477
iteration 0700: loss: 1956.712
iteration 0800: loss: 1956.230
iteration 0900: loss: 1957.157
====> Epoch: 048 Train loss: 1956.4687  took : 8.925892353057861
====> Test loss: 1951.3998
iteration 0000: loss: 1957.319
iteration 0100: loss: 1954.704
iteration 0200: loss: 1955.708
iteration 0300: loss: 1956.157
iteration 0400: loss: 1956.772
iteration 0500: loss: 1955.876
iteration 0600: loss: 1956.812
iteration 0700: loss: 1957.648
iteration 0800: loss: 1957.391
iteration 0900: loss: 1955.762
====> Epoch: 049 Train loss: 1956.4722  took : 8.876515626907349
====> Test loss: 1951.5607
iteration 0000: loss: 1955.862
iteration 0100: loss: 1956.887
iteration 0200: loss: 1956.246
iteration 0300: loss: 1956.207
iteration 0400: loss: 1955.355
iteration 0500: loss: 1955.975
iteration 0600: loss: 1957.237
iteration 0700: loss: 1956.181
iteration 0800: loss: 1955.228
iteration 0900: loss: 1956.823
====> Epoch: 050 Train loss: 1956.4571  took : 8.67384147644043
====> Test loss: 1951.6142
====> [MM-VAE] Time: 569.133s or 00:09:29
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'print_freq': 100,
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5162.991
iteration 0100: loss: 4140.450
iteration 0200: loss: 4094.686
iteration 0300: loss: 4021.196
iteration 0400: loss: 4016.759
iteration 0500: loss: 4015.201
iteration 0600: loss: 4000.684
iteration 0700: loss: 4012.172
iteration 0800: loss: 3994.129
iteration 0900: loss: 3994.257
iteration 1000: loss: 3980.010
iteration 1100: loss: 3993.365
iteration 1200: loss: 3975.663
iteration 1300: loss: 3978.903
iteration 1400: loss: 3990.935
iteration 1500: loss: 3980.083
iteration 1600: loss: 3984.729
iteration 1700: loss: 3983.487
iteration 1800: loss: 3972.144
====> Epoch: 001 Train loss: 4012.9159  took : 141.1466679573059
====> Test loss: 3982.6773
iteration 0000: loss: 3968.521
iteration 0100: loss: 3982.572
iteration 0200: loss: 3974.272
iteration 0300: loss: 3973.038
iteration 0400: loss: 3977.801
iteration 0500: loss: 3982.058
iteration 0600: loss: 3979.962
iteration 0700: loss: 3971.545
iteration 0800: loss: 3974.766
iteration 0900: loss: 3972.138
iteration 1000: loss: 3975.899
iteration 1100: loss: 3978.913
iteration 1200: loss: 3970.679
iteration 1300: loss: 3969.229
iteration 1400: loss: 3970.216
iteration 1500: loss: 3968.380
iteration 1600: loss: 3972.452
iteration 1700: loss: 3970.818
iteration 1800: loss: 3973.474
====> Epoch: 002 Train loss: 3974.9837  took : 138.26027250289917
====> Test loss: 3975.8927
iteration 0000: loss: 3979.211
iteration 0100: loss: 3973.923
iteration 0200: loss: 3964.275
iteration 0300: loss: 3970.247
iteration 0400: loss: 3966.062
iteration 0500: loss: 3970.795
iteration 0600: loss: 3971.930
iteration 0700: loss: 3970.834
iteration 0800: loss: 3970.845
iteration 0900: loss: 3969.604
iteration 1000: loss: 3962.908
iteration 1100: loss: 3964.298
iteration 1200: loss: 3978.495
iteration 1300: loss: 3976.374
iteration 1400: loss: 3966.781
iteration 1500: loss: 3969.461
iteration 1600: loss: 3970.980
iteration 1700: loss: 3972.527
iteration 1800: loss: 3978.887
====> Epoch: 003 Train loss: 3970.3520  took : 139.8389322757721
====> Test loss: 3973.8709
iteration 0000: loss: 3974.811
iteration 0100: loss: 3973.514
iteration 0200: loss: 3970.634
iteration 0300: loss: 3972.470
iteration 0400: loss: 3966.317
iteration 0500: loss: 3972.303
iteration 0600: loss: 3970.654
iteration 0700: loss: 3978.440
iteration 0800: loss: 3976.635
iteration 0900: loss: 3967.300
iteration 1000: loss: 3967.706
iteration 1100: loss: 3969.651
iteration 1200: loss: 3969.510
iteration 1300: loss: 3967.771
iteration 1400: loss: 3970.941
iteration 1500: loss: 3958.140
iteration 1600: loss: 3963.756
iteration 1700: loss: 3971.807
iteration 1800: loss: 3966.935
====> Epoch: 004 Train loss: 3968.2953  took : 140.45936584472656
====> Test loss: 3972.3611
iteration 0000: loss: 3964.755
iteration 0100: loss: 3963.737
iteration 0200: loss: 3970.132
iteration 0300: loss: 3969.719
iteration 0400: loss: 3970.190
iteration 0500: loss: 3970.204
iteration 0600: loss: 3963.409
iteration 0700: loss: 3971.038
iteration 0800: loss: 3958.847
iteration 0900: loss: 3968.465
iteration 1000: loss: 3966.646
iteration 1100: loss: 3959.806
iteration 1200: loss: 3963.486
iteration 1300: loss: 3968.659
iteration 1400: loss: 3972.353
iteration 1500: loss: 3983.087
iteration 1600: loss: 3965.702
iteration 1700: loss: 3957.942
iteration 1800: loss: 3963.875
====> Epoch: 005 Train loss: 3966.9358  took : 140.49986171722412
====> Test loss: 3971.3945
iteration 0000: loss: 3965.290
iteration 0100: loss: 3977.166
iteration 0200: loss: 3961.256
iteration 0300: loss: 3961.913
iteration 0400: loss: 3961.691
iteration 0500: loss: 3975.145
iteration 0600: loss: 3961.699
iteration 0700: loss: 3957.571
iteration 0800: loss: 3957.365
iteration 0900: loss: 3963.396
iteration 1000: loss: 3965.133
iteration 1100: loss: 3967.214
iteration 1200: loss: 3966.607
iteration 1300: loss: 3964.371
iteration 1400: loss: 3980.225
iteration 1500: loss: 3964.728
iteration 1600: loss: 3968.048
iteration 1700: loss: 3964.153
iteration 1800: loss: 3956.565
====> Epoch: 006 Train loss: 3966.0223  took : 139.96228122711182
====> Test loss: 3971.0366
iteration 0000: loss: 3967.043
iteration 0100: loss: 3966.414
iteration 0200: loss: 3965.126
iteration 0300: loss: 3966.637
iteration 0400: loss: 3972.167
iteration 0500: loss: 3961.411
iteration 0600: loss: 3971.858
iteration 0700: loss: 3971.749
iteration 0800: loss: 3960.037
iteration 0900: loss: 3955.399
iteration 1000: loss: 3959.225
iteration 1100: loss: 3966.077
iteration 1200: loss: 3964.096
iteration 1300: loss: 3964.417
iteration 1400: loss: 3964.300
iteration 1500: loss: 3966.599
iteration 1600: loss: 3966.691
iteration 1700: loss: 3963.775
iteration 1800: loss: 3973.604
====> Epoch: 007 Train loss: 3965.1958  took : 139.56708478927612
====> Test loss: 3970.1985
iteration 0000: loss: 3966.664
iteration 0100: loss: 3967.889
iteration 0200: loss: 3960.544
iteration 0300: loss: 3959.754
iteration 0400: loss: 3964.168
iteration 0500: loss: 3960.662
iteration 0600: loss: 3976.629
iteration 0700: loss: 3964.564
iteration 0800: loss: 3966.648
iteration 0900: loss: 3970.497
iteration 1000: loss: 3967.775
iteration 1100: loss: 3960.325
iteration 1200: loss: 3973.381
iteration 1300: loss: 3962.569
iteration 1400: loss: 3971.488
iteration 1500: loss: 3963.651
iteration 1600: loss: 3965.896
iteration 1700: loss: 3963.997
iteration 1800: loss: 3974.470
====> Epoch: 008 Train loss: 3964.8271  took : 138.8344531059265
====> Test loss: 3969.5600
iteration 0000: loss: 3967.777
iteration 0100: loss: 3966.131
iteration 0200: loss: 3962.406
iteration 0300: loss: 3954.447
iteration 0400: loss: 3959.977
iteration 0500: loss: 3960.738
iteration 0600: loss: 3961.977
iteration 0700: loss: 3957.682
iteration 0800: loss: 3969.639
iteration 0900: loss: 3966.974
iteration 1000: loss: 3972.440
iteration 1100: loss: 3968.523
iteration 1200: loss: 3959.874
iteration 1300: loss: 3956.282
iteration 1400: loss: 3968.470
iteration 1500: loss: 3966.294
iteration 1600: loss: 3964.349
iteration 1700: loss: 3955.203
iteration 1800: loss: 3964.931
====> Epoch: 009 Train loss: 3964.1811  took : 139.04154229164124
====> Test loss: 3969.4188
iteration 0000: loss: 3964.291
iteration 0100: loss: 3959.269
iteration 0200: loss: 3976.133
iteration 0300: loss: 3961.560
iteration 0400: loss: 3974.478
iteration 0500: loss: 3969.757
iteration 0600: loss: 3959.292
iteration 0700: loss: 3968.760
iteration 0800: loss: 3966.808
iteration 0900: loss: 3964.545
iteration 1000: loss: 3969.406
iteration 1100: loss: 3959.246
iteration 1200: loss: 3960.044
iteration 1300: loss: 3963.249
iteration 1400: loss: 3973.879
iteration 1500: loss: 3961.746
iteration 1600: loss: 3962.338
iteration 1700: loss: 3957.969
iteration 1800: loss: 3953.495
====> Epoch: 010 Train loss: 3963.6331  took : 139.63361144065857
====> Test loss: 3968.4911
iteration 0000: loss: 3967.493
iteration 0100: loss: 3959.696
iteration 0200: loss: 3959.411
iteration 0300: loss: 3964.020
iteration 0400: loss: 3967.131
iteration 0500: loss: 3954.655
iteration 0600: loss: 3966.287
iteration 0700: loss: 3958.718
iteration 0800: loss: 3959.147
iteration 0900: loss: 3964.666
iteration 1000: loss: 3968.050
iteration 1100: loss: 3966.828
iteration 1200: loss: 3962.378
iteration 1300: loss: 3965.467
iteration 1400: loss: 3959.123
iteration 1500: loss: 3959.850
iteration 1600: loss: 3968.456
iteration 1700: loss: 3964.208
iteration 1800: loss: 3967.958
====> Epoch: 011 Train loss: 3963.2432  took : 139.1822955608368
====> Test loss: 3967.9494
iteration 0000: loss: 3963.127
iteration 0100: loss: 3966.397
iteration 0200: loss: 3959.013
iteration 0300: loss: 3959.946
iteration 0400: loss: 3957.453
iteration 0500: loss: 3958.309
iteration 0600: loss: 3955.720
iteration 0700: loss: 3964.624
iteration 0800: loss: 3969.024
iteration 0900: loss: 3956.510
iteration 1000: loss: 3961.380
iteration 1100: loss: 3969.626
iteration 1200: loss: 3960.058
iteration 1300: loss: 3963.901
iteration 1400: loss: 3962.943
iteration 1500: loss: 3969.396
iteration 1600: loss: 3960.576
iteration 1700: loss: 3960.920
iteration 1800: loss: 3961.067
====> Epoch: 012 Train loss: 3962.9835  took : 138.7705430984497
====> Test loss: 3969.2273
iteration 0000: loss: 3961.914
iteration 0100: loss: 3961.387
iteration 0200: loss: 3957.740
iteration 0300: loss: 3967.007
iteration 0400: loss: 3966.131
iteration 0500: loss: 3967.275
iteration 0600: loss: 3967.038
iteration 0700: loss: 3962.503
iteration 0800: loss: 3963.907
iteration 0900: loss: 3965.426
iteration 1000: loss: 3975.308
iteration 1100: loss: 3967.646
iteration 1200: loss: 3958.708
iteration 1300: loss: 3958.484
iteration 1400: loss: 3959.590
iteration 1500: loss: 3967.786
iteration 1600: loss: 3964.816
iteration 1700: loss: 3960.844
iteration 1800: loss: 3965.379
====> Epoch: 013 Train loss: 3962.5942  took : 140.18213987350464
====> Test loss: 3968.3088
iteration 0000: loss: 3962.809
iteration 0100: loss: 3968.790
iteration 0200: loss: 3965.598
iteration 0300: loss: 3960.817
iteration 0400: loss: 3954.211
iteration 0500: loss: 3965.719
iteration 0600: loss: 3958.510
iteration 0700: loss: 3961.037
iteration 0800: loss: 3959.917
iteration 0900: loss: 3969.746
iteration 1000: loss: 3961.869
iteration 1100: loss: 3965.167
iteration 1200: loss: 3965.134
iteration 1300: loss: 3964.936
iteration 1400: loss: 3962.667
iteration 1500: loss: 3967.626
iteration 1600: loss: 3962.382
iteration 1700: loss: 3963.936
iteration 1800: loss: 3963.106
====> Epoch: 014 Train loss: 3962.2565  took : 139.3468461036682
====> Test loss: 3967.7663
iteration 0000: loss: 3963.956
iteration 0100: loss: 3957.170
iteration 0200: loss: 3960.554
iteration 0300: loss: 3963.226
iteration 0400: loss: 3965.835
iteration 0500: loss: 3959.083
iteration 0600: loss: 3961.387
iteration 0700: loss: 3961.834
iteration 0800: loss: 3971.943
iteration 0900: loss: 3961.916
iteration 1000: loss: 3969.974
iteration 1100: loss: 3964.588
iteration 1200: loss: 3970.993
iteration 1300: loss: 3957.609
iteration 1400: loss: 3957.917
iteration 1500: loss: 3961.409
iteration 1600: loss: 3953.346
iteration 1700: loss: 3963.427
iteration 1800: loss: 3967.840
====> Epoch: 015 Train loss: 3961.9545  took : 139.17723107337952
====> Test loss: 3967.4493
iteration 0000: loss: 3957.070
iteration 0100: loss: 3968.490
iteration 0200: loss: 3962.042
iteration 0300: loss: 3956.339
iteration 0400: loss: 3956.663
iteration 0500: loss: 3965.973
iteration 0600: loss: 3958.140
iteration 0700: loss: 3968.462
iteration 0800: loss: 3967.933
iteration 0900: loss: 3959.114
iteration 1000: loss: 3966.544
iteration 1100: loss: 3964.211
iteration 1200: loss: 3962.769
iteration 1300: loss: 3963.919
iteration 1400: loss: 3964.158
iteration 1500: loss: 3967.367
iteration 1600: loss: 3958.420
iteration 1700: loss: 3954.072
iteration 1800: loss: 3969.208
====> Epoch: 016 Train loss: 3961.9704  took : 139.9981746673584
====> Test loss: 3967.4567
iteration 0000: loss: 3968.049
iteration 0100: loss: 3970.080
iteration 0200: loss: 3964.333
iteration 0300: loss: 3955.349
iteration 0400: loss: 3965.344
iteration 0500: loss: 3963.068
iteration 0600: loss: 3956.521
iteration 0700: loss: 3957.522
iteration 0800: loss: 3961.463
iteration 0900: loss: 3964.170
iteration 1000: loss: 3959.017
iteration 1100: loss: 3968.280
iteration 1200: loss: 3968.759
iteration 1300: loss: 3963.444
iteration 1400: loss: 3969.836
iteration 1500: loss: 3961.223
iteration 1600: loss: 3957.791
iteration 1700: loss: 3968.665
iteration 1800: loss: 3965.612
====> Epoch: 017 Train loss: 3962.0020  took : 140.23323345184326
====> Test loss: 3967.8331
iteration 0000: loss: 3953.268
iteration 0100: loss: 3959.577
iteration 0200: loss: 3954.136
iteration 0300: loss: 3967.451
iteration 0400: loss: 3960.949
iteration 0500: loss: 3966.951
iteration 0600: loss: 3949.779
iteration 0700: loss: 3958.380
iteration 0800: loss: 3960.367
iteration 0900: loss: 3960.475
iteration 1000: loss: 3957.880
iteration 1100: loss: 3972.457
iteration 1200: loss: 3956.976
iteration 1300: loss: 3958.355
iteration 1400: loss: 3961.142
iteration 1500: loss: 3962.733
iteration 1600: loss: 3950.587
iteration 1700: loss: 3968.263
iteration 1800: loss: 3959.909
====> Epoch: 018 Train loss: 3961.5213  took : 138.9645960330963
====> Test loss: 3967.3912
iteration 0000: loss: 3956.729
iteration 0100: loss: 3958.601
iteration 0200: loss: 3966.643
iteration 0300: loss: 3966.223
iteration 0400: loss: 3955.382
iteration 0500: loss: 3956.939
iteration 0600: loss: 3955.289
iteration 0700: loss: 3957.762
iteration 0800: loss: 3965.044
iteration 0900: loss: 3962.696
iteration 1000: loss: 3965.079
iteration 1100: loss: 3968.995
iteration 1200: loss: 3958.500
iteration 1300: loss: 3963.676
iteration 1400: loss: 3965.073
iteration 1500: loss: 3962.161
iteration 1600: loss: 3957.034
iteration 1700: loss: 3961.884
iteration 1800: loss: 3962.219
====> Epoch: 019 Train loss: 3961.5127  took : 139.0391640663147
====> Test loss: 3966.8875
iteration 0000: loss: 3964.634
iteration 0100: loss: 3955.010
iteration 0200: loss: 3961.659
iteration 0300: loss: 3956.822
iteration 0400: loss: 3960.244
iteration 0500: loss: 3955.829
iteration 0600: loss: 3964.617
iteration 0700: loss: 3969.856
iteration 0800: loss: 3958.342
iteration 0900: loss: 3958.721
iteration 1000: loss: 3963.894
iteration 1100: loss: 3958.566
iteration 1200: loss: 3965.793
iteration 1300: loss: 3958.105
iteration 1400: loss: 3963.670
iteration 1500: loss: 3957.841
iteration 1600: loss: 3966.507
iteration 1700: loss: 3954.297
iteration 1800: loss: 3960.610
====> Epoch: 020 Train loss: 3961.0903  took : 140.26214218139648
====> Test loss: 3966.6686
iteration 0000: loss: 3966.551
iteration 0100: loss: 3962.573
iteration 0200: loss: 3968.604
iteration 0300: loss: 3966.460
iteration 0400: loss: 3961.061
iteration 0500: loss: 3957.125
iteration 0600: loss: 3961.616
iteration 0700: loss: 3955.491
iteration 0800: loss: 3965.553
iteration 0900: loss: 3958.523
iteration 1000: loss: 3962.480
iteration 1100: loss: 3961.941
iteration 1200: loss: 3962.421
iteration 1300: loss: 3963.403
iteration 1400: loss: 3958.534
iteration 1500: loss: 3956.687
iteration 1600: loss: 3953.559
iteration 1700: loss: 3965.347
iteration 1800: loss: 3959.957
====> Epoch: 021 Train loss: 3960.9292  took : 140.08876991271973
====> Test loss: 3966.9508
iteration 0000: loss: 3959.315
iteration 0100: loss: 3967.101
iteration 0200: loss: 3960.963
iteration 0300: loss: 3960.135
iteration 0400: loss: 3959.391
iteration 0500: loss: 3962.432
iteration 0600: loss: 3971.915
iteration 0700: loss: 3963.079
iteration 0800: loss: 3955.115
iteration 0900: loss: 3957.200
iteration 1000: loss: 3966.527
iteration 1100: loss: 3958.167
iteration 1200: loss: 3955.031
iteration 1300: loss: 3963.438
iteration 1400: loss: 3954.546
iteration 1500: loss: 3960.714
iteration 1600: loss: 3957.456
iteration 1700: loss: 3951.107
iteration 1800: loss: 3966.913
====> Epoch: 022 Train loss: 3960.8072  took : 140.11200714111328
====> Test loss: 3966.5870
iteration 0000: loss: 3963.445
iteration 0100: loss: 3963.262
iteration 0200: loss: 3970.546
iteration 0300: loss: 3956.335
iteration 0400: loss: 3960.800
iteration 0500: loss: 3955.345
iteration 0600: loss: 3968.271
iteration 0700: loss: 3958.634
iteration 0800: loss: 3964.270
iteration 0900: loss: 3966.566
iteration 1000: loss: 3957.302
iteration 1100: loss: 3951.542
iteration 1200: loss: 3962.701
iteration 1300: loss: 3967.946
iteration 1400: loss: 3961.280
iteration 1500: loss: 3955.744
iteration 1600: loss: 3961.223
iteration 1700: loss: 3959.770
iteration 1800: loss: 3952.362
====> Epoch: 023 Train loss: 3960.6899  took : 140.24197101593018
====> Test loss: 3966.8189
iteration 0000: loss: 3960.003
iteration 0100: loss: 3962.398
iteration 0200: loss: 3966.785
iteration 0300: loss: 3959.020
iteration 0400: loss: 3960.313
iteration 0500: loss: 3962.261
iteration 0600: loss: 3969.372
iteration 0700: loss: 3957.028
iteration 0800: loss: 3952.505
iteration 0900: loss: 3964.051
iteration 1000: loss: 3964.287
iteration 1100: loss: 3963.375
iteration 1200: loss: 3959.080
iteration 1300: loss: 3963.641
iteration 1400: loss: 3960.034
iteration 1500: loss: 3957.494
iteration 1600: loss: 3964.238
iteration 1700: loss: 3950.648
iteration 1800: loss: 3960.597
====> Epoch: 024 Train loss: 3960.8493  took : 139.57332968711853
====> Test loss: 3966.1390
iteration 0000: loss: 3959.693
iteration 0100: loss: 3969.353
iteration 0200: loss: 3970.172
iteration 0300: loss: 3953.039
iteration 0400: loss: 3958.633
iteration 0500: loss: 3957.201
iteration 0600: loss: 3954.564
iteration 0700: loss: 3959.448
iteration 0800: loss: 3960.937
iteration 0900: loss: 3956.382
iteration 1000: loss: 3960.791
iteration 1100: loss: 3962.757
iteration 1200: loss: 3965.042
iteration 1300: loss: 3956.022
iteration 1400: loss: 3969.689
iteration 1500: loss: 3964.292
iteration 1600: loss: 3963.012
iteration 1700: loss: 3967.600
iteration 1800: loss: 3953.540
====> Epoch: 025 Train loss: 3960.5529  took : 140.37157893180847
====> Test loss: 3966.3952
iteration 0000: loss: 3952.610
iteration 0100: loss: 3958.875
iteration 0200: loss: 3962.743
iteration 0300: loss: 3960.391
iteration 0400: loss: 3973.409
iteration 0500: loss: 3960.254
iteration 0600: loss: 3970.399
iteration 0700: loss: 3962.066
iteration 0800: loss: 3959.945
iteration 0900: loss: 3964.992
iteration 1000: loss: 3953.998
iteration 1100: loss: 3966.425
iteration 1200: loss: 3950.391
iteration 1300: loss: 3969.271
iteration 1400: loss: 3957.990
iteration 1500: loss: 3953.576
iteration 1600: loss: 3961.591
iteration 1700: loss: 3964.170
iteration 1800: loss: 3957.120
====> Epoch: 026 Train loss: 3960.5473  took : 139.63476729393005
====> Test loss: 3966.8357
iteration 0000: loss: 3958.452
iteration 0100: loss: 3960.777
iteration 0200: loss: 3954.720
iteration 0300: loss: 3954.440
iteration 0400: loss: 3956.186
iteration 0500: loss: 3957.536
iteration 0600: loss: 3958.904
iteration 0700: loss: 3960.066
iteration 0800: loss: 3954.717
iteration 0900: loss: 3958.654
iteration 1000: loss: 3955.779
iteration 1100: loss: 3953.803
iteration 1200: loss: 3964.723
iteration 1300: loss: 3958.720
iteration 1400: loss: 3961.731
iteration 1500: loss: 3962.355
iteration 1600: loss: 3952.183
iteration 1700: loss: 3965.923
iteration 1800: loss: 3950.296
====> Epoch: 027 Train loss: 3960.1998  took : 140.0855996608734
====> Test loss: 3965.7481
iteration 0000: loss: 3955.484
iteration 0100: loss: 3963.214
iteration 0200: loss: 3955.971
iteration 0300: loss: 3958.615
iteration 0400: loss: 3956.836
iteration 0500: loss: 3959.518
iteration 0600: loss: 3964.523
iteration 0700: loss: 3958.966
iteration 0800: loss: 3967.371
iteration 0900: loss: 3958.875
iteration 1000: loss: 3956.535
iteration 1100: loss: 3951.817
iteration 1200: loss: 3967.030
iteration 1300: loss: 3966.292
iteration 1400: loss: 3958.228
iteration 1500: loss: 3959.301
iteration 1600: loss: 3961.041
iteration 1700: loss: 3964.370
iteration 1800: loss: 3966.521
====> Epoch: 028 Train loss: 3960.4592  took : 140.44279074668884
====> Test loss: 3966.3528
iteration 0000: loss: 3956.028
iteration 0100: loss: 3960.945
iteration 0200: loss: 3961.458
iteration 0300: loss: 3963.715
iteration 0400: loss: 3966.895
iteration 0500: loss: 3954.562
iteration 0600: loss: 3962.114
iteration 0700: loss: 3971.151
iteration 0800: loss: 3958.099
iteration 0900: loss: 3963.713
iteration 1000: loss: 3957.740
iteration 1100: loss: 3954.065
iteration 1200: loss: 3957.323
iteration 1300: loss: 3956.944
iteration 1400: loss: 3966.143
iteration 1500: loss: 3955.670
iteration 1600: loss: 3959.344
iteration 1700: loss: 3956.521
iteration 1800: loss: 3963.483
====> Epoch: 029 Train loss: 3960.1797  took : 139.98474788665771
====> Test loss: 3967.0236
iteration 0000: loss: 3962.317
iteration 0100: loss: 3953.410
iteration 0200: loss: 3964.577
iteration 0300: loss: 3960.882
iteration 0400: loss: 3950.477
iteration 0500: loss: 3952.371
iteration 0600: loss: 3962.589
iteration 0700: loss: 3959.369
iteration 0800: loss: 3957.232
iteration 0900: loss: 3962.341
iteration 1000: loss: 3957.196
iteration 1100: loss: 3952.717
iteration 1200: loss: 3961.841
iteration 1300: loss: 3957.348
iteration 1400: loss: 3965.288
iteration 1500: loss: 3953.821
iteration 1600: loss: 3958.663
iteration 1700: loss: 3967.292
iteration 1800: loss: 3958.089
====> Epoch: 030 Train loss: 3960.0833  took : 140.74218010902405
====> Test loss: 3966.6444
iteration 0000: loss: 3964.587
iteration 0100: loss: 3966.403
iteration 0200: loss: 3954.955
iteration 0300: loss: 3957.061
iteration 0400: loss: 3962.794
iteration 0500: loss: 3953.733
iteration 0600: loss: 3963.245
iteration 0700: loss: 3959.686
iteration 0800: loss: 3960.786
iteration 0900: loss: 3957.858
iteration 1000: loss: 3962.595
iteration 1100: loss: 3964.364
iteration 1200: loss: 3959.452
iteration 1300: loss: 3961.118
iteration 1400: loss: 3956.475
iteration 1500: loss: 3962.870
iteration 1600: loss: 3961.236
iteration 1700: loss: 3953.438
iteration 1800: loss: 3957.562
====> Epoch: 031 Train loss: 3960.1389  took : 139.48853635787964
====> Test loss: 3965.9316
iteration 0000: loss: 3957.831
iteration 0100: loss: 3958.872
iteration 0200: loss: 3962.520
iteration 0300: loss: 3957.470
iteration 0400: loss: 3962.997
iteration 0500: loss: 3969.091
iteration 0600: loss: 3956.053
iteration 0700: loss: 3949.717
iteration 0800: loss: 3966.024
iteration 0900: loss: 3963.296
iteration 1000: loss: 3962.642
iteration 1100: loss: 3960.144
iteration 1200: loss: 3961.192
iteration 1300: loss: 3956.539
iteration 1400: loss: 3958.661
iteration 1500: loss: 3956.083
iteration 1600: loss: 3961.406
iteration 1700: loss: 3958.322
iteration 1800: loss: 3965.281
====> Epoch: 032 Train loss: 3959.6560  took : 138.05183863639832
====> Test loss: 3965.9077
iteration 0000: loss: 3962.099
iteration 0100: loss: 3960.361
iteration 0200: loss: 3949.051
iteration 0300: loss: 3961.883
iteration 0400: loss: 3964.821
iteration 0500: loss: 3957.881
iteration 0600: loss: 3962.864
iteration 0700: loss: 3962.665
iteration 0800: loss: 3958.489
iteration 0900: loss: 3961.569
iteration 1000: loss: 3960.057
iteration 1100: loss: 3958.251
iteration 1200: loss: 3962.011
iteration 1300: loss: 3958.805
iteration 1400: loss: 3956.641
iteration 1500: loss: 3946.420
iteration 1600: loss: 3966.330
iteration 1700: loss: 3963.981
iteration 1800: loss: 3961.338
====> Epoch: 033 Train loss: 3959.7952  took : 139.0784411430359
====> Test loss: 3966.0880
iteration 0000: loss: 3959.158
iteration 0100: loss: 3968.832
iteration 0200: loss: 3964.342
iteration 0300: loss: 3955.640
iteration 0400: loss: 3963.420
iteration 0500: loss: 3969.176
iteration 0600: loss: 3953.190
iteration 0700: loss: 3954.516
iteration 0800: loss: 3957.848
iteration 0900: loss: 3967.938
iteration 1000: loss: 3960.652
iteration 1100: loss: 3965.192
iteration 1200: loss: 3972.666
iteration 1300: loss: 3956.086
iteration 1400: loss: 3960.850
iteration 1500: loss: 3951.378
iteration 1600: loss: 3962.589
iteration 1700: loss: 3957.476
iteration 1800: loss: 3953.139
====> Epoch: 034 Train loss: 3959.8282  took : 139.82171535491943
====> Test loss: 3966.3554
iteration 0000: loss: 3961.021
iteration 0100: loss: 3962.171
iteration 0200: loss: 3949.305
iteration 0300: loss: 3953.877
iteration 0400: loss: 3957.477
iteration 0500: loss: 3973.261
iteration 0600: loss: 3970.353
iteration 0700: loss: 3955.375
iteration 0800: loss: 3964.431
iteration 0900: loss: 3952.805
iteration 1000: loss: 3960.504
iteration 1100: loss: 3961.710
iteration 1200: loss: 3955.913
iteration 1300: loss: 3960.610
iteration 1400: loss: 3965.338
iteration 1500: loss: 3956.368
iteration 1600: loss: 3961.682
iteration 1700: loss: 3950.722
iteration 1800: loss: 3969.289
====> Epoch: 035 Train loss: 3959.7144  took : 139.57198309898376
====> Test loss: 3966.2828
iteration 0000: loss: 3960.901
iteration 0100: loss: 3961.097
iteration 0200: loss: 3957.482
iteration 0300: loss: 3951.718
iteration 0400: loss: 3958.762
iteration 0500: loss: 3958.892
iteration 0600: loss: 3972.849
iteration 0700: loss: 3961.750
iteration 0800: loss: 3967.575
iteration 0900: loss: 3956.123
iteration 1000: loss: 3958.619
iteration 1100: loss: 3960.420
iteration 1200: loss: 3964.168
iteration 1300: loss: 3959.887
iteration 1400: loss: 3953.758
iteration 1500: loss: 3958.274
iteration 1600: loss: 3965.876
iteration 1700: loss: 3955.104
iteration 1800: loss: 3951.047
====> Epoch: 036 Train loss: 3959.4560  took : 138.13643884658813
====> Test loss: 3965.6597
iteration 0000: loss: 3956.587
iteration 0100: loss: 3958.490
iteration 0200: loss: 3958.939
iteration 0300: loss: 3956.811
iteration 0400: loss: 3962.102
iteration 0500: loss: 3959.005
iteration 0600: loss: 3954.282
iteration 0700: loss: 3958.959
iteration 0800: loss: 3962.430
iteration 0900: loss: 3954.908
iteration 1000: loss: 3955.226
iteration 1100: loss: 3964.006
iteration 1200: loss: 3960.885
iteration 1300: loss: 3961.894
iteration 1400: loss: 3960.650
iteration 1500: loss: 3965.098
iteration 1600: loss: 3961.921
iteration 1700: loss: 3960.819
iteration 1800: loss: 3959.115
====> Epoch: 037 Train loss: 3959.4669  took : 140.05948686599731
====> Test loss: 3965.8009
iteration 0000: loss: 3960.616
iteration 0100: loss: 3956.516
iteration 0200: loss: 3949.758
iteration 0300: loss: 3952.277
iteration 0400: loss: 3959.752
iteration 0500: loss: 3956.841
iteration 0600: loss: 3961.752
iteration 0700: loss: 3964.732
iteration 0800: loss: 3962.263
iteration 0900: loss: 3966.851
iteration 1000: loss: 3964.460
iteration 1100: loss: 3955.482
iteration 1200: loss: 3960.810
iteration 1300: loss: 3955.010
iteration 1400: loss: 3956.123
iteration 1500: loss: 3962.349
iteration 1600: loss: 3956.441
iteration 1700: loss: 3959.806
iteration 1800: loss: 3958.123
====> Epoch: 038 Train loss: 3959.4443  took : 140.45142531394958
====> Test loss: 3966.0831
iteration 0000: loss: 3966.316
iteration 0100: loss: 3961.150
iteration 0200: loss: 3954.379
iteration 0300: loss: 3958.474
iteration 0400: loss: 3957.525
iteration 0500: loss: 3970.142
iteration 0600: loss: 3956.765
iteration 0700: loss: 3954.123
iteration 0800: loss: 3960.674
iteration 0900: loss: 3952.036
iteration 1000: loss: 3967.368
iteration 1100: loss: 3958.274
iteration 1200: loss: 3958.652
iteration 1300: loss: 3959.456
iteration 1400: loss: 3962.190
iteration 1500: loss: 3960.664
iteration 1600: loss: 3969.515
iteration 1700: loss: 3969.091
iteration 1800: loss: 3967.955
====> Epoch: 039 Train loss: 3959.3603  took : 139.68271017074585
====> Test loss: 3965.2379
iteration 0000: loss: 3958.800
iteration 0100: loss: 3953.271
iteration 0200: loss: 3960.768
iteration 0300: loss: 3973.502
iteration 0400: loss: 3951.311
iteration 0500: loss: 3953.698
iteration 0600: loss: 3962.765
iteration 0700: loss: 3961.444
iteration 0800: loss: 3957.841
iteration 0900: loss: 3960.549
iteration 1000: loss: 3967.586
iteration 1100: loss: 3952.927
iteration 1200: loss: 3946.155
iteration 1300: loss: 3962.884
iteration 1400: loss: 3961.202
iteration 1500: loss: 3962.753
iteration 1600: loss: 3959.069
iteration 1700: loss: 3963.931
iteration 1800: loss: 3958.349
====> Epoch: 040 Train loss: 3959.2012  took : 140.11616468429565
====> Test loss: 3965.5889
iteration 0000: loss: 3954.566
iteration 0100: loss: 3953.313
iteration 0200: loss: 3954.726
iteration 0300: loss: 3955.266
iteration 0400: loss: 3958.793
iteration 0500: loss: 3958.726
iteration 0600: loss: 3962.605
iteration 0700: loss: 3961.743
iteration 0800: loss: 3964.922
iteration 0900: loss: 3951.418
iteration 1000: loss: 3960.206
iteration 1100: loss: 3961.980
iteration 1200: loss: 3957.937
iteration 1300: loss: 3955.857
iteration 1400: loss: 3963.529
iteration 1500: loss: 3964.838
iteration 1600: loss: 3954.004
iteration 1700: loss: 3967.525
iteration 1800: loss: 3957.578
====> Epoch: 041 Train loss: 3959.2160  took : 139.06127500534058
====> Test loss: 3965.9036
iteration 0000: loss: 3961.004
iteration 0100: loss: 3968.666
iteration 0200: loss: 3957.071
iteration 0300: loss: 3958.155
iteration 0400: loss: 3962.428
iteration 0500: loss: 3955.505
iteration 0600: loss: 3958.029
iteration 0700: loss: 3957.955
iteration 0800: loss: 3964.589
iteration 0900: loss: 3969.414
iteration 1000: loss: 3954.953
iteration 1100: loss: 3959.740
iteration 1200: loss: 3962.784
iteration 1300: loss: 3959.421
iteration 1400: loss: 3959.021
iteration 1500: loss: 3961.106
iteration 1600: loss: 3959.526
iteration 1700: loss: 3968.935
iteration 1800: loss: 3959.346
====> Epoch: 042 Train loss: 3959.1423  took : 139.8106038570404
====> Test loss: 3965.5279
iteration 0000: loss: 3963.801
iteration 0100: loss: 3956.280
iteration 0200: loss: 3957.131
iteration 0300: loss: 3957.265
iteration 0400: loss: 3959.955
iteration 0500: loss: 3967.982
iteration 0600: loss: 3960.732
iteration 0700: loss: 3965.050
iteration 0800: loss: 3954.431
iteration 0900: loss: 3962.992
iteration 1000: loss: 3957.212
iteration 1100: loss: 3958.813
iteration 1200: loss: 3959.289
iteration 1300: loss: 3953.238
iteration 1400: loss: 3957.666
iteration 1500: loss: 3963.253
iteration 1600: loss: 3959.431
iteration 1700: loss: 3963.021
iteration 1800: loss: 3961.120
====> Epoch: 043 Train loss: 3959.2733  took : 139.4664044380188
====> Test loss: 3965.3989
iteration 0000: loss: 3959.167
iteration 0100: loss: 3958.997
iteration 0200: loss: 3965.367
iteration 0300: loss: 3951.728
iteration 0400: loss: 3958.449
iteration 0500: loss: 3961.728
iteration 0600: loss: 3954.639
iteration 0700: loss: 3961.485
iteration 0800: loss: 3964.429
iteration 0900: loss: 3960.685
iteration 1000: loss: 3962.475
iteration 1100: loss: 3960.869
iteration 1200: loss: 3955.954
iteration 1300: loss: 3960.195
iteration 1400: loss: 3963.924
iteration 1500: loss: 3962.127
iteration 1600: loss: 3965.234
iteration 1700: loss: 3953.047
iteration 1800: loss: 3960.552
====> Epoch: 044 Train loss: 3959.0349  took : 139.70425534248352
====> Test loss: 3964.8905
iteration 0000: loss: 3962.432
iteration 0100: loss: 3960.416
iteration 0200: loss: 3958.371
iteration 0300: loss: 3961.476
iteration 0400: loss: 3967.669
iteration 0500: loss: 3960.597
iteration 0600: loss: 3972.077
iteration 0700: loss: 3954.604
iteration 0800: loss: 3957.079
iteration 0900: loss: 3961.670
iteration 1000: loss: 3962.246
iteration 1100: loss: 3964.450
iteration 1200: loss: 3957.436
iteration 1300: loss: 3961.798
iteration 1400: loss: 3964.890
iteration 1500: loss: 3959.713
iteration 1600: loss: 3952.724
iteration 1700: loss: 3955.323
iteration 1800: loss: 3960.148
====> Epoch: 045 Train loss: 3959.1033  took : 139.8461389541626
====> Test loss: 3964.8971
iteration 0000: loss: 3956.041
iteration 0100: loss: 3961.224
iteration 0200: loss: 3957.630
iteration 0300: loss: 3953.251
iteration 0400: loss: 3962.383
iteration 0500: loss: 3956.256
iteration 0600: loss: 3948.172
iteration 0700: loss: 3955.199
iteration 0800: loss: 3963.893
iteration 0900: loss: 3962.678
iteration 1000: loss: 3958.159
iteration 1100: loss: 3962.224
iteration 1200: loss: 3957.173
iteration 1300: loss: 3959.115
iteration 1400: loss: 3970.547
iteration 1500: loss: 3958.680
iteration 1600: loss: 3957.803
iteration 1700: loss: 3966.968
iteration 1800: loss: 3959.141
====> Epoch: 046 Train loss: 3959.0881  took : 139.35418915748596
====> Test loss: 3965.4266
iteration 0000: loss: 3959.122
iteration 0100: loss: 3952.775
iteration 0200: loss: 3960.131
iteration 0300: loss: 3960.090
iteration 0400: loss: 3958.859
iteration 0500: loss: 3960.281
iteration 0600: loss: 3953.057
iteration 0700: loss: 3956.295
iteration 0800: loss: 3962.431
iteration 0900: loss: 3956.150
iteration 1000: loss: 3955.971
iteration 1100: loss: 3948.880
iteration 1200: loss: 3961.174
iteration 1300: loss: 3959.664
iteration 1400: loss: 3952.934
iteration 1500: loss: 3964.078
iteration 1600: loss: 3955.733
iteration 1700: loss: 3955.036
iteration 1800: loss: 3963.787
====> Epoch: 047 Train loss: 3959.1905  took : 140.18032503128052
====> Test loss: 3964.8142
iteration 0000: loss: 3964.742
iteration 0100: loss: 3962.769
iteration 0200: loss: 3953.262
iteration 0300: loss: 3956.070
iteration 0400: loss: 3956.912
iteration 0500: loss: 3968.011
iteration 0600: loss: 3958.120
iteration 0700: loss: 3957.388
iteration 0800: loss: 3956.690
iteration 0900: loss: 3956.871
iteration 1000: loss: 3963.802
iteration 1100: loss: 3954.153
iteration 1200: loss: 3961.673
iteration 1300: loss: 3958.364
iteration 1400: loss: 3953.646
iteration 1500: loss: 3964.746
iteration 1600: loss: 3954.216
iteration 1700: loss: 3950.918
iteration 1800: loss: 3964.433
====> Epoch: 048 Train loss: 3958.9115  took : 139.0445945262909
====> Test loss: 3965.5290
iteration 0000: loss: 3957.296
iteration 0100: loss: 3961.937
iteration 0200: loss: 3959.033
iteration 0300: loss: 3959.065
iteration 0400: loss: 3954.763
iteration 0500: loss: 3955.858
iteration 0600: loss: 3959.875
iteration 0700: loss: 3954.822
iteration 0800: loss: 3954.368
iteration 0900: loss: 3957.919
iteration 1000: loss: 3955.538
iteration 1100: loss: 3959.494
iteration 1200: loss: 3963.488
iteration 1300: loss: 3957.283
iteration 1400: loss: 3955.142
iteration 1500: loss: 3965.295
iteration 1600: loss: 3954.876
iteration 1700: loss: 3962.164
iteration 1800: loss: 3963.331
====> Epoch: 049 Train loss: 3958.8969  took : 140.07900881767273
====> Test loss: 3964.9957
iteration 0000: loss: 3962.798
iteration 0100: loss: 3960.558
iteration 0200: loss: 3957.360
iteration 0300: loss: 3956.260
iteration 0400: loss: 3954.113
iteration 0500: loss: 3963.384
iteration 0600: loss: 3960.878
iteration 0700: loss: 3957.388
iteration 0800: loss: 3963.781
iteration 0900: loss: 3960.429
iteration 1000: loss: 3956.708
iteration 1100: loss: 3962.922
iteration 1200: loss: 3954.187
iteration 1300: loss: 3959.416
iteration 1400: loss: 3964.966
iteration 1500: loss: 3963.651
iteration 1600: loss: 3958.271
iteration 1700: loss: 3956.114
iteration 1800: loss: 3957.594
====> Epoch: 050 Train loss: 3958.8331  took : 140.0186619758606
====> Test loss: 3964.9847
====> [MM-VAE] Time: 7990.133s or 02:13:10
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_1
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1996.208
iteration 0100: loss: 1568.251
iteration 0200: loss: 1550.165
iteration 0300: loss: 1555.022
iteration 0400: loss: 1546.504
iteration 0500: loss: 1543.819
iteration 0600: loss: 1543.932
iteration 0700: loss: 1546.568
iteration 0800: loss: 1537.953
iteration 0900: loss: 1540.136
====> Epoch: 001 Train loss: 1551.8442  took : 5.9112067222595215
====> Test loss: 1531.9003
iteration 0000: loss: 1534.289
iteration 0100: loss: 1539.691
iteration 0200: loss: 1536.526
iteration 0300: loss: 1535.751
iteration 0400: loss: 1533.717
iteration 0500: loss: 1535.919
iteration 0600: loss: 1537.655
iteration 0700: loss: 1531.822
iteration 0800: loss: 1533.344
iteration 0900: loss: 1536.217
====> Epoch: 002 Train loss: 1534.3722  took : 5.8659186363220215
====> Test loss: 1527.6368
iteration 0000: loss: 1532.847
iteration 0100: loss: 1532.011
iteration 0200: loss: 1533.666
iteration 0300: loss: 1537.073
iteration 0400: loss: 1532.333
iteration 0500: loss: 1532.605
iteration 0600: loss: 1537.469
iteration 0700: loss: 1527.462
iteration 0800: loss: 1532.112
iteration 0900: loss: 1529.732
====> Epoch: 003 Train loss: 1531.4808  took : 5.921749830245972
====> Test loss: 1525.9625
iteration 0000: loss: 1533.837
iteration 0100: loss: 1530.211
iteration 0200: loss: 1528.345
iteration 0300: loss: 1530.689
iteration 0400: loss: 1528.763
iteration 0500: loss: 1529.892
iteration 0600: loss: 1530.272
iteration 0700: loss: 1533.199
iteration 0800: loss: 1533.453
iteration 0900: loss: 1526.457
====> Epoch: 004 Train loss: 1530.1742  took : 5.840069770812988
====> Test loss: 1524.8232
iteration 0000: loss: 1534.794
iteration 0100: loss: 1527.719
iteration 0200: loss: 1526.417
iteration 0300: loss: 1530.442
iteration 0400: loss: 1526.987
iteration 0500: loss: 1528.144
iteration 0600: loss: 1528.765
iteration 0700: loss: 1528.994
iteration 0800: loss: 1531.010
iteration 0900: loss: 1531.248
====> Epoch: 005 Train loss: 1529.3527  took : 5.82684326171875
====> Test loss: 1524.2582
iteration 0000: loss: 1531.881
iteration 0100: loss: 1529.194
iteration 0200: loss: 1528.059
iteration 0300: loss: 1528.684
iteration 0400: loss: 1530.046
iteration 0500: loss: 1528.810
iteration 0600: loss: 1531.610
iteration 0700: loss: 1528.652
iteration 0800: loss: 1526.687
iteration 0900: loss: 1527.634
====> Epoch: 006 Train loss: 1528.7667  took : 5.9010396003723145
====> Test loss: 1523.6574
iteration 0000: loss: 1528.615
iteration 0100: loss: 1525.734
iteration 0200: loss: 1528.623
iteration 0300: loss: 1530.125
iteration 0400: loss: 1530.943
iteration 0500: loss: 1525.443
iteration 0600: loss: 1527.034
iteration 0700: loss: 1524.748
iteration 0800: loss: 1528.962
iteration 0900: loss: 1526.665
====> Epoch: 007 Train loss: 1528.3626  took : 5.871095895767212
====> Test loss: 1523.5594
iteration 0000: loss: 1528.713
iteration 0100: loss: 1525.529
iteration 0200: loss: 1527.119
iteration 0300: loss: 1523.780
iteration 0400: loss: 1528.708
iteration 0500: loss: 1529.654
iteration 0600: loss: 1526.956
iteration 0700: loss: 1527.693
iteration 0800: loss: 1529.870
iteration 0900: loss: 1526.563
====> Epoch: 008 Train loss: 1528.0140  took : 5.8961615562438965
====> Test loss: 1523.0943
iteration 0000: loss: 1527.840
iteration 0100: loss: 1529.431
iteration 0200: loss: 1526.190
iteration 0300: loss: 1526.829
iteration 0400: loss: 1530.798
iteration 0500: loss: 1528.222
iteration 0600: loss: 1529.400
iteration 0700: loss: 1529.234
iteration 0800: loss: 1525.871
iteration 0900: loss: 1527.155
====> Epoch: 009 Train loss: 1527.7266  took : 5.906656503677368
====> Test loss: 1522.8639
iteration 0000: loss: 1527.969
iteration 0100: loss: 1526.771
iteration 0200: loss: 1526.912
iteration 0300: loss: 1525.583
iteration 0400: loss: 1526.719
iteration 0500: loss: 1528.931
iteration 0600: loss: 1527.490
iteration 0700: loss: 1532.087
iteration 0800: loss: 1528.365
iteration 0900: loss: 1529.288
====> Epoch: 010 Train loss: 1527.4730  took : 5.886085748672485
====> Test loss: 1522.6411
iteration 0000: loss: 1526.270
iteration 0100: loss: 1528.439
iteration 0200: loss: 1527.963
iteration 0300: loss: 1529.352
iteration 0400: loss: 1527.026
iteration 0500: loss: 1525.235
iteration 0600: loss: 1526.610
iteration 0700: loss: 1527.028
iteration 0800: loss: 1526.481
iteration 0900: loss: 1527.490
====> Epoch: 011 Train loss: 1527.2420  took : 5.928905725479126
====> Test loss: 1522.4936
iteration 0000: loss: 1527.770
iteration 0100: loss: 1524.928
iteration 0200: loss: 1527.624
iteration 0300: loss: 1527.391
iteration 0400: loss: 1526.855
iteration 0500: loss: 1525.838
iteration 0600: loss: 1524.887
iteration 0700: loss: 1528.299
iteration 0800: loss: 1524.776
iteration 0900: loss: 1528.088
====> Epoch: 012 Train loss: 1527.0913  took : 5.893949031829834
====> Test loss: 1522.2867
iteration 0000: loss: 1526.704
iteration 0100: loss: 1524.518
iteration 0200: loss: 1526.278
iteration 0300: loss: 1524.971
iteration 0400: loss: 1530.319
iteration 0500: loss: 1527.443
iteration 0600: loss: 1525.496
iteration 0700: loss: 1526.496
iteration 0800: loss: 1526.930
iteration 0900: loss: 1528.898
====> Epoch: 013 Train loss: 1526.9395  took : 5.881091833114624
====> Test loss: 1522.2101
iteration 0000: loss: 1528.864
iteration 0100: loss: 1528.718
iteration 0200: loss: 1527.057
iteration 0300: loss: 1524.026
iteration 0400: loss: 1525.409
iteration 0500: loss: 1525.975
iteration 0600: loss: 1525.449
iteration 0700: loss: 1528.449
iteration 0800: loss: 1525.766
iteration 0900: loss: 1530.112
====> Epoch: 014 Train loss: 1526.7732  took : 5.87260365486145
====> Test loss: 1522.0883
iteration 0000: loss: 1525.787
iteration 0100: loss: 1527.703
iteration 0200: loss: 1529.510
iteration 0300: loss: 1527.983
iteration 0400: loss: 1529.303
iteration 0500: loss: 1528.835
iteration 0600: loss: 1527.046
iteration 0700: loss: 1530.602
iteration 0800: loss: 1528.516
iteration 0900: loss: 1527.494
====> Epoch: 015 Train loss: 1526.6675  took : 5.875795841217041
====> Test loss: 1522.0414
iteration 0000: loss: 1528.429
iteration 0100: loss: 1523.963
iteration 0200: loss: 1527.606
iteration 0300: loss: 1524.262
iteration 0400: loss: 1528.049
iteration 0500: loss: 1524.564
iteration 0600: loss: 1528.179
iteration 0700: loss: 1528.046
iteration 0800: loss: 1527.712
iteration 0900: loss: 1526.224
====> Epoch: 016 Train loss: 1526.5500  took : 5.865882635116577
====> Test loss: 1521.8553
iteration 0000: loss: 1526.460
iteration 0100: loss: 1527.070
iteration 0200: loss: 1525.990
iteration 0300: loss: 1523.317
iteration 0400: loss: 1526.898
iteration 0500: loss: 1526.805
iteration 0600: loss: 1528.559
iteration 0700: loss: 1527.735
iteration 0800: loss: 1528.226
iteration 0900: loss: 1523.706
====> Epoch: 017 Train loss: 1526.4500  took : 6.015077829360962
====> Test loss: 1521.9183
iteration 0000: loss: 1529.797
iteration 0100: loss: 1523.839
iteration 0200: loss: 1525.102
iteration 0300: loss: 1525.935
iteration 0400: loss: 1524.384
iteration 0500: loss: 1526.543
iteration 0600: loss: 1528.218
iteration 0700: loss: 1528.660
iteration 0800: loss: 1526.922
iteration 0900: loss: 1527.124
====> Epoch: 018 Train loss: 1526.3315  took : 5.899123430252075
====> Test loss: 1521.8365
iteration 0000: loss: 1525.705
iteration 0100: loss: 1523.863
iteration 0200: loss: 1526.170
iteration 0300: loss: 1526.014
iteration 0400: loss: 1527.360
iteration 0500: loss: 1526.688
iteration 0600: loss: 1523.752
iteration 0700: loss: 1526.296
iteration 0800: loss: 1525.502
iteration 0900: loss: 1525.824
====> Epoch: 019 Train loss: 1526.2498  took : 5.969959020614624
====> Test loss: 1521.6684
iteration 0000: loss: 1526.775
iteration 0100: loss: 1530.472
iteration 0200: loss: 1527.572
iteration 0300: loss: 1526.390
iteration 0400: loss: 1525.283
iteration 0500: loss: 1524.866
iteration 0600: loss: 1527.559
iteration 0700: loss: 1527.489
iteration 0800: loss: 1526.044
iteration 0900: loss: 1524.698
====> Epoch: 020 Train loss: 1526.1733  took : 5.893150806427002
====> Test loss: 1521.6283
iteration 0000: loss: 1522.254
iteration 0100: loss: 1530.115
iteration 0200: loss: 1527.022
iteration 0300: loss: 1527.608
iteration 0400: loss: 1522.320
iteration 0500: loss: 1525.666
iteration 0600: loss: 1527.751
iteration 0700: loss: 1527.573
iteration 0800: loss: 1529.411
iteration 0900: loss: 1526.461
====> Epoch: 021 Train loss: 1526.1198  took : 5.941248416900635
====> Test loss: 1521.5886
iteration 0000: loss: 1525.074
iteration 0100: loss: 1524.050
iteration 0200: loss: 1524.385
iteration 0300: loss: 1524.084
iteration 0400: loss: 1525.701
iteration 0500: loss: 1529.584
iteration 0600: loss: 1528.490
iteration 0700: loss: 1528.488
iteration 0800: loss: 1525.652
iteration 0900: loss: 1525.464
====> Epoch: 022 Train loss: 1525.9818  took : 5.894293785095215
====> Test loss: 1521.4975
iteration 0000: loss: 1521.926
iteration 0100: loss: 1524.286
iteration 0200: loss: 1528.091
iteration 0300: loss: 1525.323
iteration 0400: loss: 1525.122
iteration 0500: loss: 1526.819
iteration 0600: loss: 1525.026
iteration 0700: loss: 1527.362
iteration 0800: loss: 1527.520
iteration 0900: loss: 1528.212
====> Epoch: 023 Train loss: 1525.9496  took : 5.883139371871948
====> Test loss: 1521.4905
iteration 0000: loss: 1523.401
iteration 0100: loss: 1528.414
iteration 0200: loss: 1524.394
iteration 0300: loss: 1528.753
iteration 0400: loss: 1528.050
iteration 0500: loss: 1526.412
iteration 0600: loss: 1522.734
iteration 0700: loss: 1524.920
iteration 0800: loss: 1525.183
iteration 0900: loss: 1524.730
====> Epoch: 024 Train loss: 1525.8615  took : 5.885108709335327
====> Test loss: 1521.3547
iteration 0000: loss: 1526.340
iteration 0100: loss: 1523.526
iteration 0200: loss: 1525.883
iteration 0300: loss: 1529.292
iteration 0400: loss: 1528.426
iteration 0500: loss: 1523.786
iteration 0600: loss: 1528.450
iteration 0700: loss: 1522.724
iteration 0800: loss: 1527.226
iteration 0900: loss: 1523.834
====> Epoch: 025 Train loss: 1525.8229  took : 5.908241033554077
====> Test loss: 1521.2257
iteration 0000: loss: 1524.516
iteration 0100: loss: 1525.036
iteration 0200: loss: 1524.983
iteration 0300: loss: 1526.706
iteration 0400: loss: 1525.720
iteration 0500: loss: 1523.328
iteration 0600: loss: 1528.815
iteration 0700: loss: 1526.523
iteration 0800: loss: 1524.510
iteration 0900: loss: 1528.360
====> Epoch: 026 Train loss: 1525.7314  took : 5.91359806060791
====> Test loss: 1521.3415
iteration 0000: loss: 1527.711
iteration 0100: loss: 1525.489
iteration 0200: loss: 1524.434
iteration 0300: loss: 1530.587
iteration 0400: loss: 1525.755
iteration 0500: loss: 1527.715
iteration 0600: loss: 1526.726
iteration 0700: loss: 1525.224
iteration 0800: loss: 1524.219
iteration 0900: loss: 1526.566
====> Epoch: 027 Train loss: 1525.6813  took : 5.916582822799683
====> Test loss: 1521.1502
iteration 0000: loss: 1524.065
iteration 0100: loss: 1527.304
iteration 0200: loss: 1524.854
iteration 0300: loss: 1527.354
iteration 0400: loss: 1524.286
iteration 0500: loss: 1525.385
iteration 0600: loss: 1527.938
iteration 0700: loss: 1527.093
iteration 0800: loss: 1523.489
iteration 0900: loss: 1527.366
====> Epoch: 028 Train loss: 1525.6359  took : 5.926137447357178
====> Test loss: 1521.2806
iteration 0000: loss: 1522.660
iteration 0100: loss: 1524.569
iteration 0200: loss: 1524.146
iteration 0300: loss: 1527.488
iteration 0400: loss: 1526.772
iteration 0500: loss: 1525.317
iteration 0600: loss: 1522.303
iteration 0700: loss: 1528.084
iteration 0800: loss: 1522.746
iteration 0900: loss: 1524.511
====> Epoch: 029 Train loss: 1525.5754  took : 5.910515308380127
====> Test loss: 1521.1899
iteration 0000: loss: 1525.424
iteration 0100: loss: 1525.093
iteration 0200: loss: 1524.430
iteration 0300: loss: 1527.798
iteration 0400: loss: 1524.025
iteration 0500: loss: 1525.242
iteration 0600: loss: 1524.598
iteration 0700: loss: 1526.719
iteration 0800: loss: 1527.188
iteration 0900: loss: 1528.867
====> Epoch: 030 Train loss: 1525.5630  took : 5.890235424041748
====> Test loss: 1521.2546
iteration 0000: loss: 1524.233
iteration 0100: loss: 1524.529
iteration 0200: loss: 1525.694
iteration 0300: loss: 1526.656
iteration 0400: loss: 1529.103
iteration 0500: loss: 1525.863
iteration 0600: loss: 1525.391
iteration 0700: loss: 1526.465
iteration 0800: loss: 1525.969
iteration 0900: loss: 1523.338
====> Epoch: 031 Train loss: 1525.4625  took : 5.907249689102173
====> Test loss: 1521.1411
iteration 0000: loss: 1524.553
iteration 0100: loss: 1525.139
iteration 0200: loss: 1523.223
iteration 0300: loss: 1524.152
iteration 0400: loss: 1524.849
iteration 0500: loss: 1525.032
iteration 0600: loss: 1524.457
iteration 0700: loss: 1522.758
iteration 0800: loss: 1526.998
iteration 0900: loss: 1529.546
====> Epoch: 032 Train loss: 1525.4482  took : 5.872461795806885
====> Test loss: 1520.9686
iteration 0000: loss: 1527.949
iteration 0100: loss: 1526.363
iteration 0200: loss: 1523.687
iteration 0300: loss: 1526.646
iteration 0400: loss: 1525.847
iteration 0500: loss: 1525.628
iteration 0600: loss: 1528.673
iteration 0700: loss: 1526.501
iteration 0800: loss: 1526.325
iteration 0900: loss: 1525.689
====> Epoch: 033 Train loss: 1525.3798  took : 5.898417711257935
====> Test loss: 1521.1364
iteration 0000: loss: 1526.275
iteration 0100: loss: 1525.464
iteration 0200: loss: 1523.381
iteration 0300: loss: 1526.935
iteration 0400: loss: 1525.657
iteration 0500: loss: 1526.388
iteration 0600: loss: 1523.891
iteration 0700: loss: 1525.397
iteration 0800: loss: 1523.941
iteration 0900: loss: 1525.177
====> Epoch: 034 Train loss: 1525.3482  took : 5.935662031173706
====> Test loss: 1521.0854
iteration 0000: loss: 1526.025
iteration 0100: loss: 1527.953
iteration 0200: loss: 1526.595
iteration 0300: loss: 1526.990
iteration 0400: loss: 1523.496
iteration 0500: loss: 1526.705
iteration 0600: loss: 1524.401
iteration 0700: loss: 1523.703
iteration 0800: loss: 1524.844
iteration 0900: loss: 1524.177
====> Epoch: 035 Train loss: 1525.2821  took : 5.895418882369995
====> Test loss: 1521.0319
iteration 0000: loss: 1526.353
iteration 0100: loss: 1523.980
iteration 0200: loss: 1524.441
iteration 0300: loss: 1526.736
iteration 0400: loss: 1524.124
iteration 0500: loss: 1524.512
iteration 0600: loss: 1526.136
iteration 0700: loss: 1524.313
iteration 0800: loss: 1526.106
iteration 0900: loss: 1524.160
====> Epoch: 036 Train loss: 1525.2453  took : 5.981595039367676
====> Test loss: 1521.0530
iteration 0000: loss: 1525.853
iteration 0100: loss: 1524.954
iteration 0200: loss: 1524.635
iteration 0300: loss: 1527.455
iteration 0400: loss: 1524.727
iteration 0500: loss: 1523.721
iteration 0600: loss: 1527.649
iteration 0700: loss: 1524.628
iteration 0800: loss: 1526.896
iteration 0900: loss: 1524.671
====> Epoch: 037 Train loss: 1525.2311  took : 5.911897897720337
====> Test loss: 1520.8248
iteration 0000: loss: 1525.664
iteration 0100: loss: 1525.617
iteration 0200: loss: 1524.418
iteration 0300: loss: 1524.215
iteration 0400: loss: 1523.713
iteration 0500: loss: 1526.332
iteration 0600: loss: 1522.296
iteration 0700: loss: 1528.696
iteration 0800: loss: 1527.597
iteration 0900: loss: 1523.844
====> Epoch: 038 Train loss: 1525.1671  took : 5.9169416427612305
====> Test loss: 1520.8866
iteration 0000: loss: 1526.277
iteration 0100: loss: 1524.846
iteration 0200: loss: 1521.600
iteration 0300: loss: 1523.070
iteration 0400: loss: 1526.714
iteration 0500: loss: 1522.894
iteration 0600: loss: 1527.535
iteration 0700: loss: 1526.501
iteration 0800: loss: 1525.895
iteration 0900: loss: 1527.987
====> Epoch: 039 Train loss: 1525.1319  took : 5.903129577636719
====> Test loss: 1520.8037
iteration 0000: loss: 1523.859
iteration 0100: loss: 1523.329
iteration 0200: loss: 1524.543
iteration 0300: loss: 1523.148
iteration 0400: loss: 1526.098
iteration 0500: loss: 1523.615
iteration 0600: loss: 1526.053
iteration 0700: loss: 1525.267
iteration 0800: loss: 1523.503
iteration 0900: loss: 1523.599
====> Epoch: 040 Train loss: 1525.1087  took : 5.857774257659912
====> Test loss: 1520.9472
iteration 0000: loss: 1527.847
iteration 0100: loss: 1526.446
iteration 0200: loss: 1526.253
iteration 0300: loss: 1523.212
iteration 0400: loss: 1526.451
iteration 0500: loss: 1525.334
iteration 0600: loss: 1522.811
iteration 0700: loss: 1524.176
iteration 0800: loss: 1523.941
iteration 0900: loss: 1529.297
====> Epoch: 041 Train loss: 1525.0919  took : 5.890386343002319
====> Test loss: 1520.8015
iteration 0000: loss: 1525.616
iteration 0100: loss: 1526.386
iteration 0200: loss: 1524.871
iteration 0300: loss: 1526.265
iteration 0400: loss: 1524.866
iteration 0500: loss: 1527.648
iteration 0600: loss: 1525.294
iteration 0700: loss: 1524.492
iteration 0800: loss: 1522.484
iteration 0900: loss: 1528.242
====> Epoch: 042 Train loss: 1525.0211  took : 5.889719009399414
====> Test loss: 1520.7037
iteration 0000: loss: 1526.636
iteration 0100: loss: 1526.583
iteration 0200: loss: 1523.035
iteration 0300: loss: 1527.389
iteration 0400: loss: 1526.013
iteration 0500: loss: 1526.865
iteration 0600: loss: 1525.228
iteration 0700: loss: 1526.067
iteration 0800: loss: 1524.561
iteration 0900: loss: 1522.704
====> Epoch: 043 Train loss: 1525.0145  took : 5.919315814971924
====> Test loss: 1520.7497
iteration 0000: loss: 1526.535
iteration 0100: loss: 1524.825
iteration 0200: loss: 1523.703
iteration 0300: loss: 1525.669
iteration 0400: loss: 1525.539
iteration 0500: loss: 1524.266
iteration 0600: loss: 1525.194
iteration 0700: loss: 1524.863
iteration 0800: loss: 1524.614
iteration 0900: loss: 1523.822
====> Epoch: 044 Train loss: 1524.9540  took : 5.976591110229492
====> Test loss: 1520.6871
iteration 0000: loss: 1525.770
iteration 0100: loss: 1521.170
iteration 0200: loss: 1525.047
iteration 0300: loss: 1521.966
iteration 0400: loss: 1524.898
iteration 0500: loss: 1525.814
iteration 0600: loss: 1526.703
iteration 0700: loss: 1525.859
iteration 0800: loss: 1523.508
iteration 0900: loss: 1524.814
====> Epoch: 045 Train loss: 1524.9440  took : 5.919639348983765
====> Test loss: 1520.8186
iteration 0000: loss: 1524.983
iteration 0100: loss: 1523.750
iteration 0200: loss: 1525.028
iteration 0300: loss: 1524.006
iteration 0400: loss: 1524.109
iteration 0500: loss: 1525.315
iteration 0600: loss: 1525.404
iteration 0700: loss: 1525.099
iteration 0800: loss: 1522.431
iteration 0900: loss: 1522.396
====> Epoch: 046 Train loss: 1524.9201  took : 5.903467416763306
====> Test loss: 1520.8442
iteration 0000: loss: 1522.731
iteration 0100: loss: 1525.137
iteration 0200: loss: 1525.944
iteration 0300: loss: 1523.736
iteration 0400: loss: 1524.345
iteration 0500: loss: 1526.474
iteration 0600: loss: 1524.181
iteration 0700: loss: 1530.209
iteration 0800: loss: 1523.562
iteration 0900: loss: 1525.647
====> Epoch: 047 Train loss: 1524.8933  took : 5.882341623306274
====> Test loss: 1520.6385
iteration 0000: loss: 1526.022
iteration 0100: loss: 1523.132
iteration 0200: loss: 1524.076
iteration 0300: loss: 1527.946
iteration 0400: loss: 1527.095
iteration 0500: loss: 1523.677
iteration 0600: loss: 1529.788
iteration 0700: loss: 1525.842
iteration 0800: loss: 1525.152
iteration 0900: loss: 1528.335
====> Epoch: 048 Train loss: 1524.8785  took : 5.8812432289123535
====> Test loss: 1520.6858
iteration 0000: loss: 1524.026
iteration 0100: loss: 1523.756
iteration 0200: loss: 1527.856
iteration 0300: loss: 1523.428
iteration 0400: loss: 1523.083
iteration 0500: loss: 1526.936
iteration 0600: loss: 1525.039
iteration 0700: loss: 1523.738
iteration 0800: loss: 1525.003
iteration 0900: loss: 1523.598
====> Epoch: 049 Train loss: 1524.8499  took : 5.8770575523376465
====> Test loss: 1520.5308
iteration 0000: loss: 1523.280
iteration 0100: loss: 1528.223
iteration 0200: loss: 1525.031
iteration 0300: loss: 1524.543
iteration 0400: loss: 1523.755
iteration 0500: loss: 1525.321
iteration 0600: loss: 1523.320
iteration 0700: loss: 1525.712
iteration 0800: loss: 1524.869
iteration 0900: loss: 1526.076
====> Epoch: 050 Train loss: 1524.8132  took : 5.901075124740601
====> Test loss: 1520.6170
====> [MM-VAE] Time: 450.970s or 00:07:30
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2668.104
iteration 0100: loss: 2114.261
iteration 0200: loss: 2098.857
iteration 0300: loss: 2100.762
iteration 0400: loss: 2063.169
iteration 0500: loss: 2044.172
iteration 0600: loss: 2021.914
iteration 0700: loss: 2012.579
iteration 0800: loss: 2001.736
iteration 0900: loss: 1986.596
====> Epoch: 001 Train loss: 2063.5592  took : 8.74761152267456
====> Test loss: 1968.0332
iteration 0000: loss: 1977.259
iteration 0100: loss: 1967.929
iteration 0200: loss: 1964.073
iteration 0300: loss: 1964.280
iteration 0400: loss: 1965.235
iteration 0500: loss: 1962.164
iteration 0600: loss: 1962.220
iteration 0700: loss: 1962.680
iteration 0800: loss: 1961.295
iteration 0900: loss: 1959.631
====> Epoch: 002 Train loss: 1963.6869  took : 8.744278192520142
====> Test loss: 1955.2525
iteration 0000: loss: 1961.601
iteration 0100: loss: 1960.543
iteration 0200: loss: 1958.889
iteration 0300: loss: 1959.935
iteration 0400: loss: 1959.748
iteration 0500: loss: 1959.866
iteration 0600: loss: 1959.706
iteration 0700: loss: 1961.084
iteration 0800: loss: 1959.428
iteration 0900: loss: 1960.969
====> Epoch: 003 Train loss: 1959.9340  took : 8.727163553237915
====> Test loss: 1954.0368
iteration 0000: loss: 1959.765
iteration 0100: loss: 1959.081
iteration 0200: loss: 1959.161
iteration 0300: loss: 1957.871
iteration 0400: loss: 1959.567
iteration 0500: loss: 1959.405
iteration 0600: loss: 1959.206
iteration 0700: loss: 1958.177
iteration 0800: loss: 1958.657
iteration 0900: loss: 1959.007
====> Epoch: 004 Train loss: 1959.1394  took : 8.95028305053711
====> Test loss: 1953.9541
iteration 0000: loss: 1958.355
iteration 0100: loss: 1958.743
iteration 0200: loss: 1958.666
iteration 0300: loss: 1959.293
iteration 0400: loss: 1959.338
iteration 0500: loss: 1958.243
iteration 0600: loss: 1958.248
iteration 0700: loss: 1958.208
iteration 0800: loss: 1959.845
iteration 0900: loss: 1957.429
====> Epoch: 005 Train loss: 1958.6775  took : 8.678475856781006
====> Test loss: 1953.2613
iteration 0000: loss: 1957.455
iteration 0100: loss: 1957.739
iteration 0200: loss: 1959.166
iteration 0300: loss: 1959.202
iteration 0400: loss: 1957.485
iteration 0500: loss: 1958.707
iteration 0600: loss: 1957.490
iteration 0700: loss: 1958.056
iteration 0800: loss: 1960.001
iteration 0900: loss: 1959.794
====> Epoch: 006 Train loss: 1958.4139  took : 8.710580825805664
====> Test loss: 1954.1391
iteration 0000: loss: 1958.708
iteration 0100: loss: 1958.467
iteration 0200: loss: 1958.567
iteration 0300: loss: 1958.045
iteration 0400: loss: 1958.717
iteration 0500: loss: 1958.117
iteration 0600: loss: 1959.315
iteration 0700: loss: 1957.499
iteration 0800: loss: 1958.526
iteration 0900: loss: 1957.313
====> Epoch: 007 Train loss: 1958.1949  took : 8.726173877716064
====> Test loss: 1952.7793
iteration 0000: loss: 1958.084
iteration 0100: loss: 1957.565
iteration 0200: loss: 1958.015
iteration 0300: loss: 1957.926
iteration 0400: loss: 1960.347
iteration 0500: loss: 1957.538
iteration 0600: loss: 1957.925
iteration 0700: loss: 1956.851
iteration 0800: loss: 1957.441
iteration 0900: loss: 1958.218
====> Epoch: 008 Train loss: 1958.0700  took : 8.589396476745605
====> Test loss: 1952.5425
iteration 0000: loss: 1956.650
iteration 0100: loss: 1958.688
iteration 0200: loss: 1957.834
iteration 0300: loss: 1958.771
iteration 0400: loss: 1957.031
iteration 0500: loss: 1958.998
iteration 0600: loss: 1956.077
iteration 0700: loss: 1957.512
iteration 0800: loss: 1957.299
iteration 0900: loss: 1958.172
====> Epoch: 009 Train loss: 1957.9502  took : 8.651618242263794
====> Test loss: 1952.3647
iteration 0000: loss: 1957.727
iteration 0100: loss: 1957.632
iteration 0200: loss: 1956.648
iteration 0300: loss: 1958.804
iteration 0400: loss: 1957.736
iteration 0500: loss: 1958.822
iteration 0600: loss: 1957.809
iteration 0700: loss: 1957.510
iteration 0800: loss: 1957.258
iteration 0900: loss: 1957.166
====> Epoch: 010 Train loss: 1957.9008  took : 8.702828884124756
====> Test loss: 1952.5674
iteration 0000: loss: 1957.063
iteration 0100: loss: 1958.749
iteration 0200: loss: 1957.738
iteration 0300: loss: 1957.521
iteration 0400: loss: 1958.094
iteration 0500: loss: 1957.445
iteration 0600: loss: 1957.771
iteration 0700: loss: 1957.938
iteration 0800: loss: 1958.573
iteration 0900: loss: 1956.301
====> Epoch: 011 Train loss: 1957.8145  took : 8.847001075744629
====> Test loss: 1952.4553
iteration 0000: loss: 1956.966
iteration 0100: loss: 1958.860
iteration 0200: loss: 1958.351
iteration 0300: loss: 1956.823
iteration 0400: loss: 1956.755
iteration 0500: loss: 1959.013
iteration 0600: loss: 1956.312
iteration 0700: loss: 1958.389
iteration 0800: loss: 1957.426
iteration 0900: loss: 1956.930
====> Epoch: 012 Train loss: 1957.7490  took : 8.642569303512573
====> Test loss: 1952.2467
iteration 0000: loss: 1956.671
iteration 0100: loss: 1958.511
iteration 0200: loss: 1958.329
iteration 0300: loss: 1956.438
iteration 0400: loss: 1959.101
iteration 0500: loss: 1957.768
iteration 0600: loss: 1956.763
iteration 0700: loss: 1956.735
iteration 0800: loss: 1958.053
iteration 0900: loss: 1956.062
====> Epoch: 013 Train loss: 1957.5534  took : 8.813181638717651
====> Test loss: 1952.1079
iteration 0000: loss: 1957.268
iteration 0100: loss: 1958.571
iteration 0200: loss: 1955.666
iteration 0300: loss: 1956.778
iteration 0400: loss: 1957.889
iteration 0500: loss: 1957.724
iteration 0600: loss: 1957.554
iteration 0700: loss: 1956.771
iteration 0800: loss: 1957.730
iteration 0900: loss: 1958.230
====> Epoch: 014 Train loss: 1957.2864  took : 8.613283395767212
====> Test loss: 1951.8929
iteration 0000: loss: 1956.873
iteration 0100: loss: 1957.734
iteration 0200: loss: 1957.024
iteration 0300: loss: 1956.415
iteration 0400: loss: 1957.986
iteration 0500: loss: 1957.182
iteration 0600: loss: 1958.614
iteration 0700: loss: 1957.539
iteration 0800: loss: 1956.860
iteration 0900: loss: 1956.744
====> Epoch: 015 Train loss: 1957.3231  took : 8.643579483032227
====> Test loss: 1952.0271
iteration 0000: loss: 1956.558
iteration 0100: loss: 1959.411
iteration 0200: loss: 1959.490
iteration 0300: loss: 1958.915
iteration 0400: loss: 1957.179
iteration 0500: loss: 1957.591
iteration 0600: loss: 1958.206
iteration 0700: loss: 1957.378
iteration 0800: loss: 1956.485
iteration 0900: loss: 1957.504
====> Epoch: 016 Train loss: 1957.2133  took : 8.746452569961548
====> Test loss: 1952.1566
iteration 0000: loss: 1957.909
iteration 0100: loss: 1959.720
iteration 0200: loss: 1957.519
iteration 0300: loss: 1955.982
iteration 0400: loss: 1957.570
iteration 0500: loss: 1958.439
iteration 0600: loss: 1957.356
iteration 0700: loss: 1958.284
iteration 0800: loss: 1956.810
iteration 0900: loss: 1956.708
====> Epoch: 017 Train loss: 1957.2074  took : 8.638267517089844
====> Test loss: 1951.8987
iteration 0000: loss: 1958.550
iteration 0100: loss: 1956.214
iteration 0200: loss: 1957.706
iteration 0300: loss: 1956.782
iteration 0400: loss: 1956.693
iteration 0500: loss: 1959.532
iteration 0600: loss: 1957.228
iteration 0700: loss: 1957.381
iteration 0800: loss: 1956.015
iteration 0900: loss: 1958.080
====> Epoch: 018 Train loss: 1957.1393  took : 8.627662181854248
====> Test loss: 1951.9972
iteration 0000: loss: 1956.182
iteration 0100: loss: 1958.207
iteration 0200: loss: 1956.285
iteration 0300: loss: 1956.841
iteration 0400: loss: 1956.807
iteration 0500: loss: 1957.381
iteration 0600: loss: 1956.294
iteration 0700: loss: 1957.156
iteration 0800: loss: 1956.830
iteration 0900: loss: 1958.312
====> Epoch: 019 Train loss: 1957.1063  took : 8.834728956222534
====> Test loss: 1952.4347
iteration 0000: loss: 1957.674
iteration 0100: loss: 1957.317
iteration 0200: loss: 1957.276
iteration 0300: loss: 1957.620
iteration 0400: loss: 1957.010
iteration 0500: loss: 1960.971
iteration 0600: loss: 1956.567
iteration 0700: loss: 1956.332
iteration 0800: loss: 1956.174
iteration 0900: loss: 1958.015
====> Epoch: 020 Train loss: 1957.1161  took : 8.7479407787323
====> Test loss: 1951.7988
iteration 0000: loss: 1955.909
iteration 0100: loss: 1956.856
iteration 0200: loss: 1958.850
iteration 0300: loss: 1957.083
iteration 0400: loss: 1956.284
iteration 0500: loss: 1959.319
iteration 0600: loss: 1957.198
iteration 0700: loss: 1956.439
iteration 0800: loss: 1956.821
iteration 0900: loss: 1956.458
====> Epoch: 021 Train loss: 1957.0209  took : 8.61491870880127
====> Test loss: 1951.6226
iteration 0000: loss: 1956.521
iteration 0100: loss: 1957.031
iteration 0200: loss: 1956.386
iteration 0300: loss: 1956.913
iteration 0400: loss: 1957.224
iteration 0500: loss: 1957.604
iteration 0600: loss: 1956.257
iteration 0700: loss: 1958.341
iteration 0800: loss: 1955.778
iteration 0900: loss: 1955.701
====> Epoch: 022 Train loss: 1956.9792  took : 8.630173921585083
====> Test loss: 1951.9982
iteration 0000: loss: 1956.948
iteration 0100: loss: 1958.552
iteration 0200: loss: 1957.053
iteration 0300: loss: 1956.701
iteration 0400: loss: 1956.748
iteration 0500: loss: 1957.109
iteration 0600: loss: 1957.277
iteration 0700: loss: 1955.689
iteration 0800: loss: 1957.751
iteration 0900: loss: 1956.495
====> Epoch: 023 Train loss: 1956.9176  took : 8.67788314819336
====> Test loss: 1951.5841
iteration 0000: loss: 1957.590
iteration 0100: loss: 1957.512
iteration 0200: loss: 1956.575
iteration 0300: loss: 1956.781
iteration 0400: loss: 1956.523
iteration 0500: loss: 1956.845
iteration 0600: loss: 1956.508
iteration 0700: loss: 1957.792
iteration 0800: loss: 1957.262
iteration 0900: loss: 1956.994
====> Epoch: 024 Train loss: 1956.9579  took : 8.778772592544556
====> Test loss: 1951.6299
iteration 0000: loss: 1958.275
iteration 0100: loss: 1956.156
iteration 0200: loss: 1957.608
iteration 0300: loss: 1955.812
iteration 0400: loss: 1956.624
iteration 0500: loss: 1957.638
iteration 0600: loss: 1957.244
iteration 0700: loss: 1957.984
iteration 0800: loss: 1956.953
iteration 0900: loss: 1956.456
====> Epoch: 025 Train loss: 1956.9491  took : 8.605233907699585
====> Test loss: 1951.6173
iteration 0000: loss: 1957.978
iteration 0100: loss: 1955.813
iteration 0200: loss: 1957.964
iteration 0300: loss: 1956.484
iteration 0400: loss: 1956.562
iteration 0500: loss: 1957.464
iteration 0600: loss: 1956.892
iteration 0700: loss: 1957.204
iteration 0800: loss: 1956.700
iteration 0900: loss: 1956.774
====> Epoch: 026 Train loss: 1956.8511  took : 8.659035682678223
====> Test loss: 1951.6340
iteration 0000: loss: 1956.995
iteration 0100: loss: 1958.110
iteration 0200: loss: 1958.579
iteration 0300: loss: 1957.151
iteration 0400: loss: 1956.729
iteration 0500: loss: 1957.107
iteration 0600: loss: 1958.778
iteration 0700: loss: 1957.078
iteration 0800: loss: 1956.011
iteration 0900: loss: 1956.440
====> Epoch: 027 Train loss: 1956.8465  took : 8.65485692024231
====> Test loss: 1951.4343
iteration 0000: loss: 1956.049
iteration 0100: loss: 1955.280
iteration 0200: loss: 1956.485
iteration 0300: loss: 1955.099
iteration 0400: loss: 1956.792
iteration 0500: loss: 1957.318
iteration 0600: loss: 1958.335
iteration 0700: loss: 1956.408
iteration 0800: loss: 1956.374
iteration 0900: loss: 1956.765
====> Epoch: 028 Train loss: 1956.7881  took : 8.630775213241577
====> Test loss: 1951.8031
iteration 0000: loss: 1957.610
iteration 0100: loss: 1957.726
iteration 0200: loss: 1957.179
iteration 0300: loss: 1955.928
iteration 0400: loss: 1957.735
iteration 0500: loss: 1956.869
iteration 0600: loss: 1956.413
iteration 0700: loss: 1956.527
iteration 0800: loss: 1956.273
iteration 0900: loss: 1955.736
====> Epoch: 029 Train loss: 1956.8264  took : 8.607003211975098
====> Test loss: 1951.8093
iteration 0000: loss: 1955.474
iteration 0100: loss: 1956.147
iteration 0200: loss: 1956.575
iteration 0300: loss: 1956.036
iteration 0400: loss: 1956.165
iteration 0500: loss: 1956.394
iteration 0600: loss: 1957.073
iteration 0700: loss: 1956.297
iteration 0800: loss: 1957.168
iteration 0900: loss: 1956.841
====> Epoch: 030 Train loss: 1956.7583  took : 8.725205421447754
====> Test loss: 1951.8067
iteration 0000: loss: 1956.359
iteration 0100: loss: 1956.870
iteration 0200: loss: 1956.921
iteration 0300: loss: 1956.255
iteration 0400: loss: 1956.349
iteration 0500: loss: 1956.073
iteration 0600: loss: 1955.775
iteration 0700: loss: 1956.114
iteration 0800: loss: 1956.542
iteration 0900: loss: 1956.621
====> Epoch: 031 Train loss: 1956.7710  took : 8.656197309494019
====> Test loss: 1951.5347
iteration 0000: loss: 1956.059
iteration 0100: loss: 1956.436
iteration 0200: loss: 1956.463
iteration 0300: loss: 1956.775
iteration 0400: loss: 1955.628
iteration 0500: loss: 1956.125
iteration 0600: loss: 1955.730
iteration 0700: loss: 1957.091
iteration 0800: loss: 1955.655
iteration 0900: loss: 1957.716
====> Epoch: 032 Train loss: 1956.7289  took : 8.62867546081543
====> Test loss: 1951.5885
iteration 0000: loss: 1956.490
iteration 0100: loss: 1956.419
iteration 0200: loss: 1956.872
iteration 0300: loss: 1957.102
iteration 0400: loss: 1957.199
iteration 0500: loss: 1954.601
iteration 0600: loss: 1955.855
iteration 0700: loss: 1956.977
iteration 0800: loss: 1955.766
iteration 0900: loss: 1957.213
====> Epoch: 033 Train loss: 1956.7415  took : 8.62794542312622
====> Test loss: 1951.6461
iteration 0000: loss: 1957.631
iteration 0100: loss: 1956.168
iteration 0200: loss: 1956.395
iteration 0300: loss: 1957.852
iteration 0400: loss: 1955.935
iteration 0500: loss: 1957.438
iteration 0600: loss: 1957.111
iteration 0700: loss: 1957.022
iteration 0800: loss: 1956.102
iteration 0900: loss: 1956.802
====> Epoch: 034 Train loss: 1956.6905  took : 8.90073299407959
====> Test loss: 1951.7190
iteration 0000: loss: 1956.523
iteration 0100: loss: 1956.623
iteration 0200: loss: 1957.044
iteration 0300: loss: 1955.408
iteration 0400: loss: 1955.693
iteration 0500: loss: 1956.465
iteration 0600: loss: 1956.840
iteration 0700: loss: 1956.361
iteration 0800: loss: 1956.493
iteration 0900: loss: 1955.955
====> Epoch: 035 Train loss: 1956.7264  took : 8.632581233978271
====> Test loss: 1951.7710
iteration 0000: loss: 1956.774
iteration 0100: loss: 1958.262
iteration 0200: loss: 1958.199
iteration 0300: loss: 1956.881
iteration 0400: loss: 1955.581
iteration 0500: loss: 1957.329
iteration 0600: loss: 1957.455
iteration 0700: loss: 1957.257
iteration 0800: loss: 1956.555
iteration 0900: loss: 1957.217
====> Epoch: 036 Train loss: 1956.6799  took : 8.629615306854248
====> Test loss: 1952.0136
iteration 0000: loss: 1956.762
iteration 0100: loss: 1957.536
iteration 0200: loss: 1956.513
iteration 0300: loss: 1957.110
iteration 0400: loss: 1956.786
iteration 0500: loss: 1956.351
iteration 0600: loss: 1956.672
iteration 0700: loss: 1955.074
iteration 0800: loss: 1956.167
iteration 0900: loss: 1956.892
====> Epoch: 037 Train loss: 1956.6748  took : 8.826438188552856
====> Test loss: 1951.5694
iteration 0000: loss: 1957.164
iteration 0100: loss: 1956.729
iteration 0200: loss: 1956.291
iteration 0300: loss: 1956.411
iteration 0400: loss: 1955.148
iteration 0500: loss: 1957.039
iteration 0600: loss: 1956.638
iteration 0700: loss: 1956.782
iteration 0800: loss: 1956.901
iteration 0900: loss: 1956.932
====> Epoch: 038 Train loss: 1956.6156  took : 8.70728063583374
====> Test loss: 1951.7286
iteration 0000: loss: 1956.662
iteration 0100: loss: 1955.563
iteration 0200: loss: 1956.561
iteration 0300: loss: 1956.884
iteration 0400: loss: 1957.052
iteration 0500: loss: 1956.406
iteration 0600: loss: 1956.539
iteration 0700: loss: 1955.817
iteration 0800: loss: 1955.964
iteration 0900: loss: 1956.570
====> Epoch: 039 Train loss: 1956.5992  took : 8.62971544265747
====> Test loss: 1951.6333
iteration 0000: loss: 1955.149
iteration 0100: loss: 1956.823
iteration 0200: loss: 1956.478
iteration 0300: loss: 1956.230
iteration 0400: loss: 1957.850
iteration 0500: loss: 1956.003
iteration 0600: loss: 1957.197
iteration 0700: loss: 1954.833
iteration 0800: loss: 1955.714
iteration 0900: loss: 1957.397
====> Epoch: 040 Train loss: 1956.6211  took : 8.621054649353027
====> Test loss: 1951.8993
iteration 0000: loss: 1957.135
iteration 0100: loss: 1957.277
iteration 0200: loss: 1957.893
iteration 0300: loss: 1957.162
iteration 0400: loss: 1955.935
iteration 0500: loss: 1956.496
iteration 0600: loss: 1956.263
iteration 0700: loss: 1956.495
iteration 0800: loss: 1957.529
iteration 0900: loss: 1955.704
====> Epoch: 041 Train loss: 1956.6213  took : 8.609428405761719
====> Test loss: 1951.7515
iteration 0000: loss: 1956.617
iteration 0100: loss: 1955.551
iteration 0200: loss: 1955.631
iteration 0300: loss: 1955.630
iteration 0400: loss: 1955.051
iteration 0500: loss: 1956.845
iteration 0600: loss: 1955.897
iteration 0700: loss: 1956.532
iteration 0800: loss: 1956.149
iteration 0900: loss: 1956.632
====> Epoch: 042 Train loss: 1956.5719  took : 8.862192869186401
====> Test loss: 1951.6616
iteration 0000: loss: 1955.806
iteration 0100: loss: 1960.729
iteration 0200: loss: 1955.584
iteration 0300: loss: 1957.550
iteration 0400: loss: 1956.189
iteration 0500: loss: 1956.290
iteration 0600: loss: 1956.261
iteration 0700: loss: 1955.953
iteration 0800: loss: 1955.688
iteration 0900: loss: 1956.806
====> Epoch: 043 Train loss: 1956.5507  took : 8.652934789657593
====> Test loss: 1951.5419
iteration 0000: loss: 1956.931
iteration 0100: loss: 1957.140
iteration 0200: loss: 1955.868
iteration 0300: loss: 1956.197
iteration 0400: loss: 1955.612
iteration 0500: loss: 1956.774
iteration 0600: loss: 1956.883
iteration 0700: loss: 1956.002
iteration 0800: loss: 1956.404
iteration 0900: loss: 1957.730
====> Epoch: 044 Train loss: 1956.5215  took : 8.66815710067749
====> Test loss: 1951.3848
iteration 0000: loss: 1956.858
iteration 0100: loss: 1956.892
iteration 0200: loss: 1957.011
iteration 0300: loss: 1957.596
iteration 0400: loss: 1956.069
iteration 0500: loss: 1955.589
iteration 0600: loss: 1957.855
iteration 0700: loss: 1956.191
iteration 0800: loss: 1956.362
iteration 0900: loss: 1957.369
====> Epoch: 045 Train loss: 1956.5649  took : 8.73507285118103
====> Test loss: 1951.4780
iteration 0000: loss: 1956.206
iteration 0100: loss: 1956.182
iteration 0200: loss: 1955.650
iteration 0300: loss: 1956.553
iteration 0400: loss: 1956.167
iteration 0500: loss: 1956.520
iteration 0600: loss: 1955.415
iteration 0700: loss: 1957.540
iteration 0800: loss: 1956.316
iteration 0900: loss: 1956.747
====> Epoch: 046 Train loss: 1956.5357  took : 8.669832944869995
====> Test loss: 1951.7885
iteration 0000: loss: 1960.502
iteration 0100: loss: 1955.432
iteration 0200: loss: 1955.993
iteration 0300: loss: 1956.619
iteration 0400: loss: 1956.816
iteration 0500: loss: 1955.650
iteration 0600: loss: 1956.392
iteration 0700: loss: 1955.551
iteration 0800: loss: 1957.116
iteration 0900: loss: 1955.310
====> Epoch: 047 Train loss: 1956.5376  took : 8.718158721923828
====> Test loss: 1951.7787
iteration 0000: loss: 1955.617
iteration 0100: loss: 1958.221
iteration 0200: loss: 1957.066
iteration 0300: loss: 1955.835
iteration 0400: loss: 1956.199
iteration 0500: loss: 1956.576
iteration 0600: loss: 1956.938
iteration 0700: loss: 1956.581
iteration 0800: loss: 1957.359
iteration 0900: loss: 1955.895
====> Epoch: 048 Train loss: 1956.5139  took : 8.637339353561401
====> Test loss: 1951.3748
iteration 0000: loss: 1956.685
iteration 0100: loss: 1956.831
iteration 0200: loss: 1955.361
iteration 0300: loss: 1957.443
iteration 0400: loss: 1957.080
iteration 0500: loss: 1955.591
iteration 0600: loss: 1955.799
iteration 0700: loss: 1955.284
iteration 0800: loss: 1956.142
iteration 0900: loss: 1955.775
====> Epoch: 049 Train loss: 1956.5169  took : 8.64014220237732
====> Test loss: 1951.4928
iteration 0000: loss: 1955.677
iteration 0100: loss: 1956.424
iteration 0200: loss: 1955.516
iteration 0300: loss: 1956.202
iteration 0400: loss: 1956.679
iteration 0500: loss: 1958.885
iteration 0600: loss: 1956.900
iteration 0700: loss: 1955.782
iteration 0800: loss: 1956.464
iteration 0900: loss: 1957.632
====> Epoch: 050 Train loss: 1956.4920  took : 8.747401714324951
====> Test loss: 1951.4510
====> [MM-VAE] Time: 569.209s or 00:09:29
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5262.674
iteration 0100: loss: 4133.722
iteration 0200: loss: 4067.652
iteration 0300: loss: 4029.404
iteration 0400: loss: 4023.059
iteration 0500: loss: 4003.500
iteration 0600: loss: 3997.475
iteration 0700: loss: 3988.839
iteration 0800: loss: 3976.524
iteration 0900: loss: 3972.270
iteration 1000: loss: 3966.291
iteration 1100: loss: 3965.337
iteration 1200: loss: 3971.288
iteration 1300: loss: 3964.388
iteration 1400: loss: 3964.491
iteration 1500: loss: 3961.277
iteration 1600: loss: 3963.192
iteration 1700: loss: 3957.884
iteration 1800: loss: 3960.116
====> Epoch: 001 Train loss: 4000.6467  took : 139.5129361152649
====> Test loss: 3962.6915
iteration 0000: loss: 3963.066
iteration 0100: loss: 3952.613
iteration 0200: loss: 3961.803
iteration 0300: loss: 3968.175
iteration 0400: loss: 3967.352
iteration 0500: loss: 3962.255
iteration 0600: loss: 3958.241
iteration 0700: loss: 3956.686
iteration 0800: loss: 3950.947
iteration 0900: loss: 3957.542
iteration 1000: loss: 3955.961
iteration 1100: loss: 3955.283
iteration 1200: loss: 3953.731
iteration 1300: loss: 3955.775
iteration 1400: loss: 3959.412
iteration 1500: loss: 3959.312
iteration 1600: loss: 3958.359
iteration 1700: loss: 3954.733
iteration 1800: loss: 3948.875
====> Epoch: 002 Train loss: 3957.8070  took : 140.7960820198059
====> Test loss: 3957.9902
iteration 0000: loss: 3956.790
iteration 0100: loss: 3956.036
iteration 0200: loss: 3959.340
iteration 0300: loss: 3955.375
iteration 0400: loss: 3953.477
iteration 0500: loss: 3956.755
iteration 0600: loss: 3953.255
iteration 0700: loss: 3958.459
iteration 0800: loss: 3951.941
iteration 0900: loss: 3959.613
iteration 1000: loss: 3951.534
iteration 1100: loss: 3949.905
iteration 1200: loss: 3950.845
iteration 1300: loss: 3955.455
iteration 1400: loss: 3952.219
iteration 1500: loss: 3955.298
iteration 1600: loss: 3955.529
iteration 1700: loss: 3955.944
iteration 1800: loss: 3961.427
====> Epoch: 003 Train loss: 3954.8721  took : 140.65868639945984
====> Test loss: 3956.0287
iteration 0000: loss: 3958.258
iteration 0100: loss: 3951.706
iteration 0200: loss: 3954.984
iteration 0300: loss: 3951.374
iteration 0400: loss: 3950.359
iteration 0500: loss: 3952.101
iteration 0600: loss: 3957.142
iteration 0700: loss: 3961.208
iteration 0800: loss: 3955.599
iteration 0900: loss: 3946.660
iteration 1000: loss: 3959.648
iteration 1100: loss: 3946.002
iteration 1200: loss: 3950.936
iteration 1300: loss: 3947.576
iteration 1400: loss: 3951.238
iteration 1500: loss: 3947.591
iteration 1600: loss: 3957.332
iteration 1700: loss: 3955.075
iteration 1800: loss: 3957.705
====> Epoch: 004 Train loss: 3953.1622  took : 140.04007196426392
====> Test loss: 3954.9194
iteration 0000: loss: 3955.741
iteration 0100: loss: 3949.803
iteration 0200: loss: 3956.130
iteration 0300: loss: 3956.189
iteration 0400: loss: 3946.772
iteration 0500: loss: 3948.139
iteration 0600: loss: 3957.549
iteration 0700: loss: 3952.205
iteration 0800: loss: 3959.255
iteration 0900: loss: 3946.505
iteration 1000: loss: 3949.606
iteration 1100: loss: 3948.410
iteration 1200: loss: 3949.498
iteration 1300: loss: 3946.527
iteration 1400: loss: 3946.687
iteration 1500: loss: 3956.585
iteration 1600: loss: 3944.515
iteration 1700: loss: 3950.375
iteration 1800: loss: 3954.783
====> Epoch: 005 Train loss: 3952.0303  took : 140.56648588180542
====> Test loss: 3954.0364
iteration 0000: loss: 3951.746
iteration 0100: loss: 3954.302
iteration 0200: loss: 3955.849
iteration 0300: loss: 3949.403
iteration 0400: loss: 3949.062
iteration 0500: loss: 3950.613
iteration 0600: loss: 3952.942
iteration 0700: loss: 3959.509
iteration 0800: loss: 3953.548
iteration 0900: loss: 3955.708
iteration 1000: loss: 3960.539
iteration 1100: loss: 3948.572
iteration 1200: loss: 3947.025
iteration 1300: loss: 3951.060
iteration 1400: loss: 3949.575
iteration 1500: loss: 3944.047
iteration 1600: loss: 3957.829
iteration 1700: loss: 3945.347
iteration 1800: loss: 3950.309
====> Epoch: 006 Train loss: 3951.1773  took : 140.15813541412354
====> Test loss: 3953.6437
iteration 0000: loss: 3957.468
iteration 0100: loss: 3950.177
iteration 0200: loss: 3943.072
iteration 0300: loss: 3946.833
iteration 0400: loss: 3948.529
iteration 0500: loss: 3947.961
iteration 0600: loss: 3955.834
iteration 0700: loss: 3947.202
iteration 0800: loss: 3953.654
iteration 0900: loss: 3948.178
iteration 1000: loss: 3952.872
iteration 1100: loss: 3956.071
iteration 1200: loss: 3950.833
iteration 1300: loss: 3949.556
iteration 1400: loss: 3949.409
iteration 1500: loss: 3949.129
iteration 1600: loss: 3945.816
iteration 1700: loss: 3949.900
iteration 1800: loss: 3946.911
====> Epoch: 007 Train loss: 3950.5595  took : 140.43154621124268
====> Test loss: 3952.8027
iteration 0000: loss: 3944.495
iteration 0100: loss: 3944.833
iteration 0200: loss: 3954.222
iteration 0300: loss: 3957.367
iteration 0400: loss: 3948.560
iteration 0500: loss: 3945.987
iteration 0600: loss: 3945.126
iteration 0700: loss: 3950.879
iteration 0800: loss: 3950.926
iteration 0900: loss: 3951.239
iteration 1000: loss: 3944.382
iteration 1100: loss: 3940.258
iteration 1200: loss: 3950.889
iteration 1300: loss: 3951.784
iteration 1400: loss: 3946.220
iteration 1500: loss: 3942.909
iteration 1600: loss: 3946.671
iteration 1700: loss: 3955.453
iteration 1800: loss: 3950.355
====> Epoch: 008 Train loss: 3949.9772  took : 140.30092525482178
====> Test loss: 3952.4118
iteration 0000: loss: 3946.639
iteration 0100: loss: 3952.929
iteration 0200: loss: 3952.336
iteration 0300: loss: 3946.940
iteration 0400: loss: 3944.538
iteration 0500: loss: 3946.897
iteration 0600: loss: 3949.141
iteration 0700: loss: 3949.536
iteration 0800: loss: 3945.690
iteration 0900: loss: 3950.848
iteration 1000: loss: 3944.219
iteration 1100: loss: 3947.403
iteration 1200: loss: 3949.618
iteration 1300: loss: 3950.127
iteration 1400: loss: 3951.784
iteration 1500: loss: 3948.985
iteration 1600: loss: 3950.382
iteration 1700: loss: 3950.693
iteration 1800: loss: 3956.189
====> Epoch: 009 Train loss: 3949.5457  took : 139.61538410186768
====> Test loss: 3952.6381
iteration 0000: loss: 3952.153
iteration 0100: loss: 3948.820
iteration 0200: loss: 3941.591
iteration 0300: loss: 3945.926
iteration 0400: loss: 3955.737
iteration 0500: loss: 3947.350
iteration 0600: loss: 3948.639
iteration 0700: loss: 3947.702
iteration 0800: loss: 3953.938
iteration 0900: loss: 3952.887
iteration 1000: loss: 3954.221
iteration 1100: loss: 3945.525
iteration 1200: loss: 3950.062
iteration 1300: loss: 3952.901
iteration 1400: loss: 3945.545
iteration 1500: loss: 3946.941
iteration 1600: loss: 3947.170
iteration 1700: loss: 3951.160
iteration 1800: loss: 3952.699
====> Epoch: 010 Train loss: 3949.1095  took : 139.42854714393616
====> Test loss: 3951.7537
iteration 0000: loss: 3947.665
iteration 0100: loss: 3943.319
iteration 0200: loss: 3946.930
iteration 0300: loss: 3953.561
iteration 0400: loss: 3955.715
iteration 0500: loss: 3949.751
iteration 0600: loss: 3954.263
iteration 0700: loss: 3951.152
iteration 0800: loss: 3948.281
iteration 0900: loss: 3951.798
iteration 1000: loss: 3950.511
iteration 1100: loss: 3944.049
iteration 1200: loss: 3949.230
iteration 1300: loss: 3954.479
iteration 1400: loss: 3943.975
iteration 1500: loss: 3950.534
iteration 1600: loss: 3942.067
iteration 1700: loss: 3949.410
iteration 1800: loss: 3949.312
====> Epoch: 011 Train loss: 3948.6835  took : 140.86986303329468
====> Test loss: 3951.3428
iteration 0000: loss: 3945.926
iteration 0100: loss: 3945.832
iteration 0200: loss: 3944.893
iteration 0300: loss: 3948.569
iteration 0400: loss: 3946.093
iteration 0500: loss: 3945.445
iteration 0600: loss: 3947.203
iteration 0700: loss: 3948.157
iteration 0800: loss: 3949.311
iteration 0900: loss: 3954.837
iteration 1000: loss: 3950.573
iteration 1100: loss: 3946.378
iteration 1200: loss: 3940.237
iteration 1300: loss: 3945.759
iteration 1400: loss: 3948.437
iteration 1500: loss: 3951.698
iteration 1600: loss: 3947.355
iteration 1700: loss: 3947.183
iteration 1800: loss: 3947.533
====> Epoch: 012 Train loss: 3948.2409  took : 140.16247248649597
====> Test loss: 3951.2410
iteration 0000: loss: 3951.929
iteration 0100: loss: 3948.365
iteration 0200: loss: 3948.255
iteration 0300: loss: 3945.454
iteration 0400: loss: 3945.242
iteration 0500: loss: 3949.806
iteration 0600: loss: 3952.028
iteration 0700: loss: 3949.896
iteration 0800: loss: 3947.196
iteration 0900: loss: 3947.344
iteration 1000: loss: 3952.967
iteration 1100: loss: 3943.451
iteration 1200: loss: 3952.599
iteration 1300: loss: 3944.969
iteration 1400: loss: 3946.475
iteration 1500: loss: 3947.536
iteration 1600: loss: 3955.053
iteration 1700: loss: 3950.019
iteration 1800: loss: 3948.071
====> Epoch: 013 Train loss: 3947.8928  took : 140.9242136478424
====> Test loss: 3950.8348
iteration 0000: loss: 3946.993
iteration 0100: loss: 3949.362
iteration 0200: loss: 3949.290
iteration 0300: loss: 3952.726
iteration 0400: loss: 3947.744
iteration 0500: loss: 3947.877
iteration 0600: loss: 3947.237
iteration 0700: loss: 3947.672
iteration 0800: loss: 3946.437
iteration 0900: loss: 3952.908
iteration 1000: loss: 3947.382
iteration 1100: loss: 3945.644
iteration 1200: loss: 3943.723
iteration 1300: loss: 3950.174
iteration 1400: loss: 3945.758
iteration 1500: loss: 3950.397
iteration 1600: loss: 3941.449
iteration 1700: loss: 3945.269
iteration 1800: loss: 3945.481
====> Epoch: 014 Train loss: 3947.5890  took : 139.60297632217407
====> Test loss: 3950.6227
iteration 0000: loss: 3940.122
iteration 0100: loss: 3947.370
iteration 0200: loss: 3951.991
iteration 0300: loss: 3943.859
iteration 0400: loss: 3954.215
iteration 0500: loss: 3947.577
iteration 0600: loss: 3941.938
iteration 0700: loss: 3944.011
iteration 0800: loss: 3951.711
iteration 0900: loss: 3945.983
iteration 1000: loss: 3947.033
iteration 1100: loss: 3945.965
iteration 1200: loss: 3949.300
iteration 1300: loss: 3943.985
iteration 1400: loss: 3945.221
iteration 1500: loss: 3944.188
iteration 1600: loss: 3952.070
iteration 1700: loss: 3948.254
iteration 1800: loss: 3943.108
====> Epoch: 015 Train loss: 3947.4048  took : 140.0510458946228
====> Test loss: 3950.2142
iteration 0000: loss: 3948.913
iteration 0100: loss: 3948.595
iteration 0200: loss: 3943.871
iteration 0300: loss: 3949.060
iteration 0400: loss: 3952.148
iteration 0500: loss: 3947.389
iteration 0600: loss: 3949.134
iteration 0700: loss: 3951.711
iteration 0800: loss: 3951.598
iteration 0900: loss: 3947.031
iteration 1000: loss: 3948.528
iteration 1100: loss: 3943.759
iteration 1200: loss: 3947.053
iteration 1300: loss: 3947.754
iteration 1400: loss: 3945.678
iteration 1500: loss: 3947.711
iteration 1600: loss: 3941.656
iteration 1700: loss: 3947.409
iteration 1800: loss: 3947.628
====> Epoch: 016 Train loss: 3947.1378  took : 140.58642530441284
====> Test loss: 3950.1511
iteration 0000: loss: 3945.197
iteration 0100: loss: 3952.235
iteration 0200: loss: 3947.407
iteration 0300: loss: 3937.011
iteration 0400: loss: 3952.071
iteration 0500: loss: 3942.484
iteration 0600: loss: 3946.227
iteration 0700: loss: 3946.217
iteration 0800: loss: 3942.258
iteration 0900: loss: 3941.345
iteration 1000: loss: 3946.418
iteration 1100: loss: 3948.121
iteration 1200: loss: 3941.456
iteration 1300: loss: 3947.077
iteration 1400: loss: 3945.567
iteration 1500: loss: 3942.807
iteration 1600: loss: 3942.116
iteration 1700: loss: 3948.991
iteration 1800: loss: 3944.078
====> Epoch: 017 Train loss: 3946.9057  took : 140.52212166786194
====> Test loss: 3950.0539
iteration 0000: loss: 3947.369
iteration 0100: loss: 3951.856
iteration 0200: loss: 3956.175
iteration 0300: loss: 3951.787
iteration 0400: loss: 3944.011
iteration 0500: loss: 3945.952
iteration 0600: loss: 3948.656
iteration 0700: loss: 3941.992
iteration 0800: loss: 3944.075
iteration 0900: loss: 3945.817
iteration 1000: loss: 3943.781
iteration 1100: loss: 3941.362
iteration 1200: loss: 3946.735
iteration 1300: loss: 3952.293
iteration 1400: loss: 3947.102
iteration 1500: loss: 3943.074
iteration 1600: loss: 3947.294
iteration 1700: loss: 3946.482
iteration 1800: loss: 3946.417
====> Epoch: 018 Train loss: 3946.6916  took : 140.7275140285492
====> Test loss: 3950.0731
iteration 0000: loss: 3940.051
iteration 0100: loss: 3957.053
iteration 0200: loss: 3953.438
iteration 0300: loss: 3946.564
iteration 0400: loss: 3944.860
iteration 0500: loss: 3946.105
iteration 0600: loss: 3947.443
iteration 0700: loss: 3948.612
iteration 0800: loss: 3946.747
iteration 0900: loss: 3952.374
iteration 1000: loss: 3947.939
iteration 1100: loss: 3945.807
iteration 1200: loss: 3941.070
iteration 1300: loss: 3947.203
iteration 1400: loss: 3948.165
iteration 1500: loss: 3949.042
iteration 1600: loss: 3951.689
iteration 1700: loss: 3946.727
iteration 1800: loss: 3948.416
====> Epoch: 019 Train loss: 3946.5488  took : 140.92828965187073
====> Test loss: 3949.8753
iteration 0000: loss: 3940.402
iteration 0100: loss: 3940.486
iteration 0200: loss: 3942.613
iteration 0300: loss: 3944.465
iteration 0400: loss: 3944.962
iteration 0500: loss: 3941.629
iteration 0600: loss: 3950.689
iteration 0700: loss: 3942.617
iteration 0800: loss: 3945.526
iteration 0900: loss: 3944.495
iteration 1000: loss: 3940.913
iteration 1100: loss: 3948.505
iteration 1200: loss: 3943.293
iteration 1300: loss: 3946.887
iteration 1400: loss: 3943.565
iteration 1500: loss: 3945.980
iteration 1600: loss: 3946.770
iteration 1700: loss: 3944.352
iteration 1800: loss: 3942.390
====> Epoch: 020 Train loss: 3946.3792  took : 140.77975130081177
====> Test loss: 3949.2865
iteration 0000: loss: 3944.696
iteration 0100: loss: 3946.573
iteration 0200: loss: 3947.706
iteration 0300: loss: 3945.188
iteration 0400: loss: 3946.861
iteration 0500: loss: 3947.080
iteration 0600: loss: 3949.217
iteration 0700: loss: 3943.591
iteration 0800: loss: 3944.548
iteration 0900: loss: 3953.448
iteration 1000: loss: 3945.166
iteration 1100: loss: 3947.007
iteration 1200: loss: 3942.320
iteration 1300: loss: 3944.066
iteration 1400: loss: 3944.880
iteration 1500: loss: 3944.658
iteration 1600: loss: 3951.056
iteration 1700: loss: 3946.208
iteration 1800: loss: 3946.592
====> Epoch: 021 Train loss: 3946.2585  took : 140.4221863746643
====> Test loss: 3949.6546
iteration 0000: loss: 3942.308
iteration 0100: loss: 3946.842
iteration 0200: loss: 3942.010
iteration 0300: loss: 3946.016
iteration 0400: loss: 3948.658
iteration 0500: loss: 3947.299
iteration 0600: loss: 3942.657
iteration 0700: loss: 3950.098
iteration 0800: loss: 3944.481
iteration 0900: loss: 3947.699
iteration 1000: loss: 3942.891
iteration 1100: loss: 3954.911
iteration 1200: loss: 3948.604
iteration 1300: loss: 3943.184
iteration 1400: loss: 3946.459
iteration 1500: loss: 3940.190
iteration 1600: loss: 3950.054
iteration 1700: loss: 3947.140
iteration 1800: loss: 3949.218
====> Epoch: 022 Train loss: 3946.0569  took : 140.8216197490692
====> Test loss: 3949.1600
iteration 0000: loss: 3945.779
iteration 0100: loss: 3945.392
iteration 0200: loss: 3945.415
iteration 0300: loss: 3950.500
iteration 0400: loss: 3943.927
iteration 0500: loss: 3947.300
iteration 0600: loss: 3942.061
iteration 0700: loss: 3940.157
iteration 0800: loss: 3945.758
iteration 0900: loss: 3947.814
iteration 1000: loss: 3950.167
iteration 1100: loss: 3948.538
iteration 1200: loss: 3946.283
iteration 1300: loss: 3949.404
iteration 1400: loss: 3951.284
iteration 1500: loss: 3942.277
iteration 1600: loss: 3944.018
iteration 1700: loss: 3943.222
iteration 1800: loss: 3949.013
====> Epoch: 023 Train loss: 3945.9578  took : 140.43841552734375
====> Test loss: 3948.9129
iteration 0000: loss: 3949.495
iteration 0100: loss: 3946.521
iteration 0200: loss: 3947.007
iteration 0300: loss: 3950.005
iteration 0400: loss: 3943.216
iteration 0500: loss: 3944.926
iteration 0600: loss: 3946.563
iteration 0700: loss: 3947.009
iteration 0800: loss: 3950.966
iteration 0900: loss: 3948.743
iteration 1000: loss: 3949.769
iteration 1100: loss: 3946.363
iteration 1200: loss: 3939.436
iteration 1300: loss: 3954.984
iteration 1400: loss: 3941.311
iteration 1500: loss: 3940.757
iteration 1600: loss: 3949.217
iteration 1700: loss: 3948.100
iteration 1800: loss: 3949.399
====> Epoch: 024 Train loss: 3945.7882  took : 140.43454694747925
====> Test loss: 3949.0860
iteration 0000: loss: 3947.806
iteration 0100: loss: 3942.271
iteration 0200: loss: 3945.770
iteration 0300: loss: 3948.221
iteration 0400: loss: 3946.418
iteration 0500: loss: 3944.887
iteration 0600: loss: 3944.924
iteration 0700: loss: 3951.752
iteration 0800: loss: 3942.940
iteration 0900: loss: 3946.624
iteration 1000: loss: 3954.397
iteration 1100: loss: 3941.924
iteration 1200: loss: 3940.112
iteration 1300: loss: 3943.107
iteration 1400: loss: 3949.546
iteration 1500: loss: 3945.305
iteration 1600: loss: 3950.020
iteration 1700: loss: 3944.620
iteration 1800: loss: 3944.788
====> Epoch: 025 Train loss: 3945.6412  took : 140.56334805488586
====> Test loss: 3949.3162
iteration 0000: loss: 3949.478
iteration 0100: loss: 3948.272
iteration 0200: loss: 3950.900
iteration 0300: loss: 3944.212
iteration 0400: loss: 3948.973
iteration 0500: loss: 3943.871
iteration 0600: loss: 3945.741
iteration 0700: loss: 3941.158
iteration 0800: loss: 3945.562
iteration 0900: loss: 3945.088
iteration 1000: loss: 3943.027
iteration 1100: loss: 3944.839
iteration 1200: loss: 3949.756
iteration 1300: loss: 3945.280
iteration 1400: loss: 3943.308
iteration 1500: loss: 3948.531
iteration 1600: loss: 3947.114
iteration 1700: loss: 3945.606
iteration 1800: loss: 3945.407
====> Epoch: 026 Train loss: 3945.5353  took : 140.4612898826599
====> Test loss: 3948.7069
iteration 0000: loss: 3947.452
iteration 0100: loss: 3945.409
iteration 0200: loss: 3939.093
iteration 0300: loss: 3945.237
iteration 0400: loss: 3942.207
iteration 0500: loss: 3943.533
iteration 0600: loss: 3943.208
iteration 0700: loss: 3945.275
iteration 0800: loss: 3945.220
iteration 0900: loss: 3951.045
iteration 1000: loss: 3950.985
iteration 1100: loss: 3946.146
iteration 1200: loss: 3940.101
iteration 1300: loss: 3951.559
iteration 1400: loss: 3947.117
iteration 1500: loss: 3939.712
iteration 1600: loss: 3946.452
iteration 1700: loss: 3944.042
iteration 1800: loss: 3941.182
====> Epoch: 027 Train loss: 3945.3903  took : 140.73215174674988
====> Test loss: 3948.6405
iteration 0000: loss: 3947.605
iteration 0100: loss: 3945.331
iteration 0200: loss: 3947.697
iteration 0300: loss: 3950.661
iteration 0400: loss: 3950.658
iteration 0500: loss: 3947.697
iteration 0600: loss: 3943.785
iteration 0700: loss: 3945.922
iteration 0800: loss: 3945.579
iteration 0900: loss: 3951.331
iteration 1000: loss: 3944.419
iteration 1100: loss: 3941.864
iteration 1200: loss: 3946.464
iteration 1300: loss: 3947.176
iteration 1400: loss: 3948.741
iteration 1500: loss: 3939.038
iteration 1600: loss: 3945.661
iteration 1700: loss: 3941.236
iteration 1800: loss: 3937.919
====> Epoch: 028 Train loss: 3945.2800  took : 139.6017563343048
====> Test loss: 3948.6205
iteration 0000: loss: 3947.640
iteration 0100: loss: 3943.894
iteration 0200: loss: 3940.596
iteration 0300: loss: 3947.376
iteration 0400: loss: 3942.924
iteration 0500: loss: 3941.328
iteration 0600: loss: 3943.374
iteration 0700: loss: 3946.814
iteration 0800: loss: 3944.025
iteration 0900: loss: 3945.543
iteration 1000: loss: 3947.102
iteration 1100: loss: 3940.331
iteration 1200: loss: 3943.793
iteration 1300: loss: 3944.632
iteration 1400: loss: 3942.753
iteration 1500: loss: 3951.531
iteration 1600: loss: 3938.967
iteration 1700: loss: 3941.922
iteration 1800: loss: 3941.812
====> Epoch: 029 Train loss: 3945.2097  took : 140.3088846206665
====> Test loss: 3948.3244
iteration 0000: loss: 3948.378
iteration 0100: loss: 3943.228
iteration 0200: loss: 3941.736
iteration 0300: loss: 3941.839
iteration 0400: loss: 3942.529
iteration 0500: loss: 3940.199
iteration 0600: loss: 3943.448
iteration 0700: loss: 3940.317
iteration 0800: loss: 3947.583
iteration 0900: loss: 3939.848
iteration 1000: loss: 3946.159
iteration 1100: loss: 3946.749
iteration 1200: loss: 3938.756
iteration 1300: loss: 3942.656
iteration 1400: loss: 3942.301
iteration 1500: loss: 3946.390
iteration 1600: loss: 3948.262
iteration 1700: loss: 3943.726
iteration 1800: loss: 3951.644
====> Epoch: 030 Train loss: 3945.0665  took : 140.08698344230652
====> Test loss: 3948.3159
iteration 0000: loss: 3943.905
iteration 0100: loss: 3941.561
iteration 0200: loss: 3947.697
iteration 0300: loss: 3937.555
iteration 0400: loss: 3942.987
iteration 0500: loss: 3949.754
iteration 0600: loss: 3947.272
iteration 0700: loss: 3942.382
iteration 0800: loss: 3941.220
iteration 0900: loss: 3946.628
iteration 1000: loss: 3947.798
iteration 1100: loss: 3941.431
iteration 1200: loss: 3940.369
iteration 1300: loss: 3948.684
iteration 1400: loss: 3944.627
iteration 1500: loss: 3943.630
iteration 1600: loss: 3944.044
iteration 1700: loss: 3943.658
iteration 1800: loss: 3950.180
====> Epoch: 031 Train loss: 3945.0156  took : 140.38773703575134
====> Test loss: 3948.2954
iteration 0000: loss: 3953.843
iteration 0100: loss: 3944.045
iteration 0200: loss: 3944.808
iteration 0300: loss: 3943.480
iteration 0400: loss: 3936.888
iteration 0500: loss: 3949.034
iteration 0600: loss: 3947.054
iteration 0700: loss: 3943.736
iteration 0800: loss: 3944.284
iteration 0900: loss: 3953.288
iteration 1000: loss: 3944.779
iteration 1100: loss: 3943.895
iteration 1200: loss: 3947.985
iteration 1300: loss: 3943.526
iteration 1400: loss: 3942.084
iteration 1500: loss: 3948.246
iteration 1600: loss: 3949.067
iteration 1700: loss: 3943.859
iteration 1800: loss: 3941.218
====> Epoch: 032 Train loss: 3944.8806  took : 140.06609797477722
====> Test loss: 3947.9103
iteration 0000: loss: 3946.966
iteration 0100: loss: 3948.423
iteration 0200: loss: 3944.320
iteration 0300: loss: 3934.897
iteration 0400: loss: 3947.085
iteration 0500: loss: 3949.921
iteration 0600: loss: 3944.655
iteration 0700: loss: 3941.106
iteration 0800: loss: 3948.748
iteration 0900: loss: 3950.209
iteration 1000: loss: 3943.937
iteration 1100: loss: 3944.230
iteration 1200: loss: 3935.257
iteration 1300: loss: 3947.289
iteration 1400: loss: 3936.900
iteration 1500: loss: 3941.016
iteration 1600: loss: 3944.318
iteration 1700: loss: 3945.176
iteration 1800: loss: 3943.811
====> Epoch: 033 Train loss: 3944.8278  took : 140.67790007591248
====> Test loss: 3948.1668
iteration 0000: loss: 3937.484
iteration 0100: loss: 3946.349
iteration 0200: loss: 3941.884
iteration 0300: loss: 3946.672
iteration 0400: loss: 3946.481
iteration 0500: loss: 3948.679
iteration 0600: loss: 3945.091
iteration 0700: loss: 3945.338
iteration 0800: loss: 3944.410
iteration 0900: loss: 3942.600
iteration 1000: loss: 3950.126
iteration 1100: loss: 3947.380
iteration 1200: loss: 3948.944
iteration 1300: loss: 3941.342
iteration 1400: loss: 3944.109
iteration 1500: loss: 3945.937
iteration 1600: loss: 3949.742
iteration 1700: loss: 3941.613
iteration 1800: loss: 3939.537
====> Epoch: 034 Train loss: 3944.7486  took : 139.13131737709045
====> Test loss: 3948.0856
iteration 0000: loss: 3942.478
iteration 0100: loss: 3948.441
iteration 0200: loss: 3952.753
iteration 0300: loss: 3947.711
iteration 0400: loss: 3943.701
iteration 0500: loss: 3947.905
iteration 0600: loss: 3948.950
iteration 0700: loss: 3940.823
iteration 0800: loss: 3949.927
iteration 0900: loss: 3948.934
iteration 1000: loss: 3948.818
iteration 1100: loss: 3941.665
iteration 1200: loss: 3942.702
iteration 1300: loss: 3945.872
iteration 1400: loss: 3947.993
iteration 1500: loss: 3947.502
iteration 1600: loss: 3938.783
iteration 1700: loss: 3944.928
iteration 1800: loss: 3947.197
====> Epoch: 035 Train loss: 3944.6771  took : 140.4178969860077
====> Test loss: 3947.8285
iteration 0000: loss: 3947.172
iteration 0100: loss: 3943.848
iteration 0200: loss: 3945.642
iteration 0300: loss: 3948.615
iteration 0400: loss: 3948.353
iteration 0500: loss: 3942.449
iteration 0600: loss: 3949.464
iteration 0700: loss: 3945.028
iteration 0800: loss: 3945.549
iteration 0900: loss: 3939.606
iteration 1000: loss: 3948.455
iteration 1100: loss: 3945.951
iteration 1200: loss: 3944.418
iteration 1300: loss: 3946.826
iteration 1400: loss: 3947.738
iteration 1500: loss: 3945.942
iteration 1600: loss: 3944.748
iteration 1700: loss: 3940.691
iteration 1800: loss: 3942.626
====> Epoch: 036 Train loss: 3944.5483  took : 140.67179775238037
====> Test loss: 3947.7444
iteration 0000: loss: 3945.478
iteration 0100: loss: 3936.978
iteration 0200: loss: 3942.513
iteration 0300: loss: 3944.188
iteration 0400: loss: 3938.022
iteration 0500: loss: 3941.537
iteration 0600: loss: 3942.884
iteration 0700: loss: 3948.560
iteration 0800: loss: 3942.676
iteration 0900: loss: 3945.111
iteration 1000: loss: 3943.050
iteration 1100: loss: 3945.712
iteration 1200: loss: 3938.919
iteration 1300: loss: 3946.419
iteration 1400: loss: 3944.706
iteration 1500: loss: 3943.173
iteration 1600: loss: 3943.567
iteration 1700: loss: 3944.150
iteration 1800: loss: 3945.968
====> Epoch: 037 Train loss: 3944.5614  took : 140.37772345542908
====> Test loss: 3947.7903
iteration 0000: loss: 3946.829
iteration 0100: loss: 3948.074
iteration 0200: loss: 3943.003
iteration 0300: loss: 3944.386
iteration 0400: loss: 3946.026
iteration 0500: loss: 3948.872
iteration 0600: loss: 3941.464
iteration 0700: loss: 3938.059
iteration 0800: loss: 3943.169
iteration 0900: loss: 3945.157
iteration 1000: loss: 3951.377
iteration 1100: loss: 3947.907
iteration 1200: loss: 3949.688
iteration 1300: loss: 3943.748
iteration 1400: loss: 3948.681
iteration 1500: loss: 3949.547
iteration 1600: loss: 3939.583
iteration 1700: loss: 3951.953
iteration 1800: loss: 3943.984
====> Epoch: 038 Train loss: 3944.4062  took : 139.82707619667053
====> Test loss: 3947.9424
iteration 0000: loss: 3942.477
iteration 0100: loss: 3944.227
iteration 0200: loss: 3939.004
iteration 0300: loss: 3945.966
iteration 0400: loss: 3942.528
iteration 0500: loss: 3945.507
iteration 0600: loss: 3940.877
iteration 0700: loss: 3947.635
iteration 0800: loss: 3938.650
iteration 0900: loss: 3949.028
iteration 1000: loss: 3936.945
iteration 1100: loss: 3947.806
iteration 1200: loss: 3945.141
iteration 1300: loss: 3936.485
iteration 1400: loss: 3950.509
iteration 1500: loss: 3946.549
iteration 1600: loss: 3942.976
iteration 1700: loss: 3943.899
iteration 1800: loss: 3941.983
====> Epoch: 039 Train loss: 3944.3748  took : 139.29761624336243
====> Test loss: 3947.6613
iteration 0000: loss: 3950.952
iteration 0100: loss: 3949.738
iteration 0200: loss: 3946.112
iteration 0300: loss: 3944.639
iteration 0400: loss: 3947.731
iteration 0500: loss: 3952.980
iteration 0600: loss: 3942.436
iteration 0700: loss: 3944.864
iteration 0800: loss: 3938.026
iteration 0900: loss: 3946.376
iteration 1000: loss: 3944.851
iteration 1100: loss: 3944.954
iteration 1200: loss: 3941.320
iteration 1300: loss: 3937.503
iteration 1400: loss: 3942.197
iteration 1500: loss: 3936.220
iteration 1600: loss: 3940.426
iteration 1700: loss: 3947.071
iteration 1800: loss: 3948.094
====> Epoch: 040 Train loss: 3944.2633  took : 140.48320055007935
====> Test loss: 3947.6963
iteration 0000: loss: 3937.813
iteration 0100: loss: 3945.855
iteration 0200: loss: 3942.104
iteration 0300: loss: 3942.677
iteration 0400: loss: 3944.877
iteration 0500: loss: 3941.532
iteration 0600: loss: 3942.480
iteration 0700: loss: 3945.104
iteration 0800: loss: 3944.103
iteration 0900: loss: 3950.279
iteration 1000: loss: 3947.208
iteration 1100: loss: 3944.525
iteration 1200: loss: 3943.919
iteration 1300: loss: 3944.819
iteration 1400: loss: 3947.654
iteration 1500: loss: 3944.150
iteration 1600: loss: 3945.851
iteration 1700: loss: 3949.253
iteration 1800: loss: 3946.569
====> Epoch: 041 Train loss: 3944.1402  took : 140.59476566314697
====> Test loss: 3947.4742
iteration 0000: loss: 3942.028
iteration 0100: loss: 3945.067
iteration 0200: loss: 3954.531
iteration 0300: loss: 3941.135
iteration 0400: loss: 3949.308
iteration 0500: loss: 3942.391
iteration 0600: loss: 3942.438
iteration 0700: loss: 3942.138
iteration 0800: loss: 3947.638
iteration 0900: loss: 3944.973
iteration 1000: loss: 3953.303
iteration 1100: loss: 3947.375
iteration 1200: loss: 3941.212
iteration 1300: loss: 3945.471
iteration 1400: loss: 3946.453
iteration 1500: loss: 3943.539
iteration 1600: loss: 3940.297
iteration 1700: loss: 3947.474
iteration 1800: loss: 3944.582
====> Epoch: 042 Train loss: 3944.0537  took : 138.9892454147339
====> Test loss: 3947.2794
iteration 0000: loss: 3940.694
iteration 0100: loss: 3946.807
iteration 0200: loss: 3942.688
iteration 0300: loss: 3941.391
iteration 0400: loss: 3946.245
iteration 0500: loss: 3942.517
iteration 0600: loss: 3947.370
iteration 0700: loss: 3952.633
iteration 0800: loss: 3944.385
iteration 0900: loss: 3943.404
iteration 1000: loss: 3942.456
iteration 1100: loss: 3944.636
iteration 1200: loss: 3946.295
iteration 1300: loss: 3941.332
iteration 1400: loss: 3948.641
iteration 1500: loss: 3938.879
iteration 1600: loss: 3942.225
iteration 1700: loss: 3944.008
iteration 1800: loss: 3940.684
====> Epoch: 043 Train loss: 3943.9823  took : 139.78666019439697
====> Test loss: 3947.4632
iteration 0000: loss: 3941.531
iteration 0100: loss: 3940.119
iteration 0200: loss: 3949.439
iteration 0300: loss: 3940.782
iteration 0400: loss: 3950.788
iteration 0500: loss: 3947.195
iteration 0600: loss: 3949.510
iteration 0700: loss: 3938.938
iteration 0800: loss: 3949.085
iteration 0900: loss: 3940.964
iteration 1000: loss: 3948.059
iteration 1100: loss: 3945.424
iteration 1200: loss: 3945.081
iteration 1300: loss: 3939.133
iteration 1400: loss: 3941.082
iteration 1500: loss: 3947.701
iteration 1600: loss: 3948.130
iteration 1700: loss: 3951.220
iteration 1800: loss: 3942.557
====> Epoch: 044 Train loss: 3943.9350  took : 140.80566477775574
====> Test loss: 3947.2054
iteration 0000: loss: 3939.212
iteration 0100: loss: 3941.926
iteration 0200: loss: 3942.784
iteration 0300: loss: 3949.735
iteration 0400: loss: 3939.127
iteration 0500: loss: 3946.676
iteration 0600: loss: 3946.798
iteration 0700: loss: 3947.479
iteration 0800: loss: 3943.572
iteration 0900: loss: 3947.490
iteration 1000: loss: 3946.286
iteration 1100: loss: 3939.036
iteration 1200: loss: 3936.337
iteration 1300: loss: 3944.137
iteration 1400: loss: 3942.639
iteration 1500: loss: 3942.169
iteration 1600: loss: 3957.453
iteration 1700: loss: 3943.671
iteration 1800: loss: 3941.374
====> Epoch: 045 Train loss: 3943.8511  took : 140.61736941337585
====> Test loss: 3947.3590
iteration 0000: loss: 3943.877
iteration 0100: loss: 3945.613
iteration 0200: loss: 3947.936
iteration 0300: loss: 3955.712
iteration 0400: loss: 3943.583
iteration 0500: loss: 3950.750
iteration 0600: loss: 3946.009
iteration 0700: loss: 3941.573
iteration 0800: loss: 3944.707
iteration 0900: loss: 3948.959
iteration 1000: loss: 3951.191
iteration 1100: loss: 3945.760
iteration 1200: loss: 3944.416
iteration 1300: loss: 3947.735
iteration 1400: loss: 3946.463
iteration 1500: loss: 3944.740
iteration 1600: loss: 3940.334
iteration 1700: loss: 3942.818
iteration 1800: loss: 3941.127
====> Epoch: 046 Train loss: 3943.7499  took : 139.463392496109
====> Test loss: 3946.8048
iteration 0000: loss: 3936.756
iteration 0100: loss: 3945.977
iteration 0200: loss: 3946.105
iteration 0300: loss: 3945.413
iteration 0400: loss: 3938.573
iteration 0500: loss: 3947.806
iteration 0600: loss: 3939.682
iteration 0700: loss: 3949.073
iteration 0800: loss: 3939.930
iteration 0900: loss: 3942.909
iteration 1000: loss: 3943.563
iteration 1100: loss: 3948.695
iteration 1200: loss: 3946.846
iteration 1300: loss: 3943.309
iteration 1400: loss: 3941.543
iteration 1500: loss: 3943.854
iteration 1600: loss: 3942.074
iteration 1700: loss: 3938.245
iteration 1800: loss: 3948.355
====> Epoch: 047 Train loss: 3943.6855  took : 139.810054063797
====> Test loss: 3947.2365
iteration 0000: loss: 3941.615
iteration 0100: loss: 3946.821
iteration 0200: loss: 3947.797
iteration 0300: loss: 3947.738
iteration 0400: loss: 3941.282
iteration 0500: loss: 3941.003
iteration 0600: loss: 3952.339
iteration 0700: loss: 3941.355
iteration 0800: loss: 3940.283
iteration 0900: loss: 3949.804
iteration 1000: loss: 3947.897
iteration 1100: loss: 3944.025
iteration 1200: loss: 3954.729
iteration 1300: loss: 3941.148
iteration 1400: loss: 3940.587
iteration 1500: loss: 3946.862
iteration 1600: loss: 3946.853
iteration 1700: loss: 3948.133
iteration 1800: loss: 3948.042
====> Epoch: 048 Train loss: 3943.6010  took : 139.2865755558014
====> Test loss: 3946.9740
iteration 0000: loss: 3940.719
iteration 0100: loss: 3939.347
iteration 0200: loss: 3937.727
iteration 0300: loss: 3940.439
iteration 0400: loss: 3940.457
iteration 0500: loss: 3942.956
iteration 0600: loss: 3949.817
iteration 0700: loss: 3944.551
iteration 0800: loss: 3940.562
iteration 0900: loss: 3944.705
iteration 1000: loss: 3938.078
iteration 1100: loss: 3940.984
iteration 1200: loss: 3942.262
iteration 1300: loss: 3943.380
iteration 1400: loss: 3941.365
iteration 1500: loss: 3945.257
iteration 1600: loss: 3942.446
iteration 1700: loss: 3941.263
iteration 1800: loss: 3941.751
====> Epoch: 049 Train loss: 3943.4980  took : 139.25536823272705
====> Test loss: 3947.0246
iteration 0000: loss: 3940.108
iteration 0100: loss: 3943.819
iteration 0200: loss: 3947.199
iteration 0300: loss: 3947.070
iteration 0400: loss: 3945.502
iteration 0500: loss: 3949.651
iteration 0600: loss: 3947.757
iteration 0700: loss: 3940.678
iteration 0800: loss: 3939.457
iteration 0900: loss: 3954.298
iteration 1000: loss: 3940.132
iteration 1100: loss: 3940.229
iteration 1200: loss: 3944.732
iteration 1300: loss: 3942.796
iteration 1400: loss: 3946.766
iteration 1500: loss: 3948.934
iteration 1600: loss: 3937.237
iteration 1700: loss: 3939.424
iteration 1800: loss: 3938.915
====> Epoch: 050 Train loss: 3943.4091  took : 140.11505270004272
====> Test loss: 3946.6922
====> [MM-VAE] Time: 8034.342s or 02:13:54
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_2
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1995.261
iteration 0100: loss: 1561.582
iteration 0200: loss: 1555.360
iteration 0300: loss: 1547.929
iteration 0400: loss: 1546.933
iteration 0500: loss: 1544.300
iteration 0600: loss: 1546.830
iteration 0700: loss: 1543.147
iteration 0800: loss: 1542.679
iteration 0900: loss: 1537.413
====> Epoch: 001 Train loss: 1551.5146  took : 5.910698175430298
====> Test loss: 1531.5376
iteration 0000: loss: 1536.984
iteration 0100: loss: 1533.424
iteration 0200: loss: 1538.505
iteration 0300: loss: 1534.057
iteration 0400: loss: 1533.755
iteration 0500: loss: 1533.859
iteration 0600: loss: 1529.446
iteration 0700: loss: 1532.674
iteration 0800: loss: 1532.564
iteration 0900: loss: 1536.499
====> Epoch: 002 Train loss: 1534.0595  took : 5.880536079406738
====> Test loss: 1527.1381
iteration 0000: loss: 1534.025
iteration 0100: loss: 1532.746
iteration 0200: loss: 1530.480
iteration 0300: loss: 1531.439
iteration 0400: loss: 1532.633
iteration 0500: loss: 1536.518
iteration 0600: loss: 1531.385
iteration 0700: loss: 1532.378
iteration 0800: loss: 1529.288
iteration 0900: loss: 1529.651
====> Epoch: 003 Train loss: 1531.3214  took : 5.8761680126190186
====> Test loss: 1525.6601
iteration 0000: loss: 1531.185
iteration 0100: loss: 1525.843
iteration 0200: loss: 1533.682
iteration 0300: loss: 1530.353
iteration 0400: loss: 1528.484
iteration 0500: loss: 1529.863
iteration 0600: loss: 1531.332
iteration 0700: loss: 1529.370
iteration 0800: loss: 1528.428
iteration 0900: loss: 1530.985
====> Epoch: 004 Train loss: 1530.1931  took : 5.920084476470947
====> Test loss: 1524.8260
iteration 0000: loss: 1531.341
iteration 0100: loss: 1529.877
iteration 0200: loss: 1534.062
iteration 0300: loss: 1528.354
iteration 0400: loss: 1528.020
iteration 0500: loss: 1528.578
iteration 0600: loss: 1530.813
iteration 0700: loss: 1528.656
iteration 0800: loss: 1529.122
iteration 0900: loss: 1529.669
====> Epoch: 005 Train loss: 1529.4867  took : 5.948558807373047
====> Test loss: 1524.2457
iteration 0000: loss: 1528.257
iteration 0100: loss: 1525.655
iteration 0200: loss: 1527.063
iteration 0300: loss: 1525.760
iteration 0400: loss: 1530.147
iteration 0500: loss: 1526.587
iteration 0600: loss: 1525.980
iteration 0700: loss: 1527.330
iteration 0800: loss: 1526.654
iteration 0900: loss: 1525.916
====> Epoch: 006 Train loss: 1528.9566  took : 5.901512384414673
====> Test loss: 1523.7602
iteration 0000: loss: 1528.127
iteration 0100: loss: 1527.309
iteration 0200: loss: 1526.537
iteration 0300: loss: 1528.073
iteration 0400: loss: 1531.468
iteration 0500: loss: 1529.817
iteration 0600: loss: 1529.310
iteration 0700: loss: 1528.600
iteration 0800: loss: 1532.002
iteration 0900: loss: 1528.462
====> Epoch: 007 Train loss: 1528.4865  took : 5.901577949523926
====> Test loss: 1523.3613
iteration 0000: loss: 1525.862
iteration 0100: loss: 1525.649
iteration 0200: loss: 1525.571
iteration 0300: loss: 1528.601
iteration 0400: loss: 1525.572
iteration 0500: loss: 1529.462
iteration 0600: loss: 1527.656
iteration 0700: loss: 1526.797
iteration 0800: loss: 1526.407
iteration 0900: loss: 1526.211
====> Epoch: 008 Train loss: 1528.0825  took : 5.936028480529785
====> Test loss: 1523.0661
iteration 0000: loss: 1529.823
iteration 0100: loss: 1525.038
iteration 0200: loss: 1527.241
iteration 0300: loss: 1526.251
iteration 0400: loss: 1525.934
iteration 0500: loss: 1526.480
iteration 0600: loss: 1531.438
iteration 0700: loss: 1526.803
iteration 0800: loss: 1529.989
iteration 0900: loss: 1524.557
====> Epoch: 009 Train loss: 1527.7736  took : 5.881571531295776
====> Test loss: 1522.8179
iteration 0000: loss: 1530.829
iteration 0100: loss: 1526.858
iteration 0200: loss: 1524.616
iteration 0300: loss: 1529.392
iteration 0400: loss: 1526.309
iteration 0500: loss: 1527.047
iteration 0600: loss: 1527.075
iteration 0700: loss: 1524.334
iteration 0800: loss: 1526.251
iteration 0900: loss: 1529.571
====> Epoch: 010 Train loss: 1527.4725  took : 5.926826238632202
====> Test loss: 1522.6092
iteration 0000: loss: 1527.175
iteration 0100: loss: 1528.506
iteration 0200: loss: 1525.783
iteration 0300: loss: 1530.581
iteration 0400: loss: 1528.720
iteration 0500: loss: 1530.281
iteration 0600: loss: 1526.449
iteration 0700: loss: 1528.259
iteration 0800: loss: 1526.526
iteration 0900: loss: 1524.872
====> Epoch: 011 Train loss: 1527.2532  took : 5.925313711166382
====> Test loss: 1522.3722
iteration 0000: loss: 1526.890
iteration 0100: loss: 1526.108
iteration 0200: loss: 1526.002
iteration 0300: loss: 1526.328
iteration 0400: loss: 1530.517
iteration 0500: loss: 1525.418
iteration 0600: loss: 1526.845
iteration 0700: loss: 1528.002
iteration 0800: loss: 1529.534
iteration 0900: loss: 1522.814
====> Epoch: 012 Train loss: 1527.0589  took : 5.903165817260742
====> Test loss: 1522.1545
iteration 0000: loss: 1527.570
iteration 0100: loss: 1529.062
iteration 0200: loss: 1524.942
iteration 0300: loss: 1527.937
iteration 0400: loss: 1526.173
iteration 0500: loss: 1525.759
iteration 0600: loss: 1524.932
iteration 0700: loss: 1529.542
iteration 0800: loss: 1527.030
iteration 0900: loss: 1523.446
====> Epoch: 013 Train loss: 1526.8609  took : 6.052363395690918
====> Test loss: 1522.0832
iteration 0000: loss: 1527.864
iteration 0100: loss: 1529.804
iteration 0200: loss: 1526.266
iteration 0300: loss: 1528.396
iteration 0400: loss: 1525.580
iteration 0500: loss: 1525.521
iteration 0600: loss: 1523.299
iteration 0700: loss: 1529.545
iteration 0800: loss: 1525.072
iteration 0900: loss: 1528.828
====> Epoch: 014 Train loss: 1526.6965  took : 5.920166254043579
====> Test loss: 1521.9111
iteration 0000: loss: 1525.603
iteration 0100: loss: 1526.677
iteration 0200: loss: 1527.857
iteration 0300: loss: 1524.680
iteration 0400: loss: 1527.229
iteration 0500: loss: 1525.745
iteration 0600: loss: 1523.278
iteration 0700: loss: 1530.783
iteration 0800: loss: 1526.943
iteration 0900: loss: 1523.413
====> Epoch: 015 Train loss: 1526.5407  took : 5.936524868011475
====> Test loss: 1521.7476
iteration 0000: loss: 1527.351
iteration 0100: loss: 1527.358
iteration 0200: loss: 1525.476
iteration 0300: loss: 1525.986
iteration 0400: loss: 1529.595
iteration 0500: loss: 1531.175
iteration 0600: loss: 1525.326
iteration 0700: loss: 1526.475
iteration 0800: loss: 1525.123
iteration 0900: loss: 1526.036
====> Epoch: 016 Train loss: 1526.4044  took : 5.901236057281494
====> Test loss: 1521.6431
iteration 0000: loss: 1526.716
iteration 0100: loss: 1525.340
iteration 0200: loss: 1528.762
iteration 0300: loss: 1526.230
iteration 0400: loss: 1526.899
iteration 0500: loss: 1525.964
iteration 0600: loss: 1522.328
iteration 0700: loss: 1525.349
iteration 0800: loss: 1529.012
iteration 0900: loss: 1527.362
====> Epoch: 017 Train loss: 1526.3157  took : 5.9321088790893555
====> Test loss: 1521.7474
iteration 0000: loss: 1526.664
iteration 0100: loss: 1526.355
iteration 0200: loss: 1527.968
iteration 0300: loss: 1528.991
iteration 0400: loss: 1528.249
iteration 0500: loss: 1526.248
iteration 0600: loss: 1527.786
iteration 0700: loss: 1527.531
iteration 0800: loss: 1524.031
iteration 0900: loss: 1527.317
====> Epoch: 018 Train loss: 1526.1918  took : 5.891045570373535
====> Test loss: 1521.6148
iteration 0000: loss: 1530.988
iteration 0100: loss: 1524.258
iteration 0200: loss: 1526.155
iteration 0300: loss: 1526.460
iteration 0400: loss: 1531.019
iteration 0500: loss: 1526.747
iteration 0600: loss: 1524.449
iteration 0700: loss: 1527.265
iteration 0800: loss: 1526.068
iteration 0900: loss: 1528.971
====> Epoch: 019 Train loss: 1526.0865  took : 5.899169206619263
====> Test loss: 1521.5838
iteration 0000: loss: 1527.105
iteration 0100: loss: 1527.696
iteration 0200: loss: 1526.720
iteration 0300: loss: 1525.573
iteration 0400: loss: 1526.720
iteration 0500: loss: 1525.986
iteration 0600: loss: 1525.957
iteration 0700: loss: 1525.156
iteration 0800: loss: 1526.575
iteration 0900: loss: 1525.101
====> Epoch: 020 Train loss: 1526.0035  took : 5.9125657081604
====> Test loss: 1521.3361
iteration 0000: loss: 1524.650
iteration 0100: loss: 1529.522
iteration 0200: loss: 1524.045
iteration 0300: loss: 1526.770
iteration 0400: loss: 1519.738
iteration 0500: loss: 1526.579
iteration 0600: loss: 1526.348
iteration 0700: loss: 1524.193
iteration 0800: loss: 1527.825
iteration 0900: loss: 1524.061
====> Epoch: 021 Train loss: 1525.8612  took : 5.916146993637085
====> Test loss: 1521.4281
iteration 0000: loss: 1524.741
iteration 0100: loss: 1524.549
iteration 0200: loss: 1526.890
iteration 0300: loss: 1525.917
iteration 0400: loss: 1526.669
iteration 0500: loss: 1526.235
iteration 0600: loss: 1529.396
iteration 0700: loss: 1524.283
iteration 0800: loss: 1525.827
iteration 0900: loss: 1527.062
====> Epoch: 022 Train loss: 1525.8275  took : 5.921022891998291
====> Test loss: 1521.4001
iteration 0000: loss: 1521.338
iteration 0100: loss: 1528.094
iteration 0200: loss: 1527.510
iteration 0300: loss: 1527.378
iteration 0400: loss: 1526.641
iteration 0500: loss: 1528.075
iteration 0600: loss: 1526.414
iteration 0700: loss: 1525.323
iteration 0800: loss: 1527.601
iteration 0900: loss: 1524.305
====> Epoch: 023 Train loss: 1525.7768  took : 5.940370321273804
====> Test loss: 1521.1767
iteration 0000: loss: 1525.863
iteration 0100: loss: 1527.197
iteration 0200: loss: 1522.437
iteration 0300: loss: 1523.191
iteration 0400: loss: 1524.030
iteration 0500: loss: 1527.216
iteration 0600: loss: 1526.648
iteration 0700: loss: 1526.681
iteration 0800: loss: 1525.188
iteration 0900: loss: 1526.383
====> Epoch: 024 Train loss: 1525.6867  took : 5.904626369476318
====> Test loss: 1521.0821
iteration 0000: loss: 1523.325
iteration 0100: loss: 1525.917
iteration 0200: loss: 1525.862
iteration 0300: loss: 1527.032
iteration 0400: loss: 1524.494
iteration 0500: loss: 1526.175
iteration 0600: loss: 1526.526
iteration 0700: loss: 1525.269
iteration 0800: loss: 1526.398
iteration 0900: loss: 1523.532
====> Epoch: 025 Train loss: 1525.6149  took : 5.946763277053833
====> Test loss: 1521.1845
iteration 0000: loss: 1526.449
iteration 0100: loss: 1523.129
iteration 0200: loss: 1522.669
iteration 0300: loss: 1525.271
iteration 0400: loss: 1525.738
iteration 0500: loss: 1525.568
iteration 0600: loss: 1525.198
iteration 0700: loss: 1527.115
iteration 0800: loss: 1526.482
iteration 0900: loss: 1524.189
====> Epoch: 026 Train loss: 1525.5538  took : 5.9345481395721436
====> Test loss: 1521.0921
iteration 0000: loss: 1526.514
iteration 0100: loss: 1524.735
iteration 0200: loss: 1526.069
iteration 0300: loss: 1528.219
iteration 0400: loss: 1526.730
iteration 0500: loss: 1526.392
iteration 0600: loss: 1526.328
iteration 0700: loss: 1527.179
iteration 0800: loss: 1522.805
iteration 0900: loss: 1524.921
====> Epoch: 027 Train loss: 1525.5139  took : 5.889901161193848
====> Test loss: 1521.0301
iteration 0000: loss: 1526.993
iteration 0100: loss: 1528.323
iteration 0200: loss: 1526.450
iteration 0300: loss: 1525.682
iteration 0400: loss: 1524.312
iteration 0500: loss: 1522.651
iteration 0600: loss: 1525.932
iteration 0700: loss: 1523.535
iteration 0800: loss: 1525.114
iteration 0900: loss: 1525.210
====> Epoch: 028 Train loss: 1525.4319  took : 5.894139051437378
====> Test loss: 1521.0291
iteration 0000: loss: 1525.797
iteration 0100: loss: 1524.541
iteration 0200: loss: 1525.891
iteration 0300: loss: 1526.789
iteration 0400: loss: 1523.796
iteration 0500: loss: 1524.652
iteration 0600: loss: 1523.107
iteration 0700: loss: 1521.353
iteration 0800: loss: 1526.796
iteration 0900: loss: 1524.279
====> Epoch: 029 Train loss: 1525.4069  took : 5.917261362075806
====> Test loss: 1520.9192
iteration 0000: loss: 1524.697
iteration 0100: loss: 1524.422
iteration 0200: loss: 1525.414
iteration 0300: loss: 1526.737
iteration 0400: loss: 1529.257
iteration 0500: loss: 1525.105
iteration 0600: loss: 1527.608
iteration 0700: loss: 1524.458
iteration 0800: loss: 1523.678
iteration 0900: loss: 1527.951
====> Epoch: 030 Train loss: 1525.3315  took : 5.988654136657715
====> Test loss: 1520.8895
iteration 0000: loss: 1524.387
iteration 0100: loss: 1524.407
iteration 0200: loss: 1525.272
iteration 0300: loss: 1524.496
iteration 0400: loss: 1525.159
iteration 0500: loss: 1526.701
iteration 0600: loss: 1527.077
iteration 0700: loss: 1526.827
iteration 0800: loss: 1525.274
iteration 0900: loss: 1526.228
====> Epoch: 031 Train loss: 1525.2732  took : 5.940133094787598
====> Test loss: 1520.7754
iteration 0000: loss: 1525.595
iteration 0100: loss: 1527.251
iteration 0200: loss: 1524.488
iteration 0300: loss: 1526.018
iteration 0400: loss: 1524.229
iteration 0500: loss: 1522.197
iteration 0600: loss: 1524.630
iteration 0700: loss: 1526.204
iteration 0800: loss: 1526.009
iteration 0900: loss: 1526.128
====> Epoch: 032 Train loss: 1525.2554  took : 5.909886360168457
====> Test loss: 1520.8122
iteration 0000: loss: 1527.606
iteration 0100: loss: 1523.028
iteration 0200: loss: 1526.533
iteration 0300: loss: 1525.580
iteration 0400: loss: 1524.737
iteration 0500: loss: 1523.659
iteration 0600: loss: 1525.476
iteration 0700: loss: 1525.432
iteration 0800: loss: 1524.169
iteration 0900: loss: 1526.486
====> Epoch: 033 Train loss: 1525.1920  took : 5.984798192977905
====> Test loss: 1520.7567
iteration 0000: loss: 1527.536
iteration 0100: loss: 1524.023
iteration 0200: loss: 1526.947
iteration 0300: loss: 1521.826
iteration 0400: loss: 1523.994
iteration 0500: loss: 1522.218
iteration 0600: loss: 1523.334
iteration 0700: loss: 1529.143
iteration 0800: loss: 1526.384
iteration 0900: loss: 1524.799
====> Epoch: 034 Train loss: 1525.1506  took : 5.8907318115234375
====> Test loss: 1520.8256
iteration 0000: loss: 1525.980
iteration 0100: loss: 1523.890
iteration 0200: loss: 1522.225
iteration 0300: loss: 1527.259
iteration 0400: loss: 1525.033
iteration 0500: loss: 1526.143
iteration 0600: loss: 1523.244
iteration 0700: loss: 1526.763
iteration 0800: loss: 1528.787
iteration 0900: loss: 1527.259
====> Epoch: 035 Train loss: 1525.1333  took : 5.941448926925659
====> Test loss: 1520.7720
iteration 0000: loss: 1525.140
iteration 0100: loss: 1524.944
iteration 0200: loss: 1524.480
iteration 0300: loss: 1524.960
iteration 0400: loss: 1523.484
iteration 0500: loss: 1526.390
iteration 0600: loss: 1525.426
iteration 0700: loss: 1527.233
iteration 0800: loss: 1525.368
iteration 0900: loss: 1526.283
====> Epoch: 036 Train loss: 1525.0921  took : 5.9027204513549805
====> Test loss: 1520.6909
iteration 0000: loss: 1524.361
iteration 0100: loss: 1523.740
iteration 0200: loss: 1524.587
iteration 0300: loss: 1525.120
iteration 0400: loss: 1524.251
iteration 0500: loss: 1524.349
iteration 0600: loss: 1526.540
iteration 0700: loss: 1527.385
iteration 0800: loss: 1525.197
iteration 0900: loss: 1526.801
====> Epoch: 037 Train loss: 1525.0376  took : 5.8964972496032715
====> Test loss: 1520.8114
iteration 0000: loss: 1526.091
iteration 0100: loss: 1526.233
iteration 0200: loss: 1526.002
iteration 0300: loss: 1524.323
iteration 0400: loss: 1522.173
iteration 0500: loss: 1522.340
iteration 0600: loss: 1524.405
iteration 0700: loss: 1524.103
iteration 0800: loss: 1526.420
iteration 0900: loss: 1523.220
====> Epoch: 038 Train loss: 1525.0481  took : 5.897140979766846
====> Test loss: 1520.6320
iteration 0000: loss: 1525.144
iteration 0100: loss: 1527.578
iteration 0200: loss: 1527.231
iteration 0300: loss: 1526.119
iteration 0400: loss: 1526.499
iteration 0500: loss: 1526.353
iteration 0600: loss: 1522.927
iteration 0700: loss: 1522.957
iteration 0800: loss: 1522.137
iteration 0900: loss: 1527.218
====> Epoch: 039 Train loss: 1524.9767  took : 5.91109037399292
====> Test loss: 1520.6472
iteration 0000: loss: 1525.877
iteration 0100: loss: 1526.445
iteration 0200: loss: 1524.662
iteration 0300: loss: 1522.331
iteration 0400: loss: 1525.191
iteration 0500: loss: 1526.381
iteration 0600: loss: 1524.400
iteration 0700: loss: 1525.485
iteration 0800: loss: 1523.385
iteration 0900: loss: 1525.885
====> Epoch: 040 Train loss: 1524.9644  took : 5.8859570026397705
====> Test loss: 1520.5782
iteration 0000: loss: 1527.038
iteration 0100: loss: 1525.770
iteration 0200: loss: 1523.939
iteration 0300: loss: 1524.800
iteration 0400: loss: 1529.542
iteration 0500: loss: 1525.567
iteration 0600: loss: 1526.095
iteration 0700: loss: 1528.160
iteration 0800: loss: 1523.137
iteration 0900: loss: 1523.436
====> Epoch: 041 Train loss: 1524.9176  took : 5.9262988567352295
====> Test loss: 1520.5515
iteration 0000: loss: 1525.090
iteration 0100: loss: 1524.675
iteration 0200: loss: 1526.016
iteration 0300: loss: 1525.638
iteration 0400: loss: 1524.339
iteration 0500: loss: 1526.374
iteration 0600: loss: 1524.921
iteration 0700: loss: 1525.704
iteration 0800: loss: 1524.346
iteration 0900: loss: 1522.709
====> Epoch: 042 Train loss: 1524.8954  took : 5.931226491928101
====> Test loss: 1520.6840
iteration 0000: loss: 1523.547
iteration 0100: loss: 1524.284
iteration 0200: loss: 1526.605
iteration 0300: loss: 1525.289
iteration 0400: loss: 1527.650
iteration 0500: loss: 1527.270
iteration 0600: loss: 1525.585
iteration 0700: loss: 1525.970
iteration 0800: loss: 1526.321
iteration 0900: loss: 1526.186
====> Epoch: 043 Train loss: 1524.8627  took : 5.977816820144653
====> Test loss: 1520.6156
iteration 0000: loss: 1525.523
iteration 0100: loss: 1525.499
iteration 0200: loss: 1522.324
iteration 0300: loss: 1525.717
iteration 0400: loss: 1525.016
iteration 0500: loss: 1526.231
iteration 0600: loss: 1525.162
iteration 0700: loss: 1523.126
iteration 0800: loss: 1526.338
iteration 0900: loss: 1525.606
====> Epoch: 044 Train loss: 1524.8473  took : 5.988062620162964
====> Test loss: 1520.7386
iteration 0000: loss: 1525.113
iteration 0100: loss: 1523.075
iteration 0200: loss: 1526.568
iteration 0300: loss: 1523.746
iteration 0400: loss: 1523.235
iteration 0500: loss: 1527.181
iteration 0600: loss: 1525.894
iteration 0700: loss: 1524.765
iteration 0800: loss: 1522.164
iteration 0900: loss: 1524.665
====> Epoch: 045 Train loss: 1524.8157  took : 5.970353841781616
====> Test loss: 1520.5455
iteration 0000: loss: 1524.954
iteration 0100: loss: 1523.856
iteration 0200: loss: 1523.330
iteration 0300: loss: 1524.214
iteration 0400: loss: 1524.245
iteration 0500: loss: 1525.659
iteration 0600: loss: 1526.029
iteration 0700: loss: 1521.519
iteration 0800: loss: 1525.400
iteration 0900: loss: 1524.171
====> Epoch: 046 Train loss: 1524.8056  took : 5.934304237365723
====> Test loss: 1520.5261
iteration 0000: loss: 1523.817
iteration 0100: loss: 1525.188
iteration 0200: loss: 1524.219
iteration 0300: loss: 1526.284
iteration 0400: loss: 1525.334
iteration 0500: loss: 1523.290
iteration 0600: loss: 1523.560
iteration 0700: loss: 1527.543
iteration 0800: loss: 1525.630
iteration 0900: loss: 1523.465
====> Epoch: 047 Train loss: 1524.7857  took : 5.911506175994873
====> Test loss: 1520.5607
iteration 0000: loss: 1524.510
iteration 0100: loss: 1522.874
iteration 0200: loss: 1524.190
iteration 0300: loss: 1526.396
iteration 0400: loss: 1523.235
iteration 0500: loss: 1526.189
iteration 0600: loss: 1524.764
iteration 0700: loss: 1521.459
iteration 0800: loss: 1526.519
iteration 0900: loss: 1527.320
====> Epoch: 048 Train loss: 1524.7350  took : 5.951728820800781
====> Test loss: 1520.5729
iteration 0000: loss: 1522.898
iteration 0100: loss: 1524.335
iteration 0200: loss: 1523.391
iteration 0300: loss: 1524.510
iteration 0400: loss: 1524.145
iteration 0500: loss: 1524.492
iteration 0600: loss: 1523.141
iteration 0700: loss: 1526.128
iteration 0800: loss: 1522.046
iteration 0900: loss: 1521.842
====> Epoch: 049 Train loss: 1524.6940  took : 5.90718674659729
====> Test loss: 1520.4831
iteration 0000: loss: 1525.460
iteration 0100: loss: 1522.277
iteration 0200: loss: 1526.268
iteration 0300: loss: 1525.549
iteration 0400: loss: 1525.176
iteration 0500: loss: 1524.164
iteration 0600: loss: 1526.387
iteration 0700: loss: 1527.633
iteration 0800: loss: 1523.689
iteration 0900: loss: 1523.757
====> Epoch: 050 Train loss: 1524.7039  took : 5.94206166267395
====> Test loss: 1520.5041
====> [MM-VAE] Time: 449.060s or 00:07:29
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2610.270
iteration 0100: loss: 2095.733
iteration 0200: loss: 2100.085
iteration 0300: loss: 2085.387
iteration 0400: loss: 2069.785
iteration 0500: loss: 2035.785
iteration 0600: loss: 2016.043
iteration 0700: loss: 1987.618
iteration 0800: loss: 1970.116
iteration 0900: loss: 1966.136
====> Epoch: 001 Train loss: 2048.7651  took : 8.768649101257324
====> Test loss: 1959.1633
iteration 0000: loss: 1966.002
iteration 0100: loss: 1964.109
iteration 0200: loss: 1962.896
iteration 0300: loss: 1961.954
iteration 0400: loss: 1960.864
iteration 0500: loss: 1963.366
iteration 0600: loss: 1961.357
iteration 0700: loss: 1959.926
iteration 0800: loss: 1959.300
iteration 0900: loss: 1960.496
====> Epoch: 002 Train loss: 1961.5303  took : 8.795473337173462
====> Test loss: 1955.1158
iteration 0000: loss: 1959.714
iteration 0100: loss: 1959.924
iteration 0200: loss: 1960.547
iteration 0300: loss: 1959.578
iteration 0400: loss: 1959.556
iteration 0500: loss: 1958.826
iteration 0600: loss: 1958.714
iteration 0700: loss: 1959.311
iteration 0800: loss: 1959.252
iteration 0900: loss: 1957.838
====> Epoch: 003 Train loss: 1959.3132  took : 8.750878810882568
====> Test loss: 1953.9333
iteration 0000: loss: 1958.720
iteration 0100: loss: 1958.131
iteration 0200: loss: 1958.176
iteration 0300: loss: 1957.519
iteration 0400: loss: 1958.646
iteration 0500: loss: 1958.398
iteration 0600: loss: 1957.911
iteration 0700: loss: 1958.760
iteration 0800: loss: 1959.936
iteration 0900: loss: 1958.126
====> Epoch: 004 Train loss: 1958.5972  took : 8.767109870910645
====> Test loss: 1952.9738
iteration 0000: loss: 1957.219
iteration 0100: loss: 1957.355
iteration 0200: loss: 1959.624
iteration 0300: loss: 1957.930
iteration 0400: loss: 1960.290
iteration 0500: loss: 1957.709
iteration 0600: loss: 1958.895
iteration 0700: loss: 1958.993
iteration 0800: loss: 1958.723
iteration 0900: loss: 1958.416
====> Epoch: 005 Train loss: 1958.2360  took : 8.758650541305542
====> Test loss: 1953.0040
iteration 0000: loss: 1958.640
iteration 0100: loss: 1957.458
iteration 0200: loss: 1956.763
iteration 0300: loss: 1957.755
iteration 0400: loss: 1958.504
iteration 0500: loss: 1957.503
iteration 0600: loss: 1958.269
iteration 0700: loss: 1957.865
iteration 0800: loss: 1957.274
iteration 0900: loss: 1957.844
====> Epoch: 006 Train loss: 1957.9810  took : 8.668431997299194
====> Test loss: 1952.5079
iteration 0000: loss: 1957.783
iteration 0100: loss: 1958.262
iteration 0200: loss: 1956.960
iteration 0300: loss: 1957.138
iteration 0400: loss: 1958.617
iteration 0500: loss: 1958.067
iteration 0600: loss: 1956.519
iteration 0700: loss: 1957.543
iteration 0800: loss: 1956.214
iteration 0900: loss: 1957.906
====> Epoch: 007 Train loss: 1957.8384  took : 8.583009958267212
====> Test loss: 1952.6872
iteration 0000: loss: 1957.594
iteration 0100: loss: 1957.216
iteration 0200: loss: 1957.594
iteration 0300: loss: 1959.215
iteration 0400: loss: 1957.802
iteration 0500: loss: 1957.714
iteration 0600: loss: 1958.326
iteration 0700: loss: 1957.042
iteration 0800: loss: 1957.354
iteration 0900: loss: 1957.364
====> Epoch: 008 Train loss: 1957.7258  took : 8.649474859237671
====> Test loss: 1952.1363
iteration 0000: loss: 1956.077
iteration 0100: loss: 1957.657
iteration 0200: loss: 1958.456
iteration 0300: loss: 1957.356
iteration 0400: loss: 1957.283
iteration 0500: loss: 1959.129
iteration 0600: loss: 1957.523
iteration 0700: loss: 1957.866
iteration 0800: loss: 1956.233
iteration 0900: loss: 1957.688
====> Epoch: 009 Train loss: 1957.5757  took : 8.638962984085083
====> Test loss: 1952.0969
iteration 0000: loss: 1956.611
iteration 0100: loss: 1956.811
iteration 0200: loss: 1957.430
iteration 0300: loss: 1957.941
iteration 0400: loss: 1956.525
iteration 0500: loss: 1956.829
iteration 0600: loss: 1957.216
iteration 0700: loss: 1957.208
iteration 0800: loss: 1957.464
iteration 0900: loss: 1956.454
====> Epoch: 010 Train loss: 1957.5426  took : 8.664161443710327
====> Test loss: 1952.1728
iteration 0000: loss: 1956.628
iteration 0100: loss: 1957.365
iteration 0200: loss: 1955.908
iteration 0300: loss: 1958.845
iteration 0400: loss: 1956.926
iteration 0500: loss: 1956.711
iteration 0600: loss: 1957.266
iteration 0700: loss: 1957.105
iteration 0800: loss: 1957.960
iteration 0900: loss: 1956.995
====> Epoch: 011 Train loss: 1957.3867  took : 8.725811243057251
====> Test loss: 1952.1371
iteration 0000: loss: 1956.923
iteration 0100: loss: 1957.259
iteration 0200: loss: 1956.721
iteration 0300: loss: 1956.604
iteration 0400: loss: 1956.550
iteration 0500: loss: 1957.582
iteration 0600: loss: 1956.928
iteration 0700: loss: 1956.812
iteration 0800: loss: 1957.300
iteration 0900: loss: 1956.691
====> Epoch: 012 Train loss: 1957.3556  took : 8.73964238166809
====> Test loss: 1952.2796
iteration 0000: loss: 1956.419
iteration 0100: loss: 1956.601
iteration 0200: loss: 1957.821
iteration 0300: loss: 1956.202
iteration 0400: loss: 1958.833
iteration 0500: loss: 1955.543
iteration 0600: loss: 1958.321
iteration 0700: loss: 1958.137
iteration 0800: loss: 1957.931
iteration 0900: loss: 1957.026
====> Epoch: 013 Train loss: 1957.2751  took : 8.738556861877441
====> Test loss: 1952.0570
iteration 0000: loss: 1956.876
iteration 0100: loss: 1956.977
iteration 0200: loss: 1957.650
iteration 0300: loss: 1957.017
iteration 0400: loss: 1957.299
iteration 0500: loss: 1956.862
iteration 0600: loss: 1957.701
iteration 0700: loss: 1957.006
iteration 0800: loss: 1955.183
iteration 0900: loss: 1957.255
====> Epoch: 014 Train loss: 1957.2553  took : 8.656110525131226
====> Test loss: 1951.8709
iteration 0000: loss: 1957.223
iteration 0100: loss: 1957.153
iteration 0200: loss: 1957.182
iteration 0300: loss: 1956.388
iteration 0400: loss: 1957.473
iteration 0500: loss: 1956.461
iteration 0600: loss: 1959.385
iteration 0700: loss: 1956.741
iteration 0800: loss: 1960.852
iteration 0900: loss: 1958.432
====> Epoch: 015 Train loss: 1957.2054  took : 8.671174764633179
====> Test loss: 1952.2660
iteration 0000: loss: 1956.623
iteration 0100: loss: 1956.453
iteration 0200: loss: 1957.427
iteration 0300: loss: 1956.852
iteration 0400: loss: 1957.896
iteration 0500: loss: 1957.004
iteration 0600: loss: 1957.300
iteration 0700: loss: 1956.690
iteration 0800: loss: 1956.747
iteration 0900: loss: 1956.985
====> Epoch: 016 Train loss: 1957.2503  took : 8.662225723266602
====> Test loss: 1951.8967
iteration 0000: loss: 1956.481
iteration 0100: loss: 1961.130
iteration 0200: loss: 1956.841
iteration 0300: loss: 1957.650
iteration 0400: loss: 1956.936
iteration 0500: loss: 1957.261
iteration 0600: loss: 1957.143
iteration 0700: loss: 1957.607
iteration 0800: loss: 1957.656
iteration 0900: loss: 1955.806
====> Epoch: 017 Train loss: 1957.1329  took : 8.766695976257324
====> Test loss: 1951.8076
iteration 0000: loss: 1956.918
iteration 0100: loss: 1956.586
iteration 0200: loss: 1957.581
iteration 0300: loss: 1956.056
iteration 0400: loss: 1956.884
iteration 0500: loss: 1957.380
iteration 0600: loss: 1955.961
iteration 0700: loss: 1956.926
iteration 0800: loss: 1957.015
iteration 0900: loss: 1957.283
====> Epoch: 018 Train loss: 1957.1507  took : 8.666616439819336
====> Test loss: 1951.5232
iteration 0000: loss: 1957.209
iteration 0100: loss: 1956.852
iteration 0200: loss: 1955.309
iteration 0300: loss: 1956.961
iteration 0400: loss: 1957.026
iteration 0500: loss: 1957.457
iteration 0600: loss: 1956.215
iteration 0700: loss: 1956.696
iteration 0800: loss: 1956.378
iteration 0900: loss: 1957.910
====> Epoch: 019 Train loss: 1957.0744  took : 8.740464210510254
====> Test loss: 1952.4622
iteration 0000: loss: 1957.639
iteration 0100: loss: 1956.416
iteration 0200: loss: 1956.573
iteration 0300: loss: 1956.700
iteration 0400: loss: 1956.652
iteration 0500: loss: 1957.099
iteration 0600: loss: 1956.026
iteration 0700: loss: 1956.395
iteration 0800: loss: 1957.391
iteration 0900: loss: 1956.255
====> Epoch: 020 Train loss: 1956.9896  took : 8.665380954742432
====> Test loss: 1951.7283
iteration 0000: loss: 1956.829
iteration 0100: loss: 1957.411
iteration 0200: loss: 1957.547
iteration 0300: loss: 1957.764
iteration 0400: loss: 1957.030
iteration 0500: loss: 1956.739
iteration 0600: loss: 1958.504
iteration 0700: loss: 1956.468
iteration 0800: loss: 1955.608
iteration 0900: loss: 1957.075
====> Epoch: 021 Train loss: 1956.9403  took : 8.62321424484253
====> Test loss: 1951.8899
iteration 0000: loss: 1956.654
iteration 0100: loss: 1956.232
iteration 0200: loss: 1956.704
iteration 0300: loss: 1957.181
iteration 0400: loss: 1957.980
iteration 0500: loss: 1956.791
iteration 0600: loss: 1956.375
iteration 0700: loss: 1957.081
iteration 0800: loss: 1956.912
iteration 0900: loss: 1957.163
====> Epoch: 022 Train loss: 1956.9704  took : 8.787277221679688
====> Test loss: 1951.7025
iteration 0000: loss: 1955.921
iteration 0100: loss: 1956.403
iteration 0200: loss: 1956.843
iteration 0300: loss: 1957.273
iteration 0400: loss: 1955.673
iteration 0500: loss: 1956.871
iteration 0600: loss: 1956.276
iteration 0700: loss: 1957.263
iteration 0800: loss: 1955.639
iteration 0900: loss: 1957.069
====> Epoch: 023 Train loss: 1956.9143  took : 8.731638669967651
====> Test loss: 1951.9234
iteration 0000: loss: 1957.448
iteration 0100: loss: 1956.355
iteration 0200: loss: 1957.167
iteration 0300: loss: 1957.409
iteration 0400: loss: 1956.752
iteration 0500: loss: 1956.618
iteration 0600: loss: 1956.345
iteration 0700: loss: 1958.177
iteration 0800: loss: 1956.500
iteration 0900: loss: 1956.427
====> Epoch: 024 Train loss: 1956.9269  took : 8.729801416397095
====> Test loss: 1951.5851
iteration 0000: loss: 1956.393
iteration 0100: loss: 1956.220
iteration 0200: loss: 1956.286
iteration 0300: loss: 1956.109
iteration 0400: loss: 1957.819
iteration 0500: loss: 1956.556
iteration 0600: loss: 1957.073
iteration 0700: loss: 1956.780
iteration 0800: loss: 1957.339
iteration 0900: loss: 1957.144
====> Epoch: 025 Train loss: 1956.8656  took : 8.659936428070068
====> Test loss: 1951.7161
iteration 0000: loss: 1956.793
iteration 0100: loss: 1956.294
iteration 0200: loss: 1955.983
iteration 0300: loss: 1956.991
iteration 0400: loss: 1956.517
iteration 0500: loss: 1957.208
iteration 0600: loss: 1957.294
iteration 0700: loss: 1957.314
iteration 0800: loss: 1955.958
iteration 0900: loss: 1957.988
====> Epoch: 026 Train loss: 1956.8020  took : 8.914621591567993
====> Test loss: 1951.3582
iteration 0000: loss: 1956.994
iteration 0100: loss: 1957.140
iteration 0200: loss: 1957.252
iteration 0300: loss: 1957.365
iteration 0400: loss: 1956.398
iteration 0500: loss: 1956.756
iteration 0600: loss: 1956.803
iteration 0700: loss: 1956.525
iteration 0800: loss: 1957.442
iteration 0900: loss: 1957.211
====> Epoch: 027 Train loss: 1956.8750  took : 8.730881690979004
====> Test loss: 1951.6328
iteration 0000: loss: 1955.554
iteration 0100: loss: 1955.828
iteration 0200: loss: 1955.919
iteration 0300: loss: 1956.346
iteration 0400: loss: 1956.546
iteration 0500: loss: 1956.607
iteration 0600: loss: 1955.225
iteration 0700: loss: 1956.317
iteration 0800: loss: 1956.701
iteration 0900: loss: 1957.781
====> Epoch: 028 Train loss: 1956.8274  took : 8.76379680633545
====> Test loss: 1951.9502
iteration 0000: loss: 1956.415
iteration 0100: loss: 1956.743
iteration 0200: loss: 1956.188
iteration 0300: loss: 1957.385
iteration 0400: loss: 1956.188
iteration 0500: loss: 1956.975
iteration 0600: loss: 1955.844
iteration 0700: loss: 1956.428
iteration 0800: loss: 1956.352
iteration 0900: loss: 1957.129
====> Epoch: 029 Train loss: 1956.8256  took : 8.744784355163574
====> Test loss: 1951.6146
iteration 0000: loss: 1956.828
iteration 0100: loss: 1955.920
iteration 0200: loss: 1956.735
iteration 0300: loss: 1956.594
iteration 0400: loss: 1956.931
iteration 0500: loss: 1956.511
iteration 0600: loss: 1958.543
iteration 0700: loss: 1956.453
iteration 0800: loss: 1958.252
iteration 0900: loss: 1956.282
====> Epoch: 030 Train loss: 1956.7740  took : 8.873655796051025
====> Test loss: 1951.6547
iteration 0000: loss: 1957.772
iteration 0100: loss: 1956.295
iteration 0200: loss: 1956.786
iteration 0300: loss: 1956.439
iteration 0400: loss: 1957.326
iteration 0500: loss: 1957.663
iteration 0600: loss: 1957.153
iteration 0700: loss: 1955.835
iteration 0800: loss: 1956.193
iteration 0900: loss: 1955.888
====> Epoch: 031 Train loss: 1956.7245  took : 8.71029281616211
====> Test loss: 1951.8183
iteration 0000: loss: 1956.190
iteration 0100: loss: 1956.669
iteration 0200: loss: 1954.998
iteration 0300: loss: 1960.415
iteration 0400: loss: 1957.520
iteration 0500: loss: 1956.017
iteration 0600: loss: 1956.348
iteration 0700: loss: 1957.207
iteration 0800: loss: 1956.732
iteration 0900: loss: 1955.520
====> Epoch: 032 Train loss: 1956.7466  took : 8.624205112457275
====> Test loss: 1951.7120
iteration 0000: loss: 1956.319
iteration 0100: loss: 1955.687
iteration 0200: loss: 1957.890
iteration 0300: loss: 1957.572
iteration 0400: loss: 1958.219
iteration 0500: loss: 1956.198
iteration 0600: loss: 1955.901
iteration 0700: loss: 1957.672
iteration 0800: loss: 1955.627
iteration 0900: loss: 1956.750
====> Epoch: 033 Train loss: 1956.6287  took : 8.707282781600952
====> Test loss: 1951.8413
iteration 0000: loss: 1958.223
iteration 0100: loss: 1956.470
iteration 0200: loss: 1957.017
iteration 0300: loss: 1956.665
iteration 0400: loss: 1956.823
iteration 0500: loss: 1956.834
iteration 0600: loss: 1956.410
iteration 0700: loss: 1957.323
iteration 0800: loss: 1956.090
iteration 0900: loss: 1956.365
====> Epoch: 034 Train loss: 1956.7045  took : 8.64374589920044
====> Test loss: 1951.5742
iteration 0000: loss: 1960.821
iteration 0100: loss: 1954.925
iteration 0200: loss: 1955.807
iteration 0300: loss: 1956.289
iteration 0400: loss: 1956.155
iteration 0500: loss: 1955.487
iteration 0600: loss: 1955.574
iteration 0700: loss: 1956.157
iteration 0800: loss: 1955.820
iteration 0900: loss: 1956.388
====> Epoch: 035 Train loss: 1956.6713  took : 8.753860712051392
====> Test loss: 1951.4195
iteration 0000: loss: 1955.120
iteration 0100: loss: 1955.376
iteration 0200: loss: 1956.958
iteration 0300: loss: 1956.821
iteration 0400: loss: 1955.973
iteration 0500: loss: 1955.936
iteration 0600: loss: 1958.371
iteration 0700: loss: 1956.240
iteration 0800: loss: 1956.285
iteration 0900: loss: 1956.589
====> Epoch: 036 Train loss: 1956.6352  took : 8.631752252578735
====> Test loss: 1951.4374
iteration 0000: loss: 1956.464
iteration 0100: loss: 1956.996
iteration 0200: loss: 1956.641
iteration 0300: loss: 1956.645
iteration 0400: loss: 1957.306
iteration 0500: loss: 1956.757
iteration 0600: loss: 1956.609
iteration 0700: loss: 1956.259
iteration 0800: loss: 1956.994
iteration 0900: loss: 1957.072
====> Epoch: 037 Train loss: 1956.6113  took : 8.735127925872803
====> Test loss: 1951.8178
iteration 0000: loss: 1956.638
iteration 0100: loss: 1956.988
iteration 0200: loss: 1956.908
iteration 0300: loss: 1956.167
iteration 0400: loss: 1956.033
iteration 0500: loss: 1956.101
iteration 0600: loss: 1956.683
iteration 0700: loss: 1956.894
iteration 0800: loss: 1956.076
iteration 0900: loss: 1956.111
====> Epoch: 038 Train loss: 1956.6059  took : 8.726446866989136
====> Test loss: 1951.5993
iteration 0000: loss: 1957.563
iteration 0100: loss: 1958.787
iteration 0200: loss: 1956.780
iteration 0300: loss: 1956.146
iteration 0400: loss: 1955.998
iteration 0500: loss: 1956.359
iteration 0600: loss: 1956.937
iteration 0700: loss: 1957.155
iteration 0800: loss: 1957.307
iteration 0900: loss: 1956.176
====> Epoch: 039 Train loss: 1956.6282  took : 8.711151599884033
====> Test loss: 1951.4766
iteration 0000: loss: 1956.423
iteration 0100: loss: 1956.784
iteration 0200: loss: 1956.896
iteration 0300: loss: 1957.284
iteration 0400: loss: 1956.291
iteration 0500: loss: 1956.457
iteration 0600: loss: 1956.646
iteration 0700: loss: 1956.270
iteration 0800: loss: 1957.059
iteration 0900: loss: 1958.499
====> Epoch: 040 Train loss: 1956.5813  took : 8.631787538528442
====> Test loss: 1951.3606
iteration 0000: loss: 1956.908
iteration 0100: loss: 1957.818
iteration 0200: loss: 1956.944
iteration 0300: loss: 1956.954
iteration 0400: loss: 1956.454
iteration 0500: loss: 1955.945
iteration 0600: loss: 1959.489
iteration 0700: loss: 1956.101
iteration 0800: loss: 1955.808
iteration 0900: loss: 1956.642
====> Epoch: 041 Train loss: 1956.5760  took : 8.63701319694519
====> Test loss: 1951.5250
iteration 0000: loss: 1956.331
iteration 0100: loss: 1956.692
iteration 0200: loss: 1957.120
iteration 0300: loss: 1956.289
iteration 0400: loss: 1955.780
iteration 0500: loss: 1956.310
iteration 0600: loss: 1957.114
iteration 0700: loss: 1956.205
iteration 0800: loss: 1958.089
iteration 0900: loss: 1955.713
====> Epoch: 042 Train loss: 1956.4964  took : 8.921263694763184
====> Test loss: 1951.4385
iteration 0000: loss: 1955.305
iteration 0100: loss: 1957.156
iteration 0200: loss: 1956.921
iteration 0300: loss: 1955.871
iteration 0400: loss: 1955.659
iteration 0500: loss: 1955.128
iteration 0600: loss: 1956.459
iteration 0700: loss: 1958.075
iteration 0800: loss: 1955.662
iteration 0900: loss: 1956.982
====> Epoch: 043 Train loss: 1956.5257  took : 8.65463137626648
====> Test loss: 1951.7394
iteration 0000: loss: 1957.247
iteration 0100: loss: 1957.760
iteration 0200: loss: 1956.787
iteration 0300: loss: 1955.663
iteration 0400: loss: 1957.046
iteration 0500: loss: 1956.235
iteration 0600: loss: 1955.954
iteration 0700: loss: 1957.289
iteration 0800: loss: 1958.067
iteration 0900: loss: 1956.221
====> Epoch: 044 Train loss: 1956.5660  took : 8.707788944244385
====> Test loss: 1951.5195
iteration 0000: loss: 1956.345
iteration 0100: loss: 1956.686
iteration 0200: loss: 1957.049
iteration 0300: loss: 1956.289
iteration 0400: loss: 1955.461
iteration 0500: loss: 1955.733
iteration 0600: loss: 1955.483
iteration 0700: loss: 1956.428
iteration 0800: loss: 1955.704
iteration 0900: loss: 1955.908
====> Epoch: 045 Train loss: 1956.5931  took : 8.730002880096436
====> Test loss: 1951.7578
iteration 0000: loss: 1956.669
iteration 0100: loss: 1956.646
iteration 0200: loss: 1957.241
iteration 0300: loss: 1957.346
iteration 0400: loss: 1956.838
iteration 0500: loss: 1956.540
iteration 0600: loss: 1957.403
iteration 0700: loss: 1955.640
iteration 0800: loss: 1956.206
iteration 0900: loss: 1956.198
====> Epoch: 046 Train loss: 1956.4747  took : 8.668713092803955
====> Test loss: 1951.2535
iteration 0000: loss: 1956.259
iteration 0100: loss: 1956.723
iteration 0200: loss: 1955.761
iteration 0300: loss: 1957.255
iteration 0400: loss: 1956.714
iteration 0500: loss: 1956.039
iteration 0600: loss: 1955.371
iteration 0700: loss: 1956.413
iteration 0800: loss: 1955.898
iteration 0900: loss: 1956.557
====> Epoch: 047 Train loss: 1956.4945  took : 8.7999906539917
====> Test loss: 1951.3977
iteration 0000: loss: 1955.792
iteration 0100: loss: 1956.939
iteration 0200: loss: 1956.044
iteration 0300: loss: 1956.425
iteration 0400: loss: 1955.638
iteration 0500: loss: 1957.491
iteration 0600: loss: 1955.793
iteration 0700: loss: 1956.042
iteration 0800: loss: 1956.453
iteration 0900: loss: 1955.262
====> Epoch: 048 Train loss: 1956.4722  took : 8.705560684204102
====> Test loss: 1951.5789
iteration 0000: loss: 1956.849
iteration 0100: loss: 1956.205
iteration 0200: loss: 1955.938
iteration 0300: loss: 1956.644
iteration 0400: loss: 1956.773
iteration 0500: loss: 1955.404
iteration 0600: loss: 1955.583
iteration 0700: loss: 1956.134
iteration 0800: loss: 1956.467
iteration 0900: loss: 1955.899
====> Epoch: 049 Train loss: 1956.4833  took : 8.772021532058716
====> Test loss: 1951.3456
iteration 0000: loss: 1956.494
iteration 0100: loss: 1955.221
iteration 0200: loss: 1956.280
iteration 0300: loss: 1957.461
iteration 0400: loss: 1956.941
iteration 0500: loss: 1959.967
iteration 0600: loss: 1957.220
iteration 0700: loss: 1955.021
iteration 0800: loss: 1955.864
iteration 0900: loss: 1958.618
====> Epoch: 050 Train loss: 1956.3910  took : 8.844453573226929
====> Test loss: 1951.5173
====> [MM-VAE] Time: 568.495s or 00:09:28
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5261.143
iteration 0100: loss: 4106.641
iteration 0200: loss: 4076.609
iteration 0300: loss: 4026.431
iteration 0400: loss: 4006.611
iteration 0500: loss: 4002.382
iteration 0600: loss: 3995.815
iteration 0700: loss: 3985.512
iteration 0800: loss: 3990.295
iteration 0900: loss: 3978.826
iteration 1000: loss: 3976.574
iteration 1100: loss: 3978.813
iteration 1200: loss: 3971.465
iteration 1300: loss: 3969.015
iteration 1400: loss: 3968.565
iteration 1500: loss: 3967.893
iteration 1600: loss: 3964.256
iteration 1700: loss: 3960.476
iteration 1800: loss: 3965.079
====> Epoch: 001 Train loss: 4003.9931  took : 140.09865927696228
====> Test loss: 3964.7880
iteration 0000: loss: 3955.336
iteration 0100: loss: 3960.708
iteration 0200: loss: 3968.004
iteration 0300: loss: 3963.360
iteration 0400: loss: 3955.591
iteration 0500: loss: 3964.310
iteration 0600: loss: 3961.722
iteration 0700: loss: 3956.956
iteration 0800: loss: 3962.066
iteration 0900: loss: 3949.586
iteration 1000: loss: 3956.657
iteration 1100: loss: 3954.216
iteration 1200: loss: 3949.995
iteration 1300: loss: 3964.499
iteration 1400: loss: 3951.561
iteration 1500: loss: 3958.875
iteration 1600: loss: 3963.749
iteration 1700: loss: 3963.153
iteration 1800: loss: 3957.663
====> Epoch: 002 Train loss: 3958.6878  took : 140.64493560791016
====> Test loss: 3958.9015
iteration 0000: loss: 3959.338
iteration 0100: loss: 3955.968
iteration 0200: loss: 3955.253
iteration 0300: loss: 3959.943
iteration 0400: loss: 3956.951
iteration 0500: loss: 3956.447
iteration 0600: loss: 3966.569
iteration 0700: loss: 3960.329
iteration 0800: loss: 3954.668
iteration 0900: loss: 3949.128
iteration 1000: loss: 3951.828
iteration 1100: loss: 3956.690
iteration 1200: loss: 3956.463
iteration 1300: loss: 3957.034
iteration 1400: loss: 3955.152
iteration 1500: loss: 3956.370
iteration 1600: loss: 3951.014
iteration 1700: loss: 3952.360
iteration 1800: loss: 3951.790
====> Epoch: 003 Train loss: 3955.5020  took : 141.04859066009521
====> Test loss: 3957.3114
iteration 0000: loss: 3954.582
iteration 0100: loss: 3957.058
iteration 0200: loss: 3957.789
iteration 0300: loss: 3958.195
iteration 0400: loss: 3952.684
iteration 0500: loss: 3957.277
iteration 0600: loss: 3953.321
iteration 0700: loss: 3953.937
iteration 0800: loss: 3951.884
iteration 0900: loss: 3947.556
iteration 1000: loss: 3950.799
iteration 1100: loss: 3946.132
iteration 1200: loss: 3955.171
iteration 1300: loss: 3954.875
iteration 1400: loss: 3954.251
iteration 1500: loss: 3956.656
iteration 1600: loss: 3950.100
iteration 1700: loss: 3947.122
iteration 1800: loss: 3947.449
====> Epoch: 004 Train loss: 3953.9555  took : 140.49026918411255
====> Test loss: 3955.8443
iteration 0000: loss: 3950.412
iteration 0100: loss: 3950.287
iteration 0200: loss: 3959.244
iteration 0300: loss: 3950.781
iteration 0400: loss: 3953.185
iteration 0500: loss: 3948.111
iteration 0600: loss: 3953.072
iteration 0700: loss: 3948.592
iteration 0800: loss: 3950.107
iteration 0900: loss: 3951.177
iteration 1000: loss: 3952.247
iteration 1100: loss: 3954.015
iteration 1200: loss: 3956.232
iteration 1300: loss: 3948.684
iteration 1400: loss: 3956.785
iteration 1500: loss: 3947.580
iteration 1600: loss: 3949.376
iteration 1700: loss: 3950.061
iteration 1800: loss: 3950.170
====> Epoch: 005 Train loss: 3952.8354  took : 139.3863286972046
====> Test loss: 3954.9838
iteration 0000: loss: 3955.707
iteration 0100: loss: 3950.525
iteration 0200: loss: 3951.636
iteration 0300: loss: 3954.498
iteration 0400: loss: 3958.747
iteration 0500: loss: 3953.223
iteration 0600: loss: 3958.516
iteration 0700: loss: 3949.888
iteration 0800: loss: 3950.155
iteration 0900: loss: 3952.542
iteration 1000: loss: 3953.069
iteration 1100: loss: 3951.529
iteration 1200: loss: 3952.643
iteration 1300: loss: 3944.974
iteration 1400: loss: 3944.999
iteration 1500: loss: 3949.587
iteration 1600: loss: 3945.854
iteration 1700: loss: 3951.420
iteration 1800: loss: 3950.140
====> Epoch: 006 Train loss: 3951.9926  took : 140.32884788513184
====> Test loss: 3954.5245
iteration 0000: loss: 3951.142
iteration 0100: loss: 3950.740
iteration 0200: loss: 3954.419
iteration 0300: loss: 3960.130
iteration 0400: loss: 3949.733
iteration 0500: loss: 3957.586
iteration 0600: loss: 3949.566
iteration 0700: loss: 3955.189
iteration 0800: loss: 3949.723
iteration 0900: loss: 3955.683
iteration 1000: loss: 3948.293
iteration 1100: loss: 3954.214
iteration 1200: loss: 3950.990
iteration 1300: loss: 3950.417
iteration 1400: loss: 3952.529
iteration 1500: loss: 3951.264
iteration 1600: loss: 3953.648
iteration 1700: loss: 3947.455
iteration 1800: loss: 3956.869
====> Epoch: 007 Train loss: 3951.1984  took : 140.43409276008606
====> Test loss: 3953.4191
iteration 0000: loss: 3951.326
iteration 0100: loss: 3954.696
iteration 0200: loss: 3953.358
iteration 0300: loss: 3947.169
iteration 0400: loss: 3951.256
iteration 0500: loss: 3952.595
iteration 0600: loss: 3951.319
iteration 0700: loss: 3946.365
iteration 0800: loss: 3955.427
iteration 0900: loss: 3959.052
iteration 1000: loss: 3946.642
iteration 1100: loss: 3957.460
iteration 1200: loss: 3953.903
iteration 1300: loss: 3946.964
iteration 1400: loss: 3960.975
iteration 1500: loss: 3950.237
iteration 1600: loss: 3947.104
iteration 1700: loss: 3946.782
iteration 1800: loss: 3952.625
====> Epoch: 008 Train loss: 3950.5922  took : 139.63335943222046
====> Test loss: 3953.1884
iteration 0000: loss: 3953.928
iteration 0100: loss: 3951.281
iteration 0200: loss: 3956.934
iteration 0300: loss: 3946.063
iteration 0400: loss: 3945.239
iteration 0500: loss: 3941.778
iteration 0600: loss: 3953.260
iteration 0700: loss: 3950.648
iteration 0800: loss: 3949.802
iteration 0900: loss: 3952.036
iteration 1000: loss: 3946.792
iteration 1100: loss: 3950.651
iteration 1200: loss: 3950.555
iteration 1300: loss: 3952.118
iteration 1400: loss: 3945.773
iteration 1500: loss: 3951.623
iteration 1600: loss: 3939.949
iteration 1700: loss: 3947.652
iteration 1800: loss: 3944.251
====> Epoch: 009 Train loss: 3950.1396  took : 140.84377551078796
====> Test loss: 3952.9313
iteration 0000: loss: 3950.772
iteration 0100: loss: 3952.850
iteration 0200: loss: 3952.377
iteration 0300: loss: 3952.072
iteration 0400: loss: 3954.775
iteration 0500: loss: 3950.885
iteration 0600: loss: 3949.270
iteration 0700: loss: 3949.018
iteration 0800: loss: 3952.782
iteration 0900: loss: 3949.038
iteration 1000: loss: 3960.061
iteration 1100: loss: 3948.772
iteration 1200: loss: 3949.486
iteration 1300: loss: 3944.456
iteration 1400: loss: 3947.589
iteration 1500: loss: 3955.111
iteration 1600: loss: 3954.328
iteration 1700: loss: 3957.692
iteration 1800: loss: 3941.090
====> Epoch: 010 Train loss: 3949.7601  took : 140.03112649917603
====> Test loss: 3952.2378
iteration 0000: loss: 3942.620
iteration 0100: loss: 3942.578
iteration 0200: loss: 3952.769
iteration 0300: loss: 3950.487
iteration 0400: loss: 3958.858
iteration 0500: loss: 3951.838
iteration 0600: loss: 3952.789
iteration 0700: loss: 3951.120
iteration 0800: loss: 3956.419
iteration 0900: loss: 3952.407
iteration 1000: loss: 3952.276
iteration 1100: loss: 3961.587
iteration 1200: loss: 3950.287
iteration 1300: loss: 3952.170
iteration 1400: loss: 3945.099
iteration 1500: loss: 3945.400
iteration 1600: loss: 3950.432
iteration 1700: loss: 3948.917
iteration 1800: loss: 3953.514
====> Epoch: 011 Train loss: 3949.3073  took : 140.49940156936646
====> Test loss: 3951.9632
iteration 0000: loss: 3955.740
iteration 0100: loss: 3944.046
iteration 0200: loss: 3951.910
iteration 0300: loss: 3950.649
iteration 0400: loss: 3941.773
iteration 0500: loss: 3944.649
iteration 0600: loss: 3946.548
iteration 0700: loss: 3945.127
iteration 0800: loss: 3945.615
iteration 0900: loss: 3951.854
iteration 1000: loss: 3947.344
iteration 1100: loss: 3943.584
iteration 1200: loss: 3947.391
iteration 1300: loss: 3948.229
iteration 1400: loss: 3943.030
iteration 1500: loss: 3949.348
iteration 1600: loss: 3949.233
iteration 1700: loss: 3944.857
iteration 1800: loss: 3947.110
====> Epoch: 012 Train loss: 3948.9579  took : 140.69054985046387
====> Test loss: 3951.8506
iteration 0000: loss: 3947.214
iteration 0100: loss: 3955.391
iteration 0200: loss: 3950.552
iteration 0300: loss: 3949.502
iteration 0400: loss: 3944.520
iteration 0500: loss: 3949.799
iteration 0600: loss: 3945.577
iteration 0700: loss: 3948.623
iteration 0800: loss: 3944.387
iteration 0900: loss: 3946.951
iteration 1000: loss: 3946.612
iteration 1100: loss: 3951.162
iteration 1200: loss: 3942.155
iteration 1300: loss: 3952.114
iteration 1400: loss: 3946.437
iteration 1500: loss: 3947.869
iteration 1600: loss: 3949.332
iteration 1700: loss: 3942.262
iteration 1800: loss: 3947.715
====> Epoch: 013 Train loss: 3948.7221  took : 140.6546287536621
====> Test loss: 3951.3724
iteration 0000: loss: 3944.622
iteration 0100: loss: 3951.729
iteration 0200: loss: 3942.926
iteration 0300: loss: 3945.820
iteration 0400: loss: 3949.693
iteration 0500: loss: 3949.508
iteration 0600: loss: 3949.656
iteration 0700: loss: 3952.894
iteration 0800: loss: 3948.983
iteration 0900: loss: 3944.098
iteration 1000: loss: 3946.842
iteration 1100: loss: 3950.492
iteration 1200: loss: 3947.207
iteration 1300: loss: 3944.977
iteration 1400: loss: 3950.396
iteration 1500: loss: 3939.327
iteration 1600: loss: 3954.888
iteration 1700: loss: 3952.221
iteration 1800: loss: 3943.270
====> Epoch: 014 Train loss: 3948.3953  took : 140.88705897331238
====> Test loss: 3951.2145
iteration 0000: loss: 3951.129
iteration 0100: loss: 3949.810
iteration 0200: loss: 3948.732
iteration 0300: loss: 3946.027
iteration 0400: loss: 3948.847
iteration 0500: loss: 3943.965
iteration 0600: loss: 3954.052
iteration 0700: loss: 3949.199
iteration 0800: loss: 3944.305
iteration 0900: loss: 3950.066
iteration 1000: loss: 3950.548
iteration 1100: loss: 3944.281
iteration 1200: loss: 3945.841
iteration 1300: loss: 3946.000
iteration 1400: loss: 3951.161
iteration 1500: loss: 3948.404
iteration 1600: loss: 3951.299
iteration 1700: loss: 3952.595
iteration 1800: loss: 3949.883
====> Epoch: 015 Train loss: 3948.1978  took : 140.64735889434814
====> Test loss: 3951.1888
iteration 0000: loss: 3949.554
iteration 0100: loss: 3956.792
iteration 0200: loss: 3951.932
iteration 0300: loss: 3949.530
iteration 0400: loss: 3945.330
iteration 0500: loss: 3952.172
iteration 0600: loss: 3947.733
iteration 0700: loss: 3944.050
iteration 0800: loss: 3948.892
iteration 0900: loss: 3944.364
iteration 1000: loss: 3952.875
iteration 1100: loss: 3954.872
iteration 1200: loss: 3952.889
iteration 1300: loss: 3948.407
iteration 1400: loss: 3949.384
iteration 1500: loss: 3951.041
iteration 1600: loss: 3945.505
iteration 1700: loss: 3948.167
iteration 1800: loss: 3944.013
====> Epoch: 016 Train loss: 3947.9118  took : 140.51288437843323
====> Test loss: 3950.8824
iteration 0000: loss: 3945.949
iteration 0100: loss: 3948.907
iteration 0200: loss: 3949.809
iteration 0300: loss: 3942.155
iteration 0400: loss: 3944.296
iteration 0500: loss: 3949.845
iteration 0600: loss: 3946.429
iteration 0700: loss: 3942.397
iteration 0800: loss: 3947.080
iteration 0900: loss: 3946.708
iteration 1000: loss: 3943.632
iteration 1100: loss: 3946.527
iteration 1200: loss: 3946.401
iteration 1300: loss: 3941.237
iteration 1400: loss: 3950.531
iteration 1500: loss: 3949.245
iteration 1600: loss: 3946.969
iteration 1700: loss: 3941.826
iteration 1800: loss: 3947.375
====> Epoch: 017 Train loss: 3947.7980  took : 140.76587629318237
====> Test loss: 3950.7274
iteration 0000: loss: 3949.205
iteration 0100: loss: 3949.646
iteration 0200: loss: 3949.657
iteration 0300: loss: 3943.405
iteration 0400: loss: 3944.922
iteration 0500: loss: 3945.020
iteration 0600: loss: 3944.005
iteration 0700: loss: 3948.511
iteration 0800: loss: 3946.930
iteration 0900: loss: 3949.033
iteration 1000: loss: 3939.878
iteration 1100: loss: 3948.022
iteration 1200: loss: 3948.499
iteration 1300: loss: 3946.615
iteration 1400: loss: 3950.235
iteration 1500: loss: 3951.050
iteration 1600: loss: 3943.235
iteration 1700: loss: 3940.781
iteration 1800: loss: 3948.630
====> Epoch: 018 Train loss: 3947.6053  took : 140.84883832931519
====> Test loss: 3950.6659
iteration 0000: loss: 3944.087
iteration 0100: loss: 3956.063
iteration 0200: loss: 3943.627
iteration 0300: loss: 3946.829
iteration 0400: loss: 3947.092
iteration 0500: loss: 3941.072
iteration 0600: loss: 3948.018
iteration 0700: loss: 3950.373
iteration 0800: loss: 3954.149
iteration 0900: loss: 3954.947
iteration 1000: loss: 3943.716
iteration 1100: loss: 3947.926
iteration 1200: loss: 3946.678
iteration 1300: loss: 3948.541
iteration 1400: loss: 3951.720
iteration 1500: loss: 3946.172
iteration 1600: loss: 3948.941
iteration 1700: loss: 3949.063
iteration 1800: loss: 3948.986
====> Epoch: 019 Train loss: 3947.3439  took : 138.85549068450928
====> Test loss: 3950.4753
iteration 0000: loss: 3947.581
iteration 0100: loss: 3943.367
iteration 0200: loss: 3949.120
iteration 0300: loss: 3948.156
iteration 0400: loss: 3948.558
iteration 0500: loss: 3950.959
iteration 0600: loss: 3950.879
iteration 0700: loss: 3943.212
iteration 0800: loss: 3953.026
iteration 0900: loss: 3948.198
iteration 1000: loss: 3944.552
iteration 1100: loss: 3954.359
iteration 1200: loss: 3952.413
iteration 1300: loss: 3944.479
iteration 1400: loss: 3939.782
iteration 1500: loss: 3944.657
iteration 1600: loss: 3944.279
iteration 1700: loss: 3947.676
iteration 1800: loss: 3944.788
====> Epoch: 020 Train loss: 3947.1745  took : 140.84160137176514
====> Test loss: 3949.9237
iteration 0000: loss: 3949.463
iteration 0100: loss: 3949.023
iteration 0200: loss: 3953.127
iteration 0300: loss: 3949.584
iteration 0400: loss: 3953.212
iteration 0500: loss: 3948.029
iteration 0600: loss: 3943.826
iteration 0700: loss: 3951.382
iteration 0800: loss: 3948.013
iteration 0900: loss: 3945.707
iteration 1000: loss: 3945.690
iteration 1100: loss: 3950.839
iteration 1200: loss: 3944.122
iteration 1300: loss: 3949.720
iteration 1400: loss: 3953.201
iteration 1500: loss: 3944.884
iteration 1600: loss: 3950.003
iteration 1700: loss: 3943.457
iteration 1800: loss: 3951.105
====> Epoch: 021 Train loss: 3946.7272  took : 140.7984709739685
====> Test loss: 3949.7311
iteration 0000: loss: 3946.229
iteration 0100: loss: 3940.906
iteration 0200: loss: 3945.775
iteration 0300: loss: 3946.588
iteration 0400: loss: 3948.574
iteration 0500: loss: 3952.562
iteration 0600: loss: 3946.071
iteration 0700: loss: 3942.634
iteration 0800: loss: 3946.705
iteration 0900: loss: 3942.839
iteration 1000: loss: 3946.709
iteration 1100: loss: 3946.643
iteration 1200: loss: 3945.993
iteration 1300: loss: 3947.028
iteration 1400: loss: 3947.299
iteration 1500: loss: 3946.999
iteration 1600: loss: 3942.658
iteration 1700: loss: 3950.927
iteration 1800: loss: 3947.738
====> Epoch: 022 Train loss: 3946.5746  took : 140.61519575119019
====> Test loss: 3949.6893
iteration 0000: loss: 3941.813
iteration 0100: loss: 3946.765
iteration 0200: loss: 3946.352
iteration 0300: loss: 3947.895
iteration 0400: loss: 3946.023
iteration 0500: loss: 3946.001
iteration 0600: loss: 3947.273
iteration 0700: loss: 3942.124
iteration 0800: loss: 3946.478
iteration 0900: loss: 3945.022
iteration 1000: loss: 3941.640
iteration 1100: loss: 3947.996
iteration 1200: loss: 3943.750
iteration 1300: loss: 3947.782
iteration 1400: loss: 3943.371
iteration 1500: loss: 3941.189
iteration 1600: loss: 3945.191
iteration 1700: loss: 3949.220
iteration 1800: loss: 3942.193
====> Epoch: 023 Train loss: 3946.3971  took : 140.40089416503906
====> Test loss: 3949.5541
iteration 0000: loss: 3946.644
iteration 0100: loss: 3948.109
iteration 0200: loss: 3945.198
iteration 0300: loss: 3937.602
iteration 0400: loss: 3943.890
iteration 0500: loss: 3953.293
iteration 0600: loss: 3948.614
iteration 0700: loss: 3950.307
iteration 0800: loss: 3944.230
iteration 0900: loss: 3945.528
iteration 1000: loss: 3947.241
iteration 1100: loss: 3944.197
iteration 1200: loss: 3941.648
iteration 1300: loss: 3945.563
iteration 1400: loss: 3941.180
iteration 1500: loss: 3940.975
iteration 1600: loss: 3943.163
iteration 1700: loss: 3942.231
iteration 1800: loss: 3948.579
====> Epoch: 024 Train loss: 3946.1404  took : 140.5028715133667
====> Test loss: 3949.4065
iteration 0000: loss: 3948.484
iteration 0100: loss: 3944.532
iteration 0200: loss: 3948.686
iteration 0300: loss: 3949.083
iteration 0400: loss: 3943.108
iteration 0500: loss: 3939.620
iteration 0600: loss: 3945.996
iteration 0700: loss: 3950.266
iteration 0800: loss: 3944.831
iteration 0900: loss: 3946.267
iteration 1000: loss: 3947.559
iteration 1100: loss: 3947.879
iteration 1200: loss: 3953.657
iteration 1300: loss: 3948.756
iteration 1400: loss: 3942.564
iteration 1500: loss: 3939.774
iteration 1600: loss: 3949.473
iteration 1700: loss: 3947.445
iteration 1800: loss: 3941.199
====> Epoch: 025 Train loss: 3946.0766  took : 139.48875665664673
====> Test loss: 3949.2721
iteration 0000: loss: 3945.941
iteration 0100: loss: 3940.232
iteration 0200: loss: 3941.812
iteration 0300: loss: 3942.030
iteration 0400: loss: 3948.748
iteration 0500: loss: 3954.093
iteration 0600: loss: 3942.553
iteration 0700: loss: 3939.541
iteration 0800: loss: 3946.063
iteration 0900: loss: 3946.451
iteration 1000: loss: 3948.157
iteration 1100: loss: 3939.651
iteration 1200: loss: 3952.586
iteration 1300: loss: 3944.752
iteration 1400: loss: 3947.636
iteration 1500: loss: 3943.035
iteration 1600: loss: 3947.919
iteration 1700: loss: 3949.785
iteration 1800: loss: 3950.240
====> Epoch: 026 Train loss: 3945.9270  took : 140.24507188796997
====> Test loss: 3948.9533
iteration 0000: loss: 3950.130
iteration 0100: loss: 3942.175
iteration 0200: loss: 3943.076
iteration 0300: loss: 3946.926
iteration 0400: loss: 3949.555
iteration 0500: loss: 3944.385
iteration 0600: loss: 3946.121
iteration 0700: loss: 3953.006
iteration 0800: loss: 3948.464
iteration 0900: loss: 3942.653
iteration 1000: loss: 3943.401
iteration 1100: loss: 3943.085
iteration 1200: loss: 3947.840
iteration 1300: loss: 3952.858
iteration 1400: loss: 3939.043
iteration 1500: loss: 3945.366
iteration 1600: loss: 3943.724
iteration 1700: loss: 3944.168
iteration 1800: loss: 3947.084
====> Epoch: 027 Train loss: 3945.8255  took : 140.6783127784729
====> Test loss: 3948.8412
iteration 0000: loss: 3947.792
iteration 0100: loss: 3945.191
iteration 0200: loss: 3944.839
iteration 0300: loss: 3950.082
iteration 0400: loss: 3946.462
iteration 0500: loss: 3950.823
iteration 0600: loss: 3944.178
iteration 0700: loss: 3944.048
iteration 0800: loss: 3950.514
iteration 0900: loss: 3949.447
iteration 1000: loss: 3945.859
iteration 1100: loss: 3941.633
iteration 1200: loss: 3946.633
iteration 1300: loss: 3941.926
iteration 1400: loss: 3942.627
iteration 1500: loss: 3946.729
iteration 1600: loss: 3948.567
iteration 1700: loss: 3943.363
iteration 1800: loss: 3941.676
====> Epoch: 028 Train loss: 3945.6914  took : 140.77876448631287
====> Test loss: 3948.7398
iteration 0000: loss: 3945.663
iteration 0100: loss: 3942.102
iteration 0200: loss: 3948.598
iteration 0300: loss: 3951.759
iteration 0400: loss: 3942.493
iteration 0500: loss: 3944.249
iteration 0600: loss: 3948.108
iteration 0700: loss: 3945.285
iteration 0800: loss: 3943.604
iteration 0900: loss: 3950.208
iteration 1000: loss: 3948.817
iteration 1100: loss: 3944.047
iteration 1200: loss: 3946.992
iteration 1300: loss: 3944.705
iteration 1400: loss: 3942.833
iteration 1500: loss: 3944.865
iteration 1600: loss: 3945.911
iteration 1700: loss: 3951.093
iteration 1800: loss: 3950.596
====> Epoch: 029 Train loss: 3945.5745  took : 140.17710733413696
====> Test loss: 3948.5311
iteration 0000: loss: 3944.709
iteration 0100: loss: 3942.706
iteration 0200: loss: 3941.021
iteration 0300: loss: 3955.185
iteration 0400: loss: 3947.979
iteration 0500: loss: 3953.684
iteration 0600: loss: 3948.116
iteration 0700: loss: 3953.003
iteration 0800: loss: 3946.398
iteration 0900: loss: 3947.578
iteration 1000: loss: 3946.648
iteration 1100: loss: 3949.605
iteration 1200: loss: 3950.833
iteration 1300: loss: 3951.429
iteration 1400: loss: 3942.350
iteration 1500: loss: 3943.645
iteration 1600: loss: 3945.136
iteration 1700: loss: 3945.995
iteration 1800: loss: 3952.297
====> Epoch: 030 Train loss: 3945.4998  took : 139.99045610427856
====> Test loss: 3948.8426
iteration 0000: loss: 3940.504
iteration 0100: loss: 3946.153
iteration 0200: loss: 3938.950
iteration 0300: loss: 3939.512
iteration 0400: loss: 3950.466
iteration 0500: loss: 3944.709
iteration 0600: loss: 3945.457
iteration 0700: loss: 3952.616
iteration 0800: loss: 3947.881
iteration 0900: loss: 3944.962
iteration 1000: loss: 3949.969
iteration 1100: loss: 3944.392
iteration 1200: loss: 3948.693
iteration 1300: loss: 3945.505
iteration 1400: loss: 3950.823
iteration 1500: loss: 3944.234
iteration 1600: loss: 3938.482
iteration 1700: loss: 3947.730
iteration 1800: loss: 3942.454
====> Epoch: 031 Train loss: 3945.3551  took : 140.54624605178833
====> Test loss: 3948.4818
iteration 0000: loss: 3942.041
iteration 0100: loss: 3941.849
iteration 0200: loss: 3950.254
iteration 0300: loss: 3944.907
iteration 0400: loss: 3943.761
iteration 0500: loss: 3947.467
iteration 0600: loss: 3942.828
iteration 0700: loss: 3943.035
iteration 0800: loss: 3950.283
iteration 0900: loss: 3947.742
iteration 1000: loss: 3946.432
iteration 1100: loss: 3947.631
iteration 1200: loss: 3943.148
iteration 1300: loss: 3942.483
iteration 1400: loss: 3943.086
iteration 1500: loss: 3943.444
iteration 1600: loss: 3946.278
iteration 1700: loss: 3949.829
iteration 1800: loss: 3952.011
====> Epoch: 032 Train loss: 3945.2521  took : 140.81160259246826
====> Test loss: 3948.4390
iteration 0000: loss: 3944.083
iteration 0100: loss: 3934.424
iteration 0200: loss: 3949.397
iteration 0300: loss: 3944.950
iteration 0400: loss: 3947.594
iteration 0500: loss: 3944.990
iteration 0600: loss: 3944.788
iteration 0700: loss: 3936.786
iteration 0800: loss: 3956.653
iteration 0900: loss: 3940.890
iteration 1000: loss: 3937.856
iteration 1100: loss: 3946.895
iteration 1200: loss: 3951.094
iteration 1300: loss: 3937.756
iteration 1400: loss: 3944.959
iteration 1500: loss: 3946.972
iteration 1600: loss: 3939.980
iteration 1700: loss: 3942.040
iteration 1800: loss: 3943.188
====> Epoch: 033 Train loss: 3945.1149  took : 140.49191451072693
====> Test loss: 3948.4547
iteration 0000: loss: 3946.649
iteration 0100: loss: 3944.652
iteration 0200: loss: 3941.122
iteration 0300: loss: 3946.047
iteration 0400: loss: 3940.809
iteration 0500: loss: 3951.767
iteration 0600: loss: 3950.665
iteration 0700: loss: 3947.276
iteration 0800: loss: 3943.552
iteration 0900: loss: 3941.058
iteration 1000: loss: 3948.806
iteration 1100: loss: 3943.870
iteration 1200: loss: 3946.927
iteration 1300: loss: 3946.213
iteration 1400: loss: 3942.088
iteration 1500: loss: 3946.304
iteration 1600: loss: 3948.862
iteration 1700: loss: 3950.554
iteration 1800: loss: 3944.951
====> Epoch: 034 Train loss: 3945.0374  took : 139.89874148368835
====> Test loss: 3948.3624
iteration 0000: loss: 3944.027
iteration 0100: loss: 3946.027
iteration 0200: loss: 3951.322
iteration 0300: loss: 3947.896
iteration 0400: loss: 3942.115
iteration 0500: loss: 3947.429
iteration 0600: loss: 3937.788
iteration 0700: loss: 3938.038
iteration 0800: loss: 3943.545
iteration 0900: loss: 3945.874
iteration 1000: loss: 3939.803
iteration 1100: loss: 3942.008
iteration 1200: loss: 3943.627
iteration 1300: loss: 3943.491
iteration 1400: loss: 3944.952
iteration 1500: loss: 3954.342
iteration 1600: loss: 3946.874
iteration 1700: loss: 3953.008
iteration 1800: loss: 3944.206
====> Epoch: 035 Train loss: 3944.9460  took : 140.3378221988678
====> Test loss: 3948.5392
iteration 0000: loss: 3942.064
iteration 0100: loss: 3941.865
iteration 0200: loss: 3942.612
iteration 0300: loss: 3942.255
iteration 0400: loss: 3940.820
iteration 0500: loss: 3949.010
iteration 0600: loss: 3940.844
iteration 0700: loss: 3940.009
iteration 0800: loss: 3947.117
iteration 0900: loss: 3943.990
iteration 1000: loss: 3944.450
iteration 1100: loss: 3944.532
iteration 1200: loss: 3941.462
iteration 1300: loss: 3942.177
iteration 1400: loss: 3942.674
iteration 1500: loss: 3938.517
iteration 1600: loss: 3944.541
iteration 1700: loss: 3948.392
iteration 1800: loss: 3944.662
====> Epoch: 036 Train loss: 3944.8817  took : 140.45141887664795
====> Test loss: 3948.1136
iteration 0000: loss: 3945.519
iteration 0100: loss: 3943.092
iteration 0200: loss: 3941.139
iteration 0300: loss: 3950.293
iteration 0400: loss: 3950.755
iteration 0500: loss: 3949.415
iteration 0600: loss: 3942.294
iteration 0700: loss: 3945.091
iteration 0800: loss: 3945.322
iteration 0900: loss: 3942.028
iteration 1000: loss: 3943.944
iteration 1100: loss: 3941.149
iteration 1200: loss: 3945.151
iteration 1300: loss: 3939.845
iteration 1400: loss: 3941.667
iteration 1500: loss: 3944.300
iteration 1600: loss: 3946.971
iteration 1700: loss: 3944.902
iteration 1800: loss: 3944.367
====> Epoch: 037 Train loss: 3944.7478  took : 141.04182243347168
====> Test loss: 3947.9939
iteration 0000: loss: 3937.079
iteration 0100: loss: 3941.645
iteration 0200: loss: 3945.403
iteration 0300: loss: 3944.784
iteration 0400: loss: 3944.493
iteration 0500: loss: 3941.046
iteration 0600: loss: 3943.889
iteration 0700: loss: 3946.544
iteration 0800: loss: 3947.861
iteration 0900: loss: 3947.771
iteration 1000: loss: 3946.027
iteration 1100: loss: 3950.387
iteration 1200: loss: 3950.471
iteration 1300: loss: 3942.967
iteration 1400: loss: 3935.851
iteration 1500: loss: 3939.410
iteration 1600: loss: 3942.245
iteration 1700: loss: 3946.077
iteration 1800: loss: 3941.229
====> Epoch: 038 Train loss: 3944.6098  took : 140.7456488609314
====> Test loss: 3948.0762
iteration 0000: loss: 3943.960
iteration 0100: loss: 3949.660
iteration 0200: loss: 3949.862
iteration 0300: loss: 3946.520
iteration 0400: loss: 3946.448
iteration 0500: loss: 3949.846
iteration 0600: loss: 3950.298
iteration 0700: loss: 3945.630
iteration 0800: loss: 3946.629
iteration 0900: loss: 3941.517
iteration 1000: loss: 3949.699
iteration 1100: loss: 3945.073
iteration 1200: loss: 3950.755
iteration 1300: loss: 3943.005
iteration 1400: loss: 3944.235
iteration 1500: loss: 3948.585
iteration 1600: loss: 3953.454
iteration 1700: loss: 3948.357
iteration 1800: loss: 3948.715
====> Epoch: 039 Train loss: 3944.5530  took : 140.53855061531067
====> Test loss: 3947.8024
iteration 0000: loss: 3946.578
iteration 0100: loss: 3944.162
iteration 0200: loss: 3945.335
iteration 0300: loss: 3938.338
iteration 0400: loss: 3940.870
iteration 0500: loss: 3940.094
iteration 0600: loss: 3945.234
iteration 0700: loss: 3947.498
iteration 0800: loss: 3941.419
iteration 0900: loss: 3942.023
iteration 1000: loss: 3952.188
iteration 1100: loss: 3942.993
iteration 1200: loss: 3940.546
iteration 1300: loss: 3941.407
iteration 1400: loss: 3949.289
iteration 1500: loss: 3940.975
iteration 1600: loss: 3944.597
iteration 1700: loss: 3951.285
iteration 1800: loss: 3945.858
====> Epoch: 040 Train loss: 3944.5072  took : 140.59503364562988
====> Test loss: 3947.6574
iteration 0000: loss: 3945.314
iteration 0100: loss: 3942.691
iteration 0200: loss: 3941.828
iteration 0300: loss: 3939.114
iteration 0400: loss: 3947.000
iteration 0500: loss: 3945.614
iteration 0600: loss: 3945.471
iteration 0700: loss: 3942.649
iteration 0800: loss: 3947.480
iteration 0900: loss: 3947.757
iteration 1000: loss: 3946.555
iteration 1100: loss: 3946.942
iteration 1200: loss: 3941.057
iteration 1300: loss: 3937.619
iteration 1400: loss: 3938.358
iteration 1500: loss: 3943.889
iteration 1600: loss: 3953.381
iteration 1700: loss: 3944.720
iteration 1800: loss: 3946.805
====> Epoch: 041 Train loss: 3944.3038  took : 140.79493975639343
====> Test loss: 3947.7085
iteration 0000: loss: 3945.125
iteration 0100: loss: 3942.600
iteration 0200: loss: 3947.638
iteration 0300: loss: 3947.357
iteration 0400: loss: 3952.816
iteration 0500: loss: 3944.933
iteration 0600: loss: 3940.173
iteration 0700: loss: 3948.006
iteration 0800: loss: 3944.011
iteration 0900: loss: 3948.337
iteration 1000: loss: 3947.456
iteration 1100: loss: 3945.175
iteration 1200: loss: 3941.090
iteration 1300: loss: 3938.456
iteration 1400: loss: 3939.184
iteration 1500: loss: 3942.919
iteration 1600: loss: 3939.813
iteration 1700: loss: 3942.340
iteration 1800: loss: 3945.042
====> Epoch: 042 Train loss: 3944.2256  took : 140.96451020240784
====> Test loss: 3947.5183
iteration 0000: loss: 3941.633
iteration 0100: loss: 3946.756
iteration 0200: loss: 3945.082
iteration 0300: loss: 3942.779
iteration 0400: loss: 3945.135
iteration 0500: loss: 3938.788
iteration 0600: loss: 3940.750
iteration 0700: loss: 3946.305
iteration 0800: loss: 3939.322
iteration 0900: loss: 3948.596
iteration 1000: loss: 3943.995
iteration 1100: loss: 3943.953
iteration 1200: loss: 3940.722
iteration 1300: loss: 3944.358
iteration 1400: loss: 3941.486
iteration 1500: loss: 3942.056
iteration 1600: loss: 3938.593
iteration 1700: loss: 3945.153
iteration 1800: loss: 3943.051
====> Epoch: 043 Train loss: 3944.2130  took : 139.0897183418274
====> Test loss: 3947.4218
iteration 0000: loss: 3945.829
iteration 0100: loss: 3947.243
iteration 0200: loss: 3945.524
iteration 0300: loss: 3940.446
iteration 0400: loss: 3936.093
iteration 0500: loss: 3948.641
iteration 0600: loss: 3945.687
iteration 0700: loss: 3945.280
iteration 0800: loss: 3940.088
iteration 0900: loss: 3951.019
iteration 1000: loss: 3939.872
iteration 1100: loss: 3947.868
iteration 1200: loss: 3943.410
iteration 1300: loss: 3940.798
iteration 1400: loss: 3946.884
iteration 1500: loss: 3940.959
iteration 1600: loss: 3944.521
iteration 1700: loss: 3949.417
iteration 1800: loss: 3944.424
====> Epoch: 044 Train loss: 3944.0107  took : 140.3463795185089
====> Test loss: 3947.4189
iteration 0000: loss: 3952.862
iteration 0100: loss: 3948.204
iteration 0200: loss: 3941.046
iteration 0300: loss: 3946.688
iteration 0400: loss: 3940.892
iteration 0500: loss: 3941.948
iteration 0600: loss: 3944.711
iteration 0700: loss: 3940.826
iteration 0800: loss: 3942.538
iteration 0900: loss: 3942.088
iteration 1000: loss: 3948.794
iteration 1100: loss: 3946.891
iteration 1200: loss: 3941.597
iteration 1300: loss: 3938.794
iteration 1400: loss: 3943.392
iteration 1500: loss: 3944.510
iteration 1600: loss: 3939.681
iteration 1700: loss: 3941.691
iteration 1800: loss: 3939.960
====> Epoch: 045 Train loss: 3943.9945  took : 140.71981835365295
====> Test loss: 3947.6173
iteration 0000: loss: 3946.561
iteration 0100: loss: 3937.043
iteration 0200: loss: 3941.620
iteration 0300: loss: 3943.784
iteration 0400: loss: 3939.491
iteration 0500: loss: 3936.409
iteration 0600: loss: 3946.796
iteration 0700: loss: 3947.685
iteration 0800: loss: 3940.392
iteration 0900: loss: 3943.438
iteration 1000: loss: 3934.202
iteration 1100: loss: 3945.845
iteration 1200: loss: 3943.369
iteration 1300: loss: 3939.811
iteration 1400: loss: 3944.259
iteration 1500: loss: 3942.608
iteration 1600: loss: 3939.919
iteration 1700: loss: 3941.491
iteration 1800: loss: 3948.472
====> Epoch: 046 Train loss: 3943.8875  took : 140.73626565933228
====> Test loss: 3947.4524
iteration 0000: loss: 3939.579
iteration 0100: loss: 3938.449
iteration 0200: loss: 3949.086
iteration 0300: loss: 3953.039
iteration 0400: loss: 3942.737
iteration 0500: loss: 3947.922
iteration 0600: loss: 3943.019
iteration 0700: loss: 3944.679
iteration 0800: loss: 3941.438
iteration 0900: loss: 3934.222
iteration 1000: loss: 3942.465
iteration 1100: loss: 3941.375
iteration 1200: loss: 3950.777
iteration 1300: loss: 3948.385
iteration 1400: loss: 3939.672
iteration 1500: loss: 3937.042
iteration 1600: loss: 3938.877
iteration 1700: loss: 3942.348
iteration 1800: loss: 3945.125
====> Epoch: 047 Train loss: 3943.7965  took : 140.84476828575134
====> Test loss: 3947.0839
iteration 0000: loss: 3940.162
iteration 0100: loss: 3936.479
iteration 0200: loss: 3946.371
iteration 0300: loss: 3941.695
iteration 0400: loss: 3938.172
iteration 0500: loss: 3945.354
iteration 0600: loss: 3945.486
iteration 0700: loss: 3941.701
iteration 0800: loss: 3941.638
iteration 0900: loss: 3941.368
iteration 1000: loss: 3946.030
iteration 1100: loss: 3944.135
iteration 1200: loss: 3942.643
iteration 1300: loss: 3947.818
iteration 1400: loss: 3948.957
iteration 1500: loss: 3948.768
iteration 1600: loss: 3941.624
iteration 1700: loss: 3945.605
iteration 1800: loss: 3943.871
====> Epoch: 048 Train loss: 3943.8449  took : 140.245463848114
====> Test loss: 3947.2283
iteration 0000: loss: 3945.868
iteration 0100: loss: 3944.082
iteration 0200: loss: 3943.390
iteration 0300: loss: 3941.865
iteration 0400: loss: 3948.245
iteration 0500: loss: 3944.692
iteration 0600: loss: 3938.392
iteration 0700: loss: 3941.812
iteration 0800: loss: 3939.969
iteration 0900: loss: 3943.653
iteration 1000: loss: 3940.563
iteration 1100: loss: 3943.735
iteration 1200: loss: 3950.250
iteration 1300: loss: 3949.721
iteration 1400: loss: 3941.875
iteration 1500: loss: 3944.912
iteration 1600: loss: 3943.471
iteration 1700: loss: 3941.786
iteration 1800: loss: 3940.196
====> Epoch: 049 Train loss: 3943.6554  took : 139.03905701637268
====> Test loss: 3946.8819
iteration 0000: loss: 3937.030
iteration 0100: loss: 3946.513
iteration 0200: loss: 3941.685
iteration 0300: loss: 3942.729
iteration 0400: loss: 3939.731
iteration 0500: loss: 3940.803
iteration 0600: loss: 3941.904
iteration 0700: loss: 3945.875
iteration 0800: loss: 3947.306
iteration 0900: loss: 3940.658
iteration 1000: loss: 3943.161
iteration 1100: loss: 3950.861
iteration 1200: loss: 3947.943
iteration 1300: loss: 3944.804
iteration 1400: loss: 3940.928
iteration 1500: loss: 3945.915
iteration 1600: loss: 3945.643
iteration 1700: loss: 3940.583
iteration 1800: loss: 3940.650
====> Epoch: 050 Train loss: 3943.4756  took : 139.852689743042
====> Test loss: 3947.1463
====> [MM-VAE] Time: 8051.517s or 02:14:11
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_3
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1994.771
iteration 0100: loss: 1570.413
iteration 0200: loss: 1550.970
iteration 0300: loss: 1551.916
iteration 0400: loss: 1548.214
iteration 0500: loss: 1545.278
iteration 0600: loss: 1545.524
iteration 0700: loss: 1538.689
iteration 0800: loss: 1538.140
iteration 0900: loss: 1538.337
====> Epoch: 001 Train loss: 1551.9740  took : 6.0752058029174805
====> Test loss: 1531.6960
iteration 0000: loss: 1536.637
iteration 0100: loss: 1530.525
iteration 0200: loss: 1535.387
iteration 0300: loss: 1534.885
iteration 0400: loss: 1533.961
iteration 0500: loss: 1534.845
iteration 0600: loss: 1533.419
iteration 0700: loss: 1532.774
iteration 0800: loss: 1529.126
iteration 0900: loss: 1534.966
====> Epoch: 002 Train loss: 1534.2397  took : 5.922759771347046
====> Test loss: 1527.6137
iteration 0000: loss: 1531.979
iteration 0100: loss: 1532.389
iteration 0200: loss: 1532.771
iteration 0300: loss: 1532.626
iteration 0400: loss: 1531.602
iteration 0500: loss: 1533.244
iteration 0600: loss: 1531.850
iteration 0700: loss: 1532.734
iteration 0800: loss: 1531.214
iteration 0900: loss: 1531.485
====> Epoch: 003 Train loss: 1531.5924  took : 5.877335071563721
====> Test loss: 1525.9342
iteration 0000: loss: 1528.474
iteration 0100: loss: 1531.778
iteration 0200: loss: 1530.680
iteration 0300: loss: 1526.925
iteration 0400: loss: 1529.867
iteration 0500: loss: 1529.866
iteration 0600: loss: 1528.250
iteration 0700: loss: 1529.610
iteration 0800: loss: 1529.957
iteration 0900: loss: 1527.829
====> Epoch: 004 Train loss: 1530.3446  took : 5.905361175537109
====> Test loss: 1524.9801
iteration 0000: loss: 1531.275
iteration 0100: loss: 1528.609
iteration 0200: loss: 1531.479
iteration 0300: loss: 1533.745
iteration 0400: loss: 1527.099
iteration 0500: loss: 1531.224
iteration 0600: loss: 1527.458
iteration 0700: loss: 1530.981
iteration 0800: loss: 1535.822
iteration 0900: loss: 1527.186
====> Epoch: 005 Train loss: 1529.5763  took : 5.875589609146118
====> Test loss: 1524.4080
iteration 0000: loss: 1530.407
iteration 0100: loss: 1530.137
iteration 0200: loss: 1533.800
iteration 0300: loss: 1529.381
iteration 0400: loss: 1528.605
iteration 0500: loss: 1531.415
iteration 0600: loss: 1528.477
iteration 0700: loss: 1529.422
iteration 0800: loss: 1527.281
iteration 0900: loss: 1530.661
====> Epoch: 006 Train loss: 1528.9768  took : 5.900662660598755
====> Test loss: 1523.8161
iteration 0000: loss: 1528.509
iteration 0100: loss: 1530.167
iteration 0200: loss: 1530.681
iteration 0300: loss: 1526.106
iteration 0400: loss: 1525.591
iteration 0500: loss: 1524.770
iteration 0600: loss: 1531.354
iteration 0700: loss: 1526.901
iteration 0800: loss: 1527.490
iteration 0900: loss: 1526.958
====> Epoch: 007 Train loss: 1528.4561  took : 5.875913858413696
====> Test loss: 1523.3821
iteration 0000: loss: 1527.294
iteration 0100: loss: 1530.222
iteration 0200: loss: 1529.118
iteration 0300: loss: 1527.909
iteration 0400: loss: 1527.298
iteration 0500: loss: 1527.953
iteration 0600: loss: 1526.391
iteration 0700: loss: 1529.048
iteration 0800: loss: 1531.221
iteration 0900: loss: 1528.021
====> Epoch: 008 Train loss: 1528.0468  took : 5.890955448150635
====> Test loss: 1523.0467
iteration 0000: loss: 1524.673
iteration 0100: loss: 1529.397
iteration 0200: loss: 1528.245
iteration 0300: loss: 1528.057
iteration 0400: loss: 1528.033
iteration 0500: loss: 1526.196
iteration 0600: loss: 1528.049
iteration 0700: loss: 1526.158
iteration 0800: loss: 1527.893
iteration 0900: loss: 1526.549
====> Epoch: 009 Train loss: 1527.7621  took : 5.895281553268433
====> Test loss: 1522.9104
iteration 0000: loss: 1527.222
iteration 0100: loss: 1528.410
iteration 0200: loss: 1529.543
iteration 0300: loss: 1530.557
iteration 0400: loss: 1528.816
iteration 0500: loss: 1527.163
iteration 0600: loss: 1527.587
iteration 0700: loss: 1526.005
iteration 0800: loss: 1527.853
iteration 0900: loss: 1528.203
====> Epoch: 010 Train loss: 1527.4794  took : 5.941869258880615
====> Test loss: 1522.5883
iteration 0000: loss: 1528.126
iteration 0100: loss: 1523.231
iteration 0200: loss: 1528.294
iteration 0300: loss: 1528.996
iteration 0400: loss: 1525.859
iteration 0500: loss: 1527.394
iteration 0600: loss: 1529.538
iteration 0700: loss: 1528.703
iteration 0800: loss: 1531.782
iteration 0900: loss: 1525.793
====> Epoch: 011 Train loss: 1527.2380  took : 5.907250881195068
====> Test loss: 1522.4358
iteration 0000: loss: 1528.439
iteration 0100: loss: 1527.324
iteration 0200: loss: 1526.761
iteration 0300: loss: 1527.946
iteration 0400: loss: 1529.864
iteration 0500: loss: 1525.145
iteration 0600: loss: 1529.017
iteration 0700: loss: 1524.036
iteration 0800: loss: 1527.146
iteration 0900: loss: 1527.503
====> Epoch: 012 Train loss: 1527.0824  took : 5.962601184844971
====> Test loss: 1522.2937
iteration 0000: loss: 1528.532
iteration 0100: loss: 1530.204
iteration 0200: loss: 1526.234
iteration 0300: loss: 1527.026
iteration 0400: loss: 1524.954
iteration 0500: loss: 1525.886
iteration 0600: loss: 1521.693
iteration 0700: loss: 1533.180
iteration 0800: loss: 1527.384
iteration 0900: loss: 1528.663
====> Epoch: 013 Train loss: 1526.9170  took : 5.916750431060791
====> Test loss: 1522.0684
iteration 0000: loss: 1526.592
iteration 0100: loss: 1523.692
iteration 0200: loss: 1524.052
iteration 0300: loss: 1523.648
iteration 0400: loss: 1528.339
iteration 0500: loss: 1523.601
iteration 0600: loss: 1528.474
iteration 0700: loss: 1526.135
iteration 0800: loss: 1527.319
iteration 0900: loss: 1523.149
====> Epoch: 014 Train loss: 1526.7565  took : 5.907585859298706
====> Test loss: 1522.0313
iteration 0000: loss: 1526.985
iteration 0100: loss: 1525.745
iteration 0200: loss: 1526.485
iteration 0300: loss: 1526.382
iteration 0400: loss: 1525.958
iteration 0500: loss: 1523.546
iteration 0600: loss: 1525.140
iteration 0700: loss: 1525.137
iteration 0800: loss: 1526.610
iteration 0900: loss: 1524.344
====> Epoch: 015 Train loss: 1526.6060  took : 5.928321361541748
====> Test loss: 1521.8473
iteration 0000: loss: 1527.210
iteration 0100: loss: 1528.493
iteration 0200: loss: 1522.692
iteration 0300: loss: 1527.298
iteration 0400: loss: 1525.602
iteration 0500: loss: 1525.477
iteration 0600: loss: 1527.754
iteration 0700: loss: 1527.000
iteration 0800: loss: 1527.054
iteration 0900: loss: 1524.791
====> Epoch: 016 Train loss: 1526.4864  took : 5.91623330116272
====> Test loss: 1521.7298
iteration 0000: loss: 1525.217
iteration 0100: loss: 1523.772
iteration 0200: loss: 1524.215
iteration 0300: loss: 1527.767
iteration 0400: loss: 1526.392
iteration 0500: loss: 1527.422
iteration 0600: loss: 1527.717
iteration 0700: loss: 1526.865
iteration 0800: loss: 1526.627
iteration 0900: loss: 1526.517
====> Epoch: 017 Train loss: 1526.3962  took : 5.929188966751099
====> Test loss: 1521.7427
iteration 0000: loss: 1529.843
iteration 0100: loss: 1522.500
iteration 0200: loss: 1526.644
iteration 0300: loss: 1528.790
iteration 0400: loss: 1528.495
iteration 0500: loss: 1527.511
iteration 0600: loss: 1527.057
iteration 0700: loss: 1522.000
iteration 0800: loss: 1524.515
iteration 0900: loss: 1527.681
====> Epoch: 018 Train loss: 1526.2537  took : 5.905672073364258
====> Test loss: 1521.6497
iteration 0000: loss: 1524.964
iteration 0100: loss: 1526.530
iteration 0200: loss: 1526.048
iteration 0300: loss: 1525.331
iteration 0400: loss: 1529.957
iteration 0500: loss: 1531.048
iteration 0600: loss: 1527.316
iteration 0700: loss: 1527.124
iteration 0800: loss: 1530.406
iteration 0900: loss: 1528.733
====> Epoch: 019 Train loss: 1526.1902  took : 5.958727598190308
====> Test loss: 1521.5339
iteration 0000: loss: 1527.298
iteration 0100: loss: 1526.603
iteration 0200: loss: 1527.619
iteration 0300: loss: 1525.452
iteration 0400: loss: 1524.018
iteration 0500: loss: 1529.249
iteration 0600: loss: 1525.423
iteration 0700: loss: 1525.033
iteration 0800: loss: 1528.577
iteration 0900: loss: 1528.435
====> Epoch: 020 Train loss: 1526.0831  took : 5.967267036437988
====> Test loss: 1521.5869
iteration 0000: loss: 1525.781
iteration 0100: loss: 1526.872
iteration 0200: loss: 1525.818
iteration 0300: loss: 1524.229
iteration 0400: loss: 1523.552
iteration 0500: loss: 1525.004
iteration 0600: loss: 1528.135
iteration 0700: loss: 1527.394
iteration 0800: loss: 1526.680
iteration 0900: loss: 1528.221
====> Epoch: 021 Train loss: 1525.9957  took : 5.936008453369141
====> Test loss: 1521.4078
iteration 0000: loss: 1528.143
iteration 0100: loss: 1522.478
iteration 0200: loss: 1522.232
iteration 0300: loss: 1524.995
iteration 0400: loss: 1526.266
iteration 0500: loss: 1528.615
iteration 0600: loss: 1526.353
iteration 0700: loss: 1525.581
iteration 0800: loss: 1526.033
iteration 0900: loss: 1524.045
====> Epoch: 022 Train loss: 1525.9277  took : 5.932636499404907
====> Test loss: 1521.4348
iteration 0000: loss: 1524.899
iteration 0100: loss: 1524.346
iteration 0200: loss: 1524.829
iteration 0300: loss: 1524.089
iteration 0400: loss: 1523.902
iteration 0500: loss: 1526.687
iteration 0600: loss: 1526.847
iteration 0700: loss: 1524.742
iteration 0800: loss: 1527.370
iteration 0900: loss: 1528.941
====> Epoch: 023 Train loss: 1525.8588  took : 6.00246000289917
====> Test loss: 1521.2730
iteration 0000: loss: 1526.830
iteration 0100: loss: 1527.093
iteration 0200: loss: 1524.465
iteration 0300: loss: 1526.993
iteration 0400: loss: 1526.332
iteration 0500: loss: 1525.063
iteration 0600: loss: 1523.652
iteration 0700: loss: 1526.450
iteration 0800: loss: 1526.775
iteration 0900: loss: 1523.953
====> Epoch: 024 Train loss: 1525.7700  took : 5.9298834800720215
====> Test loss: 1521.2519
iteration 0000: loss: 1528.298
iteration 0100: loss: 1525.386
iteration 0200: loss: 1527.295
iteration 0300: loss: 1524.860
iteration 0400: loss: 1527.304
iteration 0500: loss: 1523.956
iteration 0600: loss: 1522.229
iteration 0700: loss: 1525.041
iteration 0800: loss: 1527.140
iteration 0900: loss: 1527.709
====> Epoch: 025 Train loss: 1525.7173  took : 5.948619604110718
====> Test loss: 1521.1327
iteration 0000: loss: 1523.505
iteration 0100: loss: 1525.278
iteration 0200: loss: 1526.135
iteration 0300: loss: 1525.559
iteration 0400: loss: 1526.814
iteration 0500: loss: 1525.171
iteration 0600: loss: 1528.234
iteration 0700: loss: 1523.815
iteration 0800: loss: 1525.124
iteration 0900: loss: 1524.750
====> Epoch: 026 Train loss: 1525.6587  took : 5.919909477233887
====> Test loss: 1521.1261
iteration 0000: loss: 1522.207
iteration 0100: loss: 1526.499
iteration 0200: loss: 1526.179
iteration 0300: loss: 1528.015
iteration 0400: loss: 1525.276
iteration 0500: loss: 1525.520
iteration 0600: loss: 1524.703
iteration 0700: loss: 1526.782
iteration 0800: loss: 1524.034
iteration 0900: loss: 1525.654
====> Epoch: 027 Train loss: 1525.6121  took : 5.92627739906311
====> Test loss: 1521.0866
iteration 0000: loss: 1523.783
iteration 0100: loss: 1525.752
iteration 0200: loss: 1524.044
iteration 0300: loss: 1526.961
iteration 0400: loss: 1526.661
iteration 0500: loss: 1522.406
iteration 0600: loss: 1524.699
iteration 0700: loss: 1525.676
iteration 0800: loss: 1523.545
iteration 0900: loss: 1525.639
====> Epoch: 028 Train loss: 1525.5423  took : 5.900076150894165
====> Test loss: 1521.0393
iteration 0000: loss: 1524.998
iteration 0100: loss: 1527.679
iteration 0200: loss: 1525.796
iteration 0300: loss: 1527.007
iteration 0400: loss: 1527.623
iteration 0500: loss: 1524.556
iteration 0600: loss: 1526.902
iteration 0700: loss: 1525.694
iteration 0800: loss: 1524.303
iteration 0900: loss: 1526.098
====> Epoch: 029 Train loss: 1525.5223  took : 5.9370646476745605
====> Test loss: 1520.9725
iteration 0000: loss: 1523.829
iteration 0100: loss: 1526.272
iteration 0200: loss: 1525.107
iteration 0300: loss: 1525.823
iteration 0400: loss: 1524.285
iteration 0500: loss: 1523.105
iteration 0600: loss: 1526.557
iteration 0700: loss: 1525.980
iteration 0800: loss: 1522.151
iteration 0900: loss: 1523.220
====> Epoch: 030 Train loss: 1525.4994  took : 5.871570348739624
====> Test loss: 1521.0319
iteration 0000: loss: 1525.516
iteration 0100: loss: 1524.860
iteration 0200: loss: 1525.437
iteration 0300: loss: 1529.686
iteration 0400: loss: 1526.620
iteration 0500: loss: 1523.872
iteration 0600: loss: 1522.919
iteration 0700: loss: 1528.886
iteration 0800: loss: 1525.960
iteration 0900: loss: 1526.807
====> Epoch: 031 Train loss: 1525.4086  took : 5.915334939956665
====> Test loss: 1520.9479
iteration 0000: loss: 1527.026
iteration 0100: loss: 1523.710
iteration 0200: loss: 1526.254
iteration 0300: loss: 1525.300
iteration 0400: loss: 1528.305
iteration 0500: loss: 1522.404
iteration 0600: loss: 1524.308
iteration 0700: loss: 1524.858
iteration 0800: loss: 1523.434
iteration 0900: loss: 1525.136
====> Epoch: 032 Train loss: 1525.3652  took : 5.935757875442505
====> Test loss: 1521.0086
iteration 0000: loss: 1528.881
iteration 0100: loss: 1526.583
iteration 0200: loss: 1524.431
iteration 0300: loss: 1528.697
iteration 0400: loss: 1522.944
iteration 0500: loss: 1524.234
iteration 0600: loss: 1527.308
iteration 0700: loss: 1524.478
iteration 0800: loss: 1525.943
iteration 0900: loss: 1524.465
====> Epoch: 033 Train loss: 1525.2869  took : 5.918860912322998
====> Test loss: 1521.0471
iteration 0000: loss: 1521.839
iteration 0100: loss: 1524.583
iteration 0200: loss: 1521.566
iteration 0300: loss: 1528.241
iteration 0400: loss: 1526.201
iteration 0500: loss: 1524.585
iteration 0600: loss: 1524.430
iteration 0700: loss: 1527.684
iteration 0800: loss: 1527.697
iteration 0900: loss: 1528.231
====> Epoch: 034 Train loss: 1525.3132  took : 5.899990797042847
====> Test loss: 1520.8955
iteration 0000: loss: 1525.010
iteration 0100: loss: 1524.895
iteration 0200: loss: 1525.152
iteration 0300: loss: 1526.287
iteration 0400: loss: 1525.604
iteration 0500: loss: 1526.587
iteration 0600: loss: 1526.360
iteration 0700: loss: 1524.121
iteration 0800: loss: 1525.261
iteration 0900: loss: 1525.490
====> Epoch: 035 Train loss: 1525.2829  took : 5.94786524772644
====> Test loss: 1520.9886
iteration 0000: loss: 1525.637
iteration 0100: loss: 1523.583
iteration 0200: loss: 1526.742
iteration 0300: loss: 1525.581
iteration 0400: loss: 1527.708
iteration 0500: loss: 1526.270
iteration 0600: loss: 1525.115
iteration 0700: loss: 1525.513
iteration 0800: loss: 1522.041
iteration 0900: loss: 1526.340
====> Epoch: 036 Train loss: 1525.2115  took : 5.89778995513916
====> Test loss: 1520.8203
iteration 0000: loss: 1524.757
iteration 0100: loss: 1525.753
iteration 0200: loss: 1525.814
iteration 0300: loss: 1527.334
iteration 0400: loss: 1524.018
iteration 0500: loss: 1527.037
iteration 0600: loss: 1522.436
iteration 0700: loss: 1523.819
iteration 0800: loss: 1523.976
iteration 0900: loss: 1522.315
====> Epoch: 037 Train loss: 1525.1652  took : 5.944647550582886
====> Test loss: 1520.8146
iteration 0000: loss: 1523.978
iteration 0100: loss: 1524.616
iteration 0200: loss: 1526.502
iteration 0300: loss: 1528.409
iteration 0400: loss: 1527.053
iteration 0500: loss: 1523.871
iteration 0600: loss: 1523.826
iteration 0700: loss: 1523.766
iteration 0800: loss: 1525.742
iteration 0900: loss: 1526.031
====> Epoch: 038 Train loss: 1525.1561  took : 5.916970729827881
====> Test loss: 1520.9366
iteration 0000: loss: 1528.036
iteration 0100: loss: 1526.486
iteration 0200: loss: 1526.287
iteration 0300: loss: 1525.929
iteration 0400: loss: 1526.330
iteration 0500: loss: 1524.167
iteration 0600: loss: 1525.274
iteration 0700: loss: 1525.060
iteration 0800: loss: 1526.794
iteration 0900: loss: 1524.842
====> Epoch: 039 Train loss: 1525.1480  took : 5.9443676471710205
====> Test loss: 1520.8656
iteration 0000: loss: 1525.633
iteration 0100: loss: 1523.536
iteration 0200: loss: 1525.002
iteration 0300: loss: 1527.217
iteration 0400: loss: 1523.548
iteration 0500: loss: 1523.316
iteration 0600: loss: 1525.516
iteration 0700: loss: 1527.765
iteration 0800: loss: 1527.905
iteration 0900: loss: 1525.668
====> Epoch: 040 Train loss: 1525.0851  took : 5.933250665664673
====> Test loss: 1520.7478
iteration 0000: loss: 1522.884
iteration 0100: loss: 1525.630
iteration 0200: loss: 1526.573
iteration 0300: loss: 1523.591
iteration 0400: loss: 1525.300
iteration 0500: loss: 1520.810
iteration 0600: loss: 1524.302
iteration 0700: loss: 1524.877
iteration 0800: loss: 1522.628
iteration 0900: loss: 1526.415
====> Epoch: 041 Train loss: 1525.0771  took : 5.937262773513794
====> Test loss: 1520.6468
iteration 0000: loss: 1524.549
iteration 0100: loss: 1524.125
iteration 0200: loss: 1523.270
iteration 0300: loss: 1524.248
iteration 0400: loss: 1522.357
iteration 0500: loss: 1529.243
iteration 0600: loss: 1523.663
iteration 0700: loss: 1523.306
iteration 0800: loss: 1525.980
iteration 0900: loss: 1527.578
====> Epoch: 042 Train loss: 1525.0242  took : 5.881545782089233
====> Test loss: 1520.6553
iteration 0000: loss: 1523.132
iteration 0100: loss: 1526.247
iteration 0200: loss: 1526.271
iteration 0300: loss: 1524.542
iteration 0400: loss: 1524.035
iteration 0500: loss: 1525.635
iteration 0600: loss: 1522.395
iteration 0700: loss: 1524.189
iteration 0800: loss: 1525.304
iteration 0900: loss: 1524.133
====> Epoch: 043 Train loss: 1525.0103  took : 5.9259865283966064
====> Test loss: 1520.6481
iteration 0000: loss: 1524.094
iteration 0100: loss: 1523.616
iteration 0200: loss: 1526.561
iteration 0300: loss: 1523.911
iteration 0400: loss: 1527.998
iteration 0500: loss: 1522.640
iteration 0600: loss: 1524.201
iteration 0700: loss: 1525.284
iteration 0800: loss: 1523.013
iteration 0900: loss: 1526.062
====> Epoch: 044 Train loss: 1524.9547  took : 5.993410110473633
====> Test loss: 1520.7776
iteration 0000: loss: 1522.394
iteration 0100: loss: 1527.867
iteration 0200: loss: 1527.735
iteration 0300: loss: 1526.127
iteration 0400: loss: 1525.115
iteration 0500: loss: 1526.764
iteration 0600: loss: 1529.103
iteration 0700: loss: 1525.981
iteration 0800: loss: 1524.475
iteration 0900: loss: 1527.337
====> Epoch: 045 Train loss: 1524.9255  took : 5.915879249572754
====> Test loss: 1520.6307
iteration 0000: loss: 1524.574
iteration 0100: loss: 1525.266
iteration 0200: loss: 1528.064
iteration 0300: loss: 1526.900
iteration 0400: loss: 1526.908
iteration 0500: loss: 1524.500
iteration 0600: loss: 1524.262
iteration 0700: loss: 1525.353
iteration 0800: loss: 1528.676
iteration 0900: loss: 1525.354
====> Epoch: 046 Train loss: 1524.9254  took : 5.923967361450195
====> Test loss: 1520.5418
iteration 0000: loss: 1526.203
iteration 0100: loss: 1525.738
iteration 0200: loss: 1523.874
iteration 0300: loss: 1524.135
iteration 0400: loss: 1526.163
iteration 0500: loss: 1523.305
iteration 0600: loss: 1525.785
iteration 0700: loss: 1526.090
iteration 0800: loss: 1525.325
iteration 0900: loss: 1520.845
====> Epoch: 047 Train loss: 1524.8940  took : 5.930893182754517
====> Test loss: 1520.5676
iteration 0000: loss: 1524.026
iteration 0100: loss: 1522.837
iteration 0200: loss: 1527.276
iteration 0300: loss: 1526.499
iteration 0400: loss: 1524.857
iteration 0500: loss: 1523.687
iteration 0600: loss: 1526.168
iteration 0700: loss: 1522.322
iteration 0800: loss: 1525.086
iteration 0900: loss: 1523.520
====> Epoch: 048 Train loss: 1524.8678  took : 5.897096633911133
====> Test loss: 1520.6269
iteration 0000: loss: 1524.098
iteration 0100: loss: 1523.122
iteration 0200: loss: 1526.709
iteration 0300: loss: 1524.212
iteration 0400: loss: 1521.104
iteration 0500: loss: 1526.900
iteration 0600: loss: 1523.358
iteration 0700: loss: 1524.394
iteration 0800: loss: 1526.917
iteration 0900: loss: 1528.528
====> Epoch: 049 Train loss: 1524.8648  took : 5.938375473022461
====> Test loss: 1520.5545
iteration 0000: loss: 1522.542
iteration 0100: loss: 1525.943
iteration 0200: loss: 1525.645
iteration 0300: loss: 1525.215
iteration 0400: loss: 1525.453
iteration 0500: loss: 1525.012
iteration 0600: loss: 1525.716
iteration 0700: loss: 1526.062
iteration 0800: loss: 1522.917
iteration 0900: loss: 1526.389
====> Epoch: 050 Train loss: 1524.8137  took : 5.994418382644653
====> Test loss: 1520.6283
====> [MM-VAE] Time: 449.869s or 00:07:29
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2565.039
iteration 0100: loss: 2104.684
iteration 0200: loss: 2080.415
iteration 0300: loss: 2040.355
iteration 0400: loss: 2021.062
iteration 0500: loss: 2022.214
iteration 0600: loss: 2001.714
iteration 0700: loss: 1994.668
iteration 0800: loss: 1971.524
iteration 0900: loss: 1965.800
====> Epoch: 001 Train loss: 2034.0650  took : 8.70138168334961
====> Test loss: 1959.3520
iteration 0000: loss: 1965.809
iteration 0100: loss: 1964.273
iteration 0200: loss: 1963.035
iteration 0300: loss: 1961.469
iteration 0400: loss: 1961.234
iteration 0500: loss: 1960.868
iteration 0600: loss: 1961.055
iteration 0700: loss: 1961.369
iteration 0800: loss: 1962.204
iteration 0900: loss: 1959.810
====> Epoch: 002 Train loss: 1961.7762  took : 8.782462358474731
====> Test loss: 1954.9310
iteration 0000: loss: 1959.844
iteration 0100: loss: 1959.675
iteration 0200: loss: 1959.207
iteration 0300: loss: 1958.861
iteration 0400: loss: 1958.934
iteration 0500: loss: 1960.015
iteration 0600: loss: 1960.017
iteration 0700: loss: 1958.607
iteration 0800: loss: 1958.415
iteration 0900: loss: 1959.982
====> Epoch: 003 Train loss: 1959.2891  took : 8.828474760055542
====> Test loss: 1953.6116
iteration 0000: loss: 1959.188
iteration 0100: loss: 1959.365
iteration 0200: loss: 1959.160
iteration 0300: loss: 1959.366
iteration 0400: loss: 1956.715
iteration 0500: loss: 1958.618
iteration 0600: loss: 1959.032
iteration 0700: loss: 1959.057
iteration 0800: loss: 1959.581
iteration 0900: loss: 1959.713
====> Epoch: 004 Train loss: 1958.5854  took : 8.619117975234985
====> Test loss: 1953.2261
iteration 0000: loss: 1958.219
iteration 0100: loss: 1957.362
iteration 0200: loss: 1958.212
iteration 0300: loss: 1958.420
iteration 0400: loss: 1957.487
iteration 0500: loss: 1959.019
iteration 0600: loss: 1959.306
iteration 0700: loss: 1958.812
iteration 0800: loss: 1958.299
iteration 0900: loss: 1957.811
====> Epoch: 005 Train loss: 1958.2353  took : 8.74513864517212
====> Test loss: 1952.6385
iteration 0000: loss: 1958.122
iteration 0100: loss: 1957.661
iteration 0200: loss: 1958.328
iteration 0300: loss: 1958.686
iteration 0400: loss: 1958.435
iteration 0500: loss: 1959.487
iteration 0600: loss: 1958.663
iteration 0700: loss: 1957.891
iteration 0800: loss: 1957.297
iteration 0900: loss: 1957.813
====> Epoch: 006 Train loss: 1958.0463  took : 8.625194311141968
====> Test loss: 1952.2603
iteration 0000: loss: 1957.538
iteration 0100: loss: 1957.556
iteration 0200: loss: 1957.783
iteration 0300: loss: 1957.661
iteration 0400: loss: 1957.289
iteration 0500: loss: 1957.020
iteration 0600: loss: 1957.349
iteration 0700: loss: 1957.730
iteration 0800: loss: 1957.380
iteration 0900: loss: 1957.618
====> Epoch: 007 Train loss: 1957.8614  took : 8.761404752731323
====> Test loss: 1952.4512
iteration 0000: loss: 1959.480
iteration 0100: loss: 1957.244
iteration 0200: loss: 1957.419
iteration 0300: loss: 1956.811
iteration 0400: loss: 1957.228
iteration 0500: loss: 1958.550
iteration 0600: loss: 1957.594
iteration 0700: loss: 1957.005
iteration 0800: loss: 1957.666
iteration 0900: loss: 1959.112
====> Epoch: 008 Train loss: 1957.6656  took : 8.625957727432251
====> Test loss: 1952.7398
iteration 0000: loss: 1958.736
iteration 0100: loss: 1956.997
iteration 0200: loss: 1959.018
iteration 0300: loss: 1957.188
iteration 0400: loss: 1958.591
iteration 0500: loss: 1956.963
iteration 0600: loss: 1957.043
iteration 0700: loss: 1958.351
iteration 0800: loss: 1957.211
iteration 0900: loss: 1958.207
====> Epoch: 009 Train loss: 1957.5955  took : 8.81386661529541
====> Test loss: 1952.2164
iteration 0000: loss: 1958.971
iteration 0100: loss: 1956.901
iteration 0200: loss: 1957.963
iteration 0300: loss: 1960.175
iteration 0400: loss: 1956.941
iteration 0500: loss: 1958.146
iteration 0600: loss: 1956.400
iteration 0700: loss: 1957.359
iteration 0800: loss: 1956.939
iteration 0900: loss: 1956.878
====> Epoch: 010 Train loss: 1957.5072  took : 8.6378173828125
====> Test loss: 1952.1878
iteration 0000: loss: 1957.481
iteration 0100: loss: 1956.282
iteration 0200: loss: 1957.321
iteration 0300: loss: 1958.700
iteration 0400: loss: 1958.539
iteration 0500: loss: 1957.601
iteration 0600: loss: 1957.047
iteration 0700: loss: 1957.483
iteration 0800: loss: 1957.541
iteration 0900: loss: 1957.125
====> Epoch: 011 Train loss: 1957.3884  took : 8.717163562774658
====> Test loss: 1952.3422
iteration 0000: loss: 1957.215
iteration 0100: loss: 1957.512
iteration 0200: loss: 1957.087
iteration 0300: loss: 1957.184
iteration 0400: loss: 1956.061
iteration 0500: loss: 1956.338
iteration 0600: loss: 1956.023
iteration 0700: loss: 1957.560
iteration 0800: loss: 1957.485
iteration 0900: loss: 1957.219
====> Epoch: 012 Train loss: 1957.2418  took : 8.76293134689331
====> Test loss: 1952.2069
iteration 0000: loss: 1956.857
iteration 0100: loss: 1957.165
iteration 0200: loss: 1957.567
iteration 0300: loss: 1957.282
iteration 0400: loss: 1956.600
iteration 0500: loss: 1957.132
iteration 0600: loss: 1958.778
iteration 0700: loss: 1956.905
iteration 0800: loss: 1957.069
iteration 0900: loss: 1957.125
====> Epoch: 013 Train loss: 1957.2513  took : 8.655031204223633
====> Test loss: 1952.3597
iteration 0000: loss: 1956.903
iteration 0100: loss: 1956.561
iteration 0200: loss: 1957.840
iteration 0300: loss: 1957.370
iteration 0400: loss: 1958.012
iteration 0500: loss: 1957.770
iteration 0600: loss: 1956.823
iteration 0700: loss: 1957.882
iteration 0800: loss: 1958.005
iteration 0900: loss: 1957.961
====> Epoch: 014 Train loss: 1957.2201  took : 8.712839841842651
====> Test loss: 1951.9790
iteration 0000: loss: 1957.954
iteration 0100: loss: 1957.513
iteration 0200: loss: 1957.231
iteration 0300: loss: 1957.436
iteration 0400: loss: 1957.301
iteration 0500: loss: 1956.047
iteration 0600: loss: 1957.257
iteration 0700: loss: 1958.292
iteration 0800: loss: 1957.192
iteration 0900: loss: 1956.113
====> Epoch: 015 Train loss: 1957.1117  took : 8.65210747718811
====> Test loss: 1951.6626
iteration 0000: loss: 1956.561
iteration 0100: loss: 1956.978
iteration 0200: loss: 1956.876
iteration 0300: loss: 1956.722
iteration 0400: loss: 1958.126
iteration 0500: loss: 1956.200
iteration 0600: loss: 1956.897
iteration 0700: loss: 1957.172
iteration 0800: loss: 1956.675
iteration 0900: loss: 1956.335
====> Epoch: 016 Train loss: 1957.0162  took : 8.627338409423828
====> Test loss: 1951.9279
iteration 0000: loss: 1955.690
iteration 0100: loss: 1956.798
iteration 0200: loss: 1957.856
iteration 0300: loss: 1958.035
iteration 0400: loss: 1956.488
iteration 0500: loss: 1956.892
iteration 0600: loss: 1956.677
iteration 0700: loss: 1956.606
iteration 0800: loss: 1956.218
iteration 0900: loss: 1956.738
====> Epoch: 017 Train loss: 1956.9758  took : 8.711580514907837
====> Test loss: 1951.6271
iteration 0000: loss: 1956.990
iteration 0100: loss: 1956.303
iteration 0200: loss: 1958.121
iteration 0300: loss: 1957.015
iteration 0400: loss: 1956.756
iteration 0500: loss: 1956.826
iteration 0600: loss: 1957.348
iteration 0700: loss: 1956.723
iteration 0800: loss: 1958.250
iteration 0900: loss: 1957.282
====> Epoch: 018 Train loss: 1956.9910  took : 8.672350883483887
====> Test loss: 1951.6370
iteration 0000: loss: 1957.822
iteration 0100: loss: 1956.075
iteration 0200: loss: 1956.476
iteration 0300: loss: 1957.138
iteration 0400: loss: 1957.285
iteration 0500: loss: 1957.808
iteration 0600: loss: 1956.140
iteration 0700: loss: 1955.595
iteration 0800: loss: 1956.534
iteration 0900: loss: 1957.758
====> Epoch: 019 Train loss: 1956.8785  took : 8.76741886138916
====> Test loss: 1951.7702
iteration 0000: loss: 1956.225
iteration 0100: loss: 1957.004
iteration 0200: loss: 1957.220
iteration 0300: loss: 1956.757
iteration 0400: loss: 1956.447
iteration 0500: loss: 1957.889
iteration 0600: loss: 1956.406
iteration 0700: loss: 1958.044
iteration 0800: loss: 1955.879
iteration 0900: loss: 1956.242
====> Epoch: 020 Train loss: 1956.8942  took : 8.60008955001831
====> Test loss: 1951.5904
iteration 0000: loss: 1957.713
iteration 0100: loss: 1957.259
iteration 0200: loss: 1956.544
iteration 0300: loss: 1957.369
iteration 0400: loss: 1956.017
iteration 0500: loss: 1957.825
iteration 0600: loss: 1957.296
iteration 0700: loss: 1956.444
iteration 0800: loss: 1955.965
iteration 0900: loss: 1957.490
====> Epoch: 021 Train loss: 1956.8072  took : 9.016926288604736
====> Test loss: 1951.5971
iteration 0000: loss: 1956.229
iteration 0100: loss: 1955.710
iteration 0200: loss: 1957.053
iteration 0300: loss: 1956.613
iteration 0400: loss: 1956.013
iteration 0500: loss: 1956.614
iteration 0600: loss: 1955.972
iteration 0700: loss: 1955.685
iteration 0800: loss: 1958.813
iteration 0900: loss: 1956.307
====> Epoch: 022 Train loss: 1956.8333  took : 8.613600969314575
====> Test loss: 1951.8087
iteration 0000: loss: 1955.965
iteration 0100: loss: 1956.059
iteration 0200: loss: 1956.076
iteration 0300: loss: 1956.528
iteration 0400: loss: 1956.531
iteration 0500: loss: 1958.410
iteration 0600: loss: 1956.474
iteration 0700: loss: 1956.279
iteration 0800: loss: 1956.157
iteration 0900: loss: 1957.329
====> Epoch: 023 Train loss: 1956.7357  took : 8.735898733139038
====> Test loss: 1951.4277
iteration 0000: loss: 1956.818
iteration 0100: loss: 1955.699
iteration 0200: loss: 1956.324
iteration 0300: loss: 1956.700
iteration 0400: loss: 1956.084
iteration 0500: loss: 1957.833
iteration 0600: loss: 1956.935
iteration 0700: loss: 1956.867
iteration 0800: loss: 1957.184
iteration 0900: loss: 1956.373
====> Epoch: 024 Train loss: 1956.7252  took : 8.701035261154175
====> Test loss: 1951.6215
iteration 0000: loss: 1956.091
iteration 0100: loss: 1955.524
iteration 0200: loss: 1956.364
iteration 0300: loss: 1956.394
iteration 0400: loss: 1956.304
iteration 0500: loss: 1956.057
iteration 0600: loss: 1957.915
iteration 0700: loss: 1957.275
iteration 0800: loss: 1955.598
iteration 0900: loss: 1955.792
====> Epoch: 025 Train loss: 1956.7289  took : 8.780129194259644
====> Test loss: 1951.4347
iteration 0000: loss: 1955.783
iteration 0100: loss: 1957.308
iteration 0200: loss: 1957.200
iteration 0300: loss: 1956.218
iteration 0400: loss: 1958.235
iteration 0500: loss: 1956.591
iteration 0600: loss: 1955.620
iteration 0700: loss: 1955.767
iteration 0800: loss: 1957.192
iteration 0900: loss: 1955.713
====> Epoch: 026 Train loss: 1956.6785  took : 8.642977237701416
====> Test loss: 1951.7305
iteration 0000: loss: 1955.652
iteration 0100: loss: 1955.931
iteration 0200: loss: 1956.885
iteration 0300: loss: 1956.579
iteration 0400: loss: 1956.903
iteration 0500: loss: 1955.928
iteration 0600: loss: 1956.473
iteration 0700: loss: 1956.416
iteration 0800: loss: 1956.184
iteration 0900: loss: 1955.467
====> Epoch: 027 Train loss: 1956.5981  took : 8.621283769607544
====> Test loss: 1951.5454
iteration 0000: loss: 1956.892
iteration 0100: loss: 1957.311
iteration 0200: loss: 1956.475
iteration 0300: loss: 1956.918
iteration 0400: loss: 1957.358
iteration 0500: loss: 1956.982
iteration 0600: loss: 1956.677
iteration 0700: loss: 1956.119
iteration 0800: loss: 1956.391
iteration 0900: loss: 1957.060
====> Epoch: 028 Train loss: 1956.6337  took : 8.767444610595703
====> Test loss: 1951.5434
iteration 0000: loss: 1956.682
iteration 0100: loss: 1955.277
iteration 0200: loss: 1955.958
iteration 0300: loss: 1956.369
iteration 0400: loss: 1956.343
iteration 0500: loss: 1955.779
iteration 0600: loss: 1956.213
iteration 0700: loss: 1956.081
iteration 0800: loss: 1957.439
iteration 0900: loss: 1957.657
====> Epoch: 029 Train loss: 1956.5566  took : 8.650110721588135
====> Test loss: 1951.6617
iteration 0000: loss: 1956.599
iteration 0100: loss: 1956.200
iteration 0200: loss: 1957.381
iteration 0300: loss: 1956.403
iteration 0400: loss: 1956.307
iteration 0500: loss: 1955.347
iteration 0600: loss: 1955.679
iteration 0700: loss: 1957.971
iteration 0800: loss: 1955.756
iteration 0900: loss: 1956.323
====> Epoch: 030 Train loss: 1956.5435  took : 8.696812152862549
====> Test loss: 1951.4783
iteration 0000: loss: 1956.751
iteration 0100: loss: 1956.055
iteration 0200: loss: 1956.234
iteration 0300: loss: 1955.709
iteration 0400: loss: 1957.141
iteration 0500: loss: 1958.274
iteration 0600: loss: 1956.426
iteration 0700: loss: 1957.214
iteration 0800: loss: 1956.032
iteration 0900: loss: 1955.952
====> Epoch: 031 Train loss: 1956.5707  took : 8.6670503616333
====> Test loss: 1951.5873
iteration 0000: loss: 1957.636
iteration 0100: loss: 1956.120
iteration 0200: loss: 1955.997
iteration 0300: loss: 1955.554
iteration 0400: loss: 1956.981
iteration 0500: loss: 1956.150
iteration 0600: loss: 1956.787
iteration 0700: loss: 1956.017
iteration 0800: loss: 1956.160
iteration 0900: loss: 1955.778
====> Epoch: 032 Train loss: 1956.5082  took : 8.698183059692383
====> Test loss: 1951.4831
iteration 0000: loss: 1956.729
iteration 0100: loss: 1957.904
iteration 0200: loss: 1956.656
iteration 0300: loss: 1956.944
iteration 0400: loss: 1956.294
iteration 0500: loss: 1954.768
iteration 0600: loss: 1956.861
iteration 0700: loss: 1956.692
iteration 0800: loss: 1956.406
iteration 0900: loss: 1956.506
====> Epoch: 033 Train loss: 1956.5744  took : 8.789167404174805
====> Test loss: 1951.7018
iteration 0000: loss: 1957.694
iteration 0100: loss: 1956.432
iteration 0200: loss: 1957.268
iteration 0300: loss: 1955.495
iteration 0400: loss: 1956.586
iteration 0500: loss: 1957.297
iteration 0600: loss: 1956.187
iteration 0700: loss: 1956.350
iteration 0800: loss: 1956.194
iteration 0900: loss: 1957.110
====> Epoch: 034 Train loss: 1956.5295  took : 8.759971141815186
====> Test loss: 1951.2815
iteration 0000: loss: 1956.313
iteration 0100: loss: 1954.927
iteration 0200: loss: 1956.792
iteration 0300: loss: 1956.252
iteration 0400: loss: 1957.266
iteration 0500: loss: 1956.382
iteration 0600: loss: 1956.826
iteration 0700: loss: 1956.589
iteration 0800: loss: 1955.787
iteration 0900: loss: 1955.795
====> Epoch: 035 Train loss: 1956.4361  took : 8.639314413070679
====> Test loss: 1951.3693
iteration 0000: loss: 1955.511
iteration 0100: loss: 1958.480
iteration 0200: loss: 1956.751
iteration 0300: loss: 1956.813
iteration 0400: loss: 1956.594
iteration 0500: loss: 1955.545
iteration 0600: loss: 1957.276
iteration 0700: loss: 1955.730
iteration 0800: loss: 1955.068
iteration 0900: loss: 1955.376
====> Epoch: 036 Train loss: 1956.4094  took : 8.720438718795776
====> Test loss: 1951.5368
iteration 0000: loss: 1956.781
iteration 0100: loss: 1956.042
iteration 0200: loss: 1957.621
iteration 0300: loss: 1956.649
iteration 0400: loss: 1956.195
iteration 0500: loss: 1957.625
iteration 0600: loss: 1956.685
iteration 0700: loss: 1956.071
iteration 0800: loss: 1956.039
iteration 0900: loss: 1956.792
====> Epoch: 037 Train loss: 1956.4353  took : 8.694634437561035
====> Test loss: 1951.1755
iteration 0000: loss: 1955.918
iteration 0100: loss: 1958.304
iteration 0200: loss: 1955.881
iteration 0300: loss: 1957.465
iteration 0400: loss: 1957.101
iteration 0500: loss: 1957.721
iteration 0600: loss: 1957.969
iteration 0700: loss: 1957.058
iteration 0800: loss: 1956.493
iteration 0900: loss: 1955.977
====> Epoch: 038 Train loss: 1956.4634  took : 8.773979187011719
====> Test loss: 1951.5237
iteration 0000: loss: 1956.500
iteration 0100: loss: 1955.996
iteration 0200: loss: 1956.709
iteration 0300: loss: 1956.634
iteration 0400: loss: 1956.928
iteration 0500: loss: 1958.184
iteration 0600: loss: 1956.417
iteration 0700: loss: 1956.579
iteration 0800: loss: 1955.438
iteration 0900: loss: 1955.382
====> Epoch: 039 Train loss: 1956.4024  took : 8.654968023300171
====> Test loss: 1951.3370
iteration 0000: loss: 1957.797
iteration 0100: loss: 1956.203
iteration 0200: loss: 1955.817
iteration 0300: loss: 1955.060
iteration 0400: loss: 1956.047
iteration 0500: loss: 1958.043
iteration 0600: loss: 1956.906
iteration 0700: loss: 1955.872
iteration 0800: loss: 1957.695
iteration 0900: loss: 1956.001
====> Epoch: 040 Train loss: 1956.4516  took : 8.756029844284058
====> Test loss: 1951.5608
iteration 0000: loss: 1957.905
iteration 0100: loss: 1956.785
iteration 0200: loss: 1955.610
iteration 0300: loss: 1957.137
iteration 0400: loss: 1956.514
iteration 0500: loss: 1955.536
iteration 0600: loss: 1956.620
iteration 0700: loss: 1955.998
iteration 0800: loss: 1957.320
iteration 0900: loss: 1955.053
====> Epoch: 041 Train loss: 1956.4140  took : 8.754769802093506
====> Test loss: 1951.4936
iteration 0000: loss: 1955.776
iteration 0100: loss: 1956.382
iteration 0200: loss: 1957.491
iteration 0300: loss: 1956.240
iteration 0400: loss: 1956.745
iteration 0500: loss: 1955.476
iteration 0600: loss: 1955.829
iteration 0700: loss: 1955.888
iteration 0800: loss: 1956.811
iteration 0900: loss: 1955.847
====> Epoch: 042 Train loss: 1956.3663  took : 8.713812351226807
====> Test loss: 1951.9241
iteration 0000: loss: 1957.153
iteration 0100: loss: 1955.396
iteration 0200: loss: 1955.777
iteration 0300: loss: 1957.040
iteration 0400: loss: 1956.460
iteration 0500: loss: 1955.428
iteration 0600: loss: 1955.887
iteration 0700: loss: 1956.359
iteration 0800: loss: 1955.872
iteration 0900: loss: 1956.006
====> Epoch: 043 Train loss: 1956.3522  took : 8.632343292236328
====> Test loss: 1951.4616
iteration 0000: loss: 1955.738
iteration 0100: loss: 1955.922
iteration 0200: loss: 1956.560
iteration 0300: loss: 1957.377
iteration 0400: loss: 1959.163
iteration 0500: loss: 1957.043
iteration 0600: loss: 1955.175
iteration 0700: loss: 1956.375
iteration 0800: loss: 1957.504
iteration 0900: loss: 1956.089
====> Epoch: 044 Train loss: 1956.3679  took : 8.744176149368286
====> Test loss: 1951.3282
iteration 0000: loss: 1955.328
iteration 0100: loss: 1958.327
iteration 0200: loss: 1955.561
iteration 0300: loss: 1955.344
iteration 0400: loss: 1955.757
iteration 0500: loss: 1955.946
iteration 0600: loss: 1957.462
iteration 0700: loss: 1958.004
iteration 0800: loss: 1956.360
iteration 0900: loss: 1955.266
====> Epoch: 045 Train loss: 1956.3258  took : 8.70331597328186
====> Test loss: 1951.3039
iteration 0000: loss: 1956.312
iteration 0100: loss: 1956.102
iteration 0200: loss: 1956.541
iteration 0300: loss: 1955.487
iteration 0400: loss: 1955.519
iteration 0500: loss: 1957.194
iteration 0600: loss: 1956.951
iteration 0700: loss: 1955.766
iteration 0800: loss: 1956.731
iteration 0900: loss: 1957.468
====> Epoch: 046 Train loss: 1956.2681  took : 8.78929352760315
====> Test loss: 1951.3321
iteration 0000: loss: 1956.792
iteration 0100: loss: 1955.391
iteration 0200: loss: 1956.695
iteration 0300: loss: 1957.249
iteration 0400: loss: 1956.529
iteration 0500: loss: 1956.454
iteration 0600: loss: 1956.106
iteration 0700: loss: 1955.434
iteration 0800: loss: 1955.815
iteration 0900: loss: 1956.115
====> Epoch: 047 Train loss: 1956.2930  took : 8.636598348617554
====> Test loss: 1951.3925
iteration 0000: loss: 1955.397
iteration 0100: loss: 1956.312
iteration 0200: loss: 1955.432
iteration 0300: loss: 1956.575
iteration 0400: loss: 1955.491
iteration 0500: loss: 1955.939
iteration 0600: loss: 1957.086
iteration 0700: loss: 1956.823
iteration 0800: loss: 1957.222
iteration 0900: loss: 1958.075
====> Epoch: 048 Train loss: 1956.2398  took : 8.649175643920898
====> Test loss: 1951.3173
iteration 0000: loss: 1956.234
iteration 0100: loss: 1955.856
iteration 0200: loss: 1959.272
iteration 0300: loss: 1955.574
iteration 0400: loss: 1956.498
iteration 0500: loss: 1956.503
iteration 0600: loss: 1955.716
iteration 0700: loss: 1955.969
iteration 0800: loss: 1955.074
iteration 0900: loss: 1956.193
====> Epoch: 049 Train loss: 1956.2567  took : 8.626099586486816
====> Test loss: 1951.3588
iteration 0000: loss: 1955.740
iteration 0100: loss: 1956.496
iteration 0200: loss: 1956.855
iteration 0300: loss: 1956.733
iteration 0400: loss: 1956.294
iteration 0500: loss: 1956.675
iteration 0600: loss: 1957.721
iteration 0700: loss: 1955.744
iteration 0800: loss: 1955.243
iteration 0900: loss: 1955.939
====> Epoch: 050 Train loss: 1956.2379  took : 8.830125570297241
====> Test loss: 1951.5754
====> [MM-VAE] Time: 568.367s or 00:09:28
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5264.734
iteration 0100: loss: 4131.019
iteration 0200: loss: 4076.258
iteration 0300: loss: 4043.209
iteration 0400: loss: 4008.480
iteration 0500: loss: 4012.744
iteration 0600: loss: 4000.029
iteration 0700: loss: 3990.316
iteration 0800: loss: 3983.734
iteration 0900: loss: 3977.858
iteration 1000: loss: 3987.921
iteration 1100: loss: 3981.496
iteration 1200: loss: 3978.550
iteration 1300: loss: 3993.102
iteration 1400: loss: 3981.074
iteration 1500: loss: 3988.562
iteration 1600: loss: 3970.171
iteration 1700: loss: 3970.934
iteration 1800: loss: 3972.843
====> Epoch: 001 Train loss: 4013.0277  took : 140.5535523891449
====> Test loss: 3981.5260
iteration 0000: loss: 3971.480
iteration 0100: loss: 3981.792
iteration 0200: loss: 3973.248
iteration 0300: loss: 3973.474
iteration 0400: loss: 3962.737
iteration 0500: loss: 3972.116
iteration 0600: loss: 3976.528
iteration 0700: loss: 3985.869
iteration 0800: loss: 3967.252
iteration 0900: loss: 3974.150
iteration 1000: loss: 3972.675
iteration 1100: loss: 3968.316
iteration 1200: loss: 3972.060
iteration 1300: loss: 3973.918
iteration 1400: loss: 3976.710
iteration 1500: loss: 3969.219
iteration 1600: loss: 3967.422
iteration 1700: loss: 3975.272
iteration 1800: loss: 3964.751
====> Epoch: 002 Train loss: 3973.7448  took : 140.8155460357666
====> Test loss: 3975.3863
iteration 0000: loss: 3965.167
iteration 0100: loss: 3969.564
iteration 0200: loss: 3969.153
iteration 0300: loss: 3974.170
iteration 0400: loss: 3963.889
iteration 0500: loss: 3967.108
iteration 0600: loss: 3975.753
iteration 0700: loss: 3970.913
iteration 0800: loss: 3969.671
iteration 0900: loss: 3970.103
iteration 1000: loss: 3961.630
iteration 1100: loss: 3967.944
iteration 1200: loss: 3961.911
iteration 1300: loss: 3968.028
iteration 1400: loss: 3975.139
iteration 1500: loss: 3973.376
iteration 1600: loss: 3965.279
iteration 1700: loss: 3965.986
iteration 1800: loss: 3954.740
====> Epoch: 003 Train loss: 3969.9032  took : 138.85006737709045
====> Test loss: 3973.3285
iteration 0000: loss: 3976.472
iteration 0100: loss: 3969.765
iteration 0200: loss: 3966.386
iteration 0300: loss: 3965.574
iteration 0400: loss: 3966.467
iteration 0500: loss: 3966.969
iteration 0600: loss: 3969.868
iteration 0700: loss: 3970.423
iteration 0800: loss: 3975.955
iteration 0900: loss: 3974.963
iteration 1000: loss: 3970.167
iteration 1100: loss: 3967.119
iteration 1200: loss: 3976.062
iteration 1300: loss: 3966.402
iteration 1400: loss: 3966.930
iteration 1500: loss: 3967.802
iteration 1600: loss: 3970.912
iteration 1700: loss: 3971.017
iteration 1800: loss: 3972.391
====> Epoch: 004 Train loss: 3968.0705  took : 139.1151053905487
====> Test loss: 3972.4344
iteration 0000: loss: 3965.343
iteration 0100: loss: 3970.498
iteration 0200: loss: 3960.646
iteration 0300: loss: 3974.960
iteration 0400: loss: 3968.144
iteration 0500: loss: 3973.263
iteration 0600: loss: 3962.760
iteration 0700: loss: 3968.347
iteration 0800: loss: 3968.779
iteration 0900: loss: 3966.861
iteration 1000: loss: 3976.227
iteration 1100: loss: 3969.684
iteration 1200: loss: 3966.516
iteration 1300: loss: 3969.417
iteration 1400: loss: 3970.690
iteration 1500: loss: 3962.626
iteration 1600: loss: 3981.777
iteration 1700: loss: 3975.588
iteration 1800: loss: 3961.393
====> Epoch: 005 Train loss: 3966.9720  took : 140.5079746246338
====> Test loss: 3970.9392
iteration 0000: loss: 3968.370
iteration 0100: loss: 3962.872
iteration 0200: loss: 3969.533
iteration 0300: loss: 3963.796
iteration 0400: loss: 3966.827
iteration 0500: loss: 3966.396
iteration 0600: loss: 3957.358
iteration 0700: loss: 3969.527
iteration 0800: loss: 3975.644
iteration 0900: loss: 3965.638
iteration 1000: loss: 3964.924
iteration 1100: loss: 3962.038
iteration 1200: loss: 3969.162
iteration 1300: loss: 3968.802
iteration 1400: loss: 3967.248
iteration 1500: loss: 3965.209
iteration 1600: loss: 3973.758
iteration 1700: loss: 3974.479
iteration 1800: loss: 3969.772
====> Epoch: 006 Train loss: 3966.0446  took : 141.04871892929077
====> Test loss: 3970.4431
iteration 0000: loss: 3961.850
iteration 0100: loss: 3972.215
iteration 0200: loss: 3973.260
iteration 0300: loss: 3972.114
iteration 0400: loss: 3970.607
iteration 0500: loss: 3966.356
iteration 0600: loss: 3966.496
iteration 0700: loss: 3960.537
iteration 0800: loss: 3967.150
iteration 0900: loss: 3966.769
iteration 1000: loss: 3961.177
iteration 1100: loss: 3965.761
iteration 1200: loss: 3968.396
iteration 1300: loss: 3962.792
iteration 1400: loss: 3963.912
iteration 1500: loss: 3955.963
iteration 1600: loss: 3965.209
iteration 1700: loss: 3964.924
iteration 1800: loss: 3962.586
====> Epoch: 007 Train loss: 3965.3126  took : 140.75331473350525
====> Test loss: 3969.8793
iteration 0000: loss: 3966.872
iteration 0100: loss: 3959.578
iteration 0200: loss: 3962.975
iteration 0300: loss: 3972.571
iteration 0400: loss: 3964.441
iteration 0500: loss: 3961.600
iteration 0600: loss: 3972.688
iteration 0700: loss: 3967.873
iteration 0800: loss: 3972.253
iteration 0900: loss: 3964.529
iteration 1000: loss: 3966.898
iteration 1100: loss: 3968.152
iteration 1200: loss: 3966.554
iteration 1300: loss: 3962.152
iteration 1400: loss: 3968.423
iteration 1500: loss: 3965.962
iteration 1600: loss: 3962.181
iteration 1700: loss: 3961.771
iteration 1800: loss: 3964.363
====> Epoch: 008 Train loss: 3964.8104  took : 139.30622029304504
====> Test loss: 3969.7064
iteration 0000: loss: 3964.734
iteration 0100: loss: 3957.541
iteration 0200: loss: 3964.355
iteration 0300: loss: 3965.988
iteration 0400: loss: 3967.000
iteration 0500: loss: 3959.666
iteration 0600: loss: 3971.999
iteration 0700: loss: 3962.820
iteration 0800: loss: 3961.948
iteration 0900: loss: 3960.406
iteration 1000: loss: 3965.680
iteration 1100: loss: 3965.086
iteration 1200: loss: 3961.267
iteration 1300: loss: 3957.853
iteration 1400: loss: 3964.373
iteration 1500: loss: 3960.389
iteration 1600: loss: 3960.984
iteration 1700: loss: 3965.569
iteration 1800: loss: 3961.184
====> Epoch: 009 Train loss: 3964.2556  took : 139.88684272766113
====> Test loss: 3969.7423
iteration 0000: loss: 3975.406
iteration 0100: loss: 3967.432
iteration 0200: loss: 3962.194
iteration 0300: loss: 3961.446
iteration 0400: loss: 3960.138
iteration 0500: loss: 3966.032
iteration 0600: loss: 3969.742
iteration 0700: loss: 3967.093
iteration 0800: loss: 3965.689
iteration 0900: loss: 3962.367
iteration 1000: loss: 3963.607
iteration 1100: loss: 3961.956
iteration 1200: loss: 3970.169
iteration 1300: loss: 3966.399
iteration 1400: loss: 3960.828
iteration 1500: loss: 3964.079
iteration 1600: loss: 3960.633
iteration 1700: loss: 3963.025
iteration 1800: loss: 3961.804
====> Epoch: 010 Train loss: 3963.7763  took : 140.32145810127258
====> Test loss: 3968.8201
iteration 0000: loss: 3959.287
iteration 0100: loss: 3969.775
iteration 0200: loss: 3953.840
iteration 0300: loss: 3973.157
iteration 0400: loss: 3963.606
iteration 0500: loss: 3969.156
iteration 0600: loss: 3960.059
iteration 0700: loss: 3971.443
iteration 0800: loss: 3959.567
iteration 0900: loss: 3962.573
iteration 1000: loss: 3961.278
iteration 1100: loss: 3960.022
iteration 1200: loss: 3957.633
iteration 1300: loss: 3962.753
iteration 1400: loss: 3959.107
iteration 1500: loss: 3968.737
iteration 1600: loss: 3960.343
iteration 1700: loss: 3965.440
iteration 1800: loss: 3971.224
====> Epoch: 011 Train loss: 3963.4672  took : 140.15893816947937
====> Test loss: 3968.8683
iteration 0000: loss: 3956.494
iteration 0100: loss: 3959.846
iteration 0200: loss: 3961.958
iteration 0300: loss: 3968.724
iteration 0400: loss: 3956.970
iteration 0500: loss: 3960.000
iteration 0600: loss: 3959.265
iteration 0700: loss: 3964.311
iteration 0800: loss: 3961.628
iteration 0900: loss: 3956.850
iteration 1000: loss: 3962.132
iteration 1100: loss: 3947.804
iteration 1200: loss: 3961.620
iteration 1300: loss: 3960.001
iteration 1400: loss: 3968.615
iteration 1500: loss: 3963.245
iteration 1600: loss: 3958.527
iteration 1700: loss: 3966.765
iteration 1800: loss: 3965.576
====> Epoch: 012 Train loss: 3963.1389  took : 137.51722359657288
====> Test loss: 3968.7440
iteration 0000: loss: 3959.480
iteration 0100: loss: 3961.691
iteration 0200: loss: 3960.580
iteration 0300: loss: 3966.430
iteration 0400: loss: 3947.848
iteration 0500: loss: 3955.644
iteration 0600: loss: 3956.169
iteration 0700: loss: 3955.778
iteration 0800: loss: 3960.731
iteration 0900: loss: 3964.363
iteration 1000: loss: 3956.768
iteration 1100: loss: 3971.553
iteration 1200: loss: 3968.921
iteration 1300: loss: 3959.787
iteration 1400: loss: 3966.082
iteration 1500: loss: 3965.331
iteration 1600: loss: 3958.607
iteration 1700: loss: 3956.539
iteration 1800: loss: 3965.560
====> Epoch: 013 Train loss: 3962.7749  took : 140.46904039382935
====> Test loss: 3967.6488
iteration 0000: loss: 3971.037
iteration 0100: loss: 3961.721
iteration 0200: loss: 3967.889
iteration 0300: loss: 3960.627
iteration 0400: loss: 3963.187
iteration 0500: loss: 3959.960
iteration 0600: loss: 3965.719
iteration 0700: loss: 3960.792
iteration 0800: loss: 3963.339
iteration 0900: loss: 3955.435
iteration 1000: loss: 3958.296
iteration 1100: loss: 3957.733
iteration 1200: loss: 3960.025
iteration 1300: loss: 3961.674
iteration 1400: loss: 3957.882
iteration 1500: loss: 3965.167
iteration 1600: loss: 3962.298
iteration 1700: loss: 3952.678
iteration 1800: loss: 3963.570
====> Epoch: 014 Train loss: 3962.5176  took : 140.77770686149597
====> Test loss: 3968.6636
iteration 0000: loss: 3957.478
iteration 0100: loss: 3960.344
iteration 0200: loss: 3963.592
iteration 0300: loss: 3962.014
iteration 0400: loss: 3957.991
iteration 0500: loss: 3960.451
iteration 0600: loss: 3962.687
iteration 0700: loss: 3970.705
iteration 0800: loss: 3968.477
iteration 0900: loss: 3954.958
iteration 1000: loss: 3956.610
iteration 1100: loss: 3956.886
iteration 1200: loss: 3963.636
iteration 1300: loss: 3966.322
iteration 1400: loss: 3962.052
iteration 1500: loss: 3964.622
iteration 1600: loss: 3974.346
iteration 1700: loss: 3952.085
iteration 1800: loss: 3957.606
====> Epoch: 015 Train loss: 3962.1456  took : 139.60577416419983
====> Test loss: 3967.2395
iteration 0000: loss: 3960.685
iteration 0100: loss: 3964.615
iteration 0200: loss: 3955.937
iteration 0300: loss: 3965.260
iteration 0400: loss: 3959.793
iteration 0500: loss: 3967.048
iteration 0600: loss: 3970.898
iteration 0700: loss: 3959.131
iteration 0800: loss: 3956.884
iteration 0900: loss: 3956.543
iteration 1000: loss: 3957.107
iteration 1100: loss: 3962.740
iteration 1200: loss: 3954.079
iteration 1300: loss: 3960.479
iteration 1400: loss: 3971.228
iteration 1500: loss: 3959.302
iteration 1600: loss: 3971.562
iteration 1700: loss: 3964.263
iteration 1800: loss: 3968.371
====> Epoch: 016 Train loss: 3961.9708  took : 138.29637837409973
====> Test loss: 3967.3779
iteration 0000: loss: 3969.429
iteration 0100: loss: 3958.246
iteration 0200: loss: 3964.223
iteration 0300: loss: 3965.487
iteration 0400: loss: 3956.410
iteration 0500: loss: 3949.173
iteration 0600: loss: 3959.385
iteration 0700: loss: 3968.154
iteration 0800: loss: 3963.031
iteration 0900: loss: 3964.987
iteration 1000: loss: 3957.508
iteration 1100: loss: 3958.756
iteration 1200: loss: 3950.907
iteration 1300: loss: 3956.587
iteration 1400: loss: 3967.171
iteration 1500: loss: 3960.685
iteration 1600: loss: 3962.933
iteration 1700: loss: 3970.511
iteration 1800: loss: 3959.529
====> Epoch: 017 Train loss: 3961.9621  took : 139.98912906646729
====> Test loss: 3967.1689
iteration 0000: loss: 3961.424
iteration 0100: loss: 3965.465
iteration 0200: loss: 3960.630
iteration 0300: loss: 3959.066
iteration 0400: loss: 3962.035
iteration 0500: loss: 3961.092
iteration 0600: loss: 3961.990
iteration 0700: loss: 3965.788
iteration 0800: loss: 3962.951
iteration 0900: loss: 3960.003
iteration 1000: loss: 3967.286
iteration 1100: loss: 3963.788
iteration 1200: loss: 3957.459
iteration 1300: loss: 3959.991
iteration 1400: loss: 3958.321
iteration 1500: loss: 3961.056
iteration 1600: loss: 3962.517
iteration 1700: loss: 3962.626
iteration 1800: loss: 3963.499
====> Epoch: 018 Train loss: 3961.6228  took : 140.26121544837952
====> Test loss: 3966.9477
iteration 0000: loss: 3968.517
iteration 0100: loss: 3959.182
iteration 0200: loss: 3965.090
iteration 0300: loss: 3967.428
iteration 0400: loss: 3949.961
iteration 0500: loss: 3960.009
iteration 0600: loss: 3962.374
iteration 0700: loss: 3963.366
iteration 0800: loss: 3958.250
iteration 0900: loss: 3959.399
iteration 1000: loss: 3969.045
iteration 1100: loss: 3957.903
iteration 1200: loss: 3958.285
iteration 1300: loss: 3960.378
iteration 1400: loss: 3958.555
iteration 1500: loss: 3956.866
iteration 1600: loss: 3968.197
iteration 1700: loss: 3959.849
iteration 1800: loss: 3955.410
====> Epoch: 019 Train loss: 3961.2714  took : 140.5455882549286
====> Test loss: 3967.3622
iteration 0000: loss: 3961.403
iteration 0100: loss: 3960.788
iteration 0200: loss: 3967.930
iteration 0300: loss: 3969.685
iteration 0400: loss: 3955.593
iteration 0500: loss: 3961.045
iteration 0600: loss: 3960.101
iteration 0700: loss: 3963.867
iteration 0800: loss: 3956.839
iteration 0900: loss: 3965.081
iteration 1000: loss: 3961.833
iteration 1100: loss: 3963.749
iteration 1200: loss: 3966.792
iteration 1300: loss: 3967.661
iteration 1400: loss: 3967.156
iteration 1500: loss: 3963.872
iteration 1600: loss: 3958.849
iteration 1700: loss: 3957.868
iteration 1800: loss: 3965.491
====> Epoch: 020 Train loss: 3961.0838  took : 140.71665501594543
====> Test loss: 3966.8656
iteration 0000: loss: 3957.521
iteration 0100: loss: 3960.945
iteration 0200: loss: 3958.646
iteration 0300: loss: 3965.231
iteration 0400: loss: 3960.002
iteration 0500: loss: 3952.075
iteration 0600: loss: 3952.410
iteration 0700: loss: 3967.184
iteration 0800: loss: 3961.679
iteration 0900: loss: 3955.152
iteration 1000: loss: 3961.751
iteration 1100: loss: 3976.766
iteration 1200: loss: 3953.197
iteration 1300: loss: 3965.539
iteration 1400: loss: 3970.185
iteration 1500: loss: 3957.581
iteration 1600: loss: 3966.224
iteration 1700: loss: 3957.871
iteration 1800: loss: 3961.608
====> Epoch: 021 Train loss: 3961.0523  took : 139.62415766716003
====> Test loss: 3966.7157
iteration 0000: loss: 3956.899
iteration 0100: loss: 3957.803
iteration 0200: loss: 3965.958
iteration 0300: loss: 3957.425
iteration 0400: loss: 3959.732
iteration 0500: loss: 3964.643
iteration 0600: loss: 3965.205
iteration 0700: loss: 3967.915
iteration 0800: loss: 3961.159
iteration 0900: loss: 3958.736
iteration 1000: loss: 3965.519
iteration 1100: loss: 3962.263
iteration 1200: loss: 3962.661
iteration 1300: loss: 3953.770
iteration 1400: loss: 3964.824
iteration 1500: loss: 3959.772
iteration 1600: loss: 3974.539
iteration 1700: loss: 3964.221
iteration 1800: loss: 3959.521
====> Epoch: 022 Train loss: 3961.0451  took : 140.5231215953827
====> Test loss: 3966.1706
iteration 0000: loss: 3961.606
iteration 0100: loss: 3956.269
iteration 0200: loss: 3960.680
iteration 0300: loss: 3962.821
iteration 0400: loss: 3958.335
iteration 0500: loss: 3962.976
iteration 0600: loss: 3959.494
iteration 0700: loss: 3957.250
iteration 0800: loss: 3957.795
iteration 0900: loss: 3956.197
iteration 1000: loss: 3964.176
iteration 1100: loss: 3955.939
iteration 1200: loss: 3957.821
iteration 1300: loss: 3964.245
iteration 1400: loss: 3956.728
iteration 1500: loss: 3968.439
iteration 1600: loss: 3967.503
iteration 1700: loss: 3966.380
iteration 1800: loss: 3960.969
====> Epoch: 023 Train loss: 3960.9406  took : 140.37996196746826
====> Test loss: 3967.5104
iteration 0000: loss: 3959.891
iteration 0100: loss: 3958.696
iteration 0200: loss: 3956.458
iteration 0300: loss: 3956.068
iteration 0400: loss: 3956.816
iteration 0500: loss: 3961.762
iteration 0600: loss: 3966.075
iteration 0700: loss: 3963.979
iteration 0800: loss: 3973.607
iteration 0900: loss: 3965.154
iteration 1000: loss: 3966.313
iteration 1100: loss: 3962.483
iteration 1200: loss: 3963.440
iteration 1300: loss: 3960.655
iteration 1400: loss: 3959.709
iteration 1500: loss: 3953.473
iteration 1600: loss: 3957.776
iteration 1700: loss: 3955.375
iteration 1800: loss: 3957.792
====> Epoch: 024 Train loss: 3960.8847  took : 140.39512491226196
====> Test loss: 3966.2925
iteration 0000: loss: 3968.644
iteration 0100: loss: 3960.142
iteration 0200: loss: 3960.681
iteration 0300: loss: 3963.951
iteration 0400: loss: 3959.258
iteration 0500: loss: 3961.586
iteration 0600: loss: 3956.479
iteration 0700: loss: 3957.360
iteration 0800: loss: 3961.839
iteration 0900: loss: 3962.370
iteration 1000: loss: 3961.841
iteration 1100: loss: 3961.022
iteration 1200: loss: 3959.889
iteration 1300: loss: 3968.963
iteration 1400: loss: 3959.110
iteration 1500: loss: 3963.331
iteration 1600: loss: 3955.286
iteration 1700: loss: 3961.478
iteration 1800: loss: 3952.797
====> Epoch: 025 Train loss: 3960.6886  took : 140.15092277526855
====> Test loss: 3966.0915
iteration 0000: loss: 3965.246
iteration 0100: loss: 3955.374
iteration 0200: loss: 3967.921
iteration 0300: loss: 3963.533
iteration 0400: loss: 3956.672
iteration 0500: loss: 3964.526
iteration 0600: loss: 3955.547
iteration 0700: loss: 3959.302
iteration 0800: loss: 3962.824
iteration 0900: loss: 3964.150
iteration 1000: loss: 3958.523
iteration 1100: loss: 3963.379
iteration 1200: loss: 3965.149
iteration 1300: loss: 3957.583
iteration 1400: loss: 3957.177
iteration 1500: loss: 3961.791
iteration 1600: loss: 3960.066
iteration 1700: loss: 3964.083
iteration 1800: loss: 3964.727
====> Epoch: 026 Train loss: 3960.5225  took : 139.55947351455688
====> Test loss: 3966.6998
iteration 0000: loss: 3961.134
iteration 0100: loss: 3962.594
iteration 0200: loss: 3954.291
iteration 0300: loss: 3955.663
iteration 0400: loss: 3958.265
iteration 0500: loss: 3951.345
iteration 0600: loss: 3957.295
iteration 0700: loss: 3961.721
iteration 0800: loss: 3954.693
iteration 0900: loss: 3958.344
iteration 1000: loss: 3954.167
iteration 1100: loss: 3969.855
iteration 1200: loss: 3948.457
iteration 1300: loss: 3954.357
iteration 1400: loss: 3953.477
iteration 1500: loss: 3966.969
iteration 1600: loss: 3954.867
iteration 1700: loss: 3967.931
iteration 1800: loss: 3949.088
====> Epoch: 027 Train loss: 3960.6394  took : 140.2983808517456
====> Test loss: 3966.2855
iteration 0000: loss: 3961.892
iteration 0100: loss: 3961.547
iteration 0200: loss: 3969.000
iteration 0300: loss: 3957.325
iteration 0400: loss: 3961.170
iteration 0500: loss: 3963.783
iteration 0600: loss: 3956.774
iteration 0700: loss: 3959.859
iteration 0800: loss: 3956.064
iteration 0900: loss: 3953.070
iteration 1000: loss: 3958.540
iteration 1100: loss: 3957.661
iteration 1200: loss: 3962.752
iteration 1300: loss: 3958.819
iteration 1400: loss: 3964.797
iteration 1500: loss: 3962.039
iteration 1600: loss: 3960.427
iteration 1700: loss: 3958.149
iteration 1800: loss: 3958.701
====> Epoch: 028 Train loss: 3960.3139  took : 140.20131635665894
====> Test loss: 3966.6642
iteration 0000: loss: 3958.103
iteration 0100: loss: 3954.285
iteration 0200: loss: 3965.970
iteration 0300: loss: 3968.359
iteration 0400: loss: 3955.096
iteration 0500: loss: 3964.442
iteration 0600: loss: 3952.564
iteration 0700: loss: 3959.396
iteration 0800: loss: 3964.772
iteration 0900: loss: 3959.060
iteration 1000: loss: 3960.377
iteration 1100: loss: 3964.493
iteration 1200: loss: 3962.483
iteration 1300: loss: 3965.959
iteration 1400: loss: 3951.333
iteration 1500: loss: 3954.396
iteration 1600: loss: 3969.079
iteration 1700: loss: 3967.380
iteration 1800: loss: 3962.664
====> Epoch: 029 Train loss: 3960.2274  took : 139.72348856925964
====> Test loss: 3966.1149
iteration 0000: loss: 3974.198
iteration 0100: loss: 3952.297
iteration 0200: loss: 3962.458
iteration 0300: loss: 3965.538
iteration 0400: loss: 3955.695
iteration 0500: loss: 3955.774
iteration 0600: loss: 3964.118
iteration 0700: loss: 3959.762
iteration 0800: loss: 3970.898
iteration 0900: loss: 3956.550
iteration 1000: loss: 3960.051
iteration 1100: loss: 3962.164
iteration 1200: loss: 3968.599
iteration 1300: loss: 3952.416
iteration 1400: loss: 3957.514
iteration 1500: loss: 3956.544
iteration 1600: loss: 3958.421
iteration 1700: loss: 3958.292
iteration 1800: loss: 3958.400
====> Epoch: 030 Train loss: 3960.2325  took : 137.4482729434967
====> Test loss: 3966.1330
iteration 0000: loss: 3953.231
iteration 0100: loss: 3963.292
iteration 0200: loss: 3958.903
iteration 0300: loss: 3948.172
iteration 0400: loss: 3958.764
iteration 0500: loss: 3964.524
iteration 0600: loss: 3959.663
iteration 0700: loss: 3957.749
iteration 0800: loss: 3963.490
iteration 0900: loss: 3965.639
iteration 1000: loss: 3959.486
iteration 1100: loss: 3957.433
iteration 1200: loss: 3955.769
iteration 1300: loss: 3962.836
iteration 1400: loss: 3954.859
iteration 1500: loss: 3955.526
iteration 1600: loss: 3954.333
iteration 1700: loss: 3956.911
iteration 1800: loss: 3957.435
====> Epoch: 031 Train loss: 3960.2205  took : 140.05126786231995
====> Test loss: 3966.2919
iteration 0000: loss: 3957.600
iteration 0100: loss: 3959.701
iteration 0200: loss: 3958.129
iteration 0300: loss: 3959.423
iteration 0400: loss: 3956.484
iteration 0500: loss: 3957.096
iteration 0600: loss: 3967.808
iteration 0700: loss: 3959.882
iteration 0800: loss: 3955.312
iteration 0900: loss: 3979.375
iteration 1000: loss: 3956.703
iteration 1100: loss: 3966.338
iteration 1200: loss: 3964.617
iteration 1300: loss: 3953.385
iteration 1400: loss: 3968.204
iteration 1500: loss: 3961.809
iteration 1600: loss: 3956.151
iteration 1700: loss: 3950.746
iteration 1800: loss: 3962.737
====> Epoch: 032 Train loss: 3960.3474  took : 140.15434336662292
====> Test loss: 3966.0211
iteration 0000: loss: 3961.279
iteration 0100: loss: 3960.666
iteration 0200: loss: 3964.560
iteration 0300: loss: 3955.059
iteration 0400: loss: 3959.180
iteration 0500: loss: 3962.106
iteration 0600: loss: 3961.189
iteration 0700: loss: 3954.666
iteration 0800: loss: 3957.133
iteration 0900: loss: 3966.070
iteration 1000: loss: 3952.179
iteration 1100: loss: 3961.993
iteration 1200: loss: 3954.906
iteration 1300: loss: 3959.675
iteration 1400: loss: 3956.662
iteration 1500: loss: 3959.276
iteration 1600: loss: 3957.193
iteration 1700: loss: 3964.182
iteration 1800: loss: 3958.757
====> Epoch: 033 Train loss: 3960.0114  took : 139.75694370269775
====> Test loss: 3965.6880
iteration 0000: loss: 3960.749
iteration 0100: loss: 3959.151
iteration 0200: loss: 3955.501
iteration 0300: loss: 3960.498
iteration 0400: loss: 3960.246
iteration 0500: loss: 3963.783
iteration 0600: loss: 3955.578
iteration 0700: loss: 3961.064
iteration 0800: loss: 3965.045
iteration 0900: loss: 3957.996
iteration 1000: loss: 3956.621
iteration 1100: loss: 3955.825
iteration 1200: loss: 3958.909
iteration 1300: loss: 3958.885
iteration 1400: loss: 3955.129
iteration 1500: loss: 3960.909
iteration 1600: loss: 3962.411
iteration 1700: loss: 3966.339
iteration 1800: loss: 3962.539
====> Epoch: 034 Train loss: 3959.8726  took : 140.5063259601593
====> Test loss: 3965.3658
iteration 0000: loss: 3958.945
iteration 0100: loss: 3960.747
iteration 0200: loss: 3968.243
iteration 0300: loss: 3958.101
iteration 0400: loss: 3955.304
iteration 0500: loss: 3959.508
iteration 0600: loss: 3972.747
iteration 0700: loss: 3958.451
iteration 0800: loss: 3952.812
iteration 0900: loss: 3962.992
iteration 1000: loss: 3964.643
iteration 1100: loss: 3957.916
iteration 1200: loss: 3951.462
iteration 1300: loss: 3955.472
iteration 1400: loss: 3965.241
iteration 1500: loss: 3960.562
iteration 1600: loss: 3963.208
iteration 1700: loss: 3964.912
iteration 1800: loss: 3954.825
====> Epoch: 035 Train loss: 3959.8242  took : 140.22138166427612
====> Test loss: 3965.9662
iteration 0000: loss: 3961.820
iteration 0100: loss: 3964.799
iteration 0200: loss: 3963.958
iteration 0300: loss: 3955.252
iteration 0400: loss: 3963.975
iteration 0500: loss: 3963.494
iteration 0600: loss: 3959.223
iteration 0700: loss: 3947.604
iteration 0800: loss: 3976.224
iteration 0900: loss: 3958.962
iteration 1000: loss: 3963.288
iteration 1100: loss: 3962.684
iteration 1200: loss: 3960.680
iteration 1300: loss: 3961.784
iteration 1400: loss: 3957.946
iteration 1500: loss: 3956.327
iteration 1600: loss: 3953.982
iteration 1700: loss: 3955.753
iteration 1800: loss: 3965.004
====> Epoch: 036 Train loss: 3959.7570  took : 138.0841965675354
====> Test loss: 3965.9518
iteration 0000: loss: 3955.534
iteration 0100: loss: 3958.426
iteration 0200: loss: 3954.030
iteration 0300: loss: 3964.589
iteration 0400: loss: 3961.491
iteration 0500: loss: 3957.675
iteration 0600: loss: 3961.772
iteration 0700: loss: 3964.644
iteration 0800: loss: 3965.659
iteration 0900: loss: 3957.103
iteration 1000: loss: 3958.282
iteration 1100: loss: 3962.643
iteration 1200: loss: 3954.835
iteration 1300: loss: 3957.681
iteration 1400: loss: 3956.454
iteration 1500: loss: 3962.349
iteration 1600: loss: 3950.425
iteration 1700: loss: 3961.745
iteration 1800: loss: 3970.634
====> Epoch: 037 Train loss: 3959.6472  took : 139.996187210083
====> Test loss: 3965.5138
iteration 0000: loss: 3960.863
iteration 0100: loss: 3968.065
iteration 0200: loss: 3964.087
iteration 0300: loss: 3956.427
iteration 0400: loss: 3958.191
iteration 0500: loss: 3962.130
iteration 0600: loss: 3965.342
iteration 0700: loss: 3961.227
iteration 0800: loss: 3958.417
iteration 0900: loss: 3955.883
iteration 1000: loss: 3960.664
iteration 1100: loss: 3953.034
iteration 1200: loss: 3965.236
iteration 1300: loss: 3960.169
iteration 1400: loss: 3955.282
iteration 1500: loss: 3965.211
iteration 1600: loss: 3955.315
iteration 1700: loss: 3955.852
iteration 1800: loss: 3964.023
====> Epoch: 038 Train loss: 3959.8459  took : 139.21705436706543
====> Test loss: 3965.7306
iteration 0000: loss: 3958.519
iteration 0100: loss: 3958.844
iteration 0200: loss: 3958.288
iteration 0300: loss: 3952.535
iteration 0400: loss: 3964.044
iteration 0500: loss: 3960.935
iteration 0600: loss: 3956.808
iteration 0700: loss: 3952.681
iteration 0800: loss: 3960.387
iteration 0900: loss: 3964.068
iteration 1000: loss: 3963.684
iteration 1100: loss: 3967.021
iteration 1200: loss: 3965.165
iteration 1300: loss: 3961.912
iteration 1400: loss: 3954.734
iteration 1500: loss: 3962.404
iteration 1600: loss: 3960.048
iteration 1700: loss: 3953.759
iteration 1800: loss: 3959.819
====> Epoch: 039 Train loss: 3959.5781  took : 139.76180768013
====> Test loss: 3965.6184
iteration 0000: loss: 3952.710
iteration 0100: loss: 3956.979
iteration 0200: loss: 3967.674
iteration 0300: loss: 3955.809
iteration 0400: loss: 3959.011
iteration 0500: loss: 3958.183
iteration 0600: loss: 3960.299
iteration 0700: loss: 3955.293
iteration 0800: loss: 3964.418
iteration 0900: loss: 3954.734
iteration 1000: loss: 3960.165
iteration 1100: loss: 3961.684
iteration 1200: loss: 3955.346
iteration 1300: loss: 3962.994
iteration 1400: loss: 3961.856
iteration 1500: loss: 3955.831
iteration 1600: loss: 3956.896
iteration 1700: loss: 3954.264
iteration 1800: loss: 3957.008
====> Epoch: 040 Train loss: 3959.8046  took : 139.83459186553955
====> Test loss: 3965.6225
iteration 0000: loss: 3953.870
iteration 0100: loss: 3964.691
iteration 0200: loss: 3956.781
iteration 0300: loss: 3955.948
iteration 0400: loss: 3957.035
iteration 0500: loss: 3956.834
iteration 0600: loss: 3957.151
iteration 0700: loss: 3963.934
iteration 0800: loss: 3964.673
iteration 0900: loss: 3958.697
iteration 1000: loss: 3953.523
iteration 1100: loss: 3962.969
iteration 1200: loss: 3964.313
iteration 1300: loss: 3961.921
iteration 1400: loss: 3960.119
iteration 1500: loss: 3963.529
iteration 1600: loss: 3955.190
iteration 1700: loss: 3952.998
iteration 1800: loss: 3955.469
====> Epoch: 041 Train loss: 3959.5441  took : 139.72822737693787
====> Test loss: 3965.2492
iteration 0000: loss: 3955.781
iteration 0100: loss: 3953.836
iteration 0200: loss: 3964.569
iteration 0300: loss: 3954.524
iteration 0400: loss: 3960.024
iteration 0500: loss: 3954.115
iteration 0600: loss: 3956.152
iteration 0700: loss: 3968.112
iteration 0800: loss: 3958.805
iteration 0900: loss: 3958.639
iteration 1000: loss: 3954.165
iteration 1100: loss: 3955.756
iteration 1200: loss: 3958.722
iteration 1300: loss: 3957.131
iteration 1400: loss: 3957.817
iteration 1500: loss: 3963.543
iteration 1600: loss: 3962.959
iteration 1700: loss: 3957.980
iteration 1800: loss: 3962.673
====> Epoch: 042 Train loss: 3959.4341  took : 140.07184863090515
====> Test loss: 3965.1711
iteration 0000: loss: 3952.574
iteration 0100: loss: 3957.709
iteration 0200: loss: 3973.731
iteration 0300: loss: 3952.447
iteration 0400: loss: 3966.989
iteration 0500: loss: 3953.333
iteration 0600: loss: 3959.406
iteration 0700: loss: 3959.105
iteration 0800: loss: 3961.976
iteration 0900: loss: 3960.858
iteration 1000: loss: 3962.776
iteration 1100: loss: 3957.549
iteration 1200: loss: 3967.737
iteration 1300: loss: 3955.740
iteration 1400: loss: 3955.351
iteration 1500: loss: 3965.791
iteration 1600: loss: 3960.445
iteration 1700: loss: 3960.856
iteration 1800: loss: 3961.394
====> Epoch: 043 Train loss: 3959.4770  took : 140.31834363937378
====> Test loss: 3965.4011
iteration 0000: loss: 3961.662
iteration 0100: loss: 3950.151
iteration 0200: loss: 3954.242
iteration 0300: loss: 3963.168
iteration 0400: loss: 3959.369
iteration 0500: loss: 3958.767
iteration 0600: loss: 3959.463
iteration 0700: loss: 3954.667
iteration 0800: loss: 3964.729
iteration 0900: loss: 3953.658
iteration 1000: loss: 3957.207
iteration 1100: loss: 3962.047
iteration 1200: loss: 3960.125
iteration 1300: loss: 3955.061
iteration 1400: loss: 3953.132
iteration 1500: loss: 3960.848
iteration 1600: loss: 3955.809
iteration 1700: loss: 3960.986
iteration 1800: loss: 3959.372
====> Epoch: 044 Train loss: 3959.1952  took : 139.39378476142883
====> Test loss: 3966.3106
iteration 0000: loss: 3960.785
iteration 0100: loss: 3958.765
iteration 0200: loss: 3968.251
iteration 0300: loss: 3968.203
iteration 0400: loss: 3955.081
iteration 0500: loss: 3955.769
iteration 0600: loss: 3960.709
iteration 0700: loss: 3957.484
iteration 0800: loss: 3956.028
iteration 0900: loss: 3957.403
iteration 1000: loss: 3957.263
iteration 1100: loss: 3952.655
iteration 1200: loss: 3956.010
iteration 1300: loss: 3956.459
iteration 1400: loss: 3957.772
iteration 1500: loss: 3970.631
iteration 1600: loss: 3957.257
iteration 1700: loss: 3965.220
iteration 1800: loss: 3960.672
====> Epoch: 045 Train loss: 3959.2563  took : 139.81690549850464
====> Test loss: 3965.0524
iteration 0000: loss: 3956.619
iteration 0100: loss: 3957.263
iteration 0200: loss: 3955.326
iteration 0300: loss: 3964.702
iteration 0400: loss: 3954.403
iteration 0500: loss: 3958.215
iteration 0600: loss: 3955.183
iteration 0700: loss: 3961.461
iteration 0800: loss: 3957.377
iteration 0900: loss: 3959.025
iteration 1000: loss: 3956.909
iteration 1100: loss: 3962.145
iteration 1200: loss: 3964.973
iteration 1300: loss: 3962.577
iteration 1400: loss: 3962.409
iteration 1500: loss: 3955.801
iteration 1600: loss: 3950.202
iteration 1700: loss: 3954.442
iteration 1800: loss: 3957.452
====> Epoch: 046 Train loss: 3959.3095  took : 139.94506907463074
====> Test loss: 3965.3928
iteration 0000: loss: 3965.679
iteration 0100: loss: 3969.674
iteration 0200: loss: 3953.970
iteration 0300: loss: 3951.371
iteration 0400: loss: 3955.691
iteration 0500: loss: 3956.230
iteration 0600: loss: 3961.221
iteration 0700: loss: 3956.501
iteration 0800: loss: 3957.128
iteration 0900: loss: 3959.372
iteration 1000: loss: 3952.771
iteration 1100: loss: 3965.773
iteration 1200: loss: 3955.397
iteration 1300: loss: 3957.351
iteration 1400: loss: 3958.618
iteration 1500: loss: 3965.211
iteration 1600: loss: 3956.999
iteration 1700: loss: 3958.744
iteration 1800: loss: 3958.878
====> Epoch: 047 Train loss: 3959.2782  took : 137.638587474823
====> Test loss: 3966.2128
iteration 0000: loss: 3955.864
iteration 0100: loss: 3960.995
iteration 0200: loss: 3958.589
iteration 0300: loss: 3962.469
iteration 0400: loss: 3960.644
iteration 0500: loss: 3959.833
iteration 0600: loss: 3960.235
iteration 0700: loss: 3958.812
iteration 0800: loss: 3957.898
iteration 0900: loss: 3957.568
iteration 1000: loss: 3951.826
iteration 1100: loss: 3957.129
iteration 1200: loss: 3958.684
iteration 1300: loss: 3956.736
iteration 1400: loss: 3964.453
iteration 1500: loss: 3959.295
iteration 1600: loss: 3961.450
iteration 1700: loss: 3950.955
iteration 1800: loss: 3960.527
====> Epoch: 048 Train loss: 3959.0615  took : 140.4349765777588
====> Test loss: 3964.8260
iteration 0000: loss: 3956.704
iteration 0100: loss: 3948.989
iteration 0200: loss: 3947.664
iteration 0300: loss: 3958.345
iteration 0400: loss: 3961.351
iteration 0500: loss: 3967.901
iteration 0600: loss: 3955.515
iteration 0700: loss: 3967.354
iteration 0800: loss: 3962.942
iteration 0900: loss: 3963.276
iteration 1000: loss: 3961.206
iteration 1100: loss: 3958.607
iteration 1200: loss: 3964.256
iteration 1300: loss: 3959.609
iteration 1400: loss: 3963.273
iteration 1500: loss: 3962.082
iteration 1600: loss: 3966.019
iteration 1700: loss: 3957.655
iteration 1800: loss: 3952.087
====> Epoch: 049 Train loss: 3958.9828  took : 138.82829976081848
====> Test loss: 3964.9549
iteration 0000: loss: 3958.722
iteration 0100: loss: 3946.448
iteration 0200: loss: 3957.833
iteration 0300: loss: 3956.743
iteration 0400: loss: 3956.877
iteration 0500: loss: 3964.853
iteration 0600: loss: 3964.291
iteration 0700: loss: 3952.217
iteration 0800: loss: 3963.323
iteration 0900: loss: 3957.885
iteration 1000: loss: 3959.473
iteration 1100: loss: 3959.790
iteration 1200: loss: 3961.090
iteration 1300: loss: 3960.232
iteration 1400: loss: 3961.982
iteration 1500: loss: 3965.229
iteration 1600: loss: 3953.634
iteration 1700: loss: 3955.340
iteration 1800: loss: 3953.290
====> Epoch: 050 Train loss: 3959.0318  took : 138.9431276321411
====> Test loss: 3964.7946
====> [MM-VAE] Time: 7985.267s or 02:13:05
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_4
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1993.524
iteration 0100: loss: 1561.219
iteration 0200: loss: 1555.871
iteration 0300: loss: 1545.247
iteration 0400: loss: 1552.786
iteration 0500: loss: 1542.813
iteration 0600: loss: 1542.755
iteration 0700: loss: 1537.459
iteration 0800: loss: 1537.816
iteration 0900: loss: 1533.737
====> Epoch: 001 Train loss: 1551.0419  took : 5.87630033493042
====> Test loss: 1531.6262
iteration 0000: loss: 1538.727
iteration 0100: loss: 1535.260
iteration 0200: loss: 1534.888
iteration 0300: loss: 1536.174
iteration 0400: loss: 1535.584
iteration 0500: loss: 1539.712
iteration 0600: loss: 1535.173
iteration 0700: loss: 1534.192
iteration 0800: loss: 1531.385
iteration 0900: loss: 1531.758
====> Epoch: 002 Train loss: 1533.8789  took : 5.990581035614014
====> Test loss: 1527.3146
iteration 0000: loss: 1534.255
iteration 0100: loss: 1532.072
iteration 0200: loss: 1529.984
iteration 0300: loss: 1530.233
iteration 0400: loss: 1530.343
iteration 0500: loss: 1534.356
iteration 0600: loss: 1530.954
iteration 0700: loss: 1533.132
iteration 0800: loss: 1530.312
iteration 0900: loss: 1531.686
====> Epoch: 003 Train loss: 1531.0845  took : 5.8926100730896
====> Test loss: 1525.5936
iteration 0000: loss: 1532.433
iteration 0100: loss: 1529.847
iteration 0200: loss: 1529.054
iteration 0300: loss: 1532.446
iteration 0400: loss: 1530.473
iteration 0500: loss: 1528.316
iteration 0600: loss: 1527.724
iteration 0700: loss: 1531.099
iteration 0800: loss: 1529.666
iteration 0900: loss: 1533.167
====> Epoch: 004 Train loss: 1529.7895  took : 5.886993885040283
====> Test loss: 1524.6685
iteration 0000: loss: 1531.394
iteration 0100: loss: 1527.473
iteration 0200: loss: 1526.405
iteration 0300: loss: 1527.901
iteration 0400: loss: 1527.484
iteration 0500: loss: 1528.402
iteration 0600: loss: 1528.382
iteration 0700: loss: 1529.626
iteration 0800: loss: 1531.657
iteration 0900: loss: 1524.184
====> Epoch: 005 Train loss: 1529.0319  took : 5.936479091644287
====> Test loss: 1524.0629
iteration 0000: loss: 1527.070
iteration 0100: loss: 1530.497
iteration 0200: loss: 1527.774
iteration 0300: loss: 1527.588
iteration 0400: loss: 1530.200
iteration 0500: loss: 1529.116
iteration 0600: loss: 1527.540
iteration 0700: loss: 1529.819
iteration 0800: loss: 1527.568
iteration 0900: loss: 1529.036
====> Epoch: 006 Train loss: 1528.4901  took : 5.926530838012695
====> Test loss: 1523.6574
iteration 0000: loss: 1529.661
iteration 0100: loss: 1527.890
iteration 0200: loss: 1531.822
iteration 0300: loss: 1528.797
iteration 0400: loss: 1529.377
iteration 0500: loss: 1529.789
iteration 0600: loss: 1525.537
iteration 0700: loss: 1528.568
iteration 0800: loss: 1526.405
iteration 0900: loss: 1527.448
====> Epoch: 007 Train loss: 1528.1312  took : 5.946073293685913
====> Test loss: 1523.2528
iteration 0000: loss: 1525.876
iteration 0100: loss: 1528.989
iteration 0200: loss: 1531.146
iteration 0300: loss: 1524.760
iteration 0400: loss: 1529.403
iteration 0500: loss: 1528.641
iteration 0600: loss: 1528.183
iteration 0700: loss: 1526.702
iteration 0800: loss: 1526.214
iteration 0900: loss: 1526.055
====> Epoch: 008 Train loss: 1527.7841  took : 5.9065892696380615
====> Test loss: 1522.9715
iteration 0000: loss: 1525.697
iteration 0100: loss: 1523.869
iteration 0200: loss: 1522.793
iteration 0300: loss: 1529.669
iteration 0400: loss: 1526.809
iteration 0500: loss: 1526.178
iteration 0600: loss: 1527.951
iteration 0700: loss: 1527.433
iteration 0800: loss: 1527.104
iteration 0900: loss: 1525.573
====> Epoch: 009 Train loss: 1527.5650  took : 5.991089105606079
====> Test loss: 1522.7224
iteration 0000: loss: 1524.024
iteration 0100: loss: 1527.555
iteration 0200: loss: 1528.332
iteration 0300: loss: 1526.711
iteration 0400: loss: 1528.666
iteration 0500: loss: 1528.950
iteration 0600: loss: 1527.894
iteration 0700: loss: 1526.277
iteration 0800: loss: 1525.441
iteration 0900: loss: 1527.483
====> Epoch: 010 Train loss: 1527.3028  took : 5.927637100219727
====> Test loss: 1522.6089
iteration 0000: loss: 1527.535
iteration 0100: loss: 1526.753
iteration 0200: loss: 1528.485
iteration 0300: loss: 1527.236
iteration 0400: loss: 1528.280
iteration 0500: loss: 1527.884
iteration 0600: loss: 1527.435
iteration 0700: loss: 1526.981
iteration 0800: loss: 1526.980
iteration 0900: loss: 1526.800
====> Epoch: 011 Train loss: 1527.1153  took : 5.917280435562134
====> Test loss: 1522.3425
iteration 0000: loss: 1525.551
iteration 0100: loss: 1529.112
iteration 0200: loss: 1525.838
iteration 0300: loss: 1526.426
iteration 0400: loss: 1528.076
iteration 0500: loss: 1527.533
iteration 0600: loss: 1525.386
iteration 0700: loss: 1527.148
iteration 0800: loss: 1525.521
iteration 0900: loss: 1528.251
====> Epoch: 012 Train loss: 1526.9222  took : 5.917331695556641
====> Test loss: 1522.2967
iteration 0000: loss: 1528.280
iteration 0100: loss: 1528.743
iteration 0200: loss: 1528.369
iteration 0300: loss: 1526.101
iteration 0400: loss: 1526.322
iteration 0500: loss: 1524.041
iteration 0600: loss: 1523.412
iteration 0700: loss: 1523.396
iteration 0800: loss: 1527.455
iteration 0900: loss: 1529.717
====> Epoch: 013 Train loss: 1526.7957  took : 5.927804708480835
====> Test loss: 1522.0773
iteration 0000: loss: 1526.890
iteration 0100: loss: 1525.958
iteration 0200: loss: 1527.220
iteration 0300: loss: 1523.353
iteration 0400: loss: 1528.884
iteration 0500: loss: 1525.395
iteration 0600: loss: 1528.636
iteration 0700: loss: 1529.112
iteration 0800: loss: 1528.871
iteration 0900: loss: 1524.101
====> Epoch: 014 Train loss: 1526.6336  took : 5.98316502571106
====> Test loss: 1521.9248
iteration 0000: loss: 1529.999
iteration 0100: loss: 1530.639
iteration 0200: loss: 1524.920
iteration 0300: loss: 1525.670
iteration 0400: loss: 1524.038
iteration 0500: loss: 1527.850
iteration 0600: loss: 1526.659
iteration 0700: loss: 1527.906
iteration 0800: loss: 1523.800
iteration 0900: loss: 1529.749
====> Epoch: 015 Train loss: 1526.5148  took : 5.9137794971466064
====> Test loss: 1521.7752
iteration 0000: loss: 1525.512
iteration 0100: loss: 1526.514
iteration 0200: loss: 1525.124
iteration 0300: loss: 1527.515
iteration 0400: loss: 1527.546
iteration 0500: loss: 1524.700
iteration 0600: loss: 1524.141
iteration 0700: loss: 1524.007
iteration 0800: loss: 1527.150
iteration 0900: loss: 1527.977
====> Epoch: 016 Train loss: 1526.3805  took : 5.915731191635132
====> Test loss: 1521.7548
iteration 0000: loss: 1526.947
iteration 0100: loss: 1527.743
iteration 0200: loss: 1528.170
iteration 0300: loss: 1526.180
iteration 0400: loss: 1526.468
iteration 0500: loss: 1525.020
iteration 0600: loss: 1523.752
iteration 0700: loss: 1526.540
iteration 0800: loss: 1528.439
iteration 0900: loss: 1529.934
====> Epoch: 017 Train loss: 1526.2840  took : 5.915314435958862
====> Test loss: 1521.6946
iteration 0000: loss: 1523.339
iteration 0100: loss: 1524.644
iteration 0200: loss: 1527.022
iteration 0300: loss: 1526.682
iteration 0400: loss: 1526.531
iteration 0500: loss: 1525.174
iteration 0600: loss: 1524.016
iteration 0700: loss: 1527.308
iteration 0800: loss: 1528.117
iteration 0900: loss: 1533.945
====> Epoch: 018 Train loss: 1526.1987  took : 5.965005159378052
====> Test loss: 1521.6650
iteration 0000: loss: 1526.942
iteration 0100: loss: 1525.851
iteration 0200: loss: 1526.684
iteration 0300: loss: 1525.696
iteration 0400: loss: 1528.173
iteration 0500: loss: 1526.615
iteration 0600: loss: 1526.992
iteration 0700: loss: 1525.903
iteration 0800: loss: 1526.539
iteration 0900: loss: 1524.266
====> Epoch: 019 Train loss: 1526.0677  took : 5.947072505950928
====> Test loss: 1521.5915
iteration 0000: loss: 1523.993
iteration 0100: loss: 1525.235
iteration 0200: loss: 1526.629
iteration 0300: loss: 1524.570
iteration 0400: loss: 1525.133
iteration 0500: loss: 1525.998
iteration 0600: loss: 1525.391
iteration 0700: loss: 1527.067
iteration 0800: loss: 1527.035
iteration 0900: loss: 1525.787
====> Epoch: 020 Train loss: 1526.0165  took : 5.909075975418091
====> Test loss: 1521.5266
iteration 0000: loss: 1526.587
iteration 0100: loss: 1530.799
iteration 0200: loss: 1524.630
iteration 0300: loss: 1525.279
iteration 0400: loss: 1524.663
iteration 0500: loss: 1524.353
iteration 0600: loss: 1526.107
iteration 0700: loss: 1525.055
iteration 0800: loss: 1520.108
iteration 0900: loss: 1527.030
====> Epoch: 021 Train loss: 1525.9189  took : 5.90441632270813
====> Test loss: 1521.4417
iteration 0000: loss: 1526.174
iteration 0100: loss: 1525.080
iteration 0200: loss: 1527.588
iteration 0300: loss: 1523.552
iteration 0400: loss: 1528.928
iteration 0500: loss: 1523.138
iteration 0600: loss: 1525.678
iteration 0700: loss: 1525.509
iteration 0800: loss: 1523.860
iteration 0900: loss: 1522.859
====> Epoch: 022 Train loss: 1525.8349  took : 5.985181093215942
====> Test loss: 1521.3920
iteration 0000: loss: 1526.808
iteration 0100: loss: 1527.685
iteration 0200: loss: 1526.328
iteration 0300: loss: 1524.067
iteration 0400: loss: 1525.458
iteration 0500: loss: 1524.844
iteration 0600: loss: 1524.523
iteration 0700: loss: 1526.055
iteration 0800: loss: 1524.823
iteration 0900: loss: 1523.628
====> Epoch: 023 Train loss: 1525.7463  took : 5.9444239139556885
====> Test loss: 1521.2082
iteration 0000: loss: 1523.915
iteration 0100: loss: 1523.087
iteration 0200: loss: 1527.485
iteration 0300: loss: 1524.108
iteration 0400: loss: 1522.532
iteration 0500: loss: 1525.112
iteration 0600: loss: 1525.792
iteration 0700: loss: 1523.126
iteration 0800: loss: 1525.634
iteration 0900: loss: 1525.416
====> Epoch: 024 Train loss: 1525.7193  took : 5.977450847625732
====> Test loss: 1521.3959
iteration 0000: loss: 1524.566
iteration 0100: loss: 1523.903
iteration 0200: loss: 1524.899
iteration 0300: loss: 1524.742
iteration 0400: loss: 1523.040
iteration 0500: loss: 1523.921
iteration 0600: loss: 1526.338
iteration 0700: loss: 1524.190
iteration 0800: loss: 1524.417
iteration 0900: loss: 1524.470
====> Epoch: 025 Train loss: 1525.6104  took : 6.010179281234741
====> Test loss: 1521.2674
iteration 0000: loss: 1523.157
iteration 0100: loss: 1526.558
iteration 0200: loss: 1527.630
iteration 0300: loss: 1523.071
iteration 0400: loss: 1526.780
iteration 0500: loss: 1526.641
iteration 0600: loss: 1525.544
iteration 0700: loss: 1524.849
iteration 0800: loss: 1524.659
iteration 0900: loss: 1528.054
====> Epoch: 026 Train loss: 1525.5803  took : 5.867342948913574
====> Test loss: 1521.1438
iteration 0000: loss: 1524.790
iteration 0100: loss: 1524.409
iteration 0200: loss: 1524.879
iteration 0300: loss: 1524.631
iteration 0400: loss: 1525.966
iteration 0500: loss: 1525.255
iteration 0600: loss: 1525.732
iteration 0700: loss: 1524.496
iteration 0800: loss: 1523.781
iteration 0900: loss: 1525.339
====> Epoch: 027 Train loss: 1525.5239  took : 5.954268455505371
====> Test loss: 1521.0743
iteration 0000: loss: 1526.687
iteration 0100: loss: 1522.782
iteration 0200: loss: 1524.592
iteration 0300: loss: 1524.739
iteration 0400: loss: 1524.667
iteration 0500: loss: 1524.580
iteration 0600: loss: 1527.555
iteration 0700: loss: 1526.623
iteration 0800: loss: 1527.020
iteration 0900: loss: 1527.868
====> Epoch: 028 Train loss: 1525.4394  took : 5.917611598968506
====> Test loss: 1521.0456
iteration 0000: loss: 1526.019
iteration 0100: loss: 1525.109
iteration 0200: loss: 1524.521
iteration 0300: loss: 1525.233
iteration 0400: loss: 1524.499
iteration 0500: loss: 1526.607
iteration 0600: loss: 1526.029
iteration 0700: loss: 1525.712
iteration 0800: loss: 1527.874
iteration 0900: loss: 1523.653
====> Epoch: 029 Train loss: 1525.4180  took : 5.962454080581665
====> Test loss: 1521.0510
iteration 0000: loss: 1526.814
iteration 0100: loss: 1524.025
iteration 0200: loss: 1525.171
iteration 0300: loss: 1525.367
iteration 0400: loss: 1526.319
iteration 0500: loss: 1523.297
iteration 0600: loss: 1524.868
iteration 0700: loss: 1523.386
iteration 0800: loss: 1526.418
iteration 0900: loss: 1524.224
====> Epoch: 030 Train loss: 1525.3633  took : 5.944010019302368
====> Test loss: 1521.0712
iteration 0000: loss: 1525.499
iteration 0100: loss: 1523.900
iteration 0200: loss: 1529.043
iteration 0300: loss: 1524.380
iteration 0400: loss: 1522.595
iteration 0500: loss: 1525.266
iteration 0600: loss: 1528.023
iteration 0700: loss: 1524.864
iteration 0800: loss: 1524.493
iteration 0900: loss: 1524.282
====> Epoch: 031 Train loss: 1525.3093  took : 5.942060947418213
====> Test loss: 1521.0476
iteration 0000: loss: 1525.041
iteration 0100: loss: 1525.052
iteration 0200: loss: 1524.842
iteration 0300: loss: 1527.602
iteration 0400: loss: 1521.544
iteration 0500: loss: 1526.834
iteration 0600: loss: 1523.703
iteration 0700: loss: 1523.296
iteration 0800: loss: 1525.253
iteration 0900: loss: 1524.374
====> Epoch: 032 Train loss: 1525.2844  took : 5.942891359329224
====> Test loss: 1520.9086
iteration 0000: loss: 1524.726
iteration 0100: loss: 1524.485
iteration 0200: loss: 1523.414
iteration 0300: loss: 1523.364
iteration 0400: loss: 1527.714
iteration 0500: loss: 1525.238
iteration 0600: loss: 1526.782
iteration 0700: loss: 1526.231
iteration 0800: loss: 1525.015
iteration 0900: loss: 1525.801
====> Epoch: 033 Train loss: 1525.2206  took : 5.919748783111572
====> Test loss: 1520.9753
iteration 0000: loss: 1524.391
iteration 0100: loss: 1521.854
iteration 0200: loss: 1525.357
iteration 0300: loss: 1525.835
iteration 0400: loss: 1524.969
iteration 0500: loss: 1524.315
iteration 0600: loss: 1527.652
iteration 0700: loss: 1524.801
iteration 0800: loss: 1523.634
iteration 0900: loss: 1525.943
====> Epoch: 034 Train loss: 1525.2023  took : 5.953343152999878
====> Test loss: 1520.9329
iteration 0000: loss: 1523.633
iteration 0100: loss: 1525.929
iteration 0200: loss: 1525.518
iteration 0300: loss: 1526.099
iteration 0400: loss: 1525.965
iteration 0500: loss: 1524.385
iteration 0600: loss: 1525.435
iteration 0700: loss: 1524.547
iteration 0800: loss: 1526.182
iteration 0900: loss: 1527.739
====> Epoch: 035 Train loss: 1525.1111  took : 5.932172775268555
====> Test loss: 1520.8675
iteration 0000: loss: 1524.754
iteration 0100: loss: 1525.815
iteration 0200: loss: 1521.593
iteration 0300: loss: 1524.515
iteration 0400: loss: 1525.698
iteration 0500: loss: 1528.009
iteration 0600: loss: 1522.782
iteration 0700: loss: 1525.013
iteration 0800: loss: 1524.120
iteration 0900: loss: 1527.674
====> Epoch: 036 Train loss: 1525.0667  took : 5.956305980682373
====> Test loss: 1520.7811
iteration 0000: loss: 1525.788
iteration 0100: loss: 1524.869
iteration 0200: loss: 1525.637
iteration 0300: loss: 1523.013
iteration 0400: loss: 1522.826
iteration 0500: loss: 1524.178
iteration 0600: loss: 1524.772
iteration 0700: loss: 1525.700
iteration 0800: loss: 1524.387
iteration 0900: loss: 1525.091
====> Epoch: 037 Train loss: 1525.0383  took : 5.9756739139556885
====> Test loss: 1520.8396
iteration 0000: loss: 1524.015
iteration 0100: loss: 1524.080
iteration 0200: loss: 1524.347
iteration 0300: loss: 1527.640
iteration 0400: loss: 1522.888
iteration 0500: loss: 1524.127
iteration 0600: loss: 1522.948
iteration 0700: loss: 1525.105
iteration 0800: loss: 1526.860
iteration 0900: loss: 1524.511
====> Epoch: 038 Train loss: 1525.0386  took : 5.9174888134002686
====> Test loss: 1520.7929
iteration 0000: loss: 1521.523
iteration 0100: loss: 1521.521
iteration 0200: loss: 1525.937
iteration 0300: loss: 1529.404
iteration 0400: loss: 1525.779
iteration 0500: loss: 1522.104
iteration 0600: loss: 1526.894
iteration 0700: loss: 1525.234
iteration 0800: loss: 1527.649
iteration 0900: loss: 1527.543
====> Epoch: 039 Train loss: 1524.9682  took : 5.90601372718811
====> Test loss: 1520.8100
iteration 0000: loss: 1525.260
iteration 0100: loss: 1528.177
iteration 0200: loss: 1524.983
iteration 0300: loss: 1523.573
iteration 0400: loss: 1524.902
iteration 0500: loss: 1522.671
iteration 0600: loss: 1525.336
iteration 0700: loss: 1521.870
iteration 0800: loss: 1523.706
iteration 0900: loss: 1523.633
====> Epoch: 040 Train loss: 1524.9449  took : 5.930938243865967
====> Test loss: 1520.7588
iteration 0000: loss: 1526.548
iteration 0100: loss: 1522.953
iteration 0200: loss: 1525.247
iteration 0300: loss: 1523.791
iteration 0400: loss: 1525.248
iteration 0500: loss: 1521.159
iteration 0600: loss: 1527.176
iteration 0700: loss: 1526.720
iteration 0800: loss: 1524.167
iteration 0900: loss: 1524.137
====> Epoch: 041 Train loss: 1524.9392  took : 5.899641513824463
====> Test loss: 1520.7280
iteration 0000: loss: 1523.057
iteration 0100: loss: 1525.080
iteration 0200: loss: 1525.612
iteration 0300: loss: 1523.364
iteration 0400: loss: 1524.263
iteration 0500: loss: 1523.470
iteration 0600: loss: 1524.928
iteration 0700: loss: 1526.957
iteration 0800: loss: 1523.018
iteration 0900: loss: 1527.282
====> Epoch: 042 Train loss: 1524.8641  took : 6.036428689956665
====> Test loss: 1520.6814
iteration 0000: loss: 1523.079
iteration 0100: loss: 1527.298
iteration 0200: loss: 1525.635
iteration 0300: loss: 1523.276
iteration 0400: loss: 1524.099
iteration 0500: loss: 1525.662
iteration 0600: loss: 1524.878
iteration 0700: loss: 1524.314
iteration 0800: loss: 1523.363
iteration 0900: loss: 1526.536
====> Epoch: 043 Train loss: 1524.8176  took : 5.915564298629761
====> Test loss: 1520.6068
iteration 0000: loss: 1523.033
iteration 0100: loss: 1524.315
iteration 0200: loss: 1525.759
iteration 0300: loss: 1523.704
iteration 0400: loss: 1523.488
iteration 0500: loss: 1526.521
iteration 0600: loss: 1524.800
iteration 0700: loss: 1523.695
iteration 0800: loss: 1527.191
iteration 0900: loss: 1524.805
====> Epoch: 044 Train loss: 1524.7935  took : 5.90406346321106
====> Test loss: 1520.6087
iteration 0000: loss: 1525.889
iteration 0100: loss: 1525.644
iteration 0200: loss: 1524.859
iteration 0300: loss: 1523.025
iteration 0400: loss: 1520.569
iteration 0500: loss: 1523.928
iteration 0600: loss: 1524.370
iteration 0700: loss: 1526.217
iteration 0800: loss: 1524.869
iteration 0900: loss: 1527.638
====> Epoch: 045 Train loss: 1524.7483  took : 5.924950838088989
====> Test loss: 1520.6625
iteration 0000: loss: 1526.581
iteration 0100: loss: 1522.761
iteration 0200: loss: 1526.006
iteration 0300: loss: 1525.189
iteration 0400: loss: 1524.363
iteration 0500: loss: 1525.340
iteration 0600: loss: 1524.047
iteration 0700: loss: 1526.824
iteration 0800: loss: 1524.328
iteration 0900: loss: 1523.006
====> Epoch: 046 Train loss: 1524.7580  took : 5.9312744140625
====> Test loss: 1520.6108
iteration 0000: loss: 1524.237
iteration 0100: loss: 1522.996
iteration 0200: loss: 1524.549
iteration 0300: loss: 1524.589
iteration 0400: loss: 1524.517
iteration 0500: loss: 1525.514
iteration 0600: loss: 1524.748
iteration 0700: loss: 1520.937
iteration 0800: loss: 1523.902
iteration 0900: loss: 1524.644
====> Epoch: 047 Train loss: 1524.7013  took : 5.934495687484741
====> Test loss: 1520.6074
iteration 0000: loss: 1523.497
iteration 0100: loss: 1523.979
iteration 0200: loss: 1527.274
iteration 0300: loss: 1528.661
iteration 0400: loss: 1523.910
iteration 0500: loss: 1525.491
iteration 0600: loss: 1523.870
iteration 0700: loss: 1524.372
iteration 0800: loss: 1523.710
iteration 0900: loss: 1522.691
====> Epoch: 048 Train loss: 1524.6982  took : 5.932727813720703
====> Test loss: 1520.4878
iteration 0000: loss: 1529.078
iteration 0100: loss: 1525.194
iteration 0200: loss: 1527.137
iteration 0300: loss: 1523.406
iteration 0400: loss: 1522.827
iteration 0500: loss: 1524.028
iteration 0600: loss: 1522.820
iteration 0700: loss: 1526.087
iteration 0800: loss: 1520.054
iteration 0900: loss: 1524.936
====> Epoch: 049 Train loss: 1524.6592  took : 5.924188137054443
====> Test loss: 1520.5485
iteration 0000: loss: 1522.946
iteration 0100: loss: 1523.591
iteration 0200: loss: 1526.020
iteration 0300: loss: 1525.042
iteration 0400: loss: 1526.114
iteration 0500: loss: 1525.921
iteration 0600: loss: 1524.176
iteration 0700: loss: 1526.093
iteration 0800: loss: 1522.981
iteration 0900: loss: 1525.831
====> Epoch: 050 Train loss: 1524.6264  took : 5.9191954135894775
====> Test loss: 1520.6651
====> [MM-VAE] Time: 452.824s or 00:07:32
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2619.250
iteration 0100: loss: 2097.364
iteration 0200: loss: 2078.056
iteration 0300: loss: 2077.284
iteration 0400: loss: 2035.242
iteration 0500: loss: 2026.681
iteration 0600: loss: 1999.441
iteration 0700: loss: 1995.905
iteration 0800: loss: 1978.874
iteration 0900: loss: 1969.837
====> Epoch: 001 Train loss: 2042.9264  took : 8.725528478622437
====> Test loss: 1962.1856
iteration 0000: loss: 1969.455
iteration 0100: loss: 1964.272
iteration 0200: loss: 1963.567
iteration 0300: loss: 1963.532
iteration 0400: loss: 1962.676
iteration 0500: loss: 1961.513
iteration 0600: loss: 1963.232
iteration 0700: loss: 1961.136
iteration 0800: loss: 1961.438
iteration 0900: loss: 1961.158
====> Epoch: 002 Train loss: 1962.8761  took : 8.818672180175781
====> Test loss: 1955.5301
iteration 0000: loss: 1962.950
iteration 0100: loss: 1961.082
iteration 0200: loss: 1961.483
iteration 0300: loss: 1961.663
iteration 0400: loss: 1960.738
iteration 0500: loss: 1960.658
iteration 0600: loss: 1961.251
iteration 0700: loss: 1959.403
iteration 0800: loss: 1961.045
iteration 0900: loss: 1959.974
====> Epoch: 003 Train loss: 1960.7842  took : 8.707144975662231
====> Test loss: 1954.7392
iteration 0000: loss: 1961.214
iteration 0100: loss: 1960.862
iteration 0200: loss: 1960.937
iteration 0300: loss: 1961.449
iteration 0400: loss: 1958.932
iteration 0500: loss: 1959.985
iteration 0600: loss: 1961.086
iteration 0700: loss: 1959.162
iteration 0800: loss: 1959.044
iteration 0900: loss: 1958.799
====> Epoch: 004 Train loss: 1959.7875  took : 8.6906898021698
====> Test loss: 1953.8987
iteration 0000: loss: 1959.018
iteration 0100: loss: 1958.768
iteration 0200: loss: 1959.131
iteration 0300: loss: 1959.974
iteration 0400: loss: 1958.906
iteration 0500: loss: 1959.029
iteration 0600: loss: 1959.427
iteration 0700: loss: 1958.646
iteration 0800: loss: 1959.225
iteration 0900: loss: 1960.533
====> Epoch: 005 Train loss: 1959.2428  took : 8.79226803779602
====> Test loss: 1953.4919
iteration 0000: loss: 1958.577
iteration 0100: loss: 1957.434
iteration 0200: loss: 1959.288
iteration 0300: loss: 1958.896
iteration 0400: loss: 1959.842
iteration 0500: loss: 1958.071
iteration 0600: loss: 1958.873
iteration 0700: loss: 1957.588
iteration 0800: loss: 1958.453
iteration 0900: loss: 1958.755
====> Epoch: 006 Train loss: 1958.9763  took : 8.69110631942749
====> Test loss: 1953.6786
iteration 0000: loss: 1959.811
iteration 0100: loss: 1958.734
iteration 0200: loss: 1958.845
iteration 0300: loss: 1957.919
iteration 0400: loss: 1959.237
iteration 0500: loss: 1959.291
iteration 0600: loss: 1958.145
iteration 0700: loss: 1957.587
iteration 0800: loss: 1960.264
iteration 0900: loss: 1960.080
====> Epoch: 007 Train loss: 1958.7191  took : 8.61182451248169
====> Test loss: 1952.7902
iteration 0000: loss: 1958.497
iteration 0100: loss: 1958.238
iteration 0200: loss: 1959.905
iteration 0300: loss: 1957.579
iteration 0400: loss: 1958.690
iteration 0500: loss: 1959.899
iteration 0600: loss: 1958.062
iteration 0700: loss: 1957.543
iteration 0800: loss: 1959.030
iteration 0900: loss: 1960.650
====> Epoch: 008 Train loss: 1958.5269  took : 8.686881065368652
====> Test loss: 1952.7122
iteration 0000: loss: 1958.225
iteration 0100: loss: 1958.831
iteration 0200: loss: 1957.825
iteration 0300: loss: 1960.092
iteration 0400: loss: 1957.076
iteration 0500: loss: 1958.044
iteration 0600: loss: 1959.094
iteration 0700: loss: 1959.313
iteration 0800: loss: 1958.216
iteration 0900: loss: 1957.549
====> Epoch: 009 Train loss: 1958.3598  took : 8.617624521255493
====> Test loss: 1952.7707
iteration 0000: loss: 1957.354
iteration 0100: loss: 1958.542
iteration 0200: loss: 1958.086
iteration 0300: loss: 1957.132
iteration 0400: loss: 1958.413
iteration 0500: loss: 1959.172
iteration 0600: loss: 1957.979
iteration 0700: loss: 1958.356
iteration 0800: loss: 1958.048
iteration 0900: loss: 1958.247
====> Epoch: 010 Train loss: 1958.2843  took : 8.77878999710083
====> Test loss: 1952.7831
iteration 0000: loss: 1957.794
iteration 0100: loss: 1956.988
iteration 0200: loss: 1958.717
iteration 0300: loss: 1959.342
iteration 0400: loss: 1956.616
iteration 0500: loss: 1957.983
iteration 0600: loss: 1957.894
iteration 0700: loss: 1957.992
iteration 0800: loss: 1957.159
iteration 0900: loss: 1958.619
====> Epoch: 011 Train loss: 1958.2142  took : 8.888495922088623
====> Test loss: 1952.4151
iteration 0000: loss: 1956.960
iteration 0100: loss: 1957.582
iteration 0200: loss: 1958.783
iteration 0300: loss: 1957.646
iteration 0400: loss: 1957.639
iteration 0500: loss: 1959.126
iteration 0600: loss: 1957.679
iteration 0700: loss: 1957.124
iteration 0800: loss: 1959.704
iteration 0900: loss: 1958.640
====> Epoch: 012 Train loss: 1958.1023  took : 8.637818574905396
====> Test loss: 1952.4647
iteration 0000: loss: 1957.812
iteration 0100: loss: 1957.541
iteration 0200: loss: 1959.529
iteration 0300: loss: 1958.187
iteration 0400: loss: 1957.444
iteration 0500: loss: 1957.597
iteration 0600: loss: 1958.029
iteration 0700: loss: 1958.048
iteration 0800: loss: 1957.502
iteration 0900: loss: 1958.854
====> Epoch: 013 Train loss: 1958.0456  took : 8.660661458969116
====> Test loss: 1952.3335
iteration 0000: loss: 1957.168
iteration 0100: loss: 1959.574
iteration 0200: loss: 1957.033
iteration 0300: loss: 1959.938
iteration 0400: loss: 1958.080
iteration 0500: loss: 1958.307
iteration 0600: loss: 1957.946
iteration 0700: loss: 1957.814
iteration 0800: loss: 1958.617
iteration 0900: loss: 1959.053
====> Epoch: 014 Train loss: 1957.9523  took : 8.678362846374512
====> Test loss: 1952.3699
iteration 0000: loss: 1957.729
iteration 0100: loss: 1957.077
iteration 0200: loss: 1958.106
iteration 0300: loss: 1957.092
iteration 0400: loss: 1957.372
iteration 0500: loss: 1957.507
iteration 0600: loss: 1956.961
iteration 0700: loss: 1957.404
iteration 0800: loss: 1959.452
iteration 0900: loss: 1957.436
====> Epoch: 015 Train loss: 1957.8073  took : 8.772866249084473
====> Test loss: 1951.9081
iteration 0000: loss: 1957.741
iteration 0100: loss: 1959.050
iteration 0200: loss: 1958.195
iteration 0300: loss: 1958.541
iteration 0400: loss: 1957.199
iteration 0500: loss: 1958.326
iteration 0600: loss: 1957.937
iteration 0700: loss: 1957.917
iteration 0800: loss: 1957.002
iteration 0900: loss: 1958.885
====> Epoch: 016 Train loss: 1957.7708  took : 8.674183368682861
====> Test loss: 1952.1583
iteration 0000: loss: 1957.101
iteration 0100: loss: 1959.227
iteration 0200: loss: 1956.972
iteration 0300: loss: 1958.055
iteration 0400: loss: 1957.565
iteration 0500: loss: 1957.464
iteration 0600: loss: 1955.854
iteration 0700: loss: 1955.674
iteration 0800: loss: 1959.213
iteration 0900: loss: 1958.519
====> Epoch: 017 Train loss: 1957.6534  took : 8.788526058197021
====> Test loss: 1952.0131
iteration 0000: loss: 1957.380
iteration 0100: loss: 1957.550
iteration 0200: loss: 1956.911
iteration 0300: loss: 1958.269
iteration 0400: loss: 1956.447
iteration 0500: loss: 1957.486
iteration 0600: loss: 1957.704
iteration 0700: loss: 1957.079
iteration 0800: loss: 1956.569
iteration 0900: loss: 1957.301
====> Epoch: 018 Train loss: 1957.5184  took : 8.691473007202148
====> Test loss: 1952.9032
iteration 0000: loss: 1958.650
iteration 0100: loss: 1956.481
iteration 0200: loss: 1957.375
iteration 0300: loss: 1955.459
iteration 0400: loss: 1957.285
iteration 0500: loss: 1956.859
iteration 0600: loss: 1958.627
iteration 0700: loss: 1959.075
iteration 0800: loss: 1958.503
iteration 0900: loss: 1956.903
====> Epoch: 019 Train loss: 1957.5003  took : 8.909109354019165
====> Test loss: 1951.8052
iteration 0000: loss: 1956.989
iteration 0100: loss: 1956.691
iteration 0200: loss: 1957.713
iteration 0300: loss: 1956.806
iteration 0400: loss: 1958.871
iteration 0500: loss: 1957.465
iteration 0600: loss: 1956.868
iteration 0700: loss: 1956.993
iteration 0800: loss: 1957.321
iteration 0900: loss: 1958.222
====> Epoch: 020 Train loss: 1957.4238  took : 8.726935625076294
====> Test loss: 1952.2844
iteration 0000: loss: 1958.094
iteration 0100: loss: 1958.793
iteration 0200: loss: 1957.023
iteration 0300: loss: 1957.250
iteration 0400: loss: 1956.349
iteration 0500: loss: 1956.294
iteration 0600: loss: 1956.920
iteration 0700: loss: 1957.347
iteration 0800: loss: 1956.415
iteration 0900: loss: 1958.663
====> Epoch: 021 Train loss: 1957.3164  took : 8.656535625457764
====> Test loss: 1951.8973
iteration 0000: loss: 1956.883
iteration 0100: loss: 1957.591
iteration 0200: loss: 1957.158
iteration 0300: loss: 1957.074
iteration 0400: loss: 1955.977
iteration 0500: loss: 1957.717
iteration 0600: loss: 1957.696
iteration 0700: loss: 1957.994
iteration 0800: loss: 1956.857
iteration 0900: loss: 1955.750
====> Epoch: 022 Train loss: 1957.2805  took : 8.75083041191101
====> Test loss: 1952.2558
iteration 0000: loss: 1957.324
iteration 0100: loss: 1957.860
iteration 0200: loss: 1958.324
iteration 0300: loss: 1958.366
iteration 0400: loss: 1957.451
iteration 0500: loss: 1958.375
iteration 0600: loss: 1956.696
iteration 0700: loss: 1956.821
iteration 0800: loss: 1957.158
iteration 0900: loss: 1958.017
====> Epoch: 023 Train loss: 1957.2975  took : 8.771729707717896
====> Test loss: 1952.0714
iteration 0000: loss: 1956.694
iteration 0100: loss: 1957.687
iteration 0200: loss: 1956.700
iteration 0300: loss: 1959.186
iteration 0400: loss: 1956.702
iteration 0500: loss: 1957.822
iteration 0600: loss: 1956.829
iteration 0700: loss: 1956.392
iteration 0800: loss: 1956.252
iteration 0900: loss: 1956.751
====> Epoch: 024 Train loss: 1957.2583  took : 8.667659521102905
====> Test loss: 1951.9383
iteration 0000: loss: 1956.875
iteration 0100: loss: 1956.916
iteration 0200: loss: 1956.750
iteration 0300: loss: 1957.076
iteration 0400: loss: 1956.045
iteration 0500: loss: 1956.563
iteration 0600: loss: 1956.541
iteration 0700: loss: 1956.655
iteration 0800: loss: 1957.914
iteration 0900: loss: 1957.000
====> Epoch: 025 Train loss: 1957.2101  took : 8.794752359390259
====> Test loss: 1952.2578
iteration 0000: loss: 1959.271
iteration 0100: loss: 1957.435
iteration 0200: loss: 1957.062
iteration 0300: loss: 1957.709
iteration 0400: loss: 1956.494
iteration 0500: loss: 1957.763
iteration 0600: loss: 1955.933
iteration 0700: loss: 1957.220
iteration 0800: loss: 1956.714
iteration 0900: loss: 1957.152
====> Epoch: 026 Train loss: 1957.1824  took : 8.65445613861084
====> Test loss: 1951.9271
iteration 0000: loss: 1956.789
iteration 0100: loss: 1957.349
iteration 0200: loss: 1960.942
iteration 0300: loss: 1957.217
iteration 0400: loss: 1956.467
iteration 0500: loss: 1956.125
iteration 0600: loss: 1957.421
iteration 0700: loss: 1956.651
iteration 0800: loss: 1957.024
iteration 0900: loss: 1956.795
====> Epoch: 027 Train loss: 1957.1443  took : 8.645667314529419
====> Test loss: 1952.0012
iteration 0000: loss: 1956.691
iteration 0100: loss: 1957.182
iteration 0200: loss: 1957.343
iteration 0300: loss: 1960.259
iteration 0400: loss: 1956.312
iteration 0500: loss: 1956.735
iteration 0600: loss: 1956.923
iteration 0700: loss: 1958.436
iteration 0800: loss: 1957.901
iteration 0900: loss: 1956.966
====> Epoch: 028 Train loss: 1957.1215  took : 8.694998264312744
====> Test loss: 1951.9115
iteration 0000: loss: 1959.051
iteration 0100: loss: 1957.979
iteration 0200: loss: 1959.025
iteration 0300: loss: 1956.874
iteration 0400: loss: 1956.836
iteration 0500: loss: 1957.298
iteration 0600: loss: 1956.509
iteration 0700: loss: 1957.114
iteration 0800: loss: 1956.740
iteration 0900: loss: 1958.361
====> Epoch: 029 Train loss: 1957.0884  took : 8.947628021240234
====> Test loss: 1951.9335
iteration 0000: loss: 1957.815
iteration 0100: loss: 1956.169
iteration 0200: loss: 1957.369
iteration 0300: loss: 1955.326
iteration 0400: loss: 1956.899
iteration 0500: loss: 1957.042
iteration 0600: loss: 1957.052
iteration 0700: loss: 1957.891
iteration 0800: loss: 1956.531
iteration 0900: loss: 1957.149
====> Epoch: 030 Train loss: 1957.0471  took : 8.753970623016357
====> Test loss: 1951.7635
iteration 0000: loss: 1955.776
iteration 0100: loss: 1957.498
iteration 0200: loss: 1955.989
iteration 0300: loss: 1956.127
iteration 0400: loss: 1958.389
iteration 0500: loss: 1956.355
iteration 0600: loss: 1956.608
iteration 0700: loss: 1958.169
iteration 0800: loss: 1955.860
iteration 0900: loss: 1956.054
====> Epoch: 031 Train loss: 1957.0011  took : 8.640145778656006
====> Test loss: 1951.7073
iteration 0000: loss: 1956.660
iteration 0100: loss: 1955.946
iteration 0200: loss: 1957.450
iteration 0300: loss: 1957.688
iteration 0400: loss: 1956.366
iteration 0500: loss: 1956.404
iteration 0600: loss: 1957.042
iteration 0700: loss: 1956.462
iteration 0800: loss: 1957.272
iteration 0900: loss: 1957.439
====> Epoch: 032 Train loss: 1956.9929  took : 8.6968252658844
====> Test loss: 1951.7526
iteration 0000: loss: 1956.577
iteration 0100: loss: 1956.609
iteration 0200: loss: 1956.131
iteration 0300: loss: 1956.422
iteration 0400: loss: 1956.203
iteration 0500: loss: 1957.837
iteration 0600: loss: 1956.336
iteration 0700: loss: 1956.059
iteration 0800: loss: 1957.285
iteration 0900: loss: 1956.570
====> Epoch: 033 Train loss: 1956.9095  took : 8.693558931350708
====> Test loss: 1951.7729
iteration 0000: loss: 1956.568
iteration 0100: loss: 1956.648
iteration 0200: loss: 1956.704
iteration 0300: loss: 1956.330
iteration 0400: loss: 1956.526
iteration 0500: loss: 1956.725
iteration 0600: loss: 1956.322
iteration 0700: loss: 1957.163
iteration 0800: loss: 1956.514
iteration 0900: loss: 1956.283
====> Epoch: 034 Train loss: 1956.8730  took : 8.666561365127563
====> Test loss: 1951.8718
iteration 0000: loss: 1955.972
iteration 0100: loss: 1956.868
iteration 0200: loss: 1956.197
iteration 0300: loss: 1956.308
iteration 0400: loss: 1957.073
iteration 0500: loss: 1956.940
iteration 0600: loss: 1956.649
iteration 0700: loss: 1957.007
iteration 0800: loss: 1959.158
iteration 0900: loss: 1958.162
====> Epoch: 035 Train loss: 1956.8687  took : 8.737900733947754
====> Test loss: 1951.4827
iteration 0000: loss: 1956.682
iteration 0100: loss: 1957.551
iteration 0200: loss: 1957.086
iteration 0300: loss: 1955.915
iteration 0400: loss: 1957.638
iteration 0500: loss: 1955.698
iteration 0600: loss: 1956.120
iteration 0700: loss: 1956.357
iteration 0800: loss: 1956.826
iteration 0900: loss: 1956.707
====> Epoch: 036 Train loss: 1956.9048  took : 8.617686748504639
====> Test loss: 1951.7732
iteration 0000: loss: 1956.575
iteration 0100: loss: 1956.499
iteration 0200: loss: 1955.150
iteration 0300: loss: 1957.376
iteration 0400: loss: 1956.223
iteration 0500: loss: 1956.604
iteration 0600: loss: 1958.015
iteration 0700: loss: 1956.771
iteration 0800: loss: 1957.416
iteration 0900: loss: 1955.701
====> Epoch: 037 Train loss: 1956.8304  took : 8.859360456466675
====> Test loss: 1951.6733
iteration 0000: loss: 1956.511
iteration 0100: loss: 1956.353
iteration 0200: loss: 1957.124
iteration 0300: loss: 1956.247
iteration 0400: loss: 1957.567
iteration 0500: loss: 1956.405
iteration 0600: loss: 1956.398
iteration 0700: loss: 1956.275
iteration 0800: loss: 1957.198
iteration 0900: loss: 1956.740
====> Epoch: 038 Train loss: 1956.7961  took : 8.653869867324829
====> Test loss: 1951.6584
iteration 0000: loss: 1958.477
iteration 0100: loss: 1957.936
iteration 0200: loss: 1957.718
iteration 0300: loss: 1955.809
iteration 0400: loss: 1956.796
iteration 0500: loss: 1955.764
iteration 0600: loss: 1956.445
iteration 0700: loss: 1956.918
iteration 0800: loss: 1956.633
iteration 0900: loss: 1957.288
====> Epoch: 039 Train loss: 1956.8344  took : 8.706363677978516
====> Test loss: 1951.7023
iteration 0000: loss: 1957.561
iteration 0100: loss: 1957.152
iteration 0200: loss: 1955.823
iteration 0300: loss: 1955.618
iteration 0400: loss: 1956.432
iteration 0500: loss: 1957.779
iteration 0600: loss: 1957.635
iteration 0700: loss: 1956.816
iteration 0800: loss: 1957.834
iteration 0900: loss: 1956.592
====> Epoch: 040 Train loss: 1956.7844  took : 8.717759132385254
====> Test loss: 1951.6366
iteration 0000: loss: 1957.723
iteration 0100: loss: 1956.706
iteration 0200: loss: 1957.474
iteration 0300: loss: 1957.365
iteration 0400: loss: 1957.013
iteration 0500: loss: 1956.731
iteration 0600: loss: 1956.841
iteration 0700: loss: 1956.680
iteration 0800: loss: 1956.779
iteration 0900: loss: 1957.324
====> Epoch: 041 Train loss: 1956.8493  took : 8.72881817817688
====> Test loss: 1951.7272
iteration 0000: loss: 1957.094
iteration 0100: loss: 1957.528
iteration 0200: loss: 1957.210
iteration 0300: loss: 1956.326
iteration 0400: loss: 1956.929
iteration 0500: loss: 1957.265
iteration 0600: loss: 1957.901
iteration 0700: loss: 1958.426
iteration 0800: loss: 1956.738
iteration 0900: loss: 1956.202
====> Epoch: 042 Train loss: 1956.7693  took : 8.665122985839844
====> Test loss: 1951.5736
iteration 0000: loss: 1958.296
iteration 0100: loss: 1956.391
iteration 0200: loss: 1955.813
iteration 0300: loss: 1956.357
iteration 0400: loss: 1955.552
iteration 0500: loss: 1957.708
iteration 0600: loss: 1955.972
iteration 0700: loss: 1957.418
iteration 0800: loss: 1956.422
iteration 0900: loss: 1957.334
====> Epoch: 043 Train loss: 1956.7498  took : 8.63321852684021
====> Test loss: 1951.7817
iteration 0000: loss: 1956.385
iteration 0100: loss: 1955.683
iteration 0200: loss: 1956.286
iteration 0300: loss: 1956.919
iteration 0400: loss: 1957.048
iteration 0500: loss: 1956.927
iteration 0600: loss: 1957.693
iteration 0700: loss: 1957.246
iteration 0800: loss: 1956.999
iteration 0900: loss: 1957.078
====> Epoch: 044 Train loss: 1956.7180  took : 8.847506761550903
====> Test loss: 1951.5518
iteration 0000: loss: 1957.168
iteration 0100: loss: 1956.446
iteration 0200: loss: 1957.405
iteration 0300: loss: 1956.757
iteration 0400: loss: 1955.293
iteration 0500: loss: 1955.688
iteration 0600: loss: 1956.610
iteration 0700: loss: 1957.949
iteration 0800: loss: 1955.705
iteration 0900: loss: 1956.673
====> Epoch: 045 Train loss: 1956.7810  took : 8.822583436965942
====> Test loss: 1952.0681
iteration 0000: loss: 1957.012
iteration 0100: loss: 1956.165
iteration 0200: loss: 1956.324
iteration 0300: loss: 1957.630
iteration 0400: loss: 1957.844
iteration 0500: loss: 1957.719
iteration 0600: loss: 1956.437
iteration 0700: loss: 1956.007
iteration 0800: loss: 1957.066
iteration 0900: loss: 1957.349
====> Epoch: 046 Train loss: 1956.7003  took : 8.676605463027954
====> Test loss: 1951.5871
iteration 0000: loss: 1957.040
iteration 0100: loss: 1956.474
iteration 0200: loss: 1955.622
iteration 0300: loss: 1956.218
iteration 0400: loss: 1958.235
iteration 0500: loss: 1955.769
iteration 0600: loss: 1956.773
iteration 0700: loss: 1956.227
iteration 0800: loss: 1955.677
iteration 0900: loss: 1957.139
====> Epoch: 047 Train loss: 1956.6879  took : 8.791282176971436
====> Test loss: 1951.6788
iteration 0000: loss: 1957.027
iteration 0100: loss: 1956.293
iteration 0200: loss: 1956.439
iteration 0300: loss: 1956.689
iteration 0400: loss: 1958.372
iteration 0500: loss: 1956.537
iteration 0600: loss: 1956.880
iteration 0700: loss: 1957.079
iteration 0800: loss: 1956.410
iteration 0900: loss: 1955.845
====> Epoch: 048 Train loss: 1956.6679  took : 8.626624345779419
====> Test loss: 1951.6040
iteration 0000: loss: 1957.062
iteration 0100: loss: 1956.153
iteration 0200: loss: 1956.402
iteration 0300: loss: 1957.716
iteration 0400: loss: 1956.603
iteration 0500: loss: 1956.231
iteration 0600: loss: 1956.957
iteration 0700: loss: 1957.431
iteration 0800: loss: 1956.138
iteration 0900: loss: 1957.845
====> Epoch: 049 Train loss: 1956.6060  took : 8.684132814407349
====> Test loss: 1951.5427
iteration 0000: loss: 1956.547
iteration 0100: loss: 1957.519
iteration 0200: loss: 1955.979
iteration 0300: loss: 1956.089
iteration 0400: loss: 1956.095
iteration 0500: loss: 1955.838
iteration 0600: loss: 1957.374
iteration 0700: loss: 1956.465
iteration 0800: loss: 1956.402
iteration 0900: loss: 1956.593
====> Epoch: 050 Train loss: 1956.6276  took : 8.76200270652771
====> Test loss: 1951.9578
====> [MM-VAE] Time: 570.227s or 00:09:30
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5220.323
iteration 0100: loss: 4144.389
iteration 0200: loss: 4105.750
iteration 0300: loss: 4058.142
iteration 0400: loss: 4025.706
iteration 0500: loss: 4014.887
iteration 0600: loss: 4008.327
iteration 0700: loss: 4001.029
iteration 0800: loss: 3990.651
iteration 0900: loss: 3995.450
iteration 1000: loss: 3987.994
iteration 1100: loss: 3982.443
iteration 1200: loss: 3996.050
iteration 1300: loss: 3991.416
iteration 1400: loss: 3983.777
iteration 1500: loss: 3984.188
iteration 1600: loss: 3973.113
iteration 1700: loss: 3972.125
iteration 1800: loss: 3982.369
====> Epoch: 001 Train loss: 4015.7699  took : 140.3075909614563
====> Test loss: 3982.6705
iteration 0000: loss: 3992.388
iteration 0100: loss: 3982.432
iteration 0200: loss: 3977.840
iteration 0300: loss: 3975.287
iteration 0400: loss: 3972.349
iteration 0500: loss: 3973.295
iteration 0600: loss: 3981.577
iteration 0700: loss: 3984.347
iteration 0800: loss: 3979.475
iteration 0900: loss: 3971.635
iteration 1000: loss: 3970.039
iteration 1100: loss: 3968.696
iteration 1200: loss: 3977.090
iteration 1300: loss: 3968.346
iteration 1400: loss: 3975.158
iteration 1500: loss: 3969.723
iteration 1600: loss: 3968.461
iteration 1700: loss: 3977.106
iteration 1800: loss: 3970.926
====> Epoch: 002 Train loss: 3974.5893  took : 138.54349327087402
====> Test loss: 3975.4925
iteration 0000: loss: 3975.175
iteration 0100: loss: 3975.975
iteration 0200: loss: 3978.487
iteration 0300: loss: 3968.956
iteration 0400: loss: 3973.763
iteration 0500: loss: 3963.605
iteration 0600: loss: 3961.012
iteration 0700: loss: 3962.474
iteration 0800: loss: 3971.954
iteration 0900: loss: 3969.276
iteration 1000: loss: 3965.750
iteration 1100: loss: 3973.026
iteration 1200: loss: 3971.003
iteration 1300: loss: 3965.390
iteration 1400: loss: 3967.151
iteration 1500: loss: 3964.407
iteration 1600: loss: 3970.614
iteration 1700: loss: 3965.529
iteration 1800: loss: 3963.359
====> Epoch: 003 Train loss: 3970.2433  took : 140.10463166236877
====> Test loss: 3973.1760
iteration 0000: loss: 3970.430
iteration 0100: loss: 3958.726
iteration 0200: loss: 3962.867
iteration 0300: loss: 3970.813
iteration 0400: loss: 3961.780
iteration 0500: loss: 3969.474
iteration 0600: loss: 3967.975
iteration 0700: loss: 3971.361
iteration 0800: loss: 3962.395
iteration 0900: loss: 3964.677
iteration 1000: loss: 3966.696
iteration 1100: loss: 3967.142
iteration 1200: loss: 3966.402
iteration 1300: loss: 3961.877
iteration 1400: loss: 3971.150
iteration 1500: loss: 3962.835
iteration 1600: loss: 3957.714
iteration 1700: loss: 3973.350
iteration 1800: loss: 3972.128
====> Epoch: 004 Train loss: 3968.2005  took : 139.99013900756836
====> Test loss: 3972.1467
iteration 0000: loss: 3969.965
iteration 0100: loss: 3964.347
iteration 0200: loss: 3968.036
iteration 0300: loss: 3969.948
iteration 0400: loss: 3965.913
iteration 0500: loss: 3968.079
iteration 0600: loss: 3967.696
iteration 0700: loss: 3961.132
iteration 0800: loss: 3966.752
iteration 0900: loss: 3974.713
iteration 1000: loss: 3960.905
iteration 1100: loss: 3975.733
iteration 1200: loss: 3971.744
iteration 1300: loss: 3972.082
iteration 1400: loss: 3976.956
iteration 1500: loss: 3959.148
iteration 1600: loss: 3963.015
iteration 1700: loss: 3965.194
iteration 1800: loss: 3961.157
====> Epoch: 005 Train loss: 3966.9285  took : 140.5452263355255
====> Test loss: 3971.2338
iteration 0000: loss: 3965.167
iteration 0100: loss: 3966.278
iteration 0200: loss: 3967.563
iteration 0300: loss: 3964.024
iteration 0400: loss: 3966.571
iteration 0500: loss: 3962.806
iteration 0600: loss: 3959.819
iteration 0700: loss: 3968.074
iteration 0800: loss: 3967.202
iteration 0900: loss: 3969.670
iteration 1000: loss: 3962.500
iteration 1100: loss: 3972.409
iteration 1200: loss: 3967.132
iteration 1300: loss: 3971.054
iteration 1400: loss: 3961.720
iteration 1500: loss: 3963.164
iteration 1600: loss: 3959.896
iteration 1700: loss: 3964.230
iteration 1800: loss: 3962.745
====> Epoch: 006 Train loss: 3966.0159  took : 140.10499358177185
====> Test loss: 3969.9997
iteration 0000: loss: 3964.413
iteration 0100: loss: 3960.819
iteration 0200: loss: 3970.740
iteration 0300: loss: 3970.425
iteration 0400: loss: 3960.059
iteration 0500: loss: 3955.613
iteration 0600: loss: 3954.887
iteration 0700: loss: 3962.936
iteration 0800: loss: 3970.222
iteration 0900: loss: 3969.852
iteration 1000: loss: 3954.085
iteration 1100: loss: 3965.385
iteration 1200: loss: 3956.507
iteration 1300: loss: 3961.312
iteration 1400: loss: 3969.663
iteration 1500: loss: 3968.891
iteration 1600: loss: 3968.635
iteration 1700: loss: 3960.326
iteration 1800: loss: 3955.753
====> Epoch: 007 Train loss: 3965.1685  took : 140.0818748474121
====> Test loss: 3970.6330
iteration 0000: loss: 3969.461
iteration 0100: loss: 3961.149
iteration 0200: loss: 3968.947
iteration 0300: loss: 3962.265
iteration 0400: loss: 3975.285
iteration 0500: loss: 3957.703
iteration 0600: loss: 3971.272
iteration 0700: loss: 3965.996
iteration 0800: loss: 3969.829
iteration 0900: loss: 3972.573
iteration 1000: loss: 3964.171
iteration 1100: loss: 3975.845
iteration 1200: loss: 3965.186
iteration 1300: loss: 3971.230
iteration 1400: loss: 3968.162
iteration 1500: loss: 3961.660
iteration 1600: loss: 3962.634
iteration 1700: loss: 3959.883
iteration 1800: loss: 3960.345
====> Epoch: 008 Train loss: 3964.7057  took : 140.25674104690552
====> Test loss: 3969.8286
iteration 0000: loss: 3970.911
iteration 0100: loss: 3975.229
iteration 0200: loss: 3962.751
iteration 0300: loss: 3960.238
iteration 0400: loss: 3972.131
iteration 0500: loss: 3963.652
iteration 0600: loss: 3966.870
iteration 0700: loss: 3959.049
iteration 0800: loss: 3956.930
iteration 0900: loss: 3967.482
iteration 1000: loss: 3965.953
iteration 1100: loss: 3967.852
iteration 1200: loss: 3963.121
iteration 1300: loss: 3957.769
iteration 1400: loss: 3960.638
iteration 1500: loss: 3958.569
iteration 1600: loss: 3957.775
iteration 1700: loss: 3969.403
iteration 1800: loss: 3960.654
====> Epoch: 009 Train loss: 3964.2733  took : 140.01806497573853
====> Test loss: 3969.1132
iteration 0000: loss: 3959.602
iteration 0100: loss: 3967.518
iteration 0200: loss: 3961.065
iteration 0300: loss: 3954.845
iteration 0400: loss: 3969.454
iteration 0500: loss: 3964.265
iteration 0600: loss: 3967.610
iteration 0700: loss: 3959.468
iteration 0800: loss: 3965.424
iteration 0900: loss: 3962.071
iteration 1000: loss: 3964.562
iteration 1100: loss: 3964.926
iteration 1200: loss: 3962.317
iteration 1300: loss: 3971.030
iteration 1400: loss: 3961.799
iteration 1500: loss: 3956.460
iteration 1600: loss: 3962.877
iteration 1700: loss: 3972.967
iteration 1800: loss: 3959.438
====> Epoch: 010 Train loss: 3963.6172  took : 140.47396302223206
====> Test loss: 3969.0884
iteration 0000: loss: 3956.088
iteration 0100: loss: 3965.786
iteration 0200: loss: 3955.083
iteration 0300: loss: 3952.011
iteration 0400: loss: 3960.680
iteration 0500: loss: 3973.625
iteration 0600: loss: 3954.948
iteration 0700: loss: 3967.314
iteration 0800: loss: 3964.014
iteration 0900: loss: 3953.789
iteration 1000: loss: 3961.088
iteration 1100: loss: 3963.261
iteration 1200: loss: 3961.215
iteration 1300: loss: 3960.555
iteration 1400: loss: 3960.749
iteration 1500: loss: 3960.460
iteration 1600: loss: 3960.409
iteration 1700: loss: 3967.722
iteration 1800: loss: 3964.031
====> Epoch: 011 Train loss: 3963.4604  took : 140.5557963848114
====> Test loss: 3968.2584
iteration 0000: loss: 3960.825
iteration 0100: loss: 3958.099
iteration 0200: loss: 3958.484
iteration 0300: loss: 3961.973
iteration 0400: loss: 3957.837
iteration 0500: loss: 3957.636
iteration 0600: loss: 3958.234
iteration 0700: loss: 3969.980
iteration 0800: loss: 3965.403
iteration 0900: loss: 3965.081
iteration 1000: loss: 3955.916
iteration 1100: loss: 3960.258
iteration 1200: loss: 3967.451
iteration 1300: loss: 3960.280
iteration 1400: loss: 3960.988
iteration 1500: loss: 3964.712
iteration 1600: loss: 3960.208
iteration 1700: loss: 3957.343
iteration 1800: loss: 3968.722
====> Epoch: 012 Train loss: 3962.9849  took : 140.57297110557556
====> Test loss: 3967.9808
iteration 0000: loss: 3961.133
iteration 0100: loss: 3959.555
iteration 0200: loss: 3966.451
iteration 0300: loss: 3960.369
iteration 0400: loss: 3963.272
iteration 0500: loss: 3959.705
iteration 0600: loss: 3964.154
iteration 0700: loss: 3966.567
iteration 0800: loss: 3962.077
iteration 0900: loss: 3963.171
iteration 1000: loss: 3974.866
iteration 1100: loss: 3966.414
iteration 1200: loss: 3968.881
iteration 1300: loss: 3963.402
iteration 1400: loss: 3972.320
iteration 1500: loss: 3963.381
iteration 1600: loss: 3965.462
iteration 1700: loss: 3962.633
iteration 1800: loss: 3957.859
====> Epoch: 013 Train loss: 3962.5387  took : 140.00783681869507
====> Test loss: 3967.4517
iteration 0000: loss: 3965.933
iteration 0100: loss: 3957.630
iteration 0200: loss: 3955.993
iteration 0300: loss: 3965.571
iteration 0400: loss: 3956.905
iteration 0500: loss: 3960.490
iteration 0600: loss: 3960.909
iteration 0700: loss: 3956.743
iteration 0800: loss: 3962.583
iteration 0900: loss: 3963.633
iteration 1000: loss: 3958.476
iteration 1100: loss: 3964.881
iteration 1200: loss: 3962.546
iteration 1300: loss: 3960.862
iteration 1400: loss: 3965.971
iteration 1500: loss: 3966.800
iteration 1600: loss: 3962.042
iteration 1700: loss: 3956.674
iteration 1800: loss: 3963.885
====> Epoch: 014 Train loss: 3962.3412  took : 140.1894087791443
====> Test loss: 3967.4841
iteration 0000: loss: 3962.326
iteration 0100: loss: 3961.310
iteration 0200: loss: 3960.765
iteration 0300: loss: 3964.107
iteration 0400: loss: 3958.030
iteration 0500: loss: 3956.184
iteration 0600: loss: 3967.333
iteration 0700: loss: 3956.419
iteration 0800: loss: 3948.182
iteration 0900: loss: 3965.172
iteration 1000: loss: 3954.983
iteration 1100: loss: 3965.357
iteration 1200: loss: 3967.573
iteration 1300: loss: 3964.816
iteration 1400: loss: 3965.764
iteration 1500: loss: 3971.419
iteration 1600: loss: 3961.366
iteration 1700: loss: 3961.895
iteration 1800: loss: 3964.578
====> Epoch: 015 Train loss: 3961.9324  took : 139.8845555782318
====> Test loss: 3967.2703
iteration 0000: loss: 3959.426
iteration 0100: loss: 3952.960
iteration 0200: loss: 3967.865
iteration 0300: loss: 3961.033
iteration 0400: loss: 3968.695
iteration 0500: loss: 3957.770
iteration 0600: loss: 3972.077
iteration 0700: loss: 3961.904
iteration 0800: loss: 3968.184
iteration 0900: loss: 3964.558
iteration 1000: loss: 3964.729
iteration 1100: loss: 3963.450
iteration 1200: loss: 3959.042
iteration 1300: loss: 3964.797
iteration 1400: loss: 3968.819
iteration 1500: loss: 3963.504
iteration 1600: loss: 3965.196
iteration 1700: loss: 3969.243
iteration 1800: loss: 3967.842
====> Epoch: 016 Train loss: 3961.7801  took : 140.52440237998962
====> Test loss: 3967.1944
iteration 0000: loss: 3961.455
iteration 0100: loss: 3954.390
iteration 0200: loss: 3958.475
iteration 0300: loss: 3958.993
iteration 0400: loss: 3963.807
iteration 0500: loss: 3965.359
iteration 0600: loss: 3961.156
iteration 0700: loss: 3962.346
iteration 0800: loss: 3965.552
iteration 0900: loss: 3965.056
iteration 1000: loss: 3960.728
iteration 1100: loss: 3959.329
iteration 1200: loss: 3955.343
iteration 1300: loss: 3961.426
iteration 1400: loss: 3960.870
iteration 1500: loss: 3963.519
iteration 1600: loss: 3963.034
iteration 1700: loss: 3963.160
iteration 1800: loss: 3963.888
====> Epoch: 017 Train loss: 3961.5643  took : 140.49138832092285
====> Test loss: 3968.4083
iteration 0000: loss: 3960.260
iteration 0100: loss: 3962.722
iteration 0200: loss: 3963.768
iteration 0300: loss: 3954.936
iteration 0400: loss: 3962.159
iteration 0500: loss: 3955.981
iteration 0600: loss: 3960.037
iteration 0700: loss: 3956.495
iteration 0800: loss: 3959.118
iteration 0900: loss: 3960.804
iteration 1000: loss: 3962.789
iteration 1100: loss: 3960.347
iteration 1200: loss: 3957.371
iteration 1300: loss: 3963.988
iteration 1400: loss: 3961.285
iteration 1500: loss: 3967.242
iteration 1600: loss: 3961.877
iteration 1700: loss: 3964.350
iteration 1800: loss: 3960.476
====> Epoch: 018 Train loss: 3961.5439  took : 139.51289987564087
====> Test loss: 3966.7686
iteration 0000: loss: 3964.211
iteration 0100: loss: 3955.383
iteration 0200: loss: 3969.295
iteration 0300: loss: 3964.437
iteration 0400: loss: 3961.929
iteration 0500: loss: 3963.315
iteration 0600: loss: 3964.271
iteration 0700: loss: 3952.839
iteration 0800: loss: 3966.540
iteration 0900: loss: 3960.181
iteration 1000: loss: 3959.965
iteration 1100: loss: 3961.892
iteration 1200: loss: 3960.070
iteration 1300: loss: 3960.277
iteration 1400: loss: 3967.687
iteration 1500: loss: 3959.780
iteration 1600: loss: 3959.765
iteration 1700: loss: 3958.403
iteration 1800: loss: 3955.333
====> Epoch: 019 Train loss: 3961.4334  took : 138.50308561325073
====> Test loss: 3966.6808
iteration 0000: loss: 3969.763
iteration 0100: loss: 3960.068
iteration 0200: loss: 3968.730
iteration 0300: loss: 3968.741
iteration 0400: loss: 3969.729
iteration 0500: loss: 3964.979
iteration 0600: loss: 3959.096
iteration 0700: loss: 3965.954
iteration 0800: loss: 3961.378
iteration 0900: loss: 3960.563
iteration 1000: loss: 3967.875
iteration 1100: loss: 3956.895
iteration 1200: loss: 3962.768
iteration 1300: loss: 3951.392
iteration 1400: loss: 3963.219
iteration 1500: loss: 3965.639
iteration 1600: loss: 3956.084
iteration 1700: loss: 3965.964
iteration 1800: loss: 3957.150
====> Epoch: 020 Train loss: 3961.1476  took : 139.9877109527588
====> Test loss: 3966.8788
iteration 0000: loss: 3961.139
iteration 0100: loss: 3960.812
iteration 0200: loss: 3958.334
iteration 0300: loss: 3964.987
iteration 0400: loss: 3958.621
iteration 0500: loss: 3960.550
iteration 0600: loss: 3960.562
iteration 0700: loss: 3958.127
iteration 0800: loss: 3958.962
iteration 0900: loss: 3963.420
iteration 1000: loss: 3966.599
iteration 1100: loss: 3960.845
iteration 1200: loss: 3951.139
iteration 1300: loss: 3965.904
iteration 1400: loss: 3962.651
iteration 1500: loss: 3963.789
iteration 1600: loss: 3963.895
iteration 1700: loss: 3967.257
iteration 1800: loss: 3956.023
====> Epoch: 021 Train loss: 3960.7040  took : 139.52474236488342
====> Test loss: 3966.9846
iteration 0000: loss: 3966.300
iteration 0100: loss: 3958.350
iteration 0200: loss: 3962.287
iteration 0300: loss: 3963.586
iteration 0400: loss: 3959.710
iteration 0500: loss: 3957.025
iteration 0600: loss: 3961.453
iteration 0700: loss: 3970.650
iteration 0800: loss: 3960.681
iteration 0900: loss: 3963.092
iteration 1000: loss: 3970.232
iteration 1100: loss: 3953.932
iteration 1200: loss: 3955.732
iteration 1300: loss: 3958.849
iteration 1400: loss: 3960.391
iteration 1500: loss: 3952.900
iteration 1600: loss: 3958.834
iteration 1700: loss: 3955.473
iteration 1800: loss: 3958.966
====> Epoch: 022 Train loss: 3960.7144  took : 140.56845808029175
====> Test loss: 3966.4370
iteration 0000: loss: 3958.203
iteration 0100: loss: 3954.635
iteration 0200: loss: 3961.933
iteration 0300: loss: 3969.204
iteration 0400: loss: 3969.218
iteration 0500: loss: 3958.771
iteration 0600: loss: 3954.771
iteration 0700: loss: 3956.822
iteration 0800: loss: 3968.655
iteration 0900: loss: 3963.163
iteration 1000: loss: 3955.279
iteration 1100: loss: 3951.599
iteration 1200: loss: 3954.536
iteration 1300: loss: 3965.986
iteration 1400: loss: 3952.186
iteration 1500: loss: 3958.624
iteration 1600: loss: 3958.486
iteration 1700: loss: 3966.393
iteration 1800: loss: 3955.252
====> Epoch: 023 Train loss: 3960.5861  took : 140.08243346214294
====> Test loss: 3966.1326
iteration 0000: loss: 3955.147
iteration 0100: loss: 3970.716
iteration 0200: loss: 3960.424
iteration 0300: loss: 3958.869
iteration 0400: loss: 3962.144
iteration 0500: loss: 3953.920
iteration 0600: loss: 3960.099
iteration 0700: loss: 3961.599
iteration 0800: loss: 3959.605
iteration 0900: loss: 3958.524
iteration 1000: loss: 3956.977
iteration 1100: loss: 3963.062
iteration 1200: loss: 3963.674
iteration 1300: loss: 3958.848
iteration 1400: loss: 3950.061
iteration 1500: loss: 3968.953
iteration 1600: loss: 3965.678
iteration 1700: loss: 3959.285
iteration 1800: loss: 3963.212
====> Epoch: 024 Train loss: 3960.4257  took : 140.73578572273254
====> Test loss: 3965.8901
iteration 0000: loss: 3955.938
iteration 0100: loss: 3953.241
iteration 0200: loss: 3959.552
iteration 0300: loss: 3955.466
iteration 0400: loss: 3961.314
iteration 0500: loss: 3953.041
iteration 0600: loss: 3968.740
iteration 0700: loss: 3962.166
iteration 0800: loss: 3965.589
iteration 0900: loss: 3955.359
iteration 1000: loss: 3959.503
iteration 1100: loss: 3962.628
iteration 1200: loss: 3962.102
iteration 1300: loss: 3967.578
iteration 1400: loss: 3956.524
iteration 1500: loss: 3960.714
iteration 1600: loss: 3954.126
iteration 1700: loss: 3961.516
iteration 1800: loss: 3962.066
====> Epoch: 025 Train loss: 3960.2679  took : 138.78936290740967
====> Test loss: 3965.9772
iteration 0000: loss: 3952.137
iteration 0100: loss: 3968.778
iteration 0200: loss: 3966.602
iteration 0300: loss: 3955.958
iteration 0400: loss: 3957.892
iteration 0500: loss: 3971.317
iteration 0600: loss: 3957.752
iteration 0700: loss: 3961.855
iteration 0800: loss: 3960.886
iteration 0900: loss: 3957.561
iteration 1000: loss: 3968.866
iteration 1100: loss: 3951.366
iteration 1200: loss: 3957.653
iteration 1300: loss: 3959.881
iteration 1400: loss: 3956.862
iteration 1500: loss: 3958.852
iteration 1600: loss: 3956.698
iteration 1700: loss: 3970.531
iteration 1800: loss: 3968.744
====> Epoch: 026 Train loss: 3960.2346  took : 140.5599958896637
====> Test loss: 3965.6908
iteration 0000: loss: 3960.793
iteration 0100: loss: 3965.713
iteration 0200: loss: 3965.789
iteration 0300: loss: 3955.359
iteration 0400: loss: 3964.332
iteration 0500: loss: 3960.799
iteration 0600: loss: 3964.885
iteration 0700: loss: 3954.791
iteration 0800: loss: 3964.907
iteration 0900: loss: 3961.317
iteration 1000: loss: 3958.261
iteration 1100: loss: 3961.498
iteration 1200: loss: 3961.639
iteration 1300: loss: 3956.301
iteration 1400: loss: 3963.461
iteration 1500: loss: 3961.111
iteration 1600: loss: 3961.155
iteration 1700: loss: 3947.598
iteration 1800: loss: 3960.896
====> Epoch: 027 Train loss: 3960.2295  took : 139.39753437042236
====> Test loss: 3966.2688
iteration 0000: loss: 3956.822
iteration 0100: loss: 3963.590
iteration 0200: loss: 3955.715
iteration 0300: loss: 3964.154
iteration 0400: loss: 3957.884
iteration 0500: loss: 3953.347
iteration 0600: loss: 3959.144
iteration 0700: loss: 3959.429
iteration 0800: loss: 3961.465
iteration 0900: loss: 3957.272
iteration 1000: loss: 3956.887
iteration 1100: loss: 3960.902
iteration 1200: loss: 3957.391
iteration 1300: loss: 3961.153
iteration 1400: loss: 3961.960
iteration 1500: loss: 3957.780
iteration 1600: loss: 3967.171
iteration 1700: loss: 3962.921
iteration 1800: loss: 3961.037
====> Epoch: 028 Train loss: 3959.9537  took : 139.6433663368225
====> Test loss: 3965.2876
iteration 0000: loss: 3950.766
iteration 0100: loss: 3963.582
iteration 0200: loss: 3954.476
iteration 0300: loss: 3960.250
iteration 0400: loss: 3948.522
iteration 0500: loss: 3965.428
iteration 0600: loss: 3960.147
iteration 0700: loss: 3959.230
iteration 0800: loss: 3958.591
iteration 0900: loss: 3966.582
iteration 1000: loss: 3961.159
iteration 1100: loss: 3959.832
iteration 1200: loss: 3962.532
iteration 1300: loss: 3960.590
iteration 1400: loss: 3957.800
iteration 1500: loss: 3964.722
iteration 1600: loss: 3965.102
iteration 1700: loss: 3951.176
iteration 1800: loss: 3960.088
====> Epoch: 029 Train loss: 3960.0565  took : 139.62236499786377
====> Test loss: 3965.6672
iteration 0000: loss: 3960.289
iteration 0100: loss: 3961.251
iteration 0200: loss: 3958.948
iteration 0300: loss: 3960.356
iteration 0400: loss: 3952.370
iteration 0500: loss: 3956.291
iteration 0600: loss: 3953.592
iteration 0700: loss: 3957.825
iteration 0800: loss: 3962.983
iteration 0900: loss: 3958.107
iteration 1000: loss: 3961.001
iteration 1100: loss: 3961.589
iteration 1200: loss: 3963.003
iteration 1300: loss: 3968.061
iteration 1400: loss: 3955.132
iteration 1500: loss: 3951.710
iteration 1600: loss: 3954.262
iteration 1700: loss: 3966.097
iteration 1800: loss: 3968.102
====> Epoch: 030 Train loss: 3959.8496  took : 139.84023356437683
====> Test loss: 3965.6127
iteration 0000: loss: 3961.971
iteration 0100: loss: 3960.090
iteration 0200: loss: 3959.414
iteration 0300: loss: 3962.729
iteration 0400: loss: 3955.691
iteration 0500: loss: 3957.760
iteration 0600: loss: 3964.802
iteration 0700: loss: 3970.488
iteration 0800: loss: 3959.183
iteration 0900: loss: 3963.060
iteration 1000: loss: 3968.636
iteration 1100: loss: 3959.063
iteration 1200: loss: 3961.446
iteration 1300: loss: 3957.740
iteration 1400: loss: 3957.195
iteration 1500: loss: 3961.752
iteration 1600: loss: 3959.797
iteration 1700: loss: 3956.160
iteration 1800: loss: 3955.662
====> Epoch: 031 Train loss: 3959.8512  took : 140.04272294044495
====> Test loss: 3965.5878
iteration 0000: loss: 3954.900
iteration 0100: loss: 3964.712
iteration 0200: loss: 3961.486
iteration 0300: loss: 3955.101
iteration 0400: loss: 3962.552
iteration 0500: loss: 3954.964
iteration 0600: loss: 3960.915
iteration 0700: loss: 3959.635
iteration 0800: loss: 3957.620
iteration 0900: loss: 3953.543
iteration 1000: loss: 3954.777
iteration 1100: loss: 3956.627
iteration 1200: loss: 3960.434
iteration 1300: loss: 3958.201
iteration 1400: loss: 3955.393
iteration 1500: loss: 3957.848
iteration 1600: loss: 3957.244
iteration 1700: loss: 3951.689
iteration 1800: loss: 3958.802
====> Epoch: 032 Train loss: 3959.6538  took : 140.36092019081116
====> Test loss: 3965.4702
iteration 0000: loss: 3959.118
iteration 0100: loss: 3960.740
iteration 0200: loss: 3962.697
iteration 0300: loss: 3951.951
iteration 0400: loss: 3951.231
iteration 0500: loss: 3966.164
iteration 0600: loss: 3944.296
iteration 0700: loss: 3951.659
iteration 0800: loss: 3960.162
iteration 0900: loss: 3962.626
iteration 1000: loss: 3965.618
iteration 1100: loss: 3959.531
iteration 1200: loss: 3955.040
iteration 1300: loss: 3960.861
iteration 1400: loss: 3956.821
iteration 1500: loss: 3960.109
iteration 1600: loss: 3953.441
iteration 1700: loss: 3955.816
iteration 1800: loss: 3960.528
====> Epoch: 033 Train loss: 3959.7488  took : 140.2974283695221
====> Test loss: 3965.6168
iteration 0000: loss: 3959.663
iteration 0100: loss: 3955.913
iteration 0200: loss: 3959.755
iteration 0300: loss: 3953.777
iteration 0400: loss: 3962.175
iteration 0500: loss: 3964.019
iteration 0600: loss: 3959.979
iteration 0700: loss: 3965.405
iteration 0800: loss: 3959.050
iteration 0900: loss: 3953.759
iteration 1000: loss: 3956.263
iteration 1100: loss: 3956.397
iteration 1200: loss: 3954.776
iteration 1300: loss: 3962.055
iteration 1400: loss: 3957.370
iteration 1500: loss: 3958.639
iteration 1600: loss: 3954.118
iteration 1700: loss: 3961.047
iteration 1800: loss: 3955.448
====> Epoch: 034 Train loss: 3959.6839  took : 137.5650749206543
====> Test loss: 3965.4082
iteration 0000: loss: 3956.027
iteration 0100: loss: 3968.618
iteration 0200: loss: 3955.944
iteration 0300: loss: 3961.708
iteration 0400: loss: 3958.750
iteration 0500: loss: 3960.317
iteration 0600: loss: 3954.438
iteration 0700: loss: 3954.197
iteration 0800: loss: 3953.101
iteration 0900: loss: 3952.481
iteration 1000: loss: 3960.039
iteration 1100: loss: 3958.348
iteration 1200: loss: 3960.831
iteration 1300: loss: 3960.035
iteration 1400: loss: 3956.279
iteration 1500: loss: 3956.752
iteration 1600: loss: 3961.023
iteration 1700: loss: 3959.770
iteration 1800: loss: 3957.365
====> Epoch: 035 Train loss: 3959.2685  took : 139.86437249183655
====> Test loss: 3965.4316
iteration 0000: loss: 3960.108
iteration 0100: loss: 3962.251
iteration 0200: loss: 3956.001
iteration 0300: loss: 3958.888
iteration 0400: loss: 3953.217
iteration 0500: loss: 3955.821
iteration 0600: loss: 3955.895
iteration 0700: loss: 3955.374
iteration 0800: loss: 3956.372
iteration 0900: loss: 3955.150
iteration 1000: loss: 3951.990
iteration 1100: loss: 3955.717
iteration 1200: loss: 3956.798
iteration 1300: loss: 3962.586
iteration 1400: loss: 3959.808
iteration 1500: loss: 3955.450
iteration 1600: loss: 3962.324
iteration 1700: loss: 3952.837
iteration 1800: loss: 3954.035
====> Epoch: 036 Train loss: 3959.4228  took : 140.4865038394928
====> Test loss: 3965.3285
iteration 0000: loss: 3958.569
iteration 0100: loss: 3956.094
iteration 0200: loss: 3958.456
iteration 0300: loss: 3957.529
iteration 0400: loss: 3958.310
iteration 0500: loss: 3959.219
iteration 0600: loss: 3961.516
iteration 0700: loss: 3964.566
iteration 0800: loss: 3961.746
iteration 0900: loss: 3958.983
iteration 1000: loss: 3961.178
iteration 1100: loss: 3962.641
iteration 1200: loss: 3960.894
iteration 1300: loss: 3961.313
iteration 1400: loss: 3970.285
iteration 1500: loss: 3957.940
iteration 1600: loss: 3953.222
iteration 1700: loss: 3957.022
iteration 1800: loss: 3952.797
====> Epoch: 037 Train loss: 3959.2737  took : 138.98768973350525
====> Test loss: 3965.1925
iteration 0000: loss: 3955.519
iteration 0100: loss: 3967.276
iteration 0200: loss: 3951.465
iteration 0300: loss: 3953.844
iteration 0400: loss: 3952.688
iteration 0500: loss: 3953.318
iteration 0600: loss: 3962.551
iteration 0700: loss: 3955.290
iteration 0800: loss: 3950.949
iteration 0900: loss: 3954.012
iteration 1000: loss: 3954.161
iteration 1100: loss: 3958.040
iteration 1200: loss: 3961.027
iteration 1300: loss: 3951.006
iteration 1400: loss: 3953.613
iteration 1500: loss: 3956.723
iteration 1600: loss: 3959.508
iteration 1700: loss: 3953.233
iteration 1800: loss: 3956.301
====> Epoch: 038 Train loss: 3959.0741  took : 137.56747770309448
====> Test loss: 3965.3512
iteration 0000: loss: 3951.778
iteration 0100: loss: 3965.382
iteration 0200: loss: 3955.759
iteration 0300: loss: 3955.008
iteration 0400: loss: 3961.327
iteration 0500: loss: 3956.292
iteration 0600: loss: 3955.116
iteration 0700: loss: 3959.198
iteration 0800: loss: 3954.682
iteration 0900: loss: 3966.823
iteration 1000: loss: 3959.336
iteration 1100: loss: 3955.797
iteration 1200: loss: 3963.747
iteration 1300: loss: 3958.391
iteration 1400: loss: 3957.058
iteration 1500: loss: 3968.413
iteration 1600: loss: 3960.449
iteration 1700: loss: 3956.240
iteration 1800: loss: 3959.753
====> Epoch: 039 Train loss: 3959.2990  took : 140.09308075904846
====> Test loss: 3965.4952
iteration 0000: loss: 3962.206
iteration 0100: loss: 3954.775
iteration 0200: loss: 3959.161
iteration 0300: loss: 3949.034
iteration 0400: loss: 3968.962
iteration 0500: loss: 3955.283
iteration 0600: loss: 3959.604
iteration 0700: loss: 3955.709
iteration 0800: loss: 3961.997
iteration 0900: loss: 3955.286
iteration 1000: loss: 3958.937
iteration 1100: loss: 3962.620
iteration 1200: loss: 3955.599
iteration 1300: loss: 3957.787
iteration 1400: loss: 3961.921
iteration 1500: loss: 3964.417
iteration 1600: loss: 3959.735
iteration 1700: loss: 3965.167
iteration 1800: loss: 3952.272
====> Epoch: 040 Train loss: 3959.1360  took : 140.01649713516235
====> Test loss: 3965.4260
iteration 0000: loss: 3955.838
iteration 0100: loss: 3957.425
iteration 0200: loss: 3957.884
iteration 0300: loss: 3956.880
iteration 0400: loss: 3954.001
iteration 0500: loss: 3961.214
iteration 0600: loss: 3966.021
iteration 0700: loss: 3967.560
iteration 0800: loss: 3964.727
iteration 0900: loss: 3961.124
iteration 1000: loss: 3970.857
iteration 1100: loss: 3950.674
iteration 1200: loss: 3961.590
iteration 1300: loss: 3956.371
iteration 1400: loss: 3955.934
iteration 1500: loss: 3964.828
iteration 1600: loss: 3952.521
iteration 1700: loss: 3961.611
iteration 1800: loss: 3962.211
====> Epoch: 041 Train loss: 3959.0952  took : 139.87107682228088
====> Test loss: 3965.0468
iteration 0000: loss: 3954.891
iteration 0100: loss: 3957.915
iteration 0200: loss: 3954.711
iteration 0300: loss: 3954.446
iteration 0400: loss: 3957.583
iteration 0500: loss: 3964.695
iteration 0600: loss: 3956.105
iteration 0700: loss: 3958.156
iteration 0800: loss: 3965.005
iteration 0900: loss: 3966.558
iteration 1000: loss: 3964.113
iteration 1100: loss: 3963.418
iteration 1200: loss: 3960.482
iteration 1300: loss: 3963.145
iteration 1400: loss: 3958.174
iteration 1500: loss: 3958.234
iteration 1600: loss: 3959.558
iteration 1700: loss: 3965.478
iteration 1800: loss: 3960.954
====> Epoch: 042 Train loss: 3958.7051  took : 139.61626601219177
====> Test loss: 3964.8957
iteration 0000: loss: 3970.244
iteration 0100: loss: 3963.465
iteration 0200: loss: 3956.804
iteration 0300: loss: 3958.279
iteration 0400: loss: 3956.384
iteration 0500: loss: 3953.833
iteration 0600: loss: 3960.204
iteration 0700: loss: 3965.832
iteration 0800: loss: 3964.490
iteration 0900: loss: 3957.924
iteration 1000: loss: 3958.678
iteration 1100: loss: 3953.136
iteration 1200: loss: 3954.127
iteration 1300: loss: 3963.588
iteration 1400: loss: 3959.252
iteration 1500: loss: 3962.496
iteration 1600: loss: 3956.091
iteration 1700: loss: 3956.195
iteration 1800: loss: 3955.427
====> Epoch: 043 Train loss: 3958.8080  took : 139.51432967185974
====> Test loss: 3965.3993
iteration 0000: loss: 3957.827
iteration 0100: loss: 3966.101
iteration 0200: loss: 3961.701
iteration 0300: loss: 3958.846
iteration 0400: loss: 3966.069
iteration 0500: loss: 3948.915
iteration 0600: loss: 3959.586
iteration 0700: loss: 3959.287
iteration 0800: loss: 3962.507
iteration 0900: loss: 3948.579
iteration 1000: loss: 3957.831
iteration 1100: loss: 3964.411
iteration 1200: loss: 3956.916
iteration 1300: loss: 3961.260
iteration 1400: loss: 3960.238
iteration 1500: loss: 3954.399
iteration 1600: loss: 3970.528
iteration 1700: loss: 3952.716
iteration 1800: loss: 3962.735
====> Epoch: 044 Train loss: 3958.9229  took : 139.88017773628235
====> Test loss: 3964.7854
iteration 0000: loss: 3968.330
iteration 0100: loss: 3948.827
iteration 0200: loss: 3958.724
iteration 0300: loss: 3962.585
iteration 0400: loss: 3961.450
iteration 0500: loss: 3956.556
iteration 0600: loss: 3961.193
iteration 0700: loss: 3950.908
iteration 0800: loss: 3960.664
iteration 0900: loss: 3957.282
iteration 1000: loss: 3960.252
iteration 1100: loss: 3963.195
iteration 1200: loss: 3962.104
iteration 1300: loss: 3966.294
iteration 1400: loss: 3957.996
iteration 1500: loss: 3956.730
iteration 1600: loss: 3961.610
iteration 1700: loss: 3952.718
iteration 1800: loss: 3954.887
====> Epoch: 045 Train loss: 3958.6129  took : 138.9438440799713
====> Test loss: 3964.7361
iteration 0000: loss: 3963.036
iteration 0100: loss: 3958.074
iteration 0200: loss: 3960.236
iteration 0300: loss: 3956.166
iteration 0400: loss: 3969.838
iteration 0500: loss: 3952.969
iteration 0600: loss: 3952.576
iteration 0700: loss: 3951.951
iteration 0800: loss: 3969.880
iteration 0900: loss: 3959.160
iteration 1000: loss: 3954.684
iteration 1100: loss: 3950.063
iteration 1200: loss: 3954.579
iteration 1300: loss: 3961.797
iteration 1400: loss: 3961.712
iteration 1500: loss: 3962.711
iteration 1600: loss: 3963.241
iteration 1700: loss: 3959.839
iteration 1800: loss: 3953.586
====> Epoch: 046 Train loss: 3958.7380  took : 140.31499099731445
====> Test loss: 3965.0547
iteration 0000: loss: 3956.949
iteration 0100: loss: 3952.242
iteration 0200: loss: 3960.032
iteration 0300: loss: 3955.760
iteration 0400: loss: 3957.321
iteration 0500: loss: 3963.212
iteration 0600: loss: 3956.585
iteration 0700: loss: 3961.108
iteration 0800: loss: 3955.511
iteration 0900: loss: 3961.799
iteration 1000: loss: 3955.232
iteration 1100: loss: 3952.663
iteration 1200: loss: 3958.858
iteration 1300: loss: 3952.047
iteration 1400: loss: 3957.953
iteration 1500: loss: 3957.322
iteration 1600: loss: 3959.347
iteration 1700: loss: 3955.236
iteration 1800: loss: 3959.378
====> Epoch: 047 Train loss: 3958.7340  took : 140.31647205352783
====> Test loss: 3965.0255
iteration 0000: loss: 3956.369
iteration 0100: loss: 3950.599
iteration 0200: loss: 3955.270
iteration 0300: loss: 3959.014
iteration 0400: loss: 3959.926
iteration 0500: loss: 3958.904
iteration 0600: loss: 3956.114
iteration 0700: loss: 3958.419
iteration 0800: loss: 3956.505
iteration 0900: loss: 3959.474
iteration 1000: loss: 3955.974
iteration 1100: loss: 3957.987
iteration 1200: loss: 3960.504
iteration 1300: loss: 3959.782
iteration 1400: loss: 3955.318
iteration 1500: loss: 3958.021
iteration 1600: loss: 3956.709
iteration 1700: loss: 3957.513
iteration 1800: loss: 3966.883
====> Epoch: 048 Train loss: 3958.5899  took : 138.70645236968994
====> Test loss: 3964.7474
iteration 0000: loss: 3960.224
iteration 0100: loss: 3958.429
iteration 0200: loss: 3955.565
iteration 0300: loss: 3959.390
iteration 0400: loss: 3959.919
iteration 0500: loss: 3953.349
iteration 0600: loss: 3956.582
iteration 0700: loss: 3962.439
iteration 0800: loss: 3966.628
iteration 0900: loss: 3966.537
iteration 1000: loss: 3964.610
iteration 1100: loss: 3958.712
iteration 1200: loss: 3965.435
iteration 1300: loss: 3960.786
iteration 1400: loss: 3950.313
iteration 1500: loss: 3966.092
iteration 1600: loss: 3955.806
iteration 1700: loss: 3954.153
iteration 1800: loss: 3961.853
====> Epoch: 049 Train loss: 3958.6264  took : 140.20523071289062
====> Test loss: 3964.8374
iteration 0000: loss: 3953.370
iteration 0100: loss: 3961.134
iteration 0200: loss: 3964.018
iteration 0300: loss: 3953.237
iteration 0400: loss: 3961.131
iteration 0500: loss: 3963.428
iteration 0600: loss: 3962.566
iteration 0700: loss: 3963.428
iteration 0800: loss: 3959.143
iteration 0900: loss: 3959.736
iteration 1000: loss: 3965.760
iteration 1100: loss: 3954.150
iteration 1200: loss: 3962.972
iteration 1300: loss: 3953.520
iteration 1400: loss: 3949.986
iteration 1500: loss: 3950.493
iteration 1600: loss: 3950.828
iteration 1700: loss: 3960.629
iteration 1800: loss: 3949.659
====> Epoch: 050 Train loss: 3958.5575  took : 139.76720261573792
====> Test loss: 3964.6969
====> [MM-VAE] Time: 8010.285s or 02:13:30
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 0, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_0', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0', 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/train'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_0
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([4, 9, 2, 3, 9, 5, 3, 8, 5, 3, 6, 1, 4, 1, 6, 9, 1, 3, 5, 8, 4, 1, 9, 5,
        3, 7, 7, 4, 7, 5, 9, 9, 9, 4, 1, 7, 4, 8, 9, 5, 1, 2, 8, 1, 8, 6, 3, 5,
        5, 9, 4, 4, 6, 6, 3, 4, 4, 1, 9, 1, 6, 6, 2, 1, 6, 7, 1, 5, 4, 3, 3, 4,
        8, 9, 1, 1, 4, 4, 9, 8, 4, 4, 1, 3, 8, 3, 1, 1, 1, 7, 2, 4, 8, 9, 3, 4,
        9, 8, 4, 5, 3, 8, 8, 5, 8, 7, 9, 8, 9, 8, 4, 9, 3, 4, 6, 3, 1, 5, 5, 9,
        6, 1, 1, 6, 9, 2, 1, 6], device='cuda:0') 
label[0]: tensor(4, device='cuda:0') 
label[1]: tensor(9, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.1689741   0.05573804 -0.09744311 ...  0.11509392 -0.02380986
  0.1734013 ]
Average of silhouette coef: 0.114090465
---
  0   1   2   3
0 0.0 2.9 2.9 2.0 
1 2.9 0.0 2.9 2.0 
2 2.9 2.9 0.0 2.1 
3 2.0 2.0 2.1 0.0 
correlation [[ 1.         -0.49114082]
 [-0.49114082  1.        ]]
---
[[], [], [], []] [1 3 3 ... 1 3 0] [[-47.727913  32.52151 ]
 [ -9.174075  19.035284]
 [ 24.333035 -45.13523 ]
 ...
 [-40.606667 -18.562504]
 [-13.571913 -36.85549 ]
 [ 10.6521   -58.13668 ]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.114090465, 'cluster_all': array([ 0.1689741 ,  0.05573804, -0.09744311, ...,  0.11509392,
       -0.02380986,  0.1734013 ], dtype=float32), 'magnitude_avg': 0.49114081652316255, 'magnitude_all': -0.49114081652316255, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_0', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.114090465, 'magnitude_avg': 0.49114081652316255, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([5, 5, 6, 4, 4, 4, 7, 9, 9, 2, 9, 8, 3, 6, 2, 3, 3, 1, 4, 8, 2, 6, 2, 8,
        1, 9, 1, 8, 6, 3, 9, 3, 2, 8, 2, 4, 6, 7, 4, 5, 3, 2, 9, 2, 8, 6, 2, 9,
        5, 1, 1, 2, 1, 2, 3, 8, 7, 9, 4, 4, 3, 8, 8, 1, 7, 2, 6, 3, 1, 8, 6, 8,
        6, 2, 5, 5, 3, 7, 7, 5, 7, 6, 3, 8, 6, 1, 7, 1, 9, 1, 9, 6, 4, 1, 8, 2,
        7, 6, 6, 8, 7, 3, 9, 1, 4, 8, 7, 6, 1, 9, 6, 6, 5, 4, 7, 6, 6, 1, 7, 3,
        3, 8, 6, 2, 5, 6, 3, 5], device='cuda:0') 
label[0]: tensor(5, device='cuda:0') 
label[1]: tensor(5, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([4, 6, 6, 7, 5, 3, 1, 4, 3, 3, 3, 3, 3, 5, 9, 4, 3, 4, 2, 5, 1, 6, 2, 3,
        5, 3, 9, 1, 9, 5, 4, 1, 6, 4, 1, 6, 4, 5, 6, 3, 1, 4, 5, 1, 4, 8, 3, 8,
        6, 8, 3, 4, 5, 5, 3, 4, 2, 4, 3, 7, 3, 4, 1, 8, 6, 1, 7, 2, 2, 1, 4, 7,
        6, 3, 1, 4, 8, 5, 2, 7, 9, 8, 5, 3, 1, 9, 2, 4, 4, 9, 6, 5, 4, 2, 2, 8,
        9, 5, 6, 7, 8, 6, 1, 8, 5, 5, 6, 5, 5, 2, 7, 7, 6, 8, 7, 9, 3, 4, 1, 6,
        5, 1, 3, 2, 8, 2, 6, 4])
Accuracy (count): tensor(20) 
Accuracy (ratio) tensor(0.1562)
Accuracy:
 [[ 0  0  0 15  0  0  0  0  0]
 [ 0  0  0 12  0  0  0  0  0]
 [ 0  0  0 19  0  0  0  0  0]
 [ 0  0  0 20  0  0  0  0  0]
 [ 0  0  0 18  0  0  0  0  0]
 [ 0  0  0 16  0  0  0  0  0]
 [ 0  0  0  9  0  0  0  0  0]
 [ 0  0  0 11  0  0  0  0  0]
 [ 0  0  0  8  0  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]]
---
Silhouette values: [-0.07358103 -0.0938155   0.11671866 ... -0.04208773  0.1433491
  0.03807716]
Average of silhouette coef: 0.069905825
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.7 2.9 2.9 3.0 3.1 3.0 2.7 2.6 
2 2.7 0.0 2.6 2.8 2.8 2.8 3.1 2.2 2.9 
3 2.9 2.6 0.0 3.5 1.9 3.2 2.8 2.1 2.7 
4 2.9 2.8 3.5 0.0 2.7 2.4 2.6 2.5 1.6 
5 3.0 2.8 1.9 2.7 0.0 2.6 2.8 1.8 2.1 
6 3.1 2.8 3.2 2.4 2.6 0.0 3.7 2.9 2.9 
7 3.0 3.1 2.8 2.6 2.8 3.7 0.0 2.9 1.6 
8 2.7 2.2 2.1 2.5 1.8 2.9 2.9 0.0 1.8 
9 2.6 2.9 2.7 1.6 2.1 2.9 1.6 1.8 0.0 
correlation [[ 1.         -0.06511306]
 [-0.06511306  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [5 5 6 ... 3 6 2] [[-18.042715    42.58051   ]
 [  9.1905775   37.497948  ]
 [ 14.311979    59.225037  ]
 ...
 [-33.259293    37.28098   ]
 [ 45.50476     30.879229  ]
 [ -0.73449606   5.569984  ]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(60) 
Accuracy (ratio) tensor(0.1227)
Accuracy:
 [[ 0  0  0 45  0  0  0  0  0]
 [ 0  0  0 52  0  0  0  0  0]
 [ 0  0  0 57  0  0  0  0  0]
 [ 0  0  0 60  0  0  0  0  0]
 [ 0  0  0 61  0  0  0  0  0]
 [ 0  0  0 60  0  0  0  0  0]
 [ 0  0  0 57  0  0  0  0  0]
 [ 0  0  0 52  0  0  0  0  0]
 [ 0  0  0 45  0  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.2  0.1  0.0  1.7  0.2  0.1 -0.0  0.1  1.0 -0.0 -0.1 -0.0 
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.2 -1.3  0.5 -0.0 -0.1  0.4  0.2 -0.6 -0.1 -0.2 -0.0 -1.0 -0.0 
-0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.5 -0.3 -0.2  0.0 -0.4  1.1  0.2  1.3  0.1 -0.2 -0.0 -0.2 -0.0 
-0.0  0.0  0.0  0.0  0.0  0.0 -0.0  0.9  0.3 -0.3 -0.0 -0.1 -0.8 -0.1 -1.0  0.0 -0.1  0.0  0.6 -0.0 
 0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  0.2  1.0  0.5  0.0 -0.4  0.5  0.1  0.8 -0.2 -0.2 -0.0 -0.3 -0.0 
 0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0 -0.1  1.0 -0.1 -0.0 -0.1  0.6  0.9 -1.5  0.1 -0.2 -0.0 -0.3  0.0 
-0.0 -0.0  0.0  0.0  0.0  0.0 -0.0 -0.6 -0.5 -0.0 -0.0 -0.1 -1.6  0.1  0.8 -0.1 -0.1  0.0  0.5  0.0 
-0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0  0.4 -0.0 -0.0  0.0 -0.3  0.6 -1.1  0.2 -0.0 -0.1  0.0 -0.4 -0.0 
-0.0  0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.1  0.3 -0.1 -0.0 -0.1 -0.8 -0.7  0.1  0.0 -0.0  0.0  0.6 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.8286791
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.2  0.1  0.0  1.7  0.2  0.1 -0.0  0.1  1.0 -0.0 -0.1 -0.0 
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.2 -1.3  0.5 -0.0 -0.1  0.4  0.2 -0.6 -0.1 -0.2 -0.0 -1.0 -0.0 
-0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.5 -0.3 -0.2  0.0 -0.4  1.1  0.2  1.3  0.1 -0.2 -0.0 -0.2 -0.0 
-0.0  0.0  0.0  0.0  0.0  0.0 -0.0  0.9  0.3 -0.3 -0.0 -0.1 -0.8 -0.1 -1.0  0.0 -0.1  0.0  0.6 -0.0 
 0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  0.2  1.0  0.5  0.0 -0.4  0.5  0.1  0.8 -0.2 -0.2 -0.0 -0.3 -0.0 
 0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0 -0.1  1.0 -0.1 -0.0 -0.1  0.6  0.9 -1.5  0.1 -0.2 -0.0 -0.3  0.0 
-0.0 -0.0  0.0  0.0  0.0  0.0 -0.0 -0.6 -0.5 -0.0 -0.0 -0.1 -1.6  0.1  0.8 -0.1 -0.1  0.0  0.5  0.0 
-0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0  0.4 -0.0 -0.0  0.0 -0.3  0.6 -1.1  0.2 -0.0 -0.1  0.0 -0.4 -0.0 
-0.0  0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.1  0.3 -0.1 -0.0 -0.1 -0.8 -0.7  0.1  0.0 -0.0  0.0  0.6 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.4593966
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.2  0.1  0.0  1.7  0.2  0.1 -0.0  0.1  1.0 -0.0 -0.1 -0.0 
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.2 -1.3  0.5 -0.0 -0.1  0.4  0.2 -0.6 -0.1 -0.2 -0.0 -1.0 -0.0 
-0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.5 -0.3 -0.2  0.0 -0.4  1.1  0.2  1.3  0.1 -0.2 -0.0 -0.2 -0.0 
-0.0  0.0  0.0  0.0  0.0  0.0 -0.0  0.9  0.3 -0.3 -0.0 -0.1 -0.8 -0.1 -1.0  0.0 -0.1  0.0  0.6 -0.0 
 0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  0.2  1.0  0.5  0.0 -0.4  0.5  0.1  0.8 -0.2 -0.2 -0.0 -0.3 -0.0 
 0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0 -0.1  1.0 -0.1 -0.0 -0.1  0.6  0.9 -1.5  0.1 -0.2 -0.0 -0.3  0.0 
-0.0 -0.0  0.0  0.0  0.0  0.0 -0.0 -0.6 -0.5 -0.0 -0.0 -0.1 -1.6  0.1  0.8 -0.1 -0.1  0.0  0.5  0.0 
-0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0  0.4 -0.0 -0.0  0.0 -0.3  0.6 -1.1  0.2 -0.0 -0.1  0.0 -0.4 -0.0 
-0.0  0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.1  0.3 -0.1 -0.0 -0.1 -0.8 -0.7  0.1  0.0 -0.0  0.0  0.6 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.7102506
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.2  0.1  0.0  1.7  0.2  0.1 -0.0  0.1  1.0 -0.0 -0.1 -0.0 
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.2 -1.3  0.5 -0.0 -0.1  0.4  0.2 -0.6 -0.1 -0.2 -0.0 -1.0 -0.0 
-0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.5 -0.3 -0.2  0.0 -0.4  1.1  0.2  1.3  0.1 -0.2 -0.0 -0.2 -0.0 
-0.0  0.0  0.0  0.0  0.0  0.0 -0.0  0.9  0.3 -0.3 -0.0 -0.1 -0.8 -0.1 -1.0  0.0 -0.1  0.0  0.6 -0.0 
 0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  0.2  1.0  0.5  0.0 -0.4  0.5  0.1  0.8 -0.2 -0.2 -0.0 -0.3 -0.0 
 0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0 -0.1  1.0 -0.1 -0.0 -0.1  0.6  0.9 -1.5  0.1 -0.2 -0.0 -0.3  0.0 
-0.0 -0.0  0.0  0.0  0.0  0.0 -0.0 -0.6 -0.5 -0.0 -0.0 -0.1 -1.6  0.1  0.8 -0.1 -0.1  0.0  0.5  0.0 
-0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0  0.4 -0.0 -0.0  0.0 -0.3  0.6 -1.1  0.2 -0.0 -0.1  0.0 -0.4 -0.0 
-0.0  0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.1  0.3 -0.1 -0.0 -0.1 -0.8 -0.7  0.1  0.0 -0.0  0.0  0.6 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.5806746
results (all): {'reconst_0x0_avg': 0.15625, 'reconst_0x0_all': nan, 'cluster_avg': 0.069905825, 'cluster_all': array([-0.07358103, -0.0938155 ,  0.11671866, ..., -0.04208773,
        0.1433491 ,  0.03807716], dtype=float32), 'magnitude_avg': 0.06511306335959595, 'magnitude_all': -0.06511306335959595, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_0', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.15625, 'cluster_avg': 0.069905825, 'magnitude_avg': 0.06511306335959595, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 1, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_1', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1', 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/train', 'device': 'cuda'}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_1
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([1, 7, 3, 5, 3, 1, 6, 3, 7, 8, 1, 1, 4, 8, 2, 1, 1, 3, 7, 4, 3, 8, 2, 9,
        6, 2, 7, 3, 4, 2, 5, 9, 1, 2, 3, 7, 4, 2, 8, 3, 4, 2, 1, 3, 6, 4, 4, 7,
        3, 7, 5, 8, 7, 3, 9, 3, 3, 4, 7, 4, 8, 9, 1, 2, 1, 4, 7, 6, 8, 4, 6, 8,
        7, 1, 8, 3, 4, 3, 8, 4, 4, 7, 1, 6, 7, 6, 5, 7, 8, 6, 2, 4, 4, 1, 3, 8,
        6, 7, 4, 2, 1, 4, 7, 5, 4, 9, 6, 8, 1, 3, 7, 5, 3, 7, 5, 3, 5, 3, 6, 6,
        4, 2, 5, 6, 7, 9, 6, 2], device='cuda:0') 
label[0]: tensor(1, device='cuda:0') 
label[1]: tensor(7, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.20481     0.18269227  0.06356577 ... -0.06465482  0.14349425
  0.15601477]
Average of silhouette coef: 0.1254683
---
  0   1   2   3
0 0.0 3.0 2.9 2.1 
1 3.0 0.0 3.0 2.1 
2 2.9 3.0 0.0 2.0 
3 2.1 2.1 2.0 0.0 
correlation [[ 1.        -0.4112431]
 [-0.4112431  1.       ]]
---
[[], [], [], []] [0 2 3 ... 3 0 2] [[ 48.962284  -30.563839 ]
 [-11.925918  -31.410385 ]
 [ 10.683245    2.4252477]
 ...
 [ -3.7803524  -1.2531484]
 [ 28.196194   12.10202  ]
 [ -2.1700776  47.124226 ]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.1254683, 'cluster_all': array([ 0.20481   ,  0.18269227,  0.06356577, ..., -0.06465482,
        0.14349425,  0.15601477], dtype=float32), 'magnitude_avg': 0.4112430984497742, 'magnitude_all': -0.4112430984497742, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_1', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.1254683, 'magnitude_avg': 0.4112430984497742, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([2, 3, 3, 7, 9, 4, 6, 7, 8, 7, 3, 1, 1, 3, 6, 3, 1, 3, 8, 9, 8, 2, 9, 1,
        2, 8, 4, 7, 9, 3, 8, 6, 6, 6, 4, 7, 7, 7, 6, 4, 6, 7, 1, 6, 9, 2, 2, 6,
        9, 8, 7, 3, 2, 5, 8, 9, 3, 7, 3, 2, 7, 9, 9, 9, 2, 6, 9, 6, 9, 2, 4, 8,
        5, 5, 3, 4, 6, 3, 8, 6, 8, 6, 2, 8, 3, 6, 2, 4, 6, 3, 9, 9, 5, 1, 3, 6,
        1, 7, 3, 5, 8, 6, 9, 9, 4, 2, 7, 9, 9, 9, 8, 2, 2, 8, 7, 7, 8, 6, 9, 9,
        2, 9, 9, 5, 5, 3, 6, 9], device='cuda:0') 
label[0]: tensor(2, device='cuda:0') 
label[1]: tensor(3, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([2, 5, 2, 6, 5, 9, 6, 9, 8, 2, 3, 4, 7, 9, 2, 3, 3, 3, 8, 3, 2, 7, 4, 4,
        1, 9, 1, 1, 1, 2, 4, 9, 6, 3, 4, 3, 3, 5, 7, 5, 1, 3, 1, 3, 9, 1, 4, 7,
        2, 4, 3, 4, 2, 6, 4, 1, 1, 2, 2, 6, 1, 4, 8, 8, 6, 5, 6, 3, 1, 5, 1, 3,
        6, 7, 7, 6, 2, 7, 8, 4, 3, 1, 7, 6, 5, 6, 3, 2, 4, 8, 5, 3, 7, 9, 2, 8,
        9, 4, 6, 1, 6, 6, 7, 4, 3, 8, 7, 3, 9, 8, 7, 1, 9, 3, 3, 4, 9, 8, 6, 9,
        3, 4, 4, 8, 5, 9, 6, 7])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 15]
 [ 0  0  0  0  0  0  0  0 13]
 [ 0  0  0  0  0  0  0  0 21]
 [ 0  0  0  0  0  0  0  0 17]
 [ 0  0  0  0  0  0  0  0  9]
 [ 0  0  0  0  0  0  0  0 16]
 [ 0  0  0  0  0  0  0  0 13]
 [ 0  0  0  0  0  0  0  0 11]
 [ 0  0  0  0  0  0  0  0 13]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
---
Silhouette values: [ 0.0424933  -0.1614691  -0.19578812 ...  0.01121746  0.11830365
  0.03336198]
Average of silhouette coef: 0.068737805
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.8 3.0 3.2 3.2 3.2 3.1 2.6 2.8 
2 2.8 0.0 2.3 2.7 2.8 2.7 2.7 2.3 2.6 
3 3.0 2.3 0.0 3.6 2.0 3.3 2.7 1.9 2.7 
4 3.2 2.7 3.6 0.0 3.0 2.4 2.6 2.6 1.6 
5 3.2 2.8 2.0 3.0 0.0 2.6 2.7 1.9 2.1 
6 3.2 2.7 3.3 2.4 2.6 0.0 3.8 2.7 2.8 
7 3.1 2.7 2.7 2.6 2.7 3.8 0.0 2.8 1.7 
8 2.6 2.3 1.9 2.6 1.9 2.7 2.8 0.0 1.9 
9 2.8 2.6 2.7 1.6 2.1 2.8 1.7 1.9 0.0 
correlation [[ 1.         -0.09499766]
 [-0.09499766  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [2 3 3 ... 2 4 4] [[-23.266743 -26.60548 ]
 [ 13.944169  30.376432]
 [ -7.332442 -51.173485]
 ...
 [-13.355073 -36.055695]
 [-14.419588  53.60314 ]
 [ 22.922909 -40.165127]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 45]
 [ 0  0  0  0  0  0  0  0 52]
 [ 0  0  0  0  0  0  0  0 57]
 [ 0  0  0  0  0  0  0  0 60]
 [ 0  0  0  0  0  0  0  0 61]
 [ 0  0  0  0  0  0  0  0 60]
 [ 0  0  0  0  0  0  0  0 57]
 [ 0  0  0  0  0  0  0  0 52]
 [ 0  0  0  0  0  0  0  0 45]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.7 -0.3  0.1 -0.0 -0.0 -0.9  0.0  0.2 -0.1  0.0  0.9 -0.0  0.0  0.0  1.4 -0.0 -0.0 -0.0 -0.0 -0.0 
-0.3  0.3  0.6 -0.0 -0.6  0.1  0.0 -1.0 -0.8 -0.0  0.6 -0.0  0.0  0.0 -0.2  0.2  0.0 -0.0 -0.0 -0.0 
 0.0  0.6 -0.1  0.0 -1.2  0.3 -0.0  0.9 -0.5 -0.0  0.5 -0.0  0.0  0.0 -0.6 -0.3 -0.0 -0.0 -0.0 -0.0 
-0.2 -0.3 -0.3 -0.0  0.9  0.2 -0.0 -1.3  1.0  0.0 -0.3 -0.0  0.0 -0.0 -0.2 -0.1 -0.0  0.0 -0.0 -0.0 
-0.1  0.6  0.1  0.0  0.1  0.2  0.0  1.1 -0.2  0.0 -0.8 -0.0  0.0 -0.0 -0.5  0.3 -0.0 -0.0 -0.0  0.0 
 0.2  0.5  0.0 -0.0  1.5 -0.9  0.0 -0.5 -0.6  0.0 -0.1 -0.0  0.0  0.0 -0.8 -0.2  0.0 -0.0 -0.0 -0.0 
-0.3 -0.7  0.2 -0.0 -1.2  0.4 -0.0 -0.1  0.8  0.0 -0.8 -0.0  0.0  0.0  0.4 -0.2 -0.0 -0.0  0.0 -0.0 
-0.4  0.2 -0.5  0.0  0.3  0.6  0.0  0.6 -0.1  0.0  0.8  0.0  0.0 -0.0 -0.1  0.2  0.0 -0.0 -0.0 -0.0 
-0.1 -0.5  0.2 -0.0  0.3  0.6 -0.0  0.0  0.9 -0.0 -0.3 -0.0  0.0 -0.0 -0.0 -0.0 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.935553
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.7 -0.3  0.1 -0.0 -0.0 -0.9  0.0  0.2 -0.1  0.0  0.9 -0.0  0.0  0.0  1.4 -0.0 -0.0 -0.0 -0.0 -0.0 
-0.3  0.3  0.6 -0.0 -0.6  0.1  0.0 -1.0 -0.8 -0.0  0.6 -0.0  0.0  0.0 -0.2  0.2  0.0 -0.0 -0.0 -0.0 
 0.0  0.6 -0.1  0.0 -1.2  0.3 -0.0  0.9 -0.5 -0.0  0.5 -0.0  0.0  0.0 -0.6 -0.3 -0.0 -0.0 -0.0 -0.0 
-0.2 -0.3 -0.3 -0.0  0.9  0.2 -0.0 -1.3  1.0  0.0 -0.3 -0.0  0.0 -0.0 -0.2 -0.1 -0.0  0.0 -0.0 -0.0 
-0.1  0.6  0.1  0.0  0.1  0.2  0.0  1.1 -0.2  0.0 -0.8 -0.0  0.0 -0.0 -0.5  0.3 -0.0 -0.0 -0.0  0.0 
 0.2  0.5  0.0 -0.0  1.5 -0.9  0.0 -0.5 -0.6  0.0 -0.1 -0.0  0.0  0.0 -0.8 -0.2  0.0 -0.0 -0.0 -0.0 
-0.3 -0.7  0.2 -0.0 -1.2  0.4 -0.0 -0.1  0.8  0.0 -0.8 -0.0  0.0  0.0  0.4 -0.2 -0.0 -0.0  0.0 -0.0 
-0.4  0.2 -0.5  0.0  0.3  0.6  0.0  0.6 -0.1  0.0  0.8  0.0  0.0 -0.0 -0.1  0.2  0.0 -0.0 -0.0 -0.0 
-0.1 -0.5  0.2 -0.0  0.3  0.6 -0.0  0.0  0.9 -0.0 -0.3 -0.0  0.0 -0.0 -0.0 -0.0 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.6252615
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.7 -0.3  0.1 -0.0 -0.0 -0.9  0.0  0.2 -0.1  0.0  0.9 -0.0  0.0  0.0  1.4 -0.0 -0.0 -0.0 -0.0 -0.0 
-0.3  0.3  0.6 -0.0 -0.6  0.1  0.0 -1.0 -0.8 -0.0  0.6 -0.0  0.0  0.0 -0.2  0.2  0.0 -0.0 -0.0 -0.0 
 0.0  0.6 -0.1  0.0 -1.2  0.3 -0.0  0.9 -0.5 -0.0  0.5 -0.0  0.0  0.0 -0.6 -0.3 -0.0 -0.0 -0.0 -0.0 
-0.2 -0.3 -0.3 -0.0  0.9  0.2 -0.0 -1.3  1.0  0.0 -0.3 -0.0  0.0 -0.0 -0.2 -0.1 -0.0  0.0 -0.0 -0.0 
-0.1  0.6  0.1  0.0  0.1  0.2  0.0  1.1 -0.2  0.0 -0.8 -0.0  0.0 -0.0 -0.5  0.3 -0.0 -0.0 -0.0  0.0 
 0.2  0.5  0.0 -0.0  1.5 -0.9  0.0 -0.5 -0.6  0.0 -0.1 -0.0  0.0  0.0 -0.8 -0.2  0.0 -0.0 -0.0 -0.0 
-0.3 -0.7  0.2 -0.0 -1.2  0.4 -0.0 -0.1  0.8  0.0 -0.8 -0.0  0.0  0.0  0.4 -0.2 -0.0 -0.0  0.0 -0.0 
-0.4  0.2 -0.5  0.0  0.3  0.6  0.0  0.6 -0.1  0.0  0.8  0.0  0.0 -0.0 -0.1  0.2  0.0 -0.0 -0.0 -0.0 
-0.1 -0.5  0.2 -0.0  0.3  0.6 -0.0  0.0  0.9 -0.0 -0.3 -0.0  0.0 -0.0 -0.0 -0.0 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.7796433
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.7 -0.3  0.1 -0.0 -0.0 -0.9  0.0  0.2 -0.1  0.0  0.9 -0.0  0.0  0.0  1.4 -0.0 -0.0 -0.0 -0.0 -0.0 
-0.3  0.3  0.6 -0.0 -0.6  0.1  0.0 -1.0 -0.8 -0.0  0.6 -0.0  0.0  0.0 -0.2  0.2  0.0 -0.0 -0.0 -0.0 
 0.0  0.6 -0.1  0.0 -1.2  0.3 -0.0  0.9 -0.5 -0.0  0.5 -0.0  0.0  0.0 -0.6 -0.3 -0.0 -0.0 -0.0 -0.0 
-0.2 -0.3 -0.3 -0.0  0.9  0.2 -0.0 -1.3  1.0  0.0 -0.3 -0.0  0.0 -0.0 -0.2 -0.1 -0.0  0.0 -0.0 -0.0 
-0.1  0.6  0.1  0.0  0.1  0.2  0.0  1.1 -0.2  0.0 -0.8 -0.0  0.0 -0.0 -0.5  0.3 -0.0 -0.0 -0.0  0.0 
 0.2  0.5  0.0 -0.0  1.5 -0.9  0.0 -0.5 -0.6  0.0 -0.1 -0.0  0.0  0.0 -0.8 -0.2  0.0 -0.0 -0.0 -0.0 
-0.3 -0.7  0.2 -0.0 -1.2  0.4 -0.0 -0.1  0.8  0.0 -0.8 -0.0  0.0  0.0  0.4 -0.2 -0.0 -0.0  0.0 -0.0 
-0.4  0.2 -0.5  0.0  0.3  0.6  0.0  0.6 -0.1  0.0  0.8  0.0  0.0 -0.0 -0.1  0.2  0.0 -0.0 -0.0 -0.0 
-0.1 -0.5  0.2 -0.0  0.3  0.6 -0.0  0.0  0.9 -0.0 -0.3 -0.0  0.0 -0.0 -0.0 -0.0 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.2716727
results (all): {'reconst_0x0_avg': 0.1015625, 'reconst_0x0_all': nan, 'cluster_avg': 0.068737805, 'cluster_all': array([ 0.0424933 , -0.1614691 , -0.19578812, ...,  0.01121746,
        0.11830365,  0.03336198], dtype=float32), 'magnitude_avg': 0.09499766392866968, 'magnitude_all': -0.09499766392866968, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_1', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1015625, 'cluster_avg': 0.068737805, 'magnitude_avg': 0.09499766392866968, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 2, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_2', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2', 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_2
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train):/home/taka/.pyenv/versions/3.6.9/lib/python3.6/site-packages/sklearn/metrics/_plot/confusion_matrix.py:81: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([6, 4, 6, 1, 9, 6, 7, 2, 4, 6, 3, 7, 2, 1, 7, 5, 4, 3, 6, 4, 8, 8, 4, 4,
        2, 1, 3, 3, 2, 8, 1, 2, 3, 6, 4, 3, 9, 3, 7, 5, 4, 9, 3, 7, 2, 6, 9, 4,
        8, 5, 5, 1, 9, 2, 7, 1, 5, 7, 3, 1, 7, 8, 9, 2, 5, 5, 7, 5, 4, 6, 1, 8,
        9, 3, 9, 9, 1, 4, 7, 7, 6, 5, 2, 8, 1, 8, 5, 4, 3, 2, 2, 5, 1, 5, 4, 8,
        5, 7, 7, 2, 9, 3, 6, 1, 2, 6, 8, 1, 2, 5, 1, 9, 4, 1, 2, 8, 8, 4, 7, 4,
        4, 8, 4, 8, 7, 5, 2, 2], device='cuda:0') 
label[0]: tensor(6, device='cuda:0') 
label[1]: tensor(4, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.18737486  0.19305694 -0.02879728 ...  0.17745732  0.15562478
  0.1554458 ]
Average of silhouette coef: 0.11929141
---
  0   1   2   3
0 0.0 3.0 3.1 1.9 
1 3.0 0.0 3.1 1.9 
2 3.1 3.1 0.0 2.1 
3 1.9 1.9 2.1 0.0 
correlation [[ 1.         -0.47200019]
 [-0.47200019  1.        ]]
---
[[], [], [], []] [0 2 3 ... 2 2 0] [[-33.863564  32.014465]
 [-31.381214 -39.05827 ]
 [-41.966427  23.204489]
 ...
 [-34.667377 -46.76051 ]
 [-64.51604  -16.849937]
 [  8.320401  49.6462  ]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.11929141, 'cluster_all': array([ 0.18737486,  0.19305694, -0.02879728, ...,  0.17745732,
        0.15562478,  0.1554458 ], dtype=float32), 'magnitude_avg': 0.47200019441888946, 'magnitude_all': -0.47200019441888946, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_2', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.11929141, 'magnitude_avg': 0.47200019441888946, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([7, 7, 9, 6, 6, 1, 7, 5, 7, 1, 3, 8, 7, 5, 1, 9, 3, 5, 8, 8, 2, 4, 7, 6,
        5, 4, 4, 3, 9, 9, 3, 6, 5, 5, 1, 1, 3, 5, 6, 2, 3, 8, 4, 9, 4, 6, 3, 5,
        4, 9, 8, 8, 4, 4, 1, 3, 8, 1, 2, 1, 1, 8, 4, 8, 9, 1, 7, 4, 5, 4, 4, 9,
        6, 3, 3, 2, 8, 8, 7, 2, 7, 2, 3, 3, 7, 1, 2, 1, 6, 3, 9, 1, 7, 6, 8, 8,
        7, 1, 3, 2, 2, 3, 7, 1, 7, 6, 6, 4, 5, 2, 3, 2, 1, 2, 2, 6, 7, 3, 8, 3,
        5, 8, 8, 8, 6, 3, 7, 2], device='cuda:0') 
label[0]: tensor(7, device='cuda:0') 
label[1]: tensor(7, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([3, 5, 5, 4, 3, 1, 9, 2, 4, 4, 7, 7, 1, 7, 7, 7, 1, 5, 7, 8, 8, 5, 9, 2,
        3, 9, 6, 4, 3, 2, 9, 4, 5, 4, 1, 7, 5, 8, 2, 8, 5, 5, 4, 1, 5, 6, 9, 3,
        9, 6, 3, 1, 9, 1, 4, 8, 7, 9, 4, 1, 7, 5, 3, 4, 1, 6, 5, 9, 7, 3, 1, 9,
        2, 3, 2, 3, 3, 6, 4, 4, 6, 3, 2, 4, 8, 2, 9, 4, 1, 7, 5, 8, 2, 2, 7, 7,
        3, 1, 2, 1, 4, 7, 8, 3, 8, 5, 9, 3, 9, 3, 2, 7, 7, 3, 3, 7, 7, 1, 9, 7,
        8, 2, 1, 1, 5, 2, 5, 1])
Accuracy (count): tensor(15) 
Accuracy (ratio) tensor(0.1172)
Accuracy:
 [[ 0  0  0  0 17  0  0  0  0]
 [ 0  0  0  0 14  0  0  0  0]
 [ 0  0  0  0 17  1  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0  6  0  0  0  0]
 [ 0  0  0  0 18  1  0  0  0]
 [ 0  0  0  0 10  0  0  0  0]
 [ 0  0  0  0 14  0  0  0  0]]
Accuracy:
 [[0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.944 0.056 0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.947 0.053 0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]]
---
Silhouette values: [0.01776674 0.04720897 0.05744255 ... 0.12486619 0.11632337 0.09415161]
Average of silhouette coef: 0.06981349
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.9 3.0 3.1 3.0 3.1 3.1 2.4 2.6 
2 2.9 0.0 2.7 2.8 3.1 2.8 2.9 2.4 2.9 
3 3.0 2.7 0.0 3.6 1.9 3.2 2.8 2.4 2.7 
4 3.1 2.8 3.6 0.0 3.0 2.6 2.7 2.4 1.7 
5 3.0 3.1 1.9 3.0 0.0 2.6 2.8 2.0 2.1 
6 3.1 2.8 3.2 2.6 2.6 0.0 3.6 2.8 2.8 
7 3.1 2.9 2.8 2.7 2.8 3.6 0.0 3.0 1.9 
8 2.4 2.4 2.4 2.4 2.0 2.8 3.0 0.0 1.8 
9 2.6 2.9 2.7 1.7 2.1 2.8 1.9 1.8 0.0 
correlation [[ 1.         -0.14589912]
 [-0.14589912  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 7 9 ... 3 7 2] [[ 50.513134  -12.451418 ]
 [-45.234367   42.043022 ]
 [-25.55397    -3.5981443]
 ...
 [  0.8245871 -25.275566 ]
 [ 36.953144  -24.826752 ]
 [  3.7327807  29.87944  ]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5,
        6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 6, 5, 6, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 6, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(0.1309)
Accuracy:
 [[ 0  0  0  0 41  4  0  0  0]
 [ 0  0  0  0 47  5  0  0  0]
 [ 0  0  0  0 56  1  0  0  0]
 [ 0  0  0  0 58  2  0  0  0]
 [ 0  0  0  0 58  3  0  0  0]
 [ 0  0  0  0 54  6  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 49  3  0  0  0]
 [ 0  0  0  0 44  1  0  0  0]]
Accuracy:
 [[0.    0.    0.    0.    0.911 0.089 0.    0.    0.   ]
 [0.    0.    0.    0.    0.904 0.096 0.    0.    0.   ]
 [0.    0.    0.    0.    0.982 0.018 0.    0.    0.   ]
 [0.    0.    0.    0.    0.967 0.033 0.    0.    0.   ]
 [0.    0.    0.    0.    0.951 0.049 0.    0.    0.   ]
 [0.    0.    0.    0.    0.9   0.1   0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.942 0.058 0.    0.    0.   ]
 [0.    0.    0.    0.    0.978 0.022 0.    0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0 -0.1  0.1 -0.2  0.0 -0.3 -1.9 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.1  0.0 -0.0 
 0.0  0.9 -0.6  1.0 -0.0  0.6 -0.0  0.0 -0.0  0.0  0.8  0.0  0.1  0.0 -0.0 -0.2  0.0  0.4  0.0 -0.1 
 0.0 -1.0 -0.4  0.5  0.0  1.1  0.0  0.0 -0.0 -0.0 -1.0  0.0 -0.2  0.0  0.0 -0.1  0.0  0.5  0.0  0.0 
 0.0  1.0  0.2 -0.8 -0.0 -0.5  0.7  0.0 -0.0  0.0  0.4 -0.0  0.6  0.0 -0.0  0.3  0.0 -0.7 -0.0 -0.1 
 0.0 -0.9  0.4  0.5  0.0  0.1  0.5  0.0 -0.0 -0.0 -0.8 -0.0 -0.9  0.0 -0.0 -0.2  0.0 -0.6  0.0 -0.0 
 0.0  0.5  2.0  0.6  0.0  0.3  0.3  0.0 -0.0  0.0  0.0 -0.0  0.3  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0 
 0.0  0.1 -0.8 -0.7 -0.0 -0.7  0.6  0.0  0.0 -0.0 -0.3  0.0 -0.5  0.0 -0.0 -0.3  0.0  1.2 -0.0  0.0 
 0.0 -0.9 -0.2  0.4 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.5  0.0  0.4  0.0 -0.0  0.2  0.0 -0.8 -0.0 -0.2 
-0.0 -0.4 -0.1 -0.8 -0.0 -0.5  0.5 -0.0 -0.0  0.0  0.1  0.0 -0.1  0.0  0.0  0.4  0.0 -0.2  0.0 -0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.7650816
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 0.0 -0.1  0.1 -0.2  0.0 -0.3 -1.9 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.1  0.0 -0.0 
 0.0  0.9 -0.6  1.0 -0.0  0.6 -0.0  0.0 -0.0  0.0  0.8  0.0  0.1  0.0 -0.0 -0.2  0.0  0.4  0.0 -0.1 
 0.0 -1.0 -0.4  0.5  0.0  1.1  0.0  0.0 -0.0 -0.0 -1.0  0.0 -0.2  0.0  0.0 -0.1  0.0  0.5  0.0  0.0 
 0.0  1.0  0.2 -0.8 -0.0 -0.5  0.7  0.0 -0.0  0.0  0.4 -0.0  0.6  0.0 -0.0  0.3  0.0 -0.7 -0.0 -0.1 
 0.0 -0.9  0.4  0.5  0.0  0.1  0.5  0.0 -0.0 -0.0 -0.8 -0.0 -0.9  0.0 -0.0 -0.2  0.0 -0.6  0.0 -0.0 
 0.0  0.5  2.0  0.6  0.0  0.3  0.3  0.0 -0.0  0.0  0.0 -0.0  0.3  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0 
 0.0  0.1 -0.8 -0.7 -0.0 -0.7  0.6  0.0  0.0 -0.0 -0.3  0.0 -0.5  0.0 -0.0 -0.3  0.0  1.2 -0.0  0.0 
 0.0 -0.9 -0.2  0.4 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.5  0.0  0.4  0.0 -0.0  0.2  0.0 -0.8 -0.0 -0.2 
-0.0 -0.4 -0.1 -0.8 -0.0 -0.5  0.5 -0.0 -0.0  0.0  0.1  0.0 -0.1  0.0  0.0  0.4  0.0 -0.2  0.0 -0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.442656
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0 -0.1  0.1 -0.2  0.0 -0.3 -1.9 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.1  0.0 -0.0 
 0.0  0.9 -0.6  1.0 -0.0  0.6 -0.0  0.0 -0.0  0.0  0.8  0.0  0.1  0.0 -0.0 -0.2  0.0  0.4  0.0 -0.1 
 0.0 -1.0 -0.4  0.5  0.0  1.1  0.0  0.0 -0.0 -0.0 -1.0  0.0 -0.2  0.0  0.0 -0.1  0.0  0.5  0.0  0.0 
 0.0  1.0  0.2 -0.8 -0.0 -0.5  0.7  0.0 -0.0  0.0  0.4 -0.0  0.6  0.0 -0.0  0.3  0.0 -0.7 -0.0 -0.1 
 0.0 -0.9  0.4  0.5  0.0  0.1  0.5  0.0 -0.0 -0.0 -0.8 -0.0 -0.9  0.0 -0.0 -0.2  0.0 -0.6  0.0 -0.0 
 0.0  0.5  2.0  0.6  0.0  0.3  0.3  0.0 -0.0  0.0  0.0 -0.0  0.3  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0 
 0.0  0.1 -0.8 -0.7 -0.0 -0.7  0.6  0.0  0.0 -0.0 -0.3  0.0 -0.5  0.0 -0.0 -0.3  0.0  1.2 -0.0  0.0 
 0.0 -0.9 -0.2  0.4 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.5  0.0  0.4  0.0 -0.0  0.2  0.0 -0.8 -0.0 -0.2 
-0.0 -0.4 -0.1 -0.8 -0.0 -0.5  0.5 -0.0 -0.0  0.0  0.1  0.0 -0.1  0.0  0.0  0.4  0.0 -0.2  0.0 -0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.85368
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(3) 
Accuracy (ratio) tensor(0.0469)
Accuracy:
 [[3]]
Accuracy:
 [[1.]]
 0.0 -0.1  0.1 -0.2  0.0 -0.3 -1.9 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.1  0.0 -0.0 
 0.0  0.9 -0.6  1.0 -0.0  0.6 -0.0  0.0 -0.0  0.0  0.8  0.0  0.1  0.0 -0.0 -0.2  0.0  0.4  0.0 -0.1 
 0.0 -1.0 -0.4  0.5  0.0  1.1  0.0  0.0 -0.0 -0.0 -1.0  0.0 -0.2  0.0  0.0 -0.1  0.0  0.5  0.0  0.0 
 0.0  1.0  0.2 -0.8 -0.0 -0.5  0.7  0.0 -0.0  0.0  0.4 -0.0  0.6  0.0 -0.0  0.3  0.0 -0.7 -0.0 -0.1 
 0.0 -0.9  0.4  0.5  0.0  0.1  0.5 /new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
 0.0 -0.0 -0.0 -0.8 -0.0 -0.9  0.0 -0.0 -0.2  0.0 -0.6  0.0 -0.0 
 0.0  0.5  2.0  0.6  0.0  0.3  0.3  0.0 -0.0  0.0  0.0 -0.0  0.3  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0 
 0.0  0.1 -0.8 -0.7 -0.0 -0.7  0.6  0.0  0.0 -0.0 -0.3  0.0 -0.5  0.0 -0.0 -0.3  0.0  1.2 -0.0  0.0 
 0.0 -0.9 -0.2  0.4 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.5  0.0  0.4  0.0 -0.0  0.2  0.0 -0.8 -0.0 -0.2 
-0.0 -0.4 -0.1 -0.8 -0.0 -0.5  0.5 -0.0 -0.0  0.0  0.1  0.0 -0.1  0.0  0.0  0.4  0.0 -0.2  0.0 -0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.7331321
results (all): {'reconst_0x0_avg': 0.1171875, 'reconst_0x0_all': nan, 'cluster_avg': 0.06981349, 'cluster_all': array([0.01776674, 0.04720897, 0.05744255, ..., 0.12486619, 0.11632337,
       0.09415161], dtype=float32), 'magnitude_avg': 0.14589911567664224, 'magnitude_all': -0.14589911567664224, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.13087934255599976, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 1.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.046875, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_2', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1171875, 'cluster_avg': 0.06981349, 'magnitude_avg': 0.14589911567664224, 'tsne-2d_avg': nan, 'mathematics_avg': 0.13087934255599976, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 1.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.046875}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 3, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_3', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3', 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_3
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([6, 7, 3, 7, 5, 8, 7, 4, 8, 2, 9, 1, 1, 8, 1, 9, 3, 4, 6, 3, 6, 9, 6, 7,
        9, 7, 1, 7, 1, 5, 5, 2, 7, 6, 9, 5, 5, 4, 1, 7, 1, 9, 4, 1, 4, 7, 5, 8,
        8, 5, 8, 7, 7, 7, 5, 1, 3, 5, 9, 6, 1, 4, 5, 9, 3, 9, 3, 7, 5, 1, 3, 9,
        1, 8, 6, 9, 2, 6, 7, 9, 9, 7, 6, 4, 5, 2, 9, 6, 3, 7, 4, 8, 4, 7, 4, 1,
        8, 4, 7, 9, 9, 5, 3, 2, 3, 4, 8, 9, 1, 3, 9, 1, 3, 9, 4, 5, 1, 4, 6, 7,
        7, 4, 1, 5, 6, 6, 3, 4], device='cuda:0') 
label[0]: tensor(6, device='cuda:0') 
label[1]: tensor(7, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.11839978 0.15962417 0.18756859 ... 0.16023311 0.13725197 0.18179663]
Average of silhouette coef: 0.11674671
---
  0   1   2   3
0 0.0 2.9 3.1 2.1 
1 2.9 0.0 2.9 2.1 
2 3.1 2.9 0.0 2.0 
3 2.1 2.1 2.0 0.0 
correlation [[ 1.         -0.35987292]
 [-0.35987292  1.        ]]
---
[[], [], [], []] [1 1 0 ... 1 1 2] [[  6.9369683  38.355392 ]
 [-31.200378   23.265392 ]
 [ -5.9941573 -39.285633 ]
 ...
 [  1.4684254  19.85285  ]
 [-12.645359   49.364605 ]
 [ 38.25369     3.4495435]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.11674671, 'cluster_all': array([0.11839978, 0.15962417, 0.18756859, ..., 0.16023311, 0.13725197,
       0.18179663], dtype=float32), 'magnitude_avg': 0.35987292088717254, 'magnitude_all': -0.35987292088717254, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_3', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.11674671, 'magnitude_avg': 0.35987292088717254, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([2, 5, 9, 8, 5, 5, 6, 3, 7, 8, 8, 9, 4, 7, 4, 5, 3, 4, 3, 9, 1, 1, 4, 2,
        9, 7, 7, 3, 6, 2, 7, 2, 6, 8, 1, 8, 2, 5, 7, 1, 1, 8, 2, 9, 2, 4, 5, 3,
        3, 1, 4, 2, 3, 4, 4, 1, 1, 7, 2, 2, 8, 4, 4, 9, 5, 6, 1, 8, 7, 7, 5, 7,
        4, 8, 7, 6, 4, 2, 3, 9, 8, 5, 7, 7, 6, 8, 6, 6, 1, 4, 1, 7, 7, 5, 7, 4,
        1, 2, 5, 1, 4, 9, 8, 8, 1, 7, 2, 5, 9, 6, 5, 1, 4, 3, 9, 3, 5, 7, 2, 3,
        6, 7, 7, 2, 6, 4, 7, 3], device='cuda:0') 
label[0]: tensor(2, device='cuda:0') 
label[1]: tensor(5, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([7, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([5, 9, 9, 7, 1, 3, 2, 7, 9, 9, 4, 4, 5, 3, 8, 6, 6, 4, 4, 6, 3, 2, 2, 1,
        2, 1, 4, 3, 5, 8, 5, 7, 5, 4, 2, 6, 1, 9, 7, 9, 4, 7, 6, 8, 8, 5, 3, 8,
        4, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 6, 9, 4, 2, 9, 8, 6, 1, 2, 1, 6, 3, 9,
        4, 4, 4, 6, 2, 7, 5, 4, 2, 6, 4, 4, 6, 5, 1, 3, 4, 2, 8, 9, 7, 2, 8, 1,
        2, 1, 2, 3, 5, 3, 6, 1, 3, 5, 9, 6, 7, 4, 1, 1, 2, 6, 1, 1, 3, 7, 4, 8,
        8, 6, 6, 2, 6, 5, 6, 2])
Accuracy (count): tensor(8) 
Accuracy (ratio) tensor(0.0625)
Accuracy:
 [[ 0  0  0  0  0  0 21  0  0]
 [ 0  0  0  0  0  0 18  0  0]
 [ 0  0  0  0  0  0 12  0  0]
 [ 0  0  0  0  0  0 18  0  0]
 [ 0  0  0  0  0  0 11  0  0]
 [ 0  0  0  0  0  0 18  0  0]
 [ 0  0  0  0  0  0  8  1  0]
 [ 0  0  0  0  0  0 10  0  0]
 [ 0  0  0  0  0  0  9  2  0]]
Accuracy:
 [[0.    0.    0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.889 0.111 0.   ]
 [0.    0.    0.    0.    0.    0.    1.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.818 0.182 0.   ]]
---
Silhouette values: [ 0.02822629  0.00769719  0.03702607 ... -0.00581365  0.27965048
 -0.04807001]
Average of silhouette coef: 0.07039657
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.8 3.0 3.1 3.2 3.1 2.9 2.7 2.7 
2 2.8 0.0 2.4 2.7 2.9 2.7 3.2 2.3 3.0 
3 3.0 2.4 0.0 3.5 1.8 3.0 3.0 2.3 2.9 
4 3.1 2.7 3.5 0.0 2.9 2.7 2.6 2.3 1.5 
5 3.2 2.9 1.8 2.9 0.0 2.2 2.9 2.2 2.3 
6 3.1 2.7 3.0 2.7 2.2 0.0 3.7 2.7 2.8 
7 2.9 3.2 3.0 2.6 2.9 3.7 0.0 3.0 1.8 
8 2.7 2.3 2.3 2.3 2.2 2.7 3.0 0.0 1.8 
9 2.7 3.0 2.9 1.5 2.3 2.8 1.8 1.8 0.0 
correlation [[ 1.         -0.00643912]
 [-0.00643912  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [2 5 9 ... 7 1 7] [[-31.262747  21.461807]
 [ 34.299206  17.836216]
 [ 35.49696   31.793993]
 ...
 [  2.306952  -9.191505]
 [  7.419031  73.942665]
 [  9.790899 -57.818226]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7,
        7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 8, 7, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7,
        7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8,
        7, 7, 7, 8, 7, 8, 7, 7, 7]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(58) 
Accuracy (ratio) tensor(0.1186)
Accuracy:
 [[ 0  0  0  0  0  0 44  1  0]
 [ 0  0  0  0  0  0 51  1  0]
 [ 0  0  0  0  0  0 55  2  0]
 [ 0  0  0  0  0  0 58  2  0]
 [ 0  0  0  0  0  0 58  3  0]
 [ 0  0  0  0  0  0 57  3  0]
 [ 0  0  0  0  0  0 53  4  0]
 [ 0  0  0  0  0  0 47  5  0]
 [ 0  0  0  0  0  0 38  7  0]]
Accuracy:
 [[0.    0.    0.    0.    0.    0.    0.978 0.022 0.   ]
 [0.    0.    0.    0.    0.    0.    0.981 0.019 0.   ]
 [0.    0.    0.    0.    0.    0.    0.965 0.035 0.   ]
 [0.    0.    0.    0.    0.    0.    0.967 0.033 0.   ]
 [0.    0.    0.    0.    0.    0.    0.951 0.049 0.   ]
 [0.    0.    0.    0.    0.    0.    0.95  0.05  0.   ]
 [0.    0.    0.    0.    0.    0.    0.93  0.07  0.   ]
 [0.    0.    0.    0.    0.    0.    0.904 0.096 0.   ]
 [0.    0.    0.    0.    0.    0.    0.844 0.156 0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 8, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.9  0.0 -1.0  0.0 -0.0  0.3  0.0  0.0 -0.0 -0.8  0.1  0.1  0.0 -0.0 -0.1 -0.0 -1.2 -0.0  0.5 -0.0 
 0.2  0.0  0.5  0.0 -0.0 -0.7 -0.0 -0.0  0.0 -0.6  1.3  0.2  0.0 -0.6 -0.5 -0.0 -0.2  0.0 -0.1  0.0 
 1.2  0.0  0.6  0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.4 -0.2  0.0  0.5  0.3 -0.0 -1.0  0.0 -0.4  0.0 
-0.6 -0.0 -0.2 -0.0  0.0  0.0  0.0  0.0 -0.0  0.5  0.0 -0.4  0.0 -1.0  0.0 -0.0  1.2 -0.0  0.1  0.0 
 0.7  0.0  0.6 -0.0  0.0 -0.4  0.0 -0.0 -0.0  0.1 -0.9  0.4 -0.0  0.6  0.6 -0.0 -0.0  0.0 -0.1 -0.0 
-0.6 -0.0  0.4 -0.0  0.0 -1.8 -0.0 -0.0 -0.0  0.0 -0.7 -0.2  0.0  0.2  0.0 -0.0 -0.0  0.0 -0.1  0.0 
-0.1  0.0 -0.3 -0.0 -0.0  1.1 -0.0  0.0 -0.0  0.4  0.8  0.0 -0.0  1.0  0.1 -0.0  0.8  0.0  0.5 -0.0 
 0.4 -0.0  0.4  0.0  0.0  0.3 -0.0 -0.0 -0.0 -0.2 -0.6 -0.1  0.0 -0.7 -0.8 -0.0 -0.2  0.0 -0.4  0.0 
-0.3 -0.0 -0.2  0.0  0.0  0.8 -0.0  0.0 -0.0  0.5 -0.5  0.0 -0.0 -0.0 -0.3 -0.0  0.7  0.0 -0.1  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.8410523
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.9  0.0 -1.0  0.0 -0.0  0.3  0.0  0.0 -0.0 -0.8  0.1  0.1  0.0 -0.0 -0.1 -0.0 -1.2 -0.0  0.5 -0.0 
 0.2  0.0  0.5  0.0 -0.0 -0.7 -0.0 -0.0  0.0 -0.6  1.3  0.2  0.0 -0.6 -0.5 -0.0 -0.2  0.0 -0.1  0.0 
 1.2  0.0  0.6  0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.4 -0.2  0.0  0.5  0.3 -0.0 -1.0  0.0 -0.4  0.0 
-0.6 -0.0 -0.2 -0.0  0.0  0.0  0.0  0.0 -0.0  0.5  0.0 -0.4  0.0 -1.0  0.0 -0.0  1.2 -0.0  0.1  0.0 
 0.7  0.0  0.6 -0.0  0.0 -0.4  0.0 -0.0 -0.0  0.1 -0.9  0.4 -0.0  0.6  0.6 -0.0 -0.0  0.0 -0.1 -0.0 
-0.6 -0.0  0.4 -0.0  0.0 -1.8 -0.0 -0.0 -0.0  0.0 -0.7 -0.2  0.0  0.2  0.0 -0.0 -0.0  0.0 -0.1  0.0 
-0.1  0.0 -0.3 -0.0 -0.0  1.1 -0.0  0.0 -0.0  0.4  0.8  0.0 -0.0  1.0  0.1 -0.0  0.8  0.0  0.5 -0.0 
 0.4 -0.0  0.4  0.0  0.0  0.3 -0.0 -0.0 -0.0 -0.2 -0.6 -0.1  0.0 -0.7 -0.8 -0.0 -0.2  0.0 -0.4  0.0 
-0.3 -0.0 -0.2  0.0  0.0  0.8 -0.0  0.0 -0.0  0.5 -0.5  0.0 -0.0 -0.0 -0.3 -0.0  0.7  0.0 -0.1  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.3470213
pred: tensor([7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 7, 8, 7, 7, 7,
        8, 8, 7, 8, 7, 8, 8, 7, 7, 7, 8, 8, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 8, 8,
        8, 8, 7, 7, 7, 7, 8, 7, 8, 7, 7, 8, 7, 8, 7, 7]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(21) 
Accuracy (ratio) tensor(0.3281)
Accuracy:
 [[21]]
Accuracy:
 [[1.]]
-0.9  0.0 -1.0  0.0 -0.0  0.3  0.0  0.0 -0.0 -0.8  0.1  0.1  0.0 -0.0 -0.1 -0.0 -1.2 -0.0  0.5 -0.0 
 0.2  0.0  0.5  0.0 -0.0 -0.7 -0.0 -0.0  0.0 -0.6  1.3  0.2  0.0 -0.6 -0.5 -0.0 -0.2  0.0 -0.1  0.0 
 1.2  0.0  0.6  0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.4 -0.2  0.0  0.5  0.3 -0.0 -1.0  0.0 -0.4  0.0 
-0.6 -0.0 -0.2 -0.0  0.0  0.0  0.0  0.0 -0.0  0.5  0.0 -0.4  0.0 -1.0  0.0 -0.0  1.2 -0.0  0.1  0.0 
 0.7  0.0  0.6 -0.0  0.0 -0.4  0.0 -0.0 -0.0  0.1 -0.9  0.4 -0.0  0.6  0.6 -0.0 -0.0  0.0 -0.1 -0.0 
-0.6 -0.0  0.4 -0.0  0.0 -1.8 -0.0 -0.0 -0.0  0.0 -0.7 -0.2  0.0  0.2  0.0 -0.0 -0.0  0.0 -0.1  0.0 
-0.1  0.0 -0.3 -0.0 -0.0  1.1 -0.0  0.0 -0.0  0.4  0.8  0.0 -0.0  1.0  0.1 -0.0  0.8  0.0  0.5 -0.0 
 0.4 -0.0  0.4  0.0  0.0  0.3 -0.0 -0.0 -0.0 -0.2 -0.6 -0.1  0.0 -0.7 -0.8 -0.0 -0.2  0.0 -0.4  0.0 
-0.3 -0.0 -0.2  0.0  0.0  0.8 -0.0  0.0 -0.0  0.5 -0.5  0.0 -0.0 -0.0 -0.3 -0.0  0.7  0.0 -0.1  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.8213515
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.9  0.0 -1.0  0.0 -0.0  0.3  0.0  0.0 -0.0 -0.8  0.1  0.1  0.0 -0.0 -0.1 -0.0 -1.2 -0.0  0.5 -0.0 
 0.2  0.0  0.5  0.0 -0.0 -0.7 -0.0 -0.0  0.0 -0.6  1.3  0.2  0.0 -0.6 -0.5 -0.0 -0.2  0.0 -0.1  0.0 
 1.2  0.0  0.6  0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.4 -0.2  0.0  0.5  0.3 -0.0 -1.0  0.0 -0.4  0.0 
-0.6 -0.0 -0.2 -0.0  0.0  0.0  0.0  0.0 -0.0  0.5  0.0 -0.4  0.0 -1.0  0.0 -0.0  1.2 -0.0  0.1  0.0 
 0.7  0.0  0.6 -0.0  0.0 -0.4  0.0 -0.0 -0.0  0.1 -0.9  0.4 -0.0  0.6  0.6 -0.0 -0.0  0.0 -0.1 -0.0 
-0.6 -0.0  0.4 -0.0  0.0 -1.8 -0.0 -0.0 -0.0  0.0 -0.7 -0.2  0.0  0.2  0.0 -0.0 -0.0  0.0 -0.1  0.0 
-0.1  0.0 -0.3 -0.0 -0.0  1.1 -0.0  0.0 -0.0  0.4  0.8  0.0 -0.0  1.0  0.1 -0.0  0.8  0.0  0.5 -0.0 
 0.4 -0.0  0.4  0.0  0.0  0.3 -0.0 -0.0 -0.0 -0.2 -0.6 -0.1  0.0 -0.7 -0.8 -0.0 -0.2  0.0 -0.4  0.0 
-0.3 -0.0 -0.2  0.0  0.0  0.8 -0.0  0.0 -0.0  0.5 -0.5  0.0 -0.0 -0.0 -0.3 -0.0  0.7  0.0 -0.1  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.371344
results (all): {'reconst_0x0_avg': 0.0625, 'reconst_0x0_all': nan, 'cluster_avg': 0.07039657, 'cluster_all': array([ 0.02822629,  0.00769719,  0.03702607, ..., -0.00581365,
        0.27965048, -0.04807001], dtype=float32), 'magnitude_avg': 0.0064391181878335636, 'magnitude_all': -0.0064391181878335636, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11860940605401993, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.328125, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_3', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.0625, 'cluster_avg': 0.07039657, 'magnitude_avg': 0.0064391181878335636, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11860940605401993, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.328125, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4', 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_4
Run Directory:
 ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4
Arguments (after settings):
{'K': 20/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([7, 1, 3, 6, 2, 7, 9, 8, 6, 2, 3, 5, 1, 9, 9, 9, 3, 4, 6, 7, 2, 9, 1, 9,
        5, 7, 9, 5, 5, 1, 1, 7, 9, 3, 8, 4, 4, 1, 1, 4, 7, 1, 7, 3, 7, 9, 2, 6,
        8, 4, 2, 3, 5, 5, 7, 8, 9, 4, 1, 7, 5, 5, 7, 5, 7, 3, 6, 1, 2, 9, 5, 9,
        8, 8, 9, 7, 4, 7, 6, 1, 1, 6, 9, 6, 4, 5, 7, 1, 3, 1, 5, 8, 7, 8, 3, 5,
        7, 4, 8, 7, 9, 9, 9, 7, 9, 3, 6, 5, 1, 7, 6, 4, 6, 1, 9, 2, 7, 3, 5, 1,
        3, 7, 9, 5, 5, 4, 8, 7], device='cuda:0') 
label[0]: tensor(7, device='cuda:0') 
label[1]: tensor(1, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.1277502  0.21015881 0.2322956  ... 0.23127005 0.16904142 0.17860162]
Average of silhouette coef: 0.117724426
---
  0   1   2   3
0 0.0 3.0 2.8 2.0 
1 3.0 0.0 3.0 2.2 
2 2.8 3.0 0.0 2.0 
3 2.0 2.2 2.0 0.0 
correlation [[ 1.         -0.49490574]
 [-0.49490574  1.        ]]
---
[[], [], [], []] [2 2 2 ... 0 1 1] [[ 11.896985  51.36281 ]
 [-33.558876  53.240314]
 [ -8.805911  34.594887]
 ...
 [-37.013718 -39.680523]
 [ 30.757431  -7.639572]
 [ 51.42689   30.581976]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.117724426, 'cluster_all': array([0.1277502 , 0.21015881, 0.2322956 , ..., 0.23127005, 0.16904142,
       0.17860162], dtype=float32), 'magnitude_avg': 0.49490574457910597, 'magnitude_all': -0.49490574457910597, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_4', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.117724426, 'magnitude_avg': 0.49490574457910597, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([7, 7, 9, 1, 5, 9, 3, 2, 2, 8, 3, 1, 3, 4, 8, 7, 2, 2, 1, 4, 3, 4, 6, 6,
        7, 5, 7, 1, 3, 7, 6, 7, 5, 2, 3, 1, 2, 3, 6, 3, 2, 3, 4, 8, 1, 6, 6, 2,
        5, 8, 3, 3, 9, 8, 5, 5, 7, 4, 9, 8, 5, 4, 3, 3, 8, 1, 6, 2, 7, 1, 4, 4,
        3, 3, 5, 3, 5, 5, 7, 1, 7, 9, 6, 3, 3, 9, 2, 2, 6, 4, 3, 9, 6, 3, 7, 3,
        6, 7, 1, 4, 9, 6, 3, 8, 9, 2, 1, 3, 7, 2, 3, 2, 5, 9, 7, 4, 1, 6, 3, 3,
        6, 2, 8, 8, 8, 4, 8, 1], device='cuda:0') 
label[0]: tensor(7, device='cuda:0') 
label[1]: tensor(7, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([2, 9, 2, 3, 4, 1, 7, 1, 9, 2, 8, 8, 9, 2, 3, 7, 5, 1, 5, 9, 6, 7, 8, 9,
        3, 9, 2, 2, 9, 4, 4, 8, 5, 6, 2, 2, 4, 7, 8, 3, 6, 3, 6, 4, 3, 8, 9, 5,
        1, 2, 5, 2, 1, 7, 2, 4, 3, 6, 6, 2, 1, 2, 2, 1, 5, 6, 7, 4, 5, 2, 7, 1,
        1, 3, 7, 3, 5, 1, 2, 8, 4, 2, 4, 5, 1, 6, 2, 2, 1, 6, 1, 8, 2, 5, 4, 8,
        5, 3, 7, 4, 8, 1, 3, 6, 7, 1, 7, 4, 4, 6, 1, 4, 6, 4, 1, 6, 3, 9, 2, 4,
        1, 7, 2, 7, 1, 4, 6, 2])
Accuracy (count): tensor(17) 
Accuracy (ratio) tensor(0.1328)
Accuracy:
 [[ 0  0  0 19  0  0  0  0  0]
 [ 0  0  0 22  1  0  0  0  0]
 [ 0  0  0 12  0  0  0  0  0]
 [ 0  0  0 17  0  0  0  0  0]
 [ 0  0  0 11  0  0  0  0  0]
 [ 0  0  0 14  0  0  0  0  0]
 [ 0  0  0 13  0  0  0  0  0]
 [ 0  0  0  9  1  0  0  0  0]
 [ 0  0  0  9  0  0  0  0  0]]
Accuracy:
 [[0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.957 0.043 0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.9   0.1   0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]]
---
Silhouette values: [0.11937725 0.17080398 0.01155702 ... 0.04584925 0.30752388 0.1989967 ]
Average of silhouette coef: 0.071545325
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.7 3.0 3.0 2.8 3.1 2.9 2.4 2.7 
2 2.7 0.0 2.5 2.8 2.7 2.8 2.9 2.3 3.0 
3 3.0 2.5 0.0 3.6 1.9 3.3 2.7 2.1 2.9 
4 3.0 2.8 3.6 0.0 2.8 2.6 2.5 2.5 1.6 
5 2.8 2.7 1.9 2.8 0.0 2.4 2.7 1.5 2.1 
6 3.1 2.8 3.3 2.6 2.4 0.0 3.7 2.6 3.1 
7 2.9 2.9 2.7 2.5 2.7 3.7 0.0 2.6 1.9 
8 2.4 2.3 2.1 2.5 1.5 2.6 2.6 0.0 1.6 
9 2.7 3.0 2.9 1.6 2.1 3.1 1.9 1.6 0.0 
correlation [[ 1.         -0.05305178]
 [-0.05305178  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 7 9 ... 4 1 6] [[-25.80214    12.930732 ]
 [ 53.224133   -6.8039694]
 [ -3.5092542 -44.790375 ]
 ...
 [-18.258234   21.721695 ]
 [ 52.62834   -30.746885 ]
 [ 11.989915   44.703804 ]]
saved ./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4,
        4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(59) 
Accuracy (ratio) tensor(0.1207)
Accuracy:
 [[ 0  0  0 45  0  0  0  0  0]
 [ 0  0  0 51  1  0  0  0  0]
 [ 0  0  0 56  1  0  0  0  0]
 [ 0  0  0 59  1  0  0  0  0]
 [ 0  0  0 61  0  0  0  0  0]
 [ 0  0  0 59  1  0  0  0  0]
 [ 0  0  0 56  1  0  0  0  0]
 [ 0  0  0 52  0  0  0  0  0]
 [ 0  0  0 44  1  0  0  0  0]]
Accuracy:
 [[0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.981 0.019 0.    0.    0.    0.   ]
 [0.    0.    0.    0.982 0.018 0.    0.    0.    0.   ]
 [0.    0.    0.    0.983 0.017 0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.983 0.017 0.    0.    0.    0.   ]
 [0.    0.    0.    0.982 0.018 0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.978 0.022 0.    0.    0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.1 -0.0  0.4  0.0 -0.0 -1.7  0.1 -0.1 -0.7  0.0 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0 -0.0  0.0 
 0.0 -0.7 -0.0 -0.1  0.0 -0.0 -0.1  1.2 -0.6  0.6 -0.0  0.0  0.0 -0.5 -0.1 -0.1 -0.4  0.0  0.0  0.8 
 0.0  0.4 -0.0 -0.2 -0.0 -0.0  0.4 -0.5 -0.0 -0.1 -0.0 -0.0 -0.0 -1.3  0.1 -0.0  0.0  0.0  0.0  1.3 
 0.0 -0.4 -0.0  0.1 -0.0 -0.0  0.7  0.5 -0.0 -0.2  0.0 -0.0  0.0  0.9  0.4  0.0  0.2  0.0 -0.0 -1.1 
-0.0  0.3  0.0 -0.2  0.0 -0.0  0.3 -1.1 -0.2  0.5  0.0 -0.0 -0.0  0.2 -0.4  0.1  0.1  0.0 -0.0  0.6 
 0.0 -1.4  0.0 -0.0  0.0 -0.0  0.1 -0.2  1.0  0.3 -0.0 -0.0 -0.0  1.1  0.1  0.0  0.0  0.0 -0.0  0.6 
 0.0  0.4 -0.0 -0.1 -0.0 -0.0  0.3 -0.1  0.0 -0.1 -0.0  0.0 -0.0 -1.3 -0.1 -0.1  0.0  0.0 -0.0 -1.3 
-0.0  0.9 -0.0 -0.4  0.0  0.0  0.0  0.1 -0.0  0.2 -0.0  0.0 -0.0  0.4  0.2  0.1 -0.2  0.0 -0.0  0.6 
-0.0  0.8 -0.0 -0.1  0.0  0.0  0.5 -0.1 -0.1 -0.2 -0.0  0.0  0.0  0.5  0.1 -0.1  0.0  0.0 -0.0 -0.9 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.6277379
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.1 -0.0  0.4  0.0 -0.0 -1.7  0.1 -0.1 -0.7  0.0 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0 -0.0  0.0 
 0.0 -0.7 -0.0 -0.1  0.0 -0.0 -0.1  1.2 -0.6  0.6 -0.0  0.0  0.0 -0.5 -0.1 -0.1 -0.4  0.0  0.0  0.8 
 0.0  0.4 -0.0 -0.2 -0.0 -0.0  0.4 -0.5 -0.0 -0.1 -0.0 -0.0 -0.0 -1.3  0.1 -0.0  0.0  0.0  0.0  1.3 
 0.0 -0.4 -0.0  0.1 -0.0 -0.0  0.7  0.5 -0.0 -0.2  0.0 -0.0  0.0  0.9  0.4  0.0  0.2  0.0 -0.0 -1.1 
-0.0  0.3  0.0 -0.2  0.0 -0.0  0.3 -1.1 -0.2  0.5  0.0 -0.0 -0.0  0.2 -0.4  0.1  0.1  0.0 -0.0  0.6 
 0.0 -1.4  0.0 -0.0  0.0 -0.0  0.1 -0.2  1.0  0.3 -0.0 -0.0 -0.0  1.1  0.1  0.0  0.0  0.0 -0.0  0.6 
 0.0  0.4 -0.0 -0.1 -0.0 -0.0  0.3 -0.1  0.0 -0.1 -0.0  0.0 -0.0 -1.3 -0.1 -0.1  0.0  0.0 -0.0 -1.3 
-0.0  0.9 -0.0 -0.4  0.0  0.0  0.0  0.1 -0.0  0.2 -0.0  0.0 -0.0  0.4  0.2  0.1 -0.2  0.0 -0.0  0.6 
-0.0  0.8 -0.0 -0.1  0.0  0.0  0.5 -0.1 -0.1 -0.2 -0.0  0.0  0.0  0.5  0.1 -0.1  0.0  0.0 -0.0 -0.9 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.4798422
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.1 -0.0  0.4  0.0 -0.0 -1.7  0.1 -0.1 -0.7  0.0 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0 -0.0  0.0 
 0.0 -0.7 -0.0 -0.1  0.0 -0.0 -0.1  1.2 -0.6  0.6 -0.0  0.0  0.0 -0.5 -0.1 -0.1 -0.4  0.0  0.0  0.8 
 0.0  0.4 -0.0 -0.2 -0.0 -0.0  0.4 -0.5 -0.0 -0.1 -0.0 -0.0 -0.0 -1.3  0.1 -0.0  0.0  0.0  0.0  1.3 
 0.0 -0.4 -0.0  0.1 -0.0 -0.0  0.7  0.5 -0.0 -0.2  0.0 -0.0  0.0  0.9  0.4  0.0  0.2  0.0 -0.0 -1.1 
-0.0  0.3  0.0 -0.2  0.0 -0.0  0.3 -1.1 -0.2  0.5  0.0 -0.0 -0.0  0.2 -0.4  0.1  0.1  0.0 -0.0  0.6 
 0.0 -1.4  0.0 -0.0  0.0 -0.0  0.1 -0.2  1.0  0.3 -0.0 -0.0 -0.0  1.1  0.1  0.0  0.0  0.0 -0.0  0.6 
 0.0  0.4 -0.0 -0.1 -0.0 -0.0  0.3 -0.1  0.0 -0.1 -0.0  0.0 -0.0 -1.3 -0.1 -0.1  0.0  0.0 -0.0 -1.3 
-0.0  0.9 -0.0 -0.4  0.0  0.0  0.0  0.1 -0.0  0.2 -0.0  0.0 -0.0  0.4  0.2  0.1 -0.2  0.0 -0.0  0.6 
-0.0  0.8 -0.0 -0.1  0.0  0.0  0.5 -0.1 -0.1 -0.2 -0.0  0.0  0.0  0.5  0.1 -0.1  0.0  0.0 -0.0 -0.9 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.7195332
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.1 -0.0  0.4  0.0 -0.0 -1.7  0.1 -0.1 -0.7  0.0 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0 -0.0  0.0 
 0.0 -0.7 -0.0 -0.1  0.0 -0.0 -0.1  1.2 -0.6  0.6 -0.0  0.0  0.0 -0.5 -0.1 -0.1 -0.4  0.0  0.0  0.8 
 0.0  0.4 -0.0 -0.2 -0.0 -0.0  0.4 -0.5 -0.0 -0.1 -0.0 -0.0 -0.0 -1.3  0.1 -0.0  0.0  0.0  0.0  1.3 
 0.0 -0.4 -0.0  0.1 -0.0 -0.0  0.7  0.5 -0.0 -0.2  0.0 -0.0  0.0  0.9  0.4  0.0  0.2  0.0 -0.0 -1.1 
-0.0  0.3  0.0 -0.2  0.0 -0.0  0.3 -1.1 -0.2  0.5  0.0 -0.0 -0.0  0.2 -0.4  0.1  0.1  0.0 -0.0  0.6 
 0.0 -1.4  0.0 -0.0  0.0 -0.0  0.1 -0.2  1.0  0.3 -0.0 -0.0 -0.0  1.1  0.1  0.0  0.0  0.0 -0.0  0.6 
 0.0  0.4 -0.0 -0.1 -0.0 -0.0  0.3 -0.1  0.0 -0.1 -0.0  0.0 -0.0 -1.3 -0.1 -0.1  0.0  0.0 -0.0 -1.3 
-0.0  0.9 -0.0 -0.4  0.0  0.0  0.0  0.1 -0.0  0.2 -0.0  0.0 -0.0  0.4  0.2  0.1 -0.2  0.0 -0.0  0.6 
-0.0  0.8 -0.0 -0.1  0.0  0.0  0.5 -0.1 -0.1 -0.2 -0.0  0.0  0.0  0.5  0.1 -0.1  0.0  0.0 -0.0 -0.9 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.4808795
results (all): {'reconst_0x0_avg': 0.1328125, 'reconst_0x0_all': nan, 'cluster_avg': 0.071545325, 'cluster_all': array([0.11937725, 0.17080398, 0.01155702, ..., 0.04584925, 0.30752388,
       0.1989967 ], dtype=float32), 'magnitude_avg': 0.05305178159097981, 'magnitude_all': -0.05305178159097981, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12065439671278, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_4', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1328125, 'cluster_avg': 0.071545325, 'magnitude_avg': 0.05305178159097981, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12065439671278, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 0, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_0', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0', 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/train'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g1t', 'g2t', 'b6j', 'g6j', 'g9t', 'r1j', 'w8j', 'b9t', 'w5j', 'g8j', 'g1s', 'b4j', 'r1j', 'w8s', 'g3j', 'w1j', 'r1s', 'b5j', 'r9j', 'w5t', 'r4j', 'r2t', 'g6t', 'r9t', 'b2s', 'r3t', 'r5s', 'w6t', 'r9t', 'w9t', 'b8j', 'b5j', 'w7t', 'r3t', 'b4j', 'g8t', 'w4s', 'w4s', 'w4t', 'g5j', 'w6s', 'w6j', 'g9j', 'w2s', 'r7t', 'b7j', 'b7t', 'b6j', 'b3t', 'g8s', 'g3j', 'b2s', 'r9s', 'r5j', 'b6s', 'r5t', 'b8s', 'r3s', 'b5j', 'w7s', 'r6j', 'w9t', 'g9j', 'r8t', 'r2s', 'r6t', 'w7t', 'w7t', 'b3t', 'b5s', 'g8j', 'w2s', 'b4j', 'w4t', 'r4j', 'w1t', 'r1j', 'b3s', 'w4j', 'g9t', 'r4t', 'r3t', 'w3j', 'r2s', 'b9t', 'w1s', 'g3t', 'b7s', 'w3t', 'w1t', 'b9t', 'b2t', 'r3t', 'w3s', 'w5j', 'g1s', 'g6j', 'b9t', 'b2t', 'g1s', 'b6t', 'b3s', 'w4j', 'g7j', 'g6j', 'r6t', 'r5s', 'g5s', 'w6t', 'w4t', 'w2s', 'w9t', 'r3j', 'r4t', 'g3t', 'b5j', 'r8t', 'b4s', 'r2j', 'g3j', 'g4s', 'w7t', 'w6s', 'w8t', 'b6j', 'b1t', 'w7t', 'r9t') 
label[0]: g1t 
label[1]: g2t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.0213036  0.04701459 0.03424068 ... 0.09786794 0.05960179 0.02912469]
Average of silhouette coef: 0.037310094
---
  0   1   2   3
0 0.0 1.6 3.5 1.9 
1 1.6 0.0 2.0 2.4 
2 3.5 2.0 0.0 3.8 
3 1.9 2.4 3.8 0.0 
correlation [[ 1.         -0.12647434]
 [-0.12647434  1.        ]]
---
[[], [], [], []] [2 2 1 ... 3 1 1] [[-49.682392 -25.809084]
 [ 34.435677  -9.162288]
 [ 46.02471   16.416542]
 ...
 [-27.509607 -20.173   ]
 [ 45.694828  28.928722]
 [ 45.996986  46.861847]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.037310094, 'cluster_all': array([0.0213036 , 0.04701459, 0.03424068, ..., 0.09786794, 0.05960179,
       0.02912469], dtype=float32), 'magnitude_avg': 0.12647434446896438, 'magnitude_all': -0.12647434446896438, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_0', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.037310094, 'magnitude_avg': 0.12647434446896438, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g9t', 'b5j', 'w1s', 'b4j', 'g8t', 'w3s', 'w6s', 'r6s', 'r8t', 'r1s', 'g1s', 'w1t', 'w3s', 'w7j', 'r4j', 'r9t', 'r5s', 'b2j', 'r7j', 'w3s', 'b1j', 'b9s', 'b1t', 'g2s', 'b6s', 'b7s', 'w1j', 'b8t', 'g4t', 'b1t', 'b2t', 'b4s', 'b7s', 'w7s', 'b8s', 'r3j', 'g9s', 'r7t', 'g4t', 'b7t', 'r9t', 'b1s', 'r3t', 'w6t', 'r8j', 'g1j', 'g1t', 'w8s', 'g2j', 'g4s', 'r1j', 'r5t', 'b5j', 'g7s', 'w1s', 'w6s', 'g1t', 'w9s', 'r5j', 'g3j', 'b8j', 'b7s', 'r8t', 'b6t', 'w3j', 'g8t', 'w7t', 'w7s', 'r4j', 'g4j', 'r7t', 'r2j', 'g6j', 'w6j', 'r7s', 'b3s', 'b1s', 'b4j', 'b1s', 'w5j', 'b3t', 'g5s', 'w9j', 'g3j', 'r4s', 'b2s', 'r8t', 'g8t', 'r1s', 'g9s', 'w8j', 'g9t', 'b8j', 'w5s', 'r6s', 'r8t', 'w1j', 'b8s', 'b6s', 'b9s', 'b3t', 'b7t', 'r7j', 'r6j', 'b8t', 'b4t', 'b5t', 'w9j', 'g7s', 'w9s', 'w2t', 'w5j', 'g5t', 'r7t', 'g5j', 'r9t', 'w7j', 'b3s', 'b7j', 'b3j', 'r9t', 'w7t', 'r6j', 'g6j', 'g7s', 'g2j', 'r7s', 'w9j') 
label[0]: g9t 
label[1]: b5j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([4, 3, 1, 2, 5, 1, 9, 4, 8, 6, 7, 1, 2, 2, 3, 2, 8, 9, 7, 2, 3, 8, 8, 6,
        4, 7, 2, 3, 3, 3, 2, 2, 6, 1, 1, 1, 9, 7, 2, 5, 7, 3, 1, 4, 9, 6, 3, 1,
        3, 2, 4, 5, 2, 1, 4, 8, 6, 7, 2, 6, 8, 2, 1, 9, 2, 6, 8, 3, 6, 1, 1, 2,
        1, 8, 2, 9, 8, 1, 3, 7, 6, 2, 3, 2, 9, 4, 8, 7, 3, 2, 1, 2, 2, 9, 5, 1,
        1, 6, 9, 9, 2, 1, 1, 4, 4, 4, 8, 2, 2, 3, 2, 9, 1, 1, 1, 7, 3, 4, 5, 8,
        2, 5, 6, 2, 5, 2, 3, 8])
Accuracy (count): tensor(16) 
Accuracy (ratio) tensor(0.1250)
Accuracy:
 [[ 0  0 22  0  0  0  0  0  0]
 [ 0  0 28  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0 11  0  0  0  0  0  0]
 [ 0  0  7  0  0  0  0  0  0]
 [ 0  0 11  0  0  0  0  0  0]
 [ 0  0  9  0  0  0  0  0  0]
 [ 0  0 13  0  0  0  0  0  0]
 [ 0  0 11  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [-0.0709936  -0.13774508  0.09494819 ...  0.14076647  0.1714041
 -0.17012836]
Average of silhouette coef: -0.09276518
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.5 0.6 0.7 0.9 0.9 1.2 1.4 1.3 
2 0.5 0.0 0.4 0.6 0.6 0.6 0.9 1.1 0.9 
3 0.6 0.4 0.0 0.5 0.5 0.6 0.9 1.0 0.9 
4 0.7 0.6 0.5 0.0 0.3 0.4 0.6 0.8 0.7 
5 0.9 0.6 0.5 0.3 0.0 0.2 0.4 0.6 0.5 
6 0.9 0.6 0.6 0.4 0.2 0.0 0.5 0.6 0.5 
7 1.2 0.9 0.9 0.6 0.4 0.5 0.0 0.4 0.2 
8 1.4 1.1 1.0 0.8 0.6 0.6 0.4 0.0 0.4 
9 1.3 0.9 0.9 0.7 0.5 0.5 0.2 0.4 0.0 
correlation [[1.         0.90374473]
 [0.90374473 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [9 5 1 ... 1 1 3] [[ 57.70121     5.6130657]
 [-24.66593   -32.47358  ]
 [-41.062397    3.7484655]
 ...
 [-38.18982    37.386143 ]
 [ 12.144233   49.39619  ]
 [-33.32669    35.424824 ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(57) 
Accuracy (ratio) tensor(0.1166)
Accuracy:
 [[ 0  0 45  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 61  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 45  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.2 -0.2 -0.2 -0.0 -0.2  0.3  0.1 -0.0 -0.0 -0.0  0.4 -0.0  0.0  0.0 -0.3  0.0  0.0 -0.0 -0.0 
 0.0 -0.1 -0.2 -0.1 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.0  0.3 -0.0  0.0 -0.0 -0.2  0.0  0.0  0.0 -0.0 
 0.0 -0.2 -0.1 -0.0 -0.0 -0.0 -0.0  0.1  0.0 -0.0 -0.0  0.1  0.0  0.0 -0.0 -0.3  0.0  0.0  0.0 -0.0 
 0.1 -0.2  0.1 -0.1 -0.0  0.2  0.1  0.2 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.0 -0.1  0.0  0.0  0.0 -0.0 
 0.0  0.0  0.1 -0.0 -0.0  0.3 -0.0  0.2 -0.1 -0.0 -0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.1  0.0 -0.0 -0.1 -0.0  0.4 -0.0  0.1 -0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.0 -0.1  0.1  0.2  0.0  0.7 -0.0  0.3 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1 -0.0  0.1  0.1  0.0  0.8 -0.0  0.3 -0.3 -0.0 -0.0 -0.2  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1  0.0  0.1  0.1  0.0  0.7 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.3702041
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.2 -0.2 -0.2 -0.0 -0.2  0.3  0.1 -0.0 -0.0 -0.0  0.4 -0.0  0.0  0.0 -0.3  0.0  0.0 -0.0 -0.0 
 0.0 -0.1 -0.2 -0.1 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.0  0.3 -0.0  0.0 -0.0 -0.2  0.0  0.0  0.0 -0.0 
 0.0 -0.2 -0.1 -0.0 -0.0 -0.0 -0.0  0.1  0.0 -0.0 -0.0  0.1  0.0  0.0 -0.0 -0.3  0.0  0.0  0.0 -0.0 
 0.1 -0.2  0.1 -0.1 -0.0  0.2  0.1  0.2 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.0 -0.1  0.0  0.0  0.0 -0.0 
 0.0  0.0  0.1 -0.0 -0.0  0.3 -0.0  0.2 -0.1 -0.0 -0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.1  0.0 -0.0 -0.1 -0.0  0.4 -0.0  0.1 -0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.0 -0.1  0.1  0.2  0.0  0.7 -0.0  0.3 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1 -0.0  0.1  0.1  0.0  0.8 -0.0  0.3 -0.3 -0.0 -0.0 -0.2  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1  0.0  0.1  0.1  0.0  0.7 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation./new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
 
true answer: 5 
indices: 5 
distance: 0.4147807
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.2 -0.2 -0.2 -0.0 -0.2  0.3  0.1 -0.0 -0.0 -0.0  0.4 -0.0  0.0  0.0 -0.3  0.0  0.0 -0.0 -0.0 
 0.0 -0.1 -0.2 -0.1 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.0  0.3 -0.0  0.0 -0.0 -0.2  0.0  0.0  0.0 -0.0 
 0.0 -0.2 -0.1 -0.0 -0.0 -0.0 -0.0  0.1  0.0 -0.0 -0.0  0.1  0.0  0.0 -0.0 -0.3  0.0  0.0  0.0 -0.0 
 0.1 -0.2  0.1 -0.1 -0.0  0.2  0.1  0.2 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.0 -0.1  0.0  0.0  0.0 -0.0 
 0.0  0.0  0.1 -0.0 -0.0  0.3 -0.0  0.2 -0.1 -0.0 -0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.1  0.0 -0.0 -0.1 -0.0  0.4 -0.0  0.1 -0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.0 -0.1  0.1  0.2  0.0  0.7 -0.0  0.3 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1 -0.0  0.1  0.1  0.0  0.8 -0.0  0.3 -0.3 -0.0 -0.0 -0.2  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1  0.0  0.1  0.1  0.0  0.7 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 9 
distance: 0.43536624
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.2 -0.2 -0.2 -0.0 -0.2  0.3  0.1 -0.0 -0.0 -0.0  0.4 -0.0  0.0  0.0 -0.3  0.0  0.0 -0.0 -0.0 
 0.0 -0.1 -0.2 -0.1 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.0  0.3 -0.0  0.0 -0.0 -0.2  0.0  0.0  0.0 -0.0 
 0.0 -0.2 -0.1 -0.0 -0.0 -0.0 -0.0  0.1  0.0 -0.0 -0.0  0.1  0.0  0.0 -0.0 -0.3  0.0  0.0  0.0 -0.0 
 0.1 -0.2  0.1 -0.1 -0.0  0.2  0.1  0.2 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.0 -0.1  0.0  0.0  0.0 -0.0 
 0.0  0.0  0.1 -0.0 -0.0  0.3 -0.0  0.2 -0.1 -0.0 -0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.1  0.0 -0.0 -0.1 -0.0  0.4 -0.0  0.1 -0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.0 -0.0  0.0 
-0.0 -0.1  0.1  0.2  0.0  0.7 -0.0  0.3 -0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1 -0.0  0.1  0.1  0.0  0.8 -0.0  0.3 -0.3 -0.0 -0.0 -0.2  0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
-0.1  0.0  0.1  0.1  0.0  0.7 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.1 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.35885757
results (all): {'reconst_0x0_avg': 0.125, 'reconst_0x0_all': nan, 'cluster_avg': -0.09276518, 'cluster_all': array([-0.0709936 , -0.13774508,  0.09494819, ...,  0.14076647,
        0.1714041 , -0.17012836], dtype=float32), 'magnitude_avg': 0.9037447282364841, 'magnitude_all': 0.9037447282364841, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_0', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.125, 'cluster_avg': -0.09276518, 'magnitude_avg': 0.9037447282364841, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g3j', 'g5s', 'w1j', 'b2s', 'b5t', 'r6t', 'w2j', 'r3t', 'r3j', 'b2s', 'b6t', 'w5s', 'w8j', 'w1j', 'w5t', 'g8t', 'r4s', 'r9s', 'r7s', 'g8s', 'b1s', 'w9j', 'r8j', 'w6s', 'g1j', 'g9t', 'g6t', 'r1j', 'r1j', 'b9j', 'r7s', 'w8s', 'b1s', 'g5t', 'w2t', 'r2j', 'g7j', 'w8t', 'b9s', 'b1s', 'b3s', 'g9s', 'w6s', 'g7j', 'w3s', 'g4s', 'r1s', 'w5t', 'w3s', 'w9s', 'b7t', 'r9s', 'b9j', 'w8s', 'g9s', 'b5s', 'g8j', 'w6t', 'w3t', 'w4j', 'g4t', 'w5t', 'g8j', 'r9s', 'b1s', 'r5t', 'w3j', 'w3j', 'r3t', 'r9s', 'g8j', 'r6s', 'b2s', 'b7t', 'w2j', 'w9s', 'b4t', 'b3s', 'w8j', 'w5j', 'w1t', 'w7j', 'g1t', 'b3j', 'g7t', 'g1t', 'w1t', 'g3s', 'b6j', 'b5s', 'g2j', 'b2s', 'r4s', 'g8t', 'g2t', 'r2s', 'w7t', 'b8t', 'b2s', 'b4s', 'g8j', 'b8t', 'r6j', 'b6t', 'b8t', 'r5s', 'r6j', 'r1s', 'r3j', 'r5j', 'b6s', 'r4s', 'g3j', 'w8t', 'w7t', 'r2s', 'b2t', 'b7s', 'r1j', 'g6t', 'b8s', 'g8s', 'g2s', 'r9t', 'w3j', 'w1j', 'g2t', 'w6s') 
label[0]: g3j 
label[1]: g5s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.06301723 0.02321015 0.10185303 ... 0.03184646 0.09494685 0.09276444]
Average of silhouette coef: 0.07679783
---
  0   1   2
0 0.0 2.9 3.4 
1 2.9 0.0 2.7 
2 3.4 2.7 0.0 
correlation [[1.         0.95741727]
 [0.95741727 1.        ]]
---
[[], [], []] [0 1 0 ... 1 0 1] [[ 41.90068    23.316204 ]
 [ 44.763638   -1.7844305]
 [-15.649875   50.7056   ]
 ...
 [ 17.996107   10.917878 ]
 [ 19.674208   -5.2931647]
 [-14.273899  -66.89102  ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.07679783, 'cluster_all': array([0.06301723, 0.02321015, 0.10185303, ..., 0.03184646, 0.09494685,
       0.09276444], dtype=float32), 'magnitude_avg': 0.9574172743080396, 'magnitude_all': 0.9574172743080396, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_0', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.07679783, 'magnitude_avg': 0.9574172743080396, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 1, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_1', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1', 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g1t', 'b8s', 'r2j', 'w3t', 'g9t', 'g8t', 'w2s', 'r3s', 'w6s', 'r3j', 'g2t', 'g8s', 'r3t', 'r6s', 'r1t', 'r4t', 'r7t', 'w4t', 'g3t', 'w6j', 'g9j', 'w4s', 'g3s', 'b3s', 'b4t', 'b7s', 'g6t', 'r6s', 'w3j', 'w5s', 'g9j', 'g7t', 'r5t', 'w7t', 'r6j', 'w3j', 'r7t', 'g2s', 'w3s', 'w9j', 'g9t', 'w5t', 'w5s', 'g6j', 'w2s', 'r7t', 'w8t', 'w2s', 'g3j', 'r8s', 'g3t', 'r3t', 'b2s', 'b9s', 'b6j', 'w3t', 'r1j', 'b8t', 'g6s', 'r8j', 'b9s', 'r7s', 'g3t', 'w8s', 'w5t', 'r1t', 'r1t', 'w8s', 'r8j', 'g7s', 'g3s', 'g9t', 'w1s', 'g4t', 'b4t', 'b6s', 'r1t', 'b4t', 'r3s', 'b3t', 'w8s', 'b6t', 'b9s', 'b9t', 'r4t', 'b2j', 'w1j', 'w4s', 'w7j', 'w2j', 'b4s', 'w1t', 'w6j', 'g4t', 'w5j', 'b1s', 'w9j', 'b5j', 'r1j', 'g7t', 'b7j', 'b1t', 'r3j', 'b2j', 'w2j', 'w2t', 'w6t', 'w5j', 'w1s', 'g7j', 'b4j', 'b8t', 'w3t', 'b7t', 'g6t', 'w1t', 'w2j', 'w2t', 'g9s', 'r7j', 'w3s', 'b3s', 'b3t', 'b4j', 'r9s', 'r2j', 'b7t', 'w8t') 
label[0]: g1t 
label[1]: b8s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.05302229  0.04472667  0.02291172 ...  0.08783615  0.00804429
 -0.11272666]
Average of silhouette coef: 0.00522223
---
  0   1   2   3
0 0.0 1.2 2.8 1.2 
1 1.2 0.0 2.4 1.6 
2 2.8 2.4 0.0 3.6 
3 1.2 1.6 3.6 0.0 
correlation [[ 1.         -0.44646985]
 [-0.44646985  1.        ]]
---
[[], [], [], []] [2 1 3 ... 1 3 0] [[-22.249014  -33.290134 ]
 [ 48.681828    3.3281517]
 [-18.41487   -33.28695  ]
 ...
 [ 50.895187   33.121593 ]
 [-66.850914    7.456256 ]
 [ 24.149855   13.443109 ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.00522223, 'cluster_all': array([-0.05302229,  0.04472667,  0.02291172, ...,  0.08783615,
        0.00804429, -0.11272666], dtype=float32), 'magnitude_avg': 0.4464698540324774, 'magnitude_all': -0.4464698540324774, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_1', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.00522223, 'magnitude_avg': 0.4464698540324774, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g1j', 'r7s', 'w9t', 'r7s', 'g3s', 'w3t', 'w9t', 'w6j', 'g3t', 'g4j', 'g9s', 'r3s', 'w5s', 'b9s', 'b9t', 'b7s', 'w7s', 'g9j', 'r9j', 'r5t', 'r7j', 'w3s', 'r4s', 'b7t', 'r3s', 'w4s', 'b4s', 'g2t', 'g6s', 'b9j', 'b9t', 'r3t', 'b8t', 'b6j', 'b7j', 'b8j', 'w3t', 'w1s', 'g3j', 'b7j', 'r3t', 'b8s', 'b4s', 'b3j', 'b6t', 'g7s', 'b9s', 'w9j', 'b1j', 'r4t', 'g8s', 'w1t', 'b6t', 'b1j', 'g9t', 'b6j', 'w7j', 'w9t', 'w4j', 'w6t', 'g9t', 'w5t', 'b5s', 'g6t', 'w5t', 'g7t', 'g1s', 'b4s', 'b9j', 'g6s', 'b1t', 'g9s', 'g9t', 'w1j', 'g2t', 'b1j', 'r2t', 'w4t', 'r1t', 'w1s', 'b7s', 'w9s', 'b8t', 'b7s', 'w2s', 'r8s', 'r9j', 'r1j', 'r1t', 'w6s', 'g3t', 'g1t', 'g2s', 'w9s', 'w6s', 'r1j', 'b1t', 'g9j', 'g8t', 'w3j', 'r9j', 'b8t', 'g9j', 'g5j', 'w5s', 'g7s', 'r4t', 'g9j', 'b9t', 'w4j', 'g8j', 'w5s', 'b7s', 'b6t', 'w1t', 'b5j', 'r4j', 'r2j', 'r6j', 'b3t', 'b9s', 'w9t', 'r3j', 'g1s', 'w3j', 'r6t', 'g4j', 'w2s') 
label[0]: g1j 
label[1]: r7s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([4, 3, 7, 4, 7, 4, 1, 9, 1, 8, 7, 5, 5, 9, 4, 3, 2, 1, 8, 1, 3, 3, 3, 9,
        7, 2, 4, 4, 5, 1, 2, 7, 7, 2, 1, 7, 9, 2, 7, 1, 7, 5, 2, 3, 8, 6, 2, 1,
        6, 1, 6, 2, 4, 7, 6, 5, 8, 1, 3, 4, 3, 2, 1, 8, 4, 3, 1, 2, 6, 4, 7, 6,
        9, 8, 2, 2, 5, 2, 9, 4, 9, 8, 6, 6, 8, 5, 3, 8, 1, 1, 1, 6, 8, 2, 4, 7,
        1, 4, 1, 2, 1, 8, 5, 7, 1, 1, 3, 6, 2, 9, 9, 8, 8, 6, 5, 5, 3, 8, 7, 9,
        3, 9, 1, 8, 7, 9, 6, 5])
Accuracy (count): tensor(11) 
Accuracy (ratio) tensor(0.0859)
Accuracy:
 [[ 0  0  0  0 21  0  0  0  0]
 [ 0  0  0  0 16  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 11  0  0  0  0]
 [ 0  0  0  0 12  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 12  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
Silhouette values: [ 0.15961336 -0.09228374  0.04779013 ... -0.10641088 -0.11955953
 -0.16112548]
Average of silhouette coef: -0.090186216
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.3 0.5 0.7 0.9 1.0 1.4 1.4 1.8 
2 0.3 0.0 0.5 0.6 0.8 0.8 1.2 1.2 1.6 
3 0.5 0.5 0.0 0.4 0.7 0.7 1.1 1.1 1.5 
4 0.7 0.6 0.4 0.0 0.7 0.8 1.1 1.1 1.4 
5 0.9 0.8 0.7 0.7 0.0 0.4 0.8 0.7 1.0 
6 1.0 0.8 0.7 0.8 0.4 0.0 0.7 0.6 0.9 
7 1.4 1.2 1.1 1.1 0.8 0.7 0.0 0.5 0.7 
8 1.4 1.2 1.1 1.1 0.7 0.6 0.5 0.0 0.7 
9 1.8 1.6 1.5 1.4 1.0 0.9 0.7 0.7 0.0 
correlation [[1.         0.93301017]
 [0.93301017 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [1 7 9 ... 2 4 4] [[-21.376766 -53.52694 ]
 [ -9.887823  47.6685  ]
 [ 55.376602  18.643656]
 ...
 [ 34.299263 -37.93205 ]
 [ 40.147713 -37.044044]
 [  7.407045   5.92219 ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(61) 
Accuracy (ratio) tensor(0.1247)
Accuracy:
 [[ 0  0  0  0 45  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 61  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 45  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.1 -0.0  0.1  0.1 -0.0  0.3  0.2 -0.0 -0.0  0.1  0.0 -0.3 -0.0  0.6 -0.0 -0.0 -0.0  0.0 -0.0 
 0.0  0.2 -0.0  0.2  0.1 -0.0  0.1  0.2  0.0 -0.0  0.1  0.0 -0.2  0.0  0.4  0.0 -0.0 -0.0 -0.0 -0.1 
 0.0  0.1 -0.0  0.1 -0.2 -0.0  0.2  0.0 -0.0  0.0  0.2  0.0 -0.2 -0.0  0.3  0.0 -0.0 -0.2 -0.0  0.1 
-0.0  0.0 -0.0  0.2 -0.2 -0.0  0.2  0.1  0.0  0.0  0.2  0.0  0.1 -0.2  0.3  0.0 -0.0 -0.0 -0.0  0.1 
-0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1  0.2  0.0  0.0  0.1  0.0  0.1  0.0 -0.2  0.0 -0.0 -0.2  0.0 -0.1 
-0.0  0.2 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0  0.0  0.2 -0.0  0.0 /new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
-0.0 -0.3  0.0 -0.0 -0.0 -0.0  0.1 
 0.0 -0.1 -0.0 -0.0 -0.2 -0.0 -0.2 -0.2  0.0  0.0 -0.1 -0.0  0.3  0.1 -0.5  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0 -0.1 -0.0 -0.0 -0.1 -0.7  0.0  0.0 -0.1 -0.0  0.0 
-0.0 -0.1 -0.0  0.0 -0.1  0.0 -0.0 -0.0  0.0 -0.0  0.3 -0.0  0.5  0.0 -1.0  0.0 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 4 
distance: 0.6475285
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 0.0  0.1 -0.0  0.1  0.1 -0.0  0.3  0.2 -0.0 -0.0  0.1  0.0 -0.3 -0.0  0.6 -0.0 -0.0 -0.0  0.0 -0.0 
 0.0  0.2 -0.0  0.2  0.1 -0.0  0.1  0.2  0.0 -0.0  0.1  0.0 -0.2  0.0  0.4  0.0 -0.0 -0.0 -0.0 -0.1 
 0.0  0.1 -0.0  0.1 -0.2 -0.0  0.2  0.0 -0.0  0.0  0.2  0.0 -0.2 -0.0  0.3  0.0 -0.0 -0.2 -0.0  0.1 
-0.0  0.0 -0.0  0.2 -0.2 -0.0  0.2  0.1  0.0  0.0  0.2  0.0  0.1 -0.2  0.3  0.0 -0.0 -0.0 -0.0  0.1 
-0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1  0.2  0.0  0.0  0.1  0.0  0.1  0.0 -0.2  0.0 -0.0 -0.2  0.0 -0.1 
-0.0  0.2 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0  0.0  0.2 -0.0  0.0 -0.0 -0.3  0.0 -0.0 -0.0 -0.0  0.1 
 0.0 -0.1 -0.0 -0.0 -0.2 -0.0 -0.2 -0.2  0.0  0.0 -0.1 -0.0  0.3  0.1 -0.5  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0 -0.1 -0.0 -0.0 -0.1 -0.7  0.0  0.0 -0.1 -0.0  0.0 
-0.0 -0.1 -0.0  0.0 -0.1  0.0 -0.0 -0.0  0.0 -0.0  0.3 -0.0  0.5  0.0 -1.0  0.0 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 0.6798638
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.1 -0.0  0.1  0.1 -0.0  0.3  0.2 -0.0 -0.0  0.1  0.0 -0.3 -0.0  0.6 -0.0 -0.0 -0.0  0.0 -0.0 
 0.0  0.2 -0.0  0.2  0.1 -0.0  0.1  0.2  0.0 -0.0  0.1  0.0 -0.2  0.0  0.4  0.0 -0.0 -0.0 -0.0 -0.1 
 0.0  0.1 -0.0  0.1 -0.2 -0.0  0.2  0.0 -0.0  0.0  0.2  0.0 -0.2 -0.0  0.3  0.0 -0.0 -0.2 -0.0  0.1 
-0.0  0.0 -0.0  0.2 -0.2 -0.0  0.2  0.1  0.0  0.0  0.2  0.0  0.1 -0.2  0.3  0.0 -0.0 -0.0 -0.0  0.1 
-0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1  0.2  0.0  0.0  0.1  0.0  0.1  0.0 -0.2  0.0 -0.0 -0.2  0.0 -0.1 
-0.0  0.2 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0  0.0  0.2 -0.0  0.0 -0.0 -0.3  0.0 -0.0 -0.0 -0.0  0.1 
 0.0 -0.1 -0.0 -0.0 -0.2 -0.0 -0.2 -0.2  0.0  0.0 -0.1 -0.0  0.3  0.1 -0.5  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0 -0.1 -0.0 -0.0 -0.1 -0.7  0.0  0.0 -0.1 -0.0  0.0 
-0.0 -0.1 -0.0  0.0 -0.1  0.0 -0.0 -0.0  0.0 -0.0  0.3 -0.0  0.5  0.0 -1.0  0.0 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.3328805
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.1 -0.0  0.1  0.1 -0.0  0.3  0.2 -0.0 -0.0  0.1  0.0 -0.3 -0.0  0.6 -0.0 -0.0 -0.0  0.0 -0.0 
 0.0  0.2 -0.0  0.2  0.1 -0.0  0.1  0.2  0.0 -0.0  0.1  0.0 -0.2  0.0  0.4  0.0 -0.0 -0.0 -0.0 -0.1 
 0.0  0.1 -0.0  0.1 -0.2 -0.0  0.2  0.0 -0.0  0.0  0.2  0.0 -0.2 -0.0  0.3  0.0 -0.0 -0.2 -0.0  0.1 
-0.0  0.0 -0.0  0.2 -0.2 -0.0  0.2  0.1  0.0  0.0  0.2  0.0  0.1 -0.2  0.3  0.0 -0.0 -0.0 -0.0  0.1 
-0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1  0.2  0.0  0.0  0.1  0.0  0.1  0.0 -0.2  0.0 -0.0 -0.2  0.0 -0.1 
-0.0  0.2 -0.0  0.0 -0.0 -0.0 -0.0 -0.1  0.0  0.0  0.2 -0.0  0.0 -0.0 -0.3  0.0 -0.0 -0.0 -0.0  0.1 
 0.0 -0.1 -0.0 -0.0 -0.2 -0.0 -0.2 -0.2  0.0  0.0 -0.1 -0.0  0.3  0.1 -0.5  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0 -0.1 -0.0 -0.0 -0.1 -0.7  0.0  0.0 -0.1 -0.0  0.0 
-0.0 -0.1 -0.0  0.0 -0.1  0.0 -0.0 -0.0  0.0 -0.0  0.3 -0.0  0.5  0.0 -1.0  0.0 -0.0 -0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.46958748
results (all): {'reconst_0x0_avg': 0.0859375, 'reconst_0x0_all': nan, 'cluster_avg': -0.090186216, 'cluster_all': array([ 0.15961336, -0.09228374,  0.04779013, ..., -0.10641088,
       -0.11955953, -0.16112548], dtype=float32), 'magnitude_avg': 0.9330101682821007, 'magnitude_all': 0.9330101682821007, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 1.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_1', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.0859375, 'cluster_avg': -0.090186216, 'magnitude_avg': 0.9330101682821007, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 1.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g9j', 'b2s', 'w7s', 'r6j', 'g5s', 'b6s', 'b9j', 'g9s', 'w4s', 'g4s', 'b7s', 'r4j', 'b5t', 'b6s', 'g8s', 'b5j', 'g1j', 'b7j', 'b3s', 'g3s', 'g5j', 'b9s', 'w1s', 'w5s', 'w8t', 'g8t', 'g4t', 'b4j', 'w7t', 'b6s', 'g4s', 'r2t', 'r3s', 'g2s', 'b6j', 'g5s', 'b1s', 'w1t', 'g7s', 'b2t', 'b9t', 'r3t', 'r9j', 'b5s', 'w2t', 'w9j', 'g7j', 'b8t', 'b1s', 'b8j', 'b8j', 'r6j', 'b1s', 'g6s', 'g1s', 'r2j', 'b6t', 'r7t', 'r6j', 'r6s', 'r9s', 'r7s', 'w7j', 'r3s', 'b2s', 'g6s', 'g9j', 'g6j', 'w6s', 'g3j', 'b1j', 'g1s', 'w7t', 'b1s', 'g4t', 'b5s', 'r5s', 'r6t', 'g4j', 'r4t', 'b9s', 'b5t', 'g7s', 'w1s', 'w1t', 'g9j', 'r5t', 'b7j', 'b2t', 'w6t', 'r1j', 'r4j', 'r9t', 'g1s', 'b6t', 'r1j', 'g9t', 'b6j', 'w4s', 'g9t', 'w4j', 'g6j', 'g6j', 'r7s', 'b6j', 'w2t', 'r9t', 'b9t', 'b4j', 'g5t', 'r7j', 'g4s', 'b1s', 'g1t', 'g1s', 'g6s', 'r4t', 'r6s', 'b5t', 'b1s', 'b1j', 'w2t', 'b5t', 'w5j', 'g4j', 'w3t', 'r1s', 'w3t') 
label[0]: g9j 
label[1]: b2s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.19489893 0.02081922 0.07961365 ... 0.02250279 0.1197018  0.10550894]
Average of silhouette coef: 0.08244529
---
  0   1   2
0 0.0 2.9 3.4 
1 2.9 0.0 3.1 
2 3.4 3.1 0.0 
correlation [[1.         0.88667688]
 [0.88667688 1.        ]]
---
[[], [], []] [0 1 1 ... 1 2 1] [[ 15.437486  -66.68704  ]
 [ 44.577442  -23.392204 ]
 [-45.583057   32.335003 ]
 ...
 [ 51.404903   10.006848 ]
 [  7.624656   -1.8243736]
 [-40.51123     2.8622053]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.08244529, 'cluster_all': array([0.19489893, 0.02081922, 0.07961365, ..., 0.02250279, 0.1197018 ,
       0.10550894], dtype=float32), 'magnitude_avg': 0.8866768806922745, 'magnitude_all': 0.8866768806922745, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_1', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.08244529, 'magnitude_avg': 0.8866768806922745, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 2, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_2', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2', 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r4j', 'b7t', 'r8t', 'g3s', 'b7j', 'g3j', 'w6s', 'r9s', 'b1s', 'r1s', 'b9s', 'r1j', 'b6s', 'b9t', 'r7t', 'r3s', 'g6s', 'g9j', 'w4t', 'r7t', 'w8j', 'r7t', 'r2j', 'w4s', 'g5j', 'b7s', 'b5j', 'w6s', 'g5j', 'w2j', 'r2j', 'w7j', 'b8s', 'g1t', 'w3s', 'r5t', 'b5j', 'g8t', 'r1s', 'w2t', 'w5j', 'g8t', 'g1j', 'w8t', 'g6s', 'w9s', 'r2t', 'g7t', 'g7t', 'g9j', 'w7s', 'w9s', 'w6s', 'r3t', 'b2t', 'w2j', 'w3t', 'g5t', 'r7t', 'b1t', 'g1t', 'r9j', 'w5s', 'b8s', 'g2j', 'b6s', 'w4j', 'w9j', 'r4s', 'w6s', 'r7s', 'g2s', 'w2j', 'w1j', 'b9j', 'g8t', 'r5t', 'g9j', 'r9t', 'w3j', 'r6s', 'w8t', 'r2s', 'g2s', 'w6s', 'b6s', 'g8t', 'w6s', 'r3s', 'g1s', 'r7j', 'r9t', 'r2j', 'w6t', 'w1j', 'b7t', 'g5j', 'r8t', 'r1t', 'w4j', 'w7t', 'w7t', 'b1s', 'w4s', 'w2s', 'r1s', 'r3s', 'r3s', 'r8s', 'r8j', 'r3s', 'r4s', 'w5s', 'w3t', 'g4j', 'r9s', 'b1t', 'r9s', 'g2t', 'g8t', 'g6t', 'w5t', 'r7t', 'g7s', 'b9s', 'g5t', 'r1j', 'g1j') 
label[0]: r4j 
label[1]: b7t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.06160979  0.13010968  0.0700217  ...  0.00757991 -0.07970091
  0.07948307]
Average of silhouette coef: 0.028178364
---
  0   1   2   3
0 0.0 2.0 1.8 3.0 
1 2.0 0.0 2.3 3.7 
2 1.8 2.3 0.0 1.5 
3 3.0 3.7 1.5 0.0 
correlation [[1.        0.5703413]
 [0.5703413 1.       ]]
---
[[], [], [], []] [3 1 3 ... 2 0 1] [[-35.52186    28.321695 ]
 [-11.6465225 -54.684566 ]
 [  7.036205  -39.313087 ]
 ...
 [-24.311241   -2.4424016]
 [ 46.49583     9.784388 ]
 [ -1.3591855 -16.103922 ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.028178364, 'cluster_all': array([ 0.06160979,  0.13010968,  0.0700217 , ...,  0.00757991,
       -0.07970091,  0.07948307], dtype=float32), 'magnitude_avg': 0.5703413000750351, 'magnitude_all': 0.5703413000750351, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_2', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.028178364, 'magnitude_avg': 0.5703413000750351, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('b1s', 'g7s', 'r3j', 'b4j', 'g4s', 'r3s', 'r8t', 'g8j', 'r2t', 'w2t', 'w6s', 'w4s', 'b3s', 'r4t', 'w5j', 'w5s', 'g1j', 'w6s', 'w5j', 'b3s', 'b5t', 'g1t', 'r6s', 'r9s', 'r3j', 'r8j', 'g4t', 'b3t', 'w9j', 'b3t', 'b1t', 'r4t', 'r7s', 'w7t', 'g8t', 'r2s', 'w3s', 'b8t', 'w8s', 'g9j', 'b5t', 'r9t', 'b3t', 'g8j', 'g5j', 'r3j', 'r7s', 'w6t', 'g1j', 'r1t', 'b1j', 'g9s', 'w4t', 'b9s', 'g4j', 'r7t', 'g7t', 'w9t', 'w8s', 'g1t', 'g3t', 'g4s', 'b7j', 'g4j', 'g7t', 'r1j', 'w6j', 'r2j', 'g7j', 'w1s', 'w9t', 'b1t', 'r4t', 'w5j', 'w3t', 'g2s', 'b8s', 'r6s', 'b5s', 'g7t', 'r1s', 'b7t', 'w4t', 'w4s', 'r5s', 'w6t', 'b7s', 'r7s', 'b4s', 'w4t', 'w3t', 'r1j', 'b8j', 'r3j', 'g7s', 'b6j', 'b8t', 'r1t', 'g6s', 'b3j', 'b6t', 'g1j', 'r7j', 'w2t', 'r3j', 'w9s', 'b7s', 'r4t', 'b4j', 'r4t', 'b3j', 'g6t', 'r2s', 'b7s', 'b1s', 'r5j', 'r1j', 'g2t', 'g3t', 'w9t', 'r2t', 'b6s', 'b1t', 'w1t', 'g5s', 'g4t', 'g6s', 'w8j') 
label[0]: b1s 
label[1]: g7s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([6, 7, 8, 5, 5, 7, 4, 3, 1, 3, 5, 1, 5, 1, 1, 8, 9, 9, 6, 1, 3, 8, 1, 6,
        3, 8, 9, 6, 2, 1, 3, 7, 7, 1, 7, 1, 8, 2, 2, 3, 3, 4, 5, 9, 3, 4, 5, 9,
        8, 6, 7, 8, 2, 9, 4, 9, 7, 2, 8, 5, 6, 5, 6, 7, 3, 9, 2, 2, 3, 6, 6, 6,
        4, 8, 6, 1, 1, 3, 5, 4, 6, 2, 8, 7, 6, 1, 9, 7, 8, 6, 7, 4, 4, 7, 1, 9,
        4, 9, 5, 4, 2, 7, 8, 8, 8, 5, 3, 5, 6, 6, 7, 9, 4, 6, 3, 6, 2, 2, 6, 1,
        7, 8, 5, 5, 2, 3, 3, 2])
Accuracy (count): tensor(12) 
Accuracy (ratio) tensor(0.0938)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 14]
 [ 0  0  0  0  0  0  0  0 13]
 [ 0  0  0  0  0  0  0  0 15]
 [ 0  0  0  0  0  0  0  0 11]
 [ 0  0  0  0  0  0  0  0 14]
 [ 0  0  0  0  0  0  0  0 19]
 [ 0  0  0  0  0  0  0  0 15]
 [ 0  0  0  0  0  0  0  0 15]
 [ 0  0  0  0  0  0  0  0 12]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
---
Silhouette values: [ 0.15556538 -0.12596384 -0.18768851 ...  0.11423182 -0.17620988
 -0.17138097]
Average of silhouette coef: -0.09870547
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.3 0.5 0.7 0.8 1.0 1.3 1.2 1.8 
2 0.3 0.0 0.5 0.6 0.8 1.0 1.2 1.1 1.8 
3 0.5 0.5 0.0 0.4 0.5 0.8 1.0 1.0 1.6 
4 0.7 0.6 0.4 0.0 0.4 0.5 0.7 0.7 1.3 
5 0.8 0.8 0.5 0.4 0.0 0.6 0.7 0.7 1.2 
6 1.0 1.0 0.8 0.5 0.6 0.0 0.4 0.6 1.0 
7 1.3 1.2 1.0 0.7 0.7 0.4 0.0 0.5 0.7 
8 1.2 1.1 1.0 0.7 0.7 0.6 0.5 0.0 0.7 
9 1.8 1.8 1.6 1.3 1.2 1.0 0.7 0.7 0.0 
correlation [[1.         0.91786315]
 [0.91786315 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [1 7 3 ... 1 7 5] [[  7.629543   68.387886 ]
 [-50.275368  -21.935299 ]
 [-14.118772   53.024673 ]
 ...
 [ 15.826097   31.844652 ]
 [ 18.889671  -33.188465 ]
 [-56.235363    4.5492315]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 45]
 [ 0  0  0  0  0  0  0  0 52]
 [ 0  0  0  0  0  0  0  0 57]
 [ 0  0  0  0  0  0  0  0 60]
 [ 0  0  0  0  0  0  0  0 61]
 [ 0  0  0  0  0  0  0  0 60]
 [ 0  0  0  0  0  0  0  0 57]
 [ 0  0  0  0  0  0  0  0 52]
 [ 0  0  0  0  0  0  0  0 45]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.1 -0.0 -0.1  0.0 -0.1  0.4  0.1  0.0  0.1  0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.2 -0.2 -0.0  0.1  0.0 -0.2  0.2 -0.0  0.1  0.1 -0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.1 -0.1  0.0  0.1  0.0  0.1  0.1  0.0  0.0 -0.3 -0.0  0.1 -0.0 -0.0  0.0 -0.0 -0.0  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.1 -0.2 -0.1 -0.1 -0.1 -0.0  0.2 -0.0 -0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.0  0.0  0.1 -0.1  0.0  0.0 -0.3  0.0  0.1 -0.3 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.1  0.0 
 0.0 -0.0 -0.2  0.0 -0.1  0.0  0.0  0.2 -0.5 -0.1 -0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.1 -0.0 -0.1 -0.1  0.0  0.1 -0.9 -0.1 -0.0 -0.2 -0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.8 -0.0 -0.0  0.1 -0.0  0.2  0.0  0.0  0.0  0.0  0.2 -0.0 
 0.0 -0.0 -0.1  0.0 -0.0 -0.1  0.0  0.0 -1.5 -0.1 -0.1  0.1 -0.0  0.0  0.0  0.0  0.0  0.0 -0.1 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 4 
distance: 0.4254577
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.1 -0.0 -0.1  0.0 -0.1  0.4  0.1  0.0  0.1  0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.2 -0.2 -0.0  0.1  0.0 -0.2  0.2 -0.0  0.1  0.1 -0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.1 -0.1  0.0  0.1  0.0  0.1  0.1  0.0  0.0 -0.3 -0.0  0.1 -0.0 -0.0  0.0 -0.0 -0.0  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.1 -0.2 -0.1 -0.1 -0.1 -0.0  0.2 -0.0 -0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.0  0.0  0.1 -0.1  0.0  0.0 -0.3  0.0  0.1 -0.3 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.1  0.0 
 0.0 -0.0 -0.2  0.0 -0.1  0.0  0.0  0.2 -0.5 -0.1 -0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.1 -0.0 -0.1 -0.1  0.0  0.1 -0.9 -0.1 -0.0 -0.2 -0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.8 -0.0 -0.0  0.1 -0.0  0.2  0.0  0.0  0.0  0.0  0.2 -0.0 
 0.0 -0.0 -0.1  0.0 -0.0 -0.1  0.0  0.0 -1.5 -0.1 -0.1  0.1 -0.0  0.0  0.0  0.0  0.0  0.0 -0.1 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 2 
distance: 0.5881235
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.1 -0.0 -0.1  0.0 -0.1  0.4  0.1  0.0  0.1  0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.2 -0.2 -0.0  0.1  0.0 -0.2  0.2 -0.0  0.1  0.1 -0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.1 -0.1  0.0  0.1  0.0  0.1  0.1  0.0  0.0 -0.3 -0.0  0.1 -0.0 -0.0  0.0 -0.0 -0.0  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.1 -0.2 -0.1 -0.1 -0.1 -0.0  0.2 -0.0 -0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.0  0.0  0.1 -0.1  0.0  0.0 -0.3  0.0  0.1 -0.3 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.1  0.0 
 0.0 -0.0 -0.2  0.0 -0.1  0.0  0.0  0.2 -0.5 -0.1 -0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.1 -0.0 -0.1 -0.1  0.0  0.1 -0.9 -0.1 -0.0 -0.2 -0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.8 -0.0 -0.0  0.1 -0.0  0.2  0.0  0.0  0.0  0.0  0.2 -0.0 
 0.0 -0.0 -0.1  0.0 -0.0 -0.1  0.0  0.0 -1.5 -0.1 -0.1  0.1 -0.0  0.0  0.0  0.0  0.0  0.0 -0.1 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.30511737
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.0 -0.1 -0.0 -0.1  0.0 -0.1  0.4  0.1  0.0  0.1  0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.2 -0.2 -0.0  0.1  0.0 -0.2  0.2 -0.0  0.1  0.1 -0.0  0.2 -0.0 -0.0  0.0 -0.0  0.1 -0.0 
 0.0 -0.0  0.1 -0.1  0.0  0.1  0.0  0.1  0.1  0.0  0.0 -0.3 -0.0  0.1 -0.0 -0.0  0.0 -0.0 -0.0  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.1 -0.2 -0.1 -0.1 -0.1 -0.0  0.2 -0.0 -0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.0  0.0  0.1 -0.1  0.0  0.0 -0.3  0.0  0.1 -0.3 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.1  0.0 
 0.0 -0.0 -0.2  0.0 -0.1  0.0  0.0  0.2 -0.5 -0.1 -0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.1  0.0 
 0.0 -0.0 -0.1 -0.0 -0.1 -0.1  0.0  0.1 -0.9 -0.1 -0.0 -0.2 -0.0 -0.0  0.0  0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.8 -0.0 -0.0  0.1 -0.0  0.2  0.0  0.0  0.0  0.0  0.2 -0.0 
 0.0 -0.0 -0.1  0.0 -0.0 -0.1  0.0  0.0 -1.5 -0.1 -0.1  0.1 -0.0  0.0  0.0  0.0  0.0  0.0 -0.1 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.49424425
results (all): {'reconst_0x0_avg': 0.09375, 'reconst_0x0_all': nan, 'cluster_avg': -0.09870547, 'cluster_all': array([ 0.15556538, -0.12596384, -0.18768851, ...,  0.11423182,
       -0.17620988, -0.17138097], dtype=float32), 'magnitude_avg': 0.9178631492336344, 'magnitude_all': 0.9178631492336344, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_2', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.09375, 'cluster_avg': -0.09870547, 'magnitude_avg': 0.9178631492336344, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r1t', 'r9j', 'g9t', 'r5j', 'w9s', 'b1t', 'w5s', 'w1t', 'r1j', 'w9t', 'w4j', 'g1t', 'g6s', 'g8j', 'g1s', 'b5j', 'w3t', 'g9t', 'g7j', 'r1j', 'g9s', 'r9t', 'r1t', 'w7j', 'b1s', 'g8j', 'w6s', 'g7s', 'w8j', 'w3s', 'r7s', 'r6t', 'g2t', 'r3j', 'w7s', 'r1t', 'r8s', 'b5t', 'g2s', 'r6j', 'g9t', 'g5t', 'r8s', 'w4t', 'g7j', 'g7s', 'r2t', 'b3j', 'r2t', 'g2j', 'g3j', 'g6j', 'r1j', 'g3t', 'w2j', 'g4t', 'b4t', 'r2j', 'r3j', 'r5j', 'b3t', 'w1j', 'r8s', 'w1s', 'r2t', 'w7j', 'r3t', 'r6j', 'g1t', 'w6t', 'r7s', 'r4s', 'r3t', 'w6j', 'b7s', 'b4t', 'g7t', 'r6j', 'g3t', 'r9j', 'g9j', 'r7t', 'r6s', 'g6t', 'r5s', 'r7j', 'b4s', 'b2s', 'g7s', 'r2s', 'r9t', 'w5s', 'r5j', 'g3j', 'g5s', 'b4t', 'r9j', 'r1t', 'g5t', 'w2s', 'g1s', 'b3j', 'r7j', 'r7j', 'b6t', 'b8s', 'r5t', 'r9t', 'g8j', 'r3j', 'r3s', 'r2s', 'g3j', 'g5t', 'g5s', 'g4s', 'b7j', 'g3j', 'b8t', 'g3s', 'b2j', 'w9j', 'r4t', 'r1t', 'g9s', 'w7j', 'b4t', 'b7j') 
label[0]: r1t 
label[1]: r9j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.00136078  0.19939275  0.24567416 ... -0.07503342  0.16874193
  0.13963608]
Average of silhouette coef: 0.080084115
---
  0   1   2
0 0.0 2.7 3.5 
1 2.7 0.0 2.6 
2 3.5 2.6 0.0 
correlation [[1.         0.99845987]
 [0.99845987 1.        ]]
---
[[], [], []] [2 0 2 ... 1 0 0] [[ 28.576824  -27.28448  ]
 [-59.432697   -7.9985633]
 [ 19.100529   60.757454 ]
 ...
 [ 42.208622  -40.54018  ]
 [-51.49646    -4.6917105]
 [-13.983155   -2.1611888]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.080084115, 'cluster_all': array([-0.00136078,  0.19939275,  0.24567416, ..., -0.07503342,
        0.16874193,  0.13963608], dtype=float32), 'magnitude_avg': 0.9984598652466594, 'magnitude_all': 0.9984598652466594, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_2', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.080084115, 'magnitude_avg': 0.9984598652466594, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 3, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_3', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3', 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('w1s', 'b3t', 'g6s', 'r6t', 'g4s', 'w5j', 'g5s', 'r6s', 'r4j', 'g8t', 'r7t', 'w6s', 'w5s', 'g9s', 'b7s', 'w9j', 'r5s', 'r1s', 'b5j', 'r8t', 'r5t', 'g9t', 'g6s', 'g5j', 'w1t', 'r7t', 'w1t', 'w4s', 'w7j', 'r5j', 'r3t', 'g2j', 'g9j', 'r2s', 'g3s', 'g3j', 'r1t', 'g8j', 'b5s', 'r2t', 'g9t', 'r5t', 'w1t', 'w7j', 'r3j', 'w3j', 'r7s', 'g3s', 'w3s', 'r4t', 'g2t', 'r2s', 'b1s', 'b5j', 'w1j', 'g8t', 'w9j', 'w3s', 'g3t', 'r5j', 'w1s', 'g8t', 'b8s', 'r9s', 'w8t', 'g7j', 'b7j', 'g1s', 'w5j', 'r4t', 'g3j', 'b6t', 'w1j', 'g1j', 'r6j', 'g5s', 'g8j', 'b5s', 'r1j', 'w5j', 'g2t', 'b1j', 'r5s', 'g6j', 'w1s', 'g4s', 'r5j', 'r3t', 'g1j', 'r2t', 'g3t', 'r7j', 'b7t', 'w1j', 'g5t', 'b9s', 'g2j', 'w7t', 'w2j', 'r5t', 'r3t', 'w4s', 'g9j', 'r1t', 'r8t', 'b1s', 'b4s', 'w6j', 'r2t', 'w9s', 'r9t', 'w9s', 'b5t', 'g7j', 'w3s', 'g7s', 'g8t', 'g7j', 'w4j', 'w4j', 'r5t', 'b6s', 'b7j', 'b9t', 'b9j', 'g2s', 'r6t', 'g4j') 
label[0]: w1s 
label[1]: b3t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.06485029  0.05597881  0.05806842 ... -0.04862753 -0.07170565
  0.03586655]
Average of silhouette coef: 0.026798684
---
  0   1   2   3
0 0.0 2.0 1.7 3.2 
1 2.0 0.0 2.7 4.0 
2 1.7 2.7 0.0 1.6 
3 3.2 4.0 1.6 0.0 
correlation [[1.         0.49857521]
 [0.49857521 1.        ]]
---
[[], [], [], []] [0 1 2 ... 0 0 2] [[-34.174015   -3.612157 ]
 [-30.310873  -21.627651 ]
 [ 54.681988   -2.805525 ]
 ...
 [  3.5536273  49.000824 ]
 [ 14.164442   32.04201  ]
 [-24.133896  -49.70555  ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.026798684, 'cluster_all': array([-0.06485029,  0.05597881,  0.05806842, ..., -0.04862753,
       -0.07170565,  0.03586655], dtype=float32), 'magnitude_avg': 0.4985752085543105, 'magnitude_all': 0.4985752085543105, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_3', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.026798684, 'magnitude_avg': 0.4985752085543105, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g3j', 'r9j', 'r7s', 'w2s', 'b8s', 'b3s', 'g3j', 'g7s', 'r2s', 'b2j', 'g6t', 'r7s', 'r8s', 'g1s', 'w8j', 'r6t', 'w8j', 'g8s', 'r7s', 'b4t', 'g5j', 'r4s', 'b7s', 'r9t', 'b9t', 'b1t', 'b1s', 'g5j', 'b4s', 'b1j', 'w1s', 'w9j', 'w6s', 'r2j', 'r1j', 'g4t', 'r2j', 'b2t', 'r8j', 'b7t', 'w4t', 'r5j', 'g8s', 'b3s', 'r5t', 'r5t', 'w7t', 'g4j', 'b1t', 'w1t', 'b3j', 'b8t', 'r5j', 'w7t', 'w6t', 'b7j', 'b6j', 'r2j', 'r5j', 'b1t', 'r9t', 'r5j', 'b8s', 'b3s', 'w6t', 'r5j', 'r3t', 'b6t', 'w8s', 'b6j', 'r5t', 'g9s', 'b2s', 'b9t', 'w6s', 'w8j', 'g9t', 'w1j', 'r9s', 'g2s', 'r8s', 'b6j', 'r4s', 'g8t', 'g9s', 'b3j', 'b4s', 'w2t', 'g6s', 'w1t', 'g8t', 'r3s', 'w6s', 'b3j', 'g9j', 'r3t', 'w8j', 'r4j', 'r6t', 'b1t', 'w2t', 'g4j', 'w7t', 'g7j', 'w9t', 'g1t', 'g4t', 'w3s', 'r4j', 'g8t', 'r7t', 'r6j', 'r2j', 'g1t', 'g3s', 'g7t', 'b8t', 'b2j', 'w2t', 'w7j', 'r9j', 'w1t', 'w1j', 'b5s', 'r9s', 'g1t', 'w2t', 'g9s') 
label[0]: g3j 
label[1]: r9j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([5, 5, 4, 2, 7, 6, 1, 7, 8, 7, 6, 9, 2, 5, 2, 6, 8, 5, 4, 3, 5, 2, 2, 9,
        1, 8, 8, 7, 8, 2, 9, 4, 1, 1, 8, 7, 1, 4, 1, 8, 4, 7, 6, 5, 8, 8, 3, 2,
        1, 2, 7, 5, 1, 7, 9, 1, 7, 5, 7, 4, 3, 8, 7, 3, 6, 6, 7, 6, 2, 9, 3, 6,
        1, 1, 6, 9, 3, 7, 9, 3, 5, 2, 8, 8, 2, 2, 2, 1, 4, 5, 7, 2, 4, 9, 8, 4,
        6, 1, 5, 1, 5, 8, 2, 6, 2, 9, 5, 6, 2, 1, 6, 2, 5, 1, 1, 2, 1, 8, 1, 9,
        6, 7, 6, 1, 4, 6, 7, 8])
Accuracy (count): tensor(16) 
Accuracy (ratio) tensor(0.1250)
Accuracy:
 [[ 0  0  0  0  0  0  0 20  0]
 [ 0  0  0  0  0  0  0 19  0]
 [ 0  0  0  0  0  0  0  7  0]
 [ 0  0  0  0  0  0  0 10  0]
 [ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 10  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
Silhouette values: [-0.17518649 -0.07349887 -0.15293738 ... -0.20444626 -0.16580708
 -0.13474534]
Average of silhouette coef: -0.10263885
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.3 0.4 0.3 0.6 0.6 0.6 0.6 0.7 
2 0.3 0.0 0.3 0.3 0.5 0.5 0.6 0.5 0.6 
3 0.4 0.3 0.0 0.3 0.5 0.4 0.5 0.4 0.6 
4 0.3 0.3 0.3 0.0 0.5 0.5 0.5 0.4 0.5 
5 0.6 0.5 0.5 0.5 0.0 0.5 0.6 0.4 0.6 
6 0.6 0.5 0.4 0.5 0.5 0.0 0.3 0.3 0.4 
7 0.6 0.6 0.5 0.5 0.6 0.3 0.0 0.4 0.4 
8 0.6 0.5 0.4 0.4 0.4 0.3 0.4 0.0 0.3 
9 0.7 0.6 0.6 0.5 0.6 0.4 0.4 0.3 0.0 
correlation [[1.         0.68702627]
 [0.68702627 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [3 9 7 ... 7 8 5] [[ -8.223709  15.876392]
 [ 52.34299   42.13283 ]
 [ 12.251047 -46.891945]
 ...
 [ 54.17314   24.176247]
 [ 41.92337   20.594677]
 [-13.561359 -57.108425]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0  0  0  0  0  0  0 45  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 61  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 45  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0 -0.0  0.1  0.1  0.2 -0.2  0.0  0.0  0.1  0.0 -0.0  0.1  0.1 -0.0  0.1  0.1  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.0  0.1  0.0 -0.2  0.0  0.0 -0.1  0.0 -0.1  0.1  0.1 -0.0  0.2  0.2  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.0 -0.0  0.0  0.0  0.1  0.0  0.1  0.0  0.1  0.0  0.1 -0.1  0.1  0.0 -0.0 
-0.0 -0.0  0.0  0.0  0.0  0.1 -0.1  0.0  0.0  0.0  0.0 -0.2 -0.0 -0.0  0.0  0.1  0.0  0.1  0.0 -0.0 
-0.0  0.0  0.0  0.1  0.1 -0.0  0.0  0.0  0.0 -0.2  0.0 -0.0 -0.2  0.1  0.0 -0.1 -0.0  0.1  0.0 -0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0  0.0  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.1  0.0  0.0 -0.2 -0.1  0.0  0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0 -0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.0  0.0 -0.1 -0.0 -0.3  0.0  0.0 
-0.0 -0.0 -0.0 -0.2  0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.0 -0.1 -0.1  0.0  0.0 -0.1 -0.1  0.0 -0.0  0.0 
-0.0 -0.0 -0.0 -0.3  0.0 -0.1  0.0 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.2 -0.0  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.3244916
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0 -0.0  0.1  0.1  0.2 -0.2  0.0  0.0  0.1  0.0 -0.0  0.1  0.1 -0.0  0.1  0.1  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.0  0.1  0.0 -0.2  0.0  0.0 -0.1  0.0 -0.1  0.1  0.1 -0.0  0.2  0.2  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.0 -0.0  0.0  0.0  0.1  0.0  0.1  0.0  0.1  0.0  0.1 -0.1  0.1  0.0 -0.0 
-0.0 -0.0  0.0  0.0  0.0  0.1 -0.1  0.0  0.0  0.0  0.0 -0.2 -0.0 -0.0  0.0  0.1  0.0  0.1  0.0 -0.0 
-0.0  0.0  0.0  0.1  0.1 -0.0  0.0  0.0  0.0 -0.2  0.0 -0.0 -0.2  0.1  0.0 -0.1 -0.0  0.1  0.0 -0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0  0.0  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.1  0.0  0.0 -0.2 -0.1  0.0  0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0 -0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.0  0.0 -0.1 -0.0 -0.3  0.0  0.0 
-0.0 -0.0 -0.0 -0.2  0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.0 -0.1 -0.1  0.0  0.0 -0.1 -0.1  0.0 -0.0  0.0 
-0.0 -0.0 -0.0 -0.3  0.0 -0.1  0.0 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.2 -0.0  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 0.33929485
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 0.0  0.0 -0.0  0.1  0.1  0.2 -0.2  0.0  0.0  0.1  0.0 -0.0  0.1  0.1 -0.0  0.1  0.1  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.0  0.1  0.0 -0.2  0.0  0.0 -0.1  0.0 -0.1  0.1  0.1 -0.0  0.2  0.2  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.0 -0.0  0.0  0.0  0.1  0.0  0.1  0.0  0.1  0.0  0.1 -0.1  0.1  0.0 -0.0 
-0.0 -0.0  0.0  0.0  0.0  0.1 -0.1  0.0  0.0  0.0  0.0 -0.2 -0.0 -0.0  0.0  0.1  0.0  0.1  0.0 -0.0 
-0.0  0.0  0.0  0.1  0.1 -0.0  0.0  0.0  0.0 -0.2  0.0 -0.0 -0.2  0.1  0.0 -0.1 -0.0  0.1  0.0 -0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0  0.0  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.1  0.0  0.0 -0.2 -0.1  0.0  0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0 -0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.0  0.0 -0.1 -0.0 -0.3  0.0  0.0 
-0.0 -0.0 -0.0 -0.2  0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.0 -0.1 -0.1  0.0  0.0 -0.1 -0.1  0.0 -0.0  0.0 
-0.0 -0.0 -0.0 -0.3  0.0 -0.1  0.0 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.2 -0.0  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.34389406
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0 -0.0  0.1  0.1  0.2 -0.2  0.0  0.0  0.1  0.0 -0.0  0.1  0.1 -0.0  0.1  0.1  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.0  0.1  0.0 -0.2  0.0  0.0 -0.1  0.0 -0.1  0.1  0.1 -0.0  0.2  0.2  0.1  0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.0 -0.0  0.0  0.0  0.1  0.0  0.1  0.0  0.1  0.0  0.1 -0.1  0.1  0.0 -0.0 
-0.0 -0.0  0.0  0.0  0.0  0.1 -0.1  0.0  0.0  0.0  0.0 -0.2 -0.0 -0.0  0.0  0.1  0.0  0.1  0.0 -0.0 
-0.0  0.0  0.0  0.1  0.1 -0.0  0.0  0.0  0.0 -0.2  0.0 -0.0 -0.2  0.1  0.0 -0.1 -0.0  0.1  0.0 -0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0  0.0  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.1  0.0  0.0 -0.2 -0.1  0.0  0.0 
-0.0 -0.0 -0.0 -0.1  0.2 -0.0 -0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.0  0.0 -0.1 -0.0 -0.3  0.0  0.0 
-0.0 -0.0 -0.0 -0.2  0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.0 -0.1 -0.1  0.0  0.0 -0.1 -0.1  0.0 -0.0  0.0 
-0.0 -0.0 -0.0 -0.3  0.0 -0.1  0.0 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.1  0.0 -0.0 -0.0 -0.2 -0.0  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.34978098
results (all): {'reconst_0x0_avg': 0.125, 'reconst_0x0_all': nan, 'cluster_avg': -0.10263885, 'cluster_all': array([-0.17518649, -0.07349887, -0.15293738, ..., -0.20444626,
       -0.16580708, -0.13474534], dtype=float32), 'magnitude_avg': 0.6870262695745853, 'magnitude_all': 0.6870262695745853, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 1.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_3', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.125, 'cluster_avg': -0.10263885, 'magnitude_avg': 0.6870262695745853, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 1.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g9s', 'g2s', 'w5t', 'w1j', 'w4t', 'g9t', 'r9s', 'r9j', 'w4s', 'r8s', 'g3j', 'w9s', 'b8s', 'r3s', 'r7s', 'b7j', 'w9s', 'g2s', 'g3t', 'r4j', 'b5s', 'w8s', 'b4j', 'w7j', 'w6j', 'w8s', 'r3j', 'b5t', 'w4t', 'g1t', 'r2j', 'w6s', 'b8t', 'g6j', 'b1j', 'g3t', 'w1j', 'b1t', 'b5j', 'r3t', 'r4j', 'w7t', 'r2j', 'g3t', 'r7s', 'w1t', 'b4s', 'b2t', 'r2s', 'r8t', 'r2s', 'g3t', 'g4t', 'g8j', 'g1t', 'w9s', 'w8s', 'w8j', 'r9t', 'g6j', 'w6s', 'b8s', 'w1t', 'r6j', 'w3t', 'b8t', 'g5j', 'b8s', 'g8s', 'r7t', 'r4t', 'b9t', 'b3j', 'g7s', 'b3t', 'b4t', 'b3t', 'r2j', 'b8t', 'r2j', 'r4j', 'g3s', 'g3s', 'w3j', 'b8j', 'w2j', 'b7j', 'w1j', 'b1s', 'r6j', 'w2t', 'w5j', 'w6s', 'r9t', 'g5j', 'w1j', 'w3t', 'r2t', 'b5j', 'w2t', 'w7s', 'b4s', 'r7j', 'r7s', 'b8s', 'w4t', 'r9t', 'r2j', 'g7s', 'w9s', 'g7j', 'r1s', 'r3s', 'r3s', 'r9s', 'g4j', 'b9j', 'r8t', 'r9t', 'r6t', 'w7t', 'r9j', 'b5t', 'r5j', 'b5s', 'b5t', 'b2s', 'g8j') 
label[0]: g9s 
label[1]: g2s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.2138515  0.03485451 0.07443673 ... 0.08610688 0.16777854 0.04652894]
Average of silhouette coef: 0.08368327
---
  0   1   2
0 0.0 3.0 3.5 
1 3.0 0.0 3.4 
2 3.5 3.4 0.0 
correlation [[1.         0.58798318]
 [0.58798318 1.        ]]
---
[[], [], []] [1 1 2 ... 1 0 1] [[ 48.617607   22.790865 ]
 [  0.8557579  10.113707 ]
 [-13.44233   -36.925575 ]
 ...
 [ 26.549429   32.371494 ]
 [-33.32286   -13.097105 ]
 [ 21.974407   28.286135 ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.08368327, 'cluster_all': array([0.2138515 , 0.03485451, 0.07443673, ..., 0.08610688, 0.16777854,
       0.04652894], dtype=float32), 'magnitude_avg': 0.5879831824249437, 'magnitude_all': 0.5879831824249437, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_3', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.08368327, 'magnitude_avg': 0.5879831824249437, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4', 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('w2t', 'w4t', 'b2t', 'w4t', 'g4j', 'w4t', 'w1t', 'w2j', 'g5j', 'w6j', 'g7t', 'w3j', 'g6s', 'g9j', 'g6j', 'w6t', 'r3s', 'r6j', 'r2s', 'g3t', 'b6t', 'b8s', 'w2t', 'r4t', 'w6t', 'b5s', 'w4j', 'w4t', 'g5s', 'b9j', 'w4t', 'r7s', 'r1j', 'g1j', 'b5j', 'r2t', 'b1t', 'g4t', 'b6j', 'g1t', 'g3t', 'g2j', 'r9s', 'b2j', 'b9j', 'b1t', 'r3j', 'r4j', 'r2j', 'b9s', 'r2j', 'r3j', 'w4j', 'b4j', 'r7j', 'b8j', 'b5s', 'g9t', 'r9s', 'b3j', 'g7j', 'b9t', 'g8j', 'r8t', 'b3s', 'g3j', 'r9s', 'r4s', 'g4s', 'b3t', 'g2j', 'b9s', 'b1s', 'g5j', 'r7s', 'b6t', 'b9t', 'r9j', 'w4j', 'w6t', 'g8j', 'w9j', 'b4j', 'g4s', 'b9j', 'r2j', 'g2j', 'w8s', 'g3t', 'r6j', 'r1j', 'r3j', 'w7j', 'r2s', 'r8s', 'g3j', 'r7j', 'r1t', 'b6j', 'g9j', 'g2j', 'r2t', 'g8t', 'g2s', 'b3t', 'w1t', 'b4s', 'r7s', 'r3s', 'r2s', 'w6s', 'b5j', 'g8s', 'b1t', 'w2s', 'r6j', 'r1s', 'g1t', 'r3j', 'w8t', 'b8s', 'r8s', 'b3s', 'g4t', 'g9s', 'w6j', 'b5j', 'w6t') 
label[0]: w2t 
label[1]: w4t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.05000482 -0.01018209  0.09302574 ... -0.04238621  0.12231138
 -0.10639187]
Average of silhouette coef: 0.058171794
---
  0   1   2   3
0 0.0 2.8 2.9 1.2 
1 2.8 0.0 3.7 2.5 
2 2.9 3.7 0.0 2.6 
3 1.2 2.5 2.6 0.0 
correlation [[ 1.         -0.79132077]
 [-0.79132077  1.        ]]
---
[[], [], [], []] [0 0 1 ... 0 1 0] [[-22.958138  -37.752106 ]
 [-30.523426   38.160973 ]
 [ -3.1793873  -6.162203 ]
 ...
 [ 57.83594     8.680006 ]
 [  1.7651109  11.389638 ]
 [-28.636557  -50.082882 ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.058171794, 'cluster_all': array([-0.05000482, -0.01018209,  0.09302574, ..., -0.04238621,
        0.12231138, -0.10639187], dtype=float32), 'magnitude_avg': 0.7913207653671002, 'magnitude_all': -0.7913207653671002, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_4', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.058171794, 'magnitude_avg': 0.7913207653671002, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g7s', 'r2t', 'r6t', 'w2s', 'g2s', 'b7t', 'b5j', 'b2t', 'w4s', 'w7j', 'w5t', 'w6j', 'g4j', 'b3s', 'b5s', 'w1t', 'g9j', 'r7j', 'r5s', 'r7j', 'b1t', 'b3s', 'w4t', 'w6j', 'g9t', 'r5j', 'r9s', 'g1t', 'r3s', 'g9s', 'g4s', 'w9s', 'w7s', 'g9t', 'b8t', 'w1j', 'b5t', 'b4j', 'r7j', 'b5j', 'g8s', 'b1j', 'r3j', 'b6t', 'w8t', 'w8j', 'r2s', 'w6s', 'r6t', 'w5t', 'g1j', 'w3j', 'w4s', 'b1s', 'b1t', 'w5s', 'b9t', 'w9j', 'g5s', 'b3s', 'g8t', 'w8t', 'r3j', 'r6j', 'w4j', 'g6s', 'g6j', 'b1s', 'r6j', 'b5j', 'w2s', 'b9s', 'w9s', 'r6j', 'g5t', 'r6s', 'w6t', 'g2j', 'g5t', 'w4t', 'w4j', 'g1s', 'r3t', 'g8t', 'r5j', 'b8t', 'w2s', 'w5j', 'b7j', 'b4t', 'g3s', 'w3j', 'w1s', 'b7t', 'g5j', 'r2j', 'b6t', 'w1j', 'w9t', 'w3j', 'w6j', 'r5j', 'w9j', 'b2j', 'g3s', 'g3j', 'r3s', 'w1s', 'w9j', 'g7j', 'g2s', 'w4t', 'r9s', 'r7j', 'b9j', 'r2t', 'r6t', 'g7s', 'b8s', 'r6j', 'r3s', 'b5t', 'w8j', 'w7s', 'g5j', 'b6t', 'r2s', 'r8t') 
label[0]: g7s 
label[1]: r2t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([8, 1, 6, 1, 1, 9, 7, 5, 5, 1, 7, 3, 4, 3, 3, 2, 3, 3, 7, 7, 9, 9, 2, 2,
        3, 7, 1, 6, 2, 9, 1, 8, 1, 3, 1, 1, 1, 3, 7, 8, 1, 2, 2, 1, 9, 8, 3, 4,
        7, 3, 9, 4, 3, 8, 4, 8, 4, 1, 9, 9, 7, 7, 5, 9, 7, 8, 1, 9, 2, 9, 2, 3,
        9, 4, 8, 7, 7, 6, 1, 5, 9, 5, 8, 1, 3, 3, 2, 2, 5, 7, 5, 7, 8, 2, 9, 6,
        4, 2, 9, 2, 7, 1, 4, 6, 6, 6, 9, 8, 9, 7, 1, 1, 2, 8, 5, 7, 1, 1, 4, 7,
        7, 4, 4, 5, 1, 4, 5, 3])
Accuracy (count): tensor(9) 
Accuracy (ratio) tensor(0.0703)
Accuracy:
 [[ 0  0  0  0  0 22  0  0  0]
 [ 0  2  0  0  0 12  0  0  0]
 [ 0  1  0  0  0 14  0  0  0]
 [ 0  0  0  0  0 12  0  0  0]
 [ 0  0  0  0  0 10  0  0  0]
 [ 0  0  0  0  0  7  0  0  0]
 [ 0  0  0  0  0 19  0  0  0]
 [ 0  0  0  0  0 12  0  0  0]
 [ 0  0  0  0  0 17  0  0  0]]
Accuracy:
 [[0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.143 0.    0.    0.    0.857 0.    0.    0.   ]
 [0.    0.067 0.    0.    0.    0.933 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]]
---
Silhouette values: [-0.19447407 -0.11386071 -0.18691131 ... -0.16760874 -0.08936714
 -0.10432553]
Average of silhouette coef: -0.12006646
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.4 0.3 0.6 0.4 0.6 0.5 0.6 0.7 
2 0.4 0.0 0.4 0.6 0.5 0.5 0.5 0.6 0.8 
3 0.3 0.4 0.0 0.5 0.3 0.4 0.5 0.3 0.6 
4 0.6 0.6 0.5 0.0 0.5 0.4 0.6 0.6 0.7 
5 0.4 0.5 0.3 0.5 0.0 0.4 0.3 0.4 0.6 
6 0.6 0.5 0.4 0.4 0.4 0.0 0.5 0.4 0.5 
7 0.5 0.5 0.5 0.6 0.3 0.5 0.0 0.6 0.7 
8 0.6 0.6 0.3 0.6 0.4 0.4 0.6 0.0 0.4 
9 0.7 0.8 0.6 0.7 0.6 0.5 0.7 0.4 0.0 
correlation [[1.         0.57424477]
 [0.57424477 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 2 6 ... 9 9 9] [[  9.06517   -60.345234 ]
 [ -0.2861572  29.692482 ]
 [-18.336308  -36.465668 ]
 ...
 [-48.73417   -13.011477 ]
 [ 59.866035   -7.4186187]
 [ 21.592276  -59.908794 ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(60) 
Accuracy (ratio) tensor(0.1227)
Accuracy:
 [[ 0  1  0  0  0 44  0  0  0]
 [ 0  0  0  0  0 52  0  0  0]
 [ 0  0  0  0  0 57  0  0  0]
 [ 0  2  0  0  0 58  0  0  0]
 [ 0  1  0  0  0 60  0  0  0]
 [ 0  0  0  0  0 60  0  0  0]
 [ 0  1  0  0  0 56  0  0  0]
 [ 0  1  0  0  0 51  0  0  0]
 [ 0  0  0  0  0 45  0  0  0]]
Accuracy:
 [[0.    0.022 0.    0.    0.    0.978 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.033 0.    0.    0.    0.967 0.    0.    0.   ]
 [0.    0.016 0.    0.    0.    0.984 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.018 0.    0.    0.    0.982 0.    0.    0.   ]
 [0.    0.019 0.    0.    0.    0.981 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(1) 
Accuracy (ratio) tensor(0.0156)
Accuracy:
 [[1]]
Accuracy:
 [[1.]]
 0.0  0.0  0.1  0.0 -0.0  0.2  0.1  0.0  0.1 -0.2 -0.1  0.0 -0.0 -0.0  0.0  0.0 -0.0  0.2 -0.2  0.1 
 0.0  0.0  0.0  0.0  0.0  0.3  0.0  0.1 -0.1 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.1  0.1  0.1  0.1 
-0.1  0.0  0.1  0.0  0.0  0.1  0.1 -0.1  0.0 -0.1 -0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.0 -0.0 
 0.0  0.0  0.1  0.0  0.0  0.0  0.1 -0.2 -0.0 -0.2  0.3  0.0 -0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1  0.0 
 0.0 -0.0  0.1  0.0  0.0  0.1  0.0  0.0  0.1  0.1  0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.1 -0.1 -0.0  0.1 
 0.0  0.0 -0.1 -0.0  0.0 -0.0  0.1 -0.0  0.0 -0.0  0.2 -0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.2  0.1 -0.0 
 0.1 -0.0  0.2 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.2 
-0.1  0.0  0.1 -0.0  0.0 -0.0  0.2 -0.1  0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.1 -0.2 
-0.0  0.0 -0.0 -0.0  0.0 -0.0  0.5 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.43220606
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0  0.1  0.0 -0.0  0.2  0.1  0.0  0.1 -0.2 -0.1  0.0 -0.0 -0.0  0.0  0.0 -0.0  0.2 -0.2  0.1 
 0.0  0.0  0.0  0.0  0.0  0.3  0.0  0.1 -0.1 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.1  0.1  0.1  0.1 
-0.1  0.0  0.1  0.0  0.0  0.1  0.1 -0.1  0.0 -0.1 -0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.0 -0.0 
 0.0  0.0  0.1  0.0  0.0  0.0  0.1 -0.2 -0.0 -0.2  0.3  0.0 -0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1  0.0 
 0.0 -0.0  0.1  0.0  0.0  0.1  0.0  0.0  0.1  0.1  0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.1 -0.1 -0.0  0.1 
 0.0  0.0 -0.1 -0.0  0.0 -0.0  0.1 -0.0  0.0 -0.0  0.2 -0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.2  0.1 -0.0 
 0.1 -0.0  0.2 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.2 
-0.1  0.0  0.1 -0.0  0.0 -0.0  0.2 -0.1  0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.1 -0.2 
-0.0  0.0 -0.0 -0.0  0.0 -0.0  0.5 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 3 
distance: 0.54905367
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0  0.1  0.0 -0.0  0.2  0.1  0.0  0.1 -0.2 -0.1  0.0 -0.0 -0.0  0.0  0.0 -0.0  0.2 -0.2  0.1 
 0.0  0.0  0.0  0.0  0.0  0.3  0.0  0.1 -0.1 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.1  0.1  0.1  0.1 
-0.1  0.0  0.1  0.0  0.0  0.1  0.1 -0.1  0.0 -0.1 -0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.0 -0.0 
 0.0  0.0  0.1  0.0  0.0  0.0  0.1 -0.2 -0.0 -0.2  0.3  0.0 -0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1  0.0 
 0.0 -0.0  0.1  0.0  0.0  0.1  0.0  0.0  0.1  0.1  0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.1 -0.1 -0.0  0.1 
 0.0  0.0 -0.1 -0.0  0.0 -0.0  0.1 -0.0  0.0 -0.0  0.2 -0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.2  0.1 -0.0 
 0.1 -0.0  0.2 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.2 
-0.1  0.0  0.1 -0.0  0.0 -0.0  0.2 -0.1  0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.1 -0.2 
-0.0  0.0 -0.0 -0.0  0.0 -0.0  0.5 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.3957076
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(63) 
Accuracy (ratio) tensor(0.9844)
Accuracy:
 [[63]]
Accuracy:
 [[1.]]
 0.0  0.0  0.1  0.0 -0.0  0.2  0.1  0.0  0.1 -0.2 -0.1  0.0 -0.0 -0.0  0.0  0.0 -0.0  0.2 -0.2  0.1 
 0.0  0.0  0.0  0.0  0.0  0.3  0.0  0.1 -0.1 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.1  0.1  0.1  0.1 
-0.1  0.0  0.1  0.0  0.0  0.1  0.1 -0.1  0.0 -0.1 -0.0 -0.0 -0.0 -0.0  0.0  0.0 -0.1  0.0 -0.0 -0.0 
 0.0  0.0  0.1  0.0  0.0  0.0  0.1 -0.2 -0.0 -0.2  0.3  0.0 -0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1  0.0 
 0.0 -0.0  0.1  0.0  0.0  /new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
0.1  0.0  0.0  0.1  0.1  0.1 -0.0 -0.0 -0.0  0.0 -0.0 -0.1 -0.1 -0.0  0.1 
 0.0  0.0 -0.1 -0.0  0.0 -0.0  0.1 -0.0  0.0 -0.0  0.2 -0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.2  0.1 -0.0 
 0.1 -0.0  0.2 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.1  0.0  0.2 
-0.1  0.0  0.1 -0.0  0.0 -0.0  0.2 -0.1  0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.1 -0.2 
-0.0  0.0 -0.0 -0.0  0.0 -0.0  0.5 -0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.2 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 8 
distance: 0.36966193
results (all): {'reconst_0x0_avg': 0.0703125, 'reconst_0x0_all': nan, 'cluster_avg': -0.12006646, 'cluster_all': array([-0.19447407, -0.11386071, -0.18691131, ..., -0.16760874,
       -0.08936714, -0.10432553], dtype=float32), 'magnitude_avg': 0.5742447693092926, 'magnitude_all': 0.5742447693092926, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.015625, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.984375, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_4', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.0703125, 'cluster_avg': -0.12006646, 'magnitude_avg': 0.5742447693092926, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_1+9-8_avg': 0.015625, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.984375}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r5j', 'w4j', 'w7j', 'b6t', 'w8t', 'b5j', 'b2t', 'g5j', 'g5s', 'b3j', 'b9s', 'g9s', 'r4j', 'b5t', 'b8s', 'b7t', 'b5t', 'r8t', 'r6j', 'r2j', 'g9t', 'g1s', 'r2j', 'g4s', 'r7s', 'g3s', 'r4j', 'b8j', 'w8t', 'w6t', 'g8j', 'b8j', 'w6j', 'g4j', 'r8j', 'r9t', 'b2s', 'g7s', 'r3j', 'r9s', 'r4t', 'w6t', 'b9s', 'w3t', 'g8t', 'w2t', 'w4s', 'b2t', 'w5t', 'g7s', 'w3s', 'w3j', 'r1s', 'r6s', 'w7j', 'r4s', 'b1s', 'r2j', 'b1j', 'b4j', 'r2s', 'w2s', 'g8s', 'b3t', 'r9s', 'r2j', 'r6t', 'g1s', 'g6s', 'r4s', 'w6t', 'b7j', 'g1j', 'w3t', 'r2t', 'b7j', 'b5s', 'b4j', 'w2j', 'b1j', 'w6j', 'g1j', 'g2t', 'r4t', 'g1s', 'b1j', 'r8s', 'g1j', 'g6j', 'r2t', 'w8t', 'g7j', 'w6t', 'g1j', 'b3t', 'r4j', 'r8j', 'g1j', 'w3t', 'r3j', 'w8s', 'g1t', 'b6s', 'w6s', 'b9t', 'b1s', 'b4t', 'b6s', 'w2j', 'b5t', 'g4s', 'g3s', 'r9t', 'r4t', 'w5s', 'b8t', 'b5s', 'g6j', 'g4j', 'r9s', 'r9s', 'r2s', 'r6t', 'r1t', 'r2j', 'r3t', 'r6s', 'w9s') 
label[0]: r5j 
label[1]: w4j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.08218647  0.11999387  0.16676359 ... -0.00511458 -0.02900886
  0.08287367]
Average of silhouette coef: 0.070139825
---
  0   1   2
0 0.0 2.7 3.5 
1 2.7 0.0 2.8 
2 3.5 2.8 0.0 
correlation [[1.         0.99917529]
 [0.99917529 1.        ]]
---
[[], [], []] [0 0 0 ... 1 1 2] [[-35.861347   12.740352 ]
 [ -4.38662    36.651936 ]
 [-59.12764     4.2492714]
 ...
 [ 29.994621    9.859046 ]
 [ 51.806404  -27.752287 ]
 [ 18.31693    24.9017   ]]
saved ./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.070139825, 'cluster_all': array([ 0.08218647,  0.11999387,  0.16676359, ..., -0.00511458,
       -0.02900886,  0.08287367], dtype=float32), 'magnitude_avg': 0.9991752941381954, 'magnitude_all': 0.9991752941381954, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_4', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.070139825, 'magnitude_avg': 0.9991752941381954, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 0, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_0', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0', 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 7, 1, 4, 3, 8, 5, 2, 5, 4, 9, 8, 6, 8, 7, 4, 3, 7, 4, 4, 4, 7, 8, 7,
        7, 3, 6, 4, 1, 5, 3, 2, 5, 5, 1, 6, 9, 1, 5, 5, 9, 4, 7, 8, 4, 8, 9, 9,
        9, 9, 5, 7, 4, 3, 4, 4, 6, 3, 2, 9, 7, 2, 7, 7, 8, 9, 6, 4, 7, 5, 3, 5,
        8, 6, 3, 5, 4, 2, 9, 1, 7, 6, 9, 7, 2, 4, 5, 2, 5, 8, 4, 8, 1, 7, 5, 2,
        8, 9, 1, 3, 5, 5, 2, 6, 7, 1, 2, 3, 4, 5, 1, 7, 7, 1, 5, 6, 3, 8, 6, 2,
        5, 2, 2, 9, 9, 1, 6, 9]), ['b2s', 'w7j', 'b1t', 'b4t', 'g3t', 'w8t', 'r5s', 'r2j', 'r5j', 'r4t', 'b9t', 'w8t', 'b6s', 'b8s', 'b7j', 'b4s', 'w3j', 'b7t', 'b4s', 'b4j', 'b4t', 'g7s', 'b8j', 'w7t', 'g7s', 'b3j', 'w6t', 'b4s', 'b1j', 'g5j', 'b3s', 'w2t', 'g5s', 'w5s', 'b1s', 'b6j', 'g9s', 'b1j', 'w5j', 'g5s', 'w9s', 'b4t', 'b7s', 'r8s', 'r4s', 'w8t', 'r9s', 'b9s', 'w9j', 'w9t', 'b5s', 'w7j', 'b4j', 'w3t', 'g4s', 'w4j', 'w6t', 'w3s', 'w2s', 'w9t', 'w7s', 'b2j', 'w7s', 'w7j', 'w8t', 'b9j', 'r6j', 'r4s', 'w7s', 'g5t', 'b3j', 'w5j', 'w8s', 'w6t', 'b3t', 'g5t', 'b4s', 'r2t', 'r9t', 'b1t', 'g7t', 'b6t', 'b9j', 'r7t', 'r2j', 'w4j', 'w5s', 'g2j', 'w5s', 'b8t', 'w4t', 'b8t', 'b1t', 'g7s', 'b5j', 'b2s', 'b8s', 'w9j', 'g1j', 'g3s', 'b5j', 'b5j', 'g2j', 'g6j', 'w7t', 'w1t', 'g2t', 'b3t', 'r4s', 'g5j', 'b1s', 'b7s', 'r7s', 'b1t', 'g5s', 'b6t', 'w3s', 'b8s', 'r6t', 'g2j', 'g5j', 'w2s', 'r2s', 'g9t', 'r9t', 'r1t', 'b6j', 'w9t']] 
label[0]: tensor([2, 7, 1, 4, 3, 8, 5, 2, 5, 4, 9, 8, 6, 8, 7, 4, 3, 7, 4, 4, 4, 7, 8, 7,
        7, 3, 6, 4, 1, 5, 3, 2, 5, 5, 1, 6, 9, 1, 5, 5, 9, 4, 7, 8, 4, 8, 9, 9,
        9, 9, 5, 7, 4, 3, 4, 4, 6, 3, 2, 9, 7, 2, 7, 7, 8, 9, 6, 4, 7, 5, 3, 5,
        8, 6, 3, 5, 4, 2, 9, 1, 7, 6, 9, 7, 2, 4, 5, 2, 5, 8, 4, 8, 1, 7, 5, 2,
        8, 9, 1, 3, 5, 5, 2, 6, 7, 1, 2, 3, 4, 5, 1, 7, 7, 1, 5, 6, 3, 8, 6, 2,
        5, 2, 2, 9, 9, 1, 6, 9]) 
label[1]: ['b2s', 'w7j', 'b1t', 'b4t', 'g3t', 'w8t', 'r5s', 'r2j', 'r5j', 'r4t', 'b9t', 'w8t', 'b6s', 'b8s', 'b7j', 'b4s', 'w3j', 'b7t', 'b4s', 'b4j', 'b4t', 'g7s', 'b8j', 'w7t', 'g7s', 'b3j', 'w6t', 'b4s', 'b1j', 'g5j', 'b3s', 'w2t', 'g5s', 'w5s', 'b1s', 'b6j', 'g9s', 'b1j', 'w5j', 'g5s', 'w9s', 'b4t', 'b7s', 'r8s', 'r4s', 'w8t', 'r9s', 'b9s', 'w9j', 'w9t', 'b5s', 'w7j', 'b4j', 'w3t', 'g4s', 'w4j', 'w6t', 'w3s', 'w2s', 'w9t', 'w7s', 'b2j', 'w7s', 'w7j', 'w8t', 'b9j', 'r6j', 'r4s', 'w7s', 'g5t', 'b3j', 'w5j', 'w8s', 'w6t', 'b3t', 'g5t', 'b4s', 'r2t', 'r9t', 'b1t', 'g7t', 'b6t', 'b9j', 'r7t', 'r2j', 'w4j', 'w5s', 'g2j', 'w5s', 'b8t', 'w4t', 'b8t', 'b1t', 'g7s', 'b5j', 'b2s', 'b8s', 'w9j', 'g1j', 'g3s', 'b5j', 'b5j', 'g2j', 'g6j', 'w7t', 'w1t', 'g2t', 'b3t', 'r4s', 'g5j', 'b1s', 'b7s', 'r7s', 'b1t', 'g5s', 'b6t', 'w3s', 'b8s', 'r6t', 'g2j', 'g5j', 'w2s', 'r2s', 'g9t', 'r9t', 'r1t', 'b6j', 'w9t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.11878116 -0.04462075  0.0862549  ...  0.15837371  0.13670895
  0.1336824 ]
Average of silhouette coef: 0.08880243
---
  0   1   2   3
0 0.0 3.0 2.8 3.1 
1 3.0 0.0 3.7 3.8 
2 2.8 3.7 0.0 3.6 
3 3.1 3.8 3.6 0.0 
correlation [[ 1.         -0.37602474]
 [-0.37602474  1.        ]]
---
[[], [], [], []] [1 0 1 ... 2 3 3] [[ 23.422514 -48.161472]
 [ 35.452137  27.973076]
 [ 68.56903  -17.224083]
 ...
 [-26.667847 -44.643856]
 [-26.340153  49.065094]
 [-36.913128   8.897259]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.08880243, 'cluster_all': array([ 0.11878116, -0.04462075,  0.0862549 , ...,  0.15837371,
        0.13670895,  0.1336824 ], dtype=float32), 'magnitude_avg': 0.37602473769777606, 'magnitude_all': -0.37602473769777606, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.08880243, 'magnitude_avg': 0.37602473769777606, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([5, 9, 5, 2, 1, 7, 5, 3, 5, 7, 3, 6, 2, 7, 6, 1, 2, 2, 8, 3, 2, 5, 1, 8,
        6, 9, 5, 4, 7, 4, 3, 1, 5, 9, 4, 3, 7, 5, 7, 7, 2, 1, 4, 8, 7, 2, 9, 2,
        1, 2, 3, 9, 8, 6, 3, 7, 9, 9, 8, 8, 1, 6, 4, 7, 7, 1, 3, 9, 3, 8, 8, 3,
        5, 9, 4, 7, 3, 5, 4, 1, 9, 9, 8, 9, 7, 8, 3, 7, 4, 9, 4, 3, 8, 7, 9, 1,
        3, 5, 5, 9, 3, 6, 8, 1, 3, 5, 9, 2, 9, 8, 9, 9, 5, 7, 8, 3, 1, 1, 3, 7,
        8, 6, 5, 6, 3, 2, 3, 4]), ['w5t', 'g9j', 'w5t', 'r2j', 'w1s', 'b7t', 'g5j', 'b3s', 'r5s', 'w7s', 'w3j', 'w6t', 'r2j', 'g7s', 'g6j', 'r1j', 'b2s', 'w2j', 'r8t', 'g3j', 'g2s', 'g5t', 'r1j', 'b8j', 'w6t', 'r9t', 'r5j', 'b4j', 'w7t', 'w4t', 'r3s', 'b1s', 'b5t', 'g9s', 'r4s', 'b3t', 'g7s', 'g5t', 'b7t', 'r7s', 'r2s', 'r1t', 'w4s', 'r8j', 'b7j', 'g2s', 'g9s', 'r2j', 'b1t', 'r2j', 'r3s', 'w9j', 'w8t', 'g6j', 'b3s', 'r7s', 'g9s', 'g9t', 'r8s', 'r8j', 'r1t', 'b6j', 'b4s', 'w7s', 'w7j', 'r1s', 'b3s', 'w9j', 'b3s', 'r8j', 'r8t', 'g3s', 'w5s', 'g9t', 'g4j', 'w7s', 'r3s', 'g5s', 'b4j', 'r1s', 'g9s', 'w9j', 'g8s', 'r9j', 'g7s', 'r8s', 'r3j', 'w7s', 'b4j', 'g9s', 'r4j', 'b3s', 'r8j', 'g7j', 'g9j', 'g1s', 'r3j', 'w5t', 'g5j', 'g9s', 'g3s', 'r6s', 'r8t', 'w1j', 'r3s', 'w5t', 'r9j', 'r2t', 'w9j', 'g8j', 'g9t', 'r9t', 'g5s', 'w7t', 'g8j', 'w3t', 'b1j', 'w1j', 'r3t', 'g7t', 'b8t', 'b6j', 'r5t', 'b6t', 'b3j', 'r2s', 'r3j', 'b4s']] 
label[0]: tensor([5, 9, 5, 2, 1, 7, 5, 3, 5, 7, 3, 6, 2, 7, 6, 1, 2, 2, 8, 3, 2, 5, 1, 8,
        6, 9, 5, 4, 7, 4, 3, 1, 5, 9, 4, 3, 7, 5, 7, 7, 2, 1, 4, 8, 7, 2, 9, 2,
        1, 2, 3, 9, 8, 6, 3, 7, 9, 9, 8, 8, 1, 6, 4, 7, 7, 1, 3, 9, 3, 8, 8, 3,
        5, 9, 4, 7, 3, 5, 4, 1, 9, 9, 8, 9, 7, 8, 3, 7, 4, 9, 4, 3, 8, 7, 9, 1,
        3, 5, 5, 9, 3, 6, 8, 1, 3, 5, 9, 2, 9, 8, 9, 9, 5, 7, 8, 3, 1, 1, 3, 7,
        8, 6, 5, 6, 3, 2, 3, 4]) 
label[1]: ['w5t', 'g9j', 'w5t', 'r2j', 'w1s', 'b7t', 'g5j', 'b3s', 'r5s', 'w7s', 'w3j', 'w6t', 'r2j', 'g7s', 'g6j', 'r1j', 'b2s', 'w2j', 'r8t', 'g3j', 'g2s', 'g5t', 'r1j', 'b8j', 'w6t', 'r9t', 'r5j', 'b4j', 'w7t', 'w4t', 'r3s', 'b1s', 'b5t', 'g9s', 'r4s', 'b3t', 'g7s', 'g5t', 'b7t', 'r7s', 'r2s', 'r1t', 'w4s', 'r8j', 'b7j', 'g2s', 'g9s', 'r2j', 'b1t', 'r2j', 'r3s', 'w9j', 'w8t', 'g6j', 'b3s', 'r7s', 'g9s', 'g9t', 'r8s', 'r8j', 'r1t', 'b6j', 'b4s', 'w7s', 'w7j', 'r1s', 'b3s', 'w9j', 'b3s', 'r8j', 'r8t', 'g3s', 'w5s', 'g9t', 'g4j', 'w7s', 'r3s', 'g5s', 'b4j', 'r1s', 'g9s', 'w9j', 'g8s', 'r9j', 'g7s', 'r8s', 'r3j', 'w7s', 'b4j', 'g9s', 'r4j', 'b3s', 'r8j', 'g7j', 'g9j', 'g1s', 'r3j', 'w5t', 'g5j', 'g9s', 'g3s', 'r6s', 'r8t', 'w1j', 'r3s', 'w5t', 'r9j', 'r2t', 'w9j', 'g8j', 'g9t', 'r9t', 'g5s', 'w7t', 'g8j', 'w3t', 'b1j', 'w1j', 'r3t', 'g7t', 'b8t', 'b6j', 'r5t', 'b6t', 'b3j', 'r2s', 'r3j', 'b4s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([4, 3, 3, 2, 7, 4, 1, 5, 9, 3, 7, 3, 9, 4, 2, 8, 6, 4, 5, 1, 7, 8, 5, 6,
        4, 9, 1, 2, 1, 4, 8, 4, 6, 3, 4, 2, 2, 1, 4, 2, 6, 7, 5, 6, 2, 4, 5, 8,
        6, 6, 9, 9, 3, 9, 9, 1, 6, 1, 3, 7, 2, 9, 1, 1, 3, 9, 2, 6, 5, 3, 7, 7,
        4, 3, 4, 8, 2, 5, 2, 6, 9, 8, 8, 3, 1, 7, 7, 4, 9, 3, 7, 5, 8, 7, 5, 7,
        2, 6, 3, 3, 3, 2, 3, 9, 4, 3, 5, 1, 6, 8, 7, 5, 9, 9, 1, 1, 6, 4, 2, 8,
        2, 8, 1, 4, 8, 9, 6, 2])
Accuracy (count): tensor(17) 
Accuracy (ratio) tensor(0.1328)
Accuracy:
 [[ 0  0 14  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0 17  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0 11  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]
 [ 0  0 13  0  0  0  0  0  0]
 [ 0  0 12  0  0  0  0  0  0]
 [ 0  0 15  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([9, 7, 3, 7, 6, 1, 3, 1, 4, 3, 2, 3, 6, 5, 1, 8, 8, 7, 7, 6, 2, 7, 7, 9,
        9, 6, 3, 8, 1, 7, 3, 1, 1, 8, 8, 9, 6, 5, 1, 6, 3, 4, 7, 5, 4, 3, 1, 1,
        1, 9, 4, 9, 4, 9, 1, 2, 9, 5, 5, 4, 8, 7, 3, 4, 2, 4, 5, 5, 9, 9, 1, 2,
        6, 4, 6, 4, 4, 5, 2, 6, 6, 1, 1, 4, 5, 5, 5, 6, 9, 5, 3, 5, 5, 7, 1, 9,
        6, 1, 6, 4, 6, 7, 1, 6, 2, 1, 7, 2, 2, 3, 3, 9, 7, 7, 9, 5, 6, 8, 5, 2,
        7, 3, 2, 9, 8, 4, 3, 6])
Accuracy (count): tensor(14) 
Accuracy (ratio) tensor(0.1094)
Accuracy:
 [[ 0  0 18  0  0  0  0  0  0]
 [ 0  0 11  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0 17  0  0  0  0  0  0]
 [ 0  0 15  0  0  0  0  0  0]
 [ 0  0  8  0  0  0  0  0  0]
 [ 0  0 15  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [0.03428865 0.10874371 0.10552531 ... 0.21492101 0.3274647  0.09462801]
Average of silhouette coef: 0.102921024
---
  1   2   3   4   5   6   7   8   9
1 0.0 3.9 4.2 4.6 4.2 4.5 4.4 4.2 4.6 
2 3.9 0.0 3.7 4.4 4.2 4.1 4.3 3.8 4.8 
3 4.2 3.7 0.0 4.5 2.6 4.4 4.2 3.9 4.7 
4 4.6 4.4 4.5 0.0 4.1 4.3 4.3 4.7 3.7 
5 4.2 4.2 2.6 4.1 0.0 3.6 4.0 3.7 3.8 
6 4.5 4.1 4.4 4.3 3.6 0.0 5.4 4.0 4.1 
7 4.4 4.3 4.2 4.3 4.0 5.4 0.0 4.4 3.1 
8 4.2 3.8 3.9 4.7 3.7 4.0 4.4 0.0 3.0 
9 4.6 4.8 4.7 3.7 3.8 4.1 3.1 3.0 0.0 
correlation [[1.         0.28577233]
 [0.28577233 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [5 9 5 ... 6 1 3] [[ 27.434048     2.1180518 ]
 [-11.170214   -32.84915   ]
 [ 43.24123    -19.026237  ]
 ...
 [ 80.72238     -0.35102952]
 [-47.62069     35.587917  ]
 [ 21.383217    28.235088  ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(57) 
Accuracy (ratio) tensor(0.1166)
Accuracy:
 [[ 0  0 45  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 61  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 45  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.1 -0.1 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.1  2.2 -0.0 -0.1 -0.0 -0.1  0.2  0.0  0.1  0.1  1.8 
 0.2 -0.1 -0.1  0.0  0.5 -0.4 -0.9 -0.5 -2.2  0.2  0.2  0.2  0.4 -0.2 -0.1  0.1  0.5 -0.0  0.2 -0.5 
 0.2 -0.1 -0.2  0.0 -0.1 -0.3  1.8  1.5 -1.4 -0.2 -0.2  0.1 -0.1 -0.4 -0.1 -0.0 -0.1  0.1 -0.4  0.0 
 0.0 -0.0  0.1 -0.1  0.3  0.2 -0.6 -0.2  0.4  0.1 -0.7  0.2 -0.4 -0.4  0.2  0.0 -2.6  0.3  0.8 -0.2 
-0.7 -0.1  0.0  0.1  0.2  0.4  2.1  0.2  0.1 -0.1 -0.1  0.2 -0.7 -0.3 -0.0 -0.1  0.3  0.1  0.2 -0.6 
-2.9 -0.2 -0.0  0.1 -0.3  0.1 -0.5 -0.0 -0.1 -0.0 -0.4  0.1 -0.1 -0.3  0.1 -0.1  0.1 -0.0 -0.4 -0.0 
 2.2 -0.0  0.0 -0.0  0.1  0.2 -0.3  0.1  0.9  0.1 -0.6  0.1 -1.1 -0.0  0.1  0.1  0.5  0.1 -0.8 -0.2 
-0.1/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
 -0.1 -0.0 -0.0 -0.6  0.1  0.4 -0.3  0.3 -0.9  0.0  0.2  2.2 -0.2  0.1  0.0  0.8  0.1  0.1 -0.3 
 0.3 -0.0  0.0 -0.0 -0.3  0.3 -0.4 -0.2  2.2 -0.7 -1.0  0.1  0.4 -0.4  0.2  0.0  0.2  0.1  0.2 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 2.9870481
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.1 -0.1 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.1  2.2 -0.0 -0.1 -0.0 -0.1  0.2  0.0  0.1  0.1  1.8 
 0.2 -0.1 -0.1  0.0  0.5 -0.4 -0.9 -0.5 -2.2  0.2  0.2  0.2  0.4 -0.2 -0.1  0.1  0.5 -0.0  0.2 -0.5 
 0.2 -0.1 -0.2  0.0 -0.1 -0.3  1.8  1.5 -1.4 -0.2 -0.2  0.1 -0.1 -0.4 -0.1 -0.0 -0.1  0.1 -0.4  0.0 
 0.0 -0.0  0.1 -0.1  0.3  0.2 -0.6 -0.2  0.4  0.1 -0.7  0.2 -0.4 -0.4  0.2  0.0 -2.6  0.3  0.8 -0.2 
-0.7 -0.1  0.0  0.1  0.2  0.4  2.1  0.2  0.1 -0.1 -0.1  0.2 -0.7 -0.3 -0.0 -0.1  0.3  0.1  0.2 -0.6 
-2.9 -0.2 -0.0  0.1 -0.3  0.1 -0.5 -0.0 -0.1 -0.0 -0.4  0.1 -0.1 -0.3  0.1 -0.1  0.1 -0.0 -0.4 -0.0 
 2.2 -0.0  0.0 -0.0  0.1  0.2 -0.3  0.1  0.9  0.1 -0.6  0.1 -1.1 -0.0  0.1  0.1  0.5  0.1 -0.8 -0.2 
-0.1 -0.1 -0.0 -0.0 -0.6  0.1  0.4 -0.3  0.3 -0.9  0.0  0.2  2.2 -0.2  0.1  0.0  0.8  0.1  0.1 -0.3 
 0.3 -0.0  0.0 -0.0 -0.3  0.3 -0.4 -0.2  2.2 -0.7 -1.0  0.1  0.4 -0.4  0.2  0.0  0.2  0.1  0.2 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 4.5727997
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.1 -0.1 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.1  2.2 -0.0 -0.1 -0.0 -0.1  0.2  0.0  0.1  0.1  1.8 
 0.2 -0.1 -0.1  0.0  0.5 -0.4 -0.9 -0.5 -2.2  0.2  0.2  0.2  0.4 -0.2 -0.1  0.1  0.5 -0.0  0.2 -0.5 
 0.2 -0.1 -0.2  0.0 -0.1 -0.3  1.8  1.5 -1.4 -0.2 -0.2  0.1 -0.1 -0.4 -0.1 -0.0 -0.1  0.1 -0.4  0.0 
 0.0 -0.0  0.1 -0.1  0.3  0.2 -0.6 -0.2  0.4  0.1 -0.7  0.2 -0.4 -0.4  0.2  0.0 -2.6  0.3  0.8 -0.2 
-0.7 -0.1  0.0  0.1  0.2  0.4  2.1  0.2  0.1 -0.1 -0.1  0.2 -0.7 -0.3 -0.0 -0.1  0.3  0.1  0.2 -0.6 
-2.9 -0.2 -0.0  0.1 -0.3  0.1 -0.5 -0.0 -0.1 -0.0 -0.4  0.1 -0.1 -0.3  0.1 -0.1  0.1 -0.0 -0.4 -0.0 
 2.2 -0.0  0.0 -0.0  0.1  0.2 -0.3  0.1  0.9  0.1 -0.6  0.1 -1.1 -0.0  0.1  0.1  0.5  0.1 -0.8 -0.2 
-0.1 -0.1 -0.0 -0.0 -0.6  0.1  0.4 -0.3  0.3 -0.9  0.0  0.2  2.2 -0.2  0.1  0.0  0.8  0.1  0.1 -0.3 
 0.3 -0.0  0.0 -0.0 -0.3  0.3 -0.4 -0.2  2.2 -0.7 -1.0  0.1  0.4 -0.4  0.2  0.0  0.2  0.1  0.2 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 3.9097037
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2 -0.1 -0.1 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.1  2.2 -0.0 -0.1 -0.0 -0.1  0.2  0.0  0.1  0.1  1.8 
 0.2 -0.1 -0.1  0.0  0.5 -0.4 -0.9 -0.5 -2.2  0.2  0.2  0.2  0.4 -0.2 -0.1  0.1  0.5 -0.0  0.2 -0.5 
 0.2 -0.1 -0.2  0.0 -0.1 -0.3  1.8  1.5 -1.4 -0.2 -0.2  0.1 -0.1 -0.4 -0.1 -0.0 -0.1  0.1 -0.4  0.0 
 0.0 -0.0  0.1 -0.1  0.3  0.2 -0.6 -0.2  0.4  0.1 -0.7  0.2 -0.4 -0.4  0.2  0.0 -2.6  0.3  0.8 -0.2 
-0.7 -0.1  0.0  0.1  0.2  0.4  2.1  0.2  0.1 -0.1 -0.1  0.2 -0.7 -0.3 -0.0 -0.1  0.3  0.1  0.2 -0.6 
-2.9 -0.2 -0.0  0.1 -0.3  0.1 -0.5 -0.0 -0.1 -0.0 -0.4  0.1 -0.1 -0.3  0.1 -0.1  0.1 -0.0 -0.4 -0.0 
 2.2 -0.0  0.0 -0.0  0.1  0.2 -0.3  0.1  0.9  0.1 -0.6  0.1 -1.1 -0.0  0.1  0.1  0.5  0.1 -0.8 -0.2 
-0.1 -0.1 -0.0 -0.0 -0.6  0.1  0.4 -0.3  0.3 -0.9  0.0  0.2  2.2 -0.2  0.1  0.0  0.8  0.1  0.1 -0.3 
 0.3 -0.0  0.0 -0.0 -0.3  0.3 -0.4 -0.2  2.2 -0.7 -1.0  0.1  0.4 -0.4  0.2  0.0  0.2  0.1  0.2 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 3.682842
results (all): {'reconst_0x0_avg': 0.1328125, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.109375, 'cross_1x0_all': nan, 'cluster_avg': 0.102921024, 'cluster_all': array([0.03428865, 0.10874371, 0.10552531, ..., 0.21492101, 0.3274647 ,
       0.09462801], dtype=float32), 'magnitude_avg': 0.28577233251804324, 'magnitude_all': 0.28577233251804324, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1328125, 'cross_1x0_avg': 0.109375, 'cluster_avg': 0.102921024, 'magnitude_avg': 0.28577233251804324, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 3, 1, 7, 9, 5, 6, 1, 5, 6, 1, 7, 2, 1, 1, 5, 9, 9, 6, 7, 4, 8, 9, 1,
        8, 4, 1, 9, 2, 2, 7, 7, 9, 6, 2, 3, 3, 6, 9, 9, 9, 3, 9, 8, 5, 9, 5, 1,
        1, 4, 1, 9, 9, 5, 2, 9, 3, 1, 3, 7, 3, 3, 4, 6, 6, 9, 7, 8, 5, 8, 2, 7,
        6, 3, 3, 3, 3, 5, 9, 8, 8, 4, 9, 8, 3, 5, 6, 9, 2, 3, 9, 4, 5, 6, 3, 2,
        8, 6, 2, 4, 7, 5, 5, 3, 8, 8, 5, 2, 6, 7, 3, 3, 5, 6, 6, 7, 1, 8, 9, 1,
        3, 8, 2, 5, 6, 3, 8, 4]), ['r7j', 'b3j', 'r1j', 'w7j', 'g9s', 'r5j', 'r6j', 'b1j', 'w5s', 'r6t', 'w1s', 'b7t', 'b2t', 'b1j', 'r1s', 'w5t', 'r9j', 'r9s', 'g6s', 'r7s', 'w4t', 'b8t', 'w9j', 'g1t', 'r8s', 'r4j', 'w1t', 'r9t', 'r2j', 'b2j', 'b7s', 'g7j', 'b9s', 'w6t', 'r2j', 'b3j', 'r3s', 'w6s', 'r9s', 'b9s', 'r9t', 'g3s', 'b9j', 'w8s', 'r5t', 'g9s', 'b5s', 'r1j', 'g1t', 'r4s', 'w1s', 'g9j', 'g9t', 'r5t', 'r2s', 'b9t', 'r3t', 'b1t', 'g3t', 'b7s', 'g3j', 'w3s', 'g4t', 'w6j', 'b6t', 'r9s', 'b7j', 'b8j', 'w5s', 'b8j', 'w2j', 'w7s', 'r6t', 'w3j', 'g3s', 'g3s', 'r3s', 'g5j', 'w9s', 'g8j', 'g8j', 'b4s', 'r9s', 'w8t', 'b3t', 'b5s', 'b6t', 'r9j', 'b2t', 'g3j', 'w9s', 'w4s', 'r5s', 'w6t', 'r3t', 'g2s', 'r8t', 'b6s', 'w2s', 'r4s', 'r7j', 'g5j', 'g5s', 'r3j', 'b8t', 'w8t', 'b5t', 'g2s', 'g6t', 'g7j', 'b3s', 'b3j', 'b5t', 'r6s', 'b6s', 'b7s', 'b1t', 'r8j', 'b9s', 'w1t', 'w3j', 'b8s', 'g2j', 'w5j', 'r6s', 'r3j', 'r8j', 'r4j']] 
label[0]: tensor([7, 3, 1, 7, 9, 5, 6, 1, 5, 6, 1, 7, 2, 1, 1, 5, 9, 9, 6, 7, 4, 8, 9, 1,
        8, 4, 1, 9, 2, 2, 7, 7, 9, 6, 2, 3, 3, 6, 9, 9, 9, 3, 9, 8, 5, 9, 5, 1,
        1, 4, 1, 9, 9, 5, 2, 9, 3, 1, 3, 7, 3, 3, 4, 6, 6, 9, 7, 8, 5, 8, 2, 7,
        6, 3, 3, 3, 3, 5, 9, 8, 8, 4, 9, 8, 3, 5, 6, 9, 2, 3, 9, 4, 5, 6, 3, 2,
        8, 6, 2, 4, 7, 5, 5, 3, 8, 8, 5, 2, 6, 7, 3, 3, 5, 6, 6, 7, 1, 8, 9, 1,
        3, 8, 2, 5, 6, 3, 8, 4]) 
label[1]: ['r7j', 'b3j', 'r1j', 'w7j', 'g9s', 'r5j', 'r6j', 'b1j', 'w5s', 'r6t', 'w1s', 'b7t', 'b2t', 'b1j', 'r1s', 'w5t', 'r9j', 'r9s', 'g6s', 'r7s', 'w4t', 'b8t', 'w9j', 'g1t', 'r8s', 'r4j', 'w1t', 'r9t', 'r2j', 'b2j', 'b7s', 'g7j', 'b9s', 'w6t', 'r2j', 'b3j', 'r3s', 'w6s', 'r9s', 'b9s', 'r9t', 'g3s', 'b9j', 'w8s', 'r5t', 'g9s', 'b5s', 'r1j', 'g1t', 'r4s', 'w1s', 'g9j', 'g9t', 'r5t', 'r2s', 'b9t', 'r3t', 'b1t', 'g3t', 'b7s', 'g3j', 'w3s', 'g4t', 'w6j', 'b6t', 'r9s', 'b7j', 'b8j', 'w5s', 'b8j', 'w2j', 'w7s', 'r6t', 'w3j', 'g3s', 'g3s', 'r3s', 'g5j', 'w9s', 'g8j', 'g8j', 'b4s', 'r9s', 'w8t', 'b3t', 'b5s', 'b6t', 'r9j', 'b2t', 'g3j', 'w9s', 'w4s', 'r5s', 'w6t', 'r3t', 'g2s', 'r8t', 'b6s', 'w2s', 'r4s', 'r7j', 'g5j', 'g5s', 'r3j', 'b8t', 'w8t', 'b5t', 'g2s', 'g6t', 'g7j', 'b3s', 'b3j', 'b5t', 'r6s', 'b6s', 'b7s', 'b1t', 'r8j', 'b9s', 'w1t', 'w3j', 'b8s', 'g2j', 'w5j', 'r6s', 'r3j', 'r8j', 'r4j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.0092476  -0.00081252 -0.00422169 ... -0.00460011 -0.00108449
 -0.0002288 ]
Average of silhouette coef: -0.0031690819
---
  0   1   2
0 0.0 0.3 0.3 
1 0.3 0.0 0.2 
2 0.3 0.2 0.0 
correlation [[1.         0.94171245]
 [0.94171245 1.        ]]
---
[[], [], []] [0 0 0 ... 0 0 2] [[ 27.995113  -41.074898 ]
 [  4.7722707  43.573956 ]
 [ 65.63817    18.904371 ]
 ...
 [ 30.843676  -39.610954 ]
 [-49.612553  -27.86337  ]
 [ 79.25743    -0.9720756]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.0031690819, 'cluster_all': array([-0.0092476 , -0.00081252, -0.00422169, ..., -0.00460011,
       -0.00108449, -0.0002288 ], dtype=float32), 'magnitude_avg': 0.9417124470340973, 'magnitude_all': 0.9417124470340973, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.0031690819, 'magnitude_avg': 0.9417124470340973, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([3, 9, 1, 6, 6, 6, 1, 9, 6, 3, 4, 5, 6, 6, 8, 1, 8, 1, 9, 9, 2, 5, 7, 1,
        3, 5, 3, 9, 1, 7, 7, 6, 1, 5, 7, 6, 3, 7, 7, 1, 4, 1, 7, 7, 8, 6, 1, 9,
        6, 1, 5, 2, 1, 2, 8, 3, 6, 5, 5, 3, 6, 9, 2, 2, 3, 3, 5, 5, 9, 5, 4, 2,
        7, 4, 3, 4, 7, 1, 4, 6, 9, 3, 4, 4, 2, 4, 1, 3, 2, 2, 1, 8, 1, 7, 1, 9,
        5, 8, 2, 7, 9, 8, 3, 4, 7, 3, 1, 9, 8, 5, 8, 2, 6, 9, 4, 8, 1, 3, 5, 2,
        5, 6, 2, 7, 2, 9, 1, 4]), ['r3t', 'b9j', 'w1j', 'g6s', 'g6j', 'b6j', 'w1s', 'r9t', 'r6j', 'g3t', 'g4t', 'g5t', 'b6j', 'b6j', 'r8t', 'w1j', 'b8s', 'b1s', 'r9j', 'w9t', 'g2s', 'g5j', 'r7s', 'g1t', 'w3s', 'w5s', 'w3j', 'b9j', 'r1j', 'w7s', 'w7s', 'r6t', 'b1j', 'r5j', 'b7s', 'w6j', 'w3t', 'w7s', 'g7s', 'b1j', 'w4j', 'w1s', 'w7j', 'r7j', 'b8j', 'w6s', 'r1s', 'w9s', 'r6t', 'g1t', 'w5t', 'w2s', 'b1t', 'b2t', 'r8s', 'r3s', 'b6s', 'b5j', 'b5t', 'w3t', 'r6s', 'b9s', 'g2s', 'g2s', 'r3s', 'r3j', 'g5s', 'g5s', 'r9s', 'b5j', 'g4s', 'b2s', 'b7s', 'w4j', 'b3s', 'b4t', 'b7s', 'g1t', 'w4j', 'b6s', 'r9j', 'g3s', 'r4t', 'b4s', 'g2j', 'b4j', 'r1j', 'w3t', 'w2j', 'b2j', 'r1s', 'g8s', 'r1s', 'g7j', 'r1t', 'w9j', 'w5t', 'g8j', 'b2t', 'b7s', 'b9s', 'b8s', 'g3s', 'w4j', 'g7t', 'r3s', 'b1s', 'b9j', 'g8s', 'w5j', 'r8t', 'r2t', 'b6j', 'g9s', 'b4j', 'r8t', 'g1s', 'w3t', 'r5s', 'b2j', 'w5t', 'g6j', 'r2j', 'r7t', 'w2t', 'r9j', 'r1t', 'g4j']] 
label[0]: tensor([3, 9, 1, 6, 6, 6, 1, 9, 6, 3, 4, 5, 6, 6, 8, 1, 8, 1, 9, 9, 2, 5, 7, 1,
        3, 5, 3, 9, 1, 7, 7, 6, 1, 5, 7, 6, 3, 7, 7, 1, 4, 1, 7, 7, 8, 6, 1, 9,
        6, 1, 5, 2, 1, 2, 8, 3, 6, 5, 5, 3, 6, 9, 2, 2, 3, 3, 5, 5, 9, 5, 4, 2,
        7, 4, 3, 4, 7, 1, 4, 6, 9, 3, 4, 4, 2, 4, 1, 3, 2, 2, 1, 8, 1, 7, 1, 9,
        5, 8, 2, 7, 9, 8, 3, 4, 7, 3, 1, 9, 8, 5, 8, 2, 6, 9, 4, 8, 1, 3, 5, 2,
        5, 6, 2, 7, 2, 9, 1, 4]) 
label[1]: ['r3t', 'b9j', 'w1j', 'g6s', 'g6j', 'b6j', 'w1s', 'r9t', 'r6j', 'g3t', 'g4t', 'g5t', 'b6j', 'b6j', 'r8t', 'w1j', 'b8s', 'b1s', 'r9j', 'w9t', 'g2s', 'g5j', 'r7s', 'g1t', 'w3s', 'w5s', 'w3j', 'b9j', 'r1j', 'w7s', 'w7s', 'r6t', 'b1j', 'r5j', 'b7s', 'w6j', 'w3t', 'w7s', 'g7s', 'b1j', 'w4j', 'w1s', 'w7j', 'r7j', 'b8j', 'w6s', 'r1s', 'w9s', 'r6t', 'g1t', 'w5t', 'w2s', 'b1t', 'b2t', 'r8s', 'r3s', 'b6s', 'b5j', 'b5t', 'w3t', 'r6s', 'b9s', 'g2s', 'g2s', 'r3s', 'r3j', 'g5s', 'g5s', 'r9s', 'b5j', 'g4s', 'b2s', 'b7s', 'w4j', 'b3s', 'b4t', 'b7s', 'g1t', 'w4j', 'b6s', 'r9j', 'g3s', 'r4t', 'b4s', 'g2j', 'b4j', 'r1j', 'w3t', 'w2j', 'b2j', 'r1s', 'g8s', 'r1s', 'g7j', 'r1t', 'w9j', 'w5t', 'g8j', 'b2t', 'b7s', 'b9s', 'b8s', 'g3s', 'w4j', 'g7t', 'r3s', 'b1s', 'b9j', 'g8s', 'w5j', 'r8t', 'r2t', 'b6j', 'g9s', 'b4j', 'r8t', 'g1s', 'w3t', 'r5s', 'b2j', 'w5t', 'g6j', 'r2j', 'r7t', 'w2t', 'r9j', 'r1t', 'g4j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.08971213 -0.1480064  -0.71104723 ... -0.01931429  0.22982101
 -0.06965043]
Average of silhouette coef: -0.07211439
---
  0   1   2   3
0 0.0 81.6 88.7 84.8 
1 81.6 0.0 7.2 3.4 
2 88.7 7.2 0.0 3.9 
3 84.8 3.4 3.9 0.0 
correlation [[1.         0.45669525]
 [0.45669525 1.        ]]
---
[[], [], [], []] [3 1 0 ... 3 0 3] [[ 26.91591    -6.626094 ]
 [  4.4114523   7.6177382]
 [ 90.67773    -4.4844885]
 ...
 [ -4.2008944  12.229289 ]
 [-77.92706   -24.184141 ]
 [ 19.48415    25.313759 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': -0.07211439, 'cluster_all': array([-0.08971213, -0.1480064 , -0.71104723, ..., -0.01931429,
        0.22982101, -0.06965043], dtype=float32), 'magnitude_avg': 0.4566952532218038, 'magnitude_all': 0.4566952532218038, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': -0.07211439, 'magnitude_avg': 0.4566952532218038, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 2, 2, 7, 3, 5, 4, 1, 9, 4, 8, 1, 5, 4, 6, 4, 6, 6, 5, 4, 6, 7, 5, 2,
        9, 9, 3, 4, 9, 7, 3, 3, 2, 3, 3, 3, 7, 8, 6, 1, 2, 3, 2, 8, 7, 7, 8, 3,
        8, 2, 1, 3, 7, 9, 4, 4, 4, 7, 9, 8, 9, 9, 2, 6, 5, 3, 3, 2, 8, 4, 8, 8,
        6, 1, 8, 1, 5, 3, 8, 7, 3, 8, 9, 9, 7, 1, 9, 3, 6, 3, 3, 3, 6, 2, 8, 1,
        3, 9, 7, 3, 2, 1, 5, 7, 7, 6, 5, 8, 4, 6, 7, 8, 4, 4, 1, 2, 2, 7, 5, 6,
        5, 1, 1, 6, 6, 5, 5, 8]), ['r2s', 'g2t', 'w2j', 'r7t', 'g3j', 'g5s', 'b4t', 'b1s', 'b9s', 'r4t', 'g8s', 'r1t', 'g5j', 'g4j', 'w6j', 'g4t', 'r6j', 'g6s', 'w5s', 'r4t', 'r6s', 'g7t', 'b5s', 'r2j', 'r9j', 'r9s', 'g3t', 'g4s', 'w9j', 'b7t', 'w3j', 'g3s', 'b2j', 'g3s', 'b3j', 'b3s', 'b7t', 'b8t', 'w6j', 'g1t', 'w2s', 'w3j', 'w2t', 'r8t', 'g7t', 'r7t', 'w8j', 'r3t', 'w8j', 'g2j', 'r1j', 'w3t', 'r7j', 'w9t', 'b4s', 'g4j', 'r4t', 'b7t', 'r9t', 'r8s', 'r9t', 'b9j', 'r2j', 'b6j', 'w5j', 'b3j', 'r3s', 'w2j', 'g8t', 'g4s', 'b8t', 'b8j', 'r6j', 'w1j', 'b8j', 'w1s', 'b5j', 'g3t', 'g8j', 'b7s', 'r3s', 'r8j', 'w9s', 'b9j', 'b7s', 'r1t', 'r9j', 'r3t', 'w6j', 'b3j', 'b3t', 'w3s', 'r6t', 'b2s', 'w8s', 'b1t', 'w3j', 'b9t', 'g7j', 'w3t', 'w2s', 'b1s', 'b5s', 'b7t', 'r7j', 'w6t', 'r5j', 'r8s', 'r4t', 'b6t', 'b7j', 'r8j', 'w4s', 'w4j', 'b1s', 'w2s', 'w2t', 'w7s', 'b5s', 'b6t', 'w5t', 'w1j', 'w1t', 'b6j', 'b6s', 'r5t', 'b5s', 'w8s']] 
label[0]: tensor([2, 2, 2, 7, 3, 5, 4, 1, 9, 4, 8, 1, 5, 4, 6, 4, 6, 6, 5, 4, 6, 7, 5, 2,
        9, 9, 3, 4, 9, 7, 3, 3, 2, 3, 3, 3, 7, 8, 6, 1, 2, 3, 2, 8, 7, 7, 8, 3,
        8, 2, 1, 3, 7, 9, 4, 4, 4, 7, 9, 8, 9, 9, 2, 6, 5, 3, 3, 2, 8, 4, 8, 8,
        6, 1, 8, 1, 5, 3, 8, 7, 3, 8, 9, 9, 7, 1, 9, 3, 6, 3, 3, 3, 6, 2, 8, 1,
        3, 9, 7, 3, 2, 1, 5, 7, 7, 6, 5, 8, 4, 6, 7, 8, 4, 4, 1, 2, 2, 7, 5, 6,
        5, 1, 1, 6, 6, 5, 5, 8]) 
label[1]: ['r2s', 'g2t', 'w2j', 'r7t', 'g3j', 'g5s', 'b4t', 'b1s', 'b9s', 'r4t', 'g8s', 'r1t', 'g5j', 'g4j', 'w6j', 'g4t', 'r6j', 'g6s', 'w5s', 'r4t', 'r6s', 'g7t', 'b5s', 'r2j', 'r9j', 'r9s', 'g3t', 'g4s', 'w9j', 'b7t', 'w3j', 'g3s', 'b2j', 'g3s', 'b3j', 'b3s', 'b7t', 'b8t', 'w6j', 'g1t', 'w2s', 'w3j', 'w2t', 'r8t', 'g7t', 'r7t', 'w8j', 'r3t', 'w8j', 'g2j', 'r1j', 'w3t', 'r7j', 'w9t', 'b4s', 'g4j', 'r4t', 'b7t', 'r9t', 'r8s', 'r9t', 'b9j', 'r2j', 'b6j', 'w5j', 'b3j', 'r3s', 'w2j', 'g8t', 'g4s', 'b8t', 'b8j', 'r6j', 'w1j', 'b8j', 'w1s', 'b5j', 'g3t', 'g8j', 'b7s', 'r3s', 'r8j', 'w9s', 'b9j', 'b7s', 'r1t', 'r9j', 'r3t', 'w6j', 'b3j', 'b3t', 'w3s', 'r6t', 'b2s', 'w8s', 'b1t', 'w3j', 'b9t', 'g7j', 'w3t', 'w2s', 'b1s', 'b5s', 'b7t', 'r7j', 'w6t', 'r5j', 'r8s', 'r4t', 'b6t', 'b7j', 'r8j', 'w4s', 'w4j', 'b1s', 'w2s', 'w2t', 'w7s', 'b5s', 'b6t', 'w5t', 'w1j', 'w1t', 'b6j', 'b6s', 'r5t', 'b5s', 'w8s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([1, 1, 5, 2, 1, 6, 5, 9, 6, 4, 6, 9, 1, 1, 3, 7, 4, 9, 8, 2, 1, 1, 7, 8,
        1, 2, 3, 8, 3, 7, 8, 1, 9, 1, 6, 8, 1, 4, 8, 3, 1, 4, 1, 7, 1, 7, 9, 7,
        9, 8, 5, 7, 1, 5, 9, 1, 8, 2, 5, 2, 4, 6, 9, 7, 5, 1, 1, 6, 2, 7, 9, 7,
        8, 1, 9, 9, 7, 5, 7, 5, 7, 8, 7, 5, 1, 7, 7, 4, 9, 4, 3, 4, 1, 9, 1, 8,
        9, 8, 4, 6, 8, 6, 8, 4, 2, 5, 6, 7, 2, 8, 4, 2, 3, 8, 2, 4, 7, 4, 6, 8,
        4, 4, 2, 4, 5, 9, 3, 2])
Accuracy (count): tensor(12) 
Accuracy (ratio) tensor(0.0938)
Accuracy:
 [[ 0 22  0  0  0  0  0  0  0]
 [ 0 12  0  0  0  0  0  0  0]
 [ 0  7  0  0  0  0  0  0  0]
 [ 0 16  0  0  0  0  0  0  0]
 [ 0 11  0  0  0  0  0  0  0]
 [ 0 10  0  0  0  0  0  0  0]
 [ 0 18  0  0  0  0  0  0  0]
 [ 0 17  0  0  0  0  0  0  0]
 [ 0 15  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([3, 9, 5, 8, 8, 2, 1, 3, 4, 1, 9, 2, 2, 4, 7, 1, 8, 1, 7, 1, 6, 1, 3, 1,
        5, 7, 9, 5, 3, 2, 1, 3, 6, 8, 6, 7, 3, 6, 2, 7, 9, 2, 9, 1, 3, 7, 1, 3,
        8, 9, 9, 7, 8, 2, 2, 5, 3, 1, 2, 2, 4, 9, 4, 6, 7, 1, 2, 1, 4, 3, 4, 4,
        2, 7, 9, 1, 7, 2, 9, 7, 4, 3, 6, 4, 8, 1, 6, 7, 7, 6, 1, 7, 5, 1, 1, 4,
        3, 5, 8, 2, 3, 6, 1, 7, 6, 7, 3, 6, 9, 6, 7, 9, 2, 7, 8, 1, 3, 6, 3, 6,
        3, 3, 7, 8, 9, 4, 3, 1])
Accuracy (count): tensor(15) 
Accuracy (ratio) tensor(0.1172)
Accuracy:
 [[ 0 21  0  0  0  0  0  0  0]
 [ 0 15  0  0  0  0  0  0  0]
 [ 0 19  0  0  0  0  0  0  0]
 [ 0 11  0  0  0  0  0  0  0]
 [ 0  6  0  0  0  0  0  0  0]
 [ 0 14  0  0  0  0  0  0  0]
 [ 0 19  0  0  0  0  0  0  0]
 [ 0 10  0  0  0  0  0  0  0]
 [ 0 13  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [-0.09802225 -0.29891762 -0.05703397 ...  0.0389637  -0.16285436
 -0.16590399]
Average of silhouette coef: -0.1553902
---
  1   2   3   4   5   6   7   8   9
1 0.0 12.2 28.0 40.9 48.8 59.5 72.1 82.8 113.1 
2 12.2 0.0 16.2 28.9 36.8 47.5 60.1 70.8 101.1 
3 28.0 16.2 0.0 13.4 21.0 31.7 44.3 54.9 85.3 
4 40.9 28.9 13.4 0.0 8.5 19.0 31.7 42.2 72.4 
5 48.8 36.8 21.0 8.5 0.0 10.9 23.5 34.2 64.4 
6 59.5 47.5 31.7 19.0 10.9 0.0 12.7 23.5 53.7 
7 72.1 60.1 44.3 31.7 23.5 12.7 0.0 10.9 41.1 
8 82.8 70.8 54.9 42.2 34.2 23.5 10.9 0.0 30.4 
9 113.1 101.1 85.3 72.4 64.4 53.7 41.1 30.4 0.0 
correlation [[1.         0.95997909]
 [0.95997909 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [2 2 2 ... 8 3 3] [[-32.397957  -36.385757 ]
 [ -2.9235983 -72.38726  ]
 [-20.033474  -46.055737 ]
 ...
 [-21.22144    50.21183  ]
 [ 37.119125    5.6060567]
 [  5.1190696  65.73237  ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0 45  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 61  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 45  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
-7.4  8.3  2.9 -8.6 -2.9 -26.7 -1.9  5.9  1.4 -12.6  1.1  11.0 -0.9 -0.5 -0.3  2.2 -6.2  21.3 -3.8  8.0 
-9.5  9.5  3.5 -10.9 -4.2 -33.9 -2.4  7.5  1.9 -16.2  1.4  16.1 -1.0 -0.7 -0.6  3.1 -7.7  26.8 -5.0  10.3 
-12.3  12.7  3.5 -13.9 -5.4 -43.5 -3.1  10.1  3.3 -20.9  1.6  18.4 -1.8 -1.0 -1.1  4.4 -11.8  35.5 -6.3  13.3 
-14.4  14.7  3.7 -16.0 -6.6 -51.2 -3.5  11.9  3.8 -24.6  1.8  25.2 -2.1 -1.3 -0.9  6.3 -13.0  40.8 -7.6  16.0 
-16.0  15.2  5.9 -17.5 -7.0 -56.1 -4.0  13.0  4.2 -26.9  2.0  25.8 -2.3 -1.5 -1.3  6.9 -15.0  45.7 -8.2  17.4 
-17.9  16.7  7.3 -19.7 -8.0 -62.6 -4.5  14.2  4.5 -30.1  2.5  29.8 -2.2 -1.6 -1.4  5.8 -16.0  51.0/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized
 -9.1  19.3 
-20.1  19.3  9.2 -22.4 -8.7 -70.5 -5.1  15.8  4.6 -33.9  2.9  32.5 -2.5 -1.6 -1.4  5.6 -17.7  57.6 -10.1  21.6 
-21.9  21.9  8.5 -24.7 -9.6 -77.3 -5.6  17.3  4.9 -37.3  3.1  34.9 -3.0 -1.7 -1.4  6.5 -19.4  62.7 -11.3  23.6 
-27.1  26.6  10.3 -30.4 -12.2 -95.7 -6.9  21.6  6.4 -46.3  3.7  44.0 -3.9 -2.2 -1.9  8.8 -24.5  77.9 -14.1  29.4 
Minimum distance of calculation. 
true answer: 2 
indices: 3 
distance: 3.1140876
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-7.4  8.3  2.9 -8.6 -2.9 -26.7 -1.9  5.9  1.4 -12.6  1.1  11.0 -0.9 -0.5 -0.3  2.2 -6.2  21.3 -3.8  8.0 
-9.5  9.5  3.5 -10.9 -4.2 -33.9 -2.4  7.5  1.9 -16.2  1.4  16.1 -1.0 -0.7 -0.6  3.1 -7.7  26.8 -5.0  10.3 
-12.3  12.7  3.5 -13.9 -5.4 -43.5 -3.1  10.1  3.3 -20.9  1.6  18.4 -1.8 -1.0 -1.1  4.4 -11.8  35.5 -6.3  13.3 
-14.4  14.7  3.7 -16.0 -6.6 -51.2 -3.5  11.9  3.8 -24.6  1.8  25.2 -2.1 -1.3 -0.9  6.3 -13.0  40.8 -7.6  16.0 
-16.0  15.2  5.9 -17.5 -7.0 -56.1 -4.0  13.0  4.2 -26.9  2.0  25.8 -2.3 -1.5 -1.3  6.9 -15.0  45.7 -8.2  17.4 
-17.9  16.7  7.3 -19.7 -8.0 -62.6 -4.5  14.2  4.5 -30.1  2.5  29.8 -2.2 -1.6 -1.4  5.8 -16.0  51.0 -9.1  19.3 
-20.1  19.3  9.2 -22.4 -8.7 -70.5 -5.1  15.8  4.6 -33.9  2.9  32.5 -2.5 -1.6 -1.4  5.6 -17.7  57.6 -10.1  21.6 
-21.9  21.9  8.5 -24.7 -9.6 -77.3 -5.6  17.3  4.9 -37.3  3.1  34.9 -3.0 -1.7 -1.4  6.5 -19.4  62.7 -11.3  23.6 
-27.1  26.6  10.3 -30.4 -12.2 -95.7 -6.9  21.6  6.4 -46.3  3.7  44.0 -3.9 -2.2 -1.9  8.8 -24.5  77.9 -14.1  29.4 
Minimum distance of calculation. 
true answer: 5 
indices: 4 
distance: 8.087563
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-7.4  8.3  2.9 -8.6 -2.9 -26.7 -1.9  5.9  1.4 -12.6  1.1  11.0 -0.9 -0.5 -0.3  2.2 -6.2  21.3 -3.8  8.0 
-9.5  9.5  3.5 -10.9 -4.2 -33.9 -2.4  7.5  1.9 -16.2  1.4  16.1 -1.0 -0.7 -0.6  3.1 -7.7  26.8 -5.0  10.3 
-12.3  12.7  3.5 -13.9 -5.4 -43.5 -3.1  10.1  3.3 -20.9  1.6  18.4 -1.8 -1.0 -1.1  4.4 -11.8  35.5 -6.3  13.3 
-14.4  14.7  3.7 -16.0 -6.6 -51.2 -3.5  11.9  3.8 -24.6  1.8  25.2 -2.1 -1.3 -0.9  6.3 -13.0  40.8 -7.6  16.0 
-16.0  15.2  5.9 -17.5 -7.0 -56.1 -4.0  13.0  4.2 -26.9  2.0  25.8 -2.3 -1.5 -1.3  6.9 -15.0  45.7 -8.2  17.4 
-17.9  16.7  7.3 -19.7 -8.0 -62.6 -4.5  14.2  4.5 -30.1  2.5  29.8 -2.2 -1.6 -1.4  5.8 -16.0  51.0 -9.1  19.3 
-20.1  19.3  9.2 -22.4 -8.7 -70.5 -5.1  15.8  4.6 -33.9  2.9  32.5 -2.5 -1.6 -1.4  5.6 -17.7  57.6 -10.1  21.6 
-21.9  21.9  8.5 -24.7 -9.6 -77.3 -5.6  17.3  4.9 -37.3  3.1  34.9 -3.0 -1.7 -1.4  6.5 -19.4  62.7 -11.3  23.6 
-27.1  26.6  10.3 -30.4 -12.2 -95.7 -6.9  21.6  6.4 -46.3  3.7  44.0 -3.9 -2.2 -1.9  8.8 -24.5  77.9 -14.1  29.4 
Minimum distance of calculation. 
true answer: 8 
indices: 8 
distance: 3.4346185
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-7.4  8.3  2.9 -8.6 -2.9 -26.7 -1.9  5.9  1.4 -12.6  1.1  11.0 -0.9 -0.5 -0.3  2.2 -6.2  21.3 -3.8  8.0 
-9.5  9.5  3.5 -10.9 -4.2 -33.9 -2.4  7.5  1.9 -16.2  1.4  16.1 -1.0 -0.7 -0.6  3.1 -7.7  26.8 -5.0  10.3 
-12.3  12.7  3.5 -13.9 -5.4 -43.5 -3.1  10.1  3.3 -20.9  1.6  18.4 -1.8 -1.0 -1.1  4.4 -11.8  35.5 -6.3  13.3 
-14.4  14.7  3.7 -16.0 -6.6 -51.2 -3.5  11.9  3.8 -24.6  1.8  25.2 -2.1 -1.3 -0.9  6.3 -13.0  40.8 -7.6  16.0 
-16.0  15.2  5.9 -17.5 -7.0 -56.1 -4.0  13.0  4.2 -26.9  2.0  25.8 -2.3 -1.5 -1.3  6.9 -15.0  45.7 -8.2  17.4 
-17.9  16.7  7.3 -19.7 -8.0 -62.6 -4.5  14.2  4.5 -30.1  2.5  29.8 -2.2 -1.6 -1.4  5.8 -16.0  51.0 -9.1  19.3 
-20.1  19.3  9.2 -22.4 -8.7 -70.5 -5.1  15.8  4.6 -33.9  2.9  32.5 -2.5 -1.6 -1.4  5.6 -17.7  57.6 -10.1  21.6 
-21.9  21.9  8.5 -24.7 -9.6 -77.3 -5.6  17.3  4.9 -37.3  3.1  34.9 -3.0 -1.7 -1.4  6.5 -19.4  62.7 -11.3  23.6 
-27.1  26.6  10.3 -30.4 -12.2 -95.7 -6.9  21.6  6.4 -46.3  3.7  44.0 -3.9 -2.2 -1.9  8.8 -24.5  77.9 -14.1  29.4 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 7.2479243
results (all): {'reconst_1x1_avg': 0.09375, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.1171875, 'cross_0x1_all': nan, 'cluster_avg': -0.1553902, 'cluster_all': array([-0.09802225, -0.29891762, -0.05703397, ...,  0.0389637 ,
       -0.16285436, -0.16590399], dtype=float32), 'magnitude_avg': 0.9599790931664716, 'magnitude_all': 0.9599790931664716, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.09375, 'cross_0x1_avg': 0.1171875, 'cluster_avg': -0.1553902, 'magnitude_avg': 0.9599790931664716, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 6, 4, 3, 3, 7, 6, 2, 9, 2, 2, 4, 1, 2, 5, 9, 2, 3, 6, 3, 3, 4, 7, 7,
        9, 5, 1, 8, 6, 6, 4, 5, 7, 5, 4, 4, 9, 1, 6, 7, 4, 1, 6, 7, 3, 1, 8, 7,
        4, 8, 4, 6, 2, 8, 7, 6, 1, 7, 8, 9, 6, 6, 6, 9, 5, 2, 6, 2, 6, 2, 5, 6,
        1, 6, 4, 7, 3, 5, 1, 6, 5, 6, 2, 5, 6, 9, 1, 5, 4, 2, 9, 8, 5, 2, 2, 5,
        6, 9, 1, 4, 9, 5, 2, 6, 6, 3, 1, 6, 5, 5, 9, 7, 6, 4, 8, 4, 3, 3, 7, 6,
        8, 9, 6, 6, 5, 4, 4, 8]), ['b2s', 'g6t', 'b4s', 'b3s', 'r3j', 'b7j', 'w6t', 'b2s', 'r9t', 'w2t', 'g2t', 'w4s', 'g1t', 'g2j', 'r5j', 'b9s', 'g2s', 'w3j', 'r6j', 'w3t', 'w3t', 'w4j', 'w7t', 'w7t', 'r9s', 'r5t', 'g1s', 'g8j', 'w6t', 'w6s', 'w4t', 'w5t', 'r7s', 'w5j', 'r4s', 'g4s', 'b9t', 'r1t', 'g6j', 'b7s', 'g4j', 'r1t', 'r6j', 'b7t', 'b3j', 'b1s', 'r8t', 'r7t', 'g4j', 'w8t', 'w4s', 'b6t', 'b2j', 'w8t', 'w7t', 'b6t', 'r1s', 'b7s', 'b8s', 'b9t', 'b6j', 'r6t', 'g6s', 'r9j', 'r5t', 'b2t', 'r6t', 'b2j', 'g6t', 'g2j', 'b5t', 'b6j', 'w1s', 'g6j', 'r4t', 'w7s', 'w3s', 'g5s', 'r1j', 'w6t', 'b5j', 'w6s', 'r2s', 'r5t', 'r6t', 'w9t', 'r1j', 'w5s', 'g4s', 'b2s', 'r9j', 'w8s', 'b5s', 'w2t', 'r2j', 'b5s', 'g6t', 'g9s', 'w1j', 'b4j', 'w9t', 'w5j', 'r2s', 'w6s', 'w6t', 'g3t', 'b1s', 'b6t', 'b5t', 'g5j', 'b9s', 'r7s', 'b6j', 'b4t', 'w8s', 'b4t', 'r3j', 'b3j', 'w7j', 'r6j', 'b8s', 'r9t', 'g6s', 'r6t', 'b5j', 'b4j', 'b4t', 'b8t']] 
label[0]: tensor([2, 6, 4, 3, 3, 7, 6, 2, 9, 2, 2, 4, 1, 2, 5, 9, 2, 3, 6, 3, 3, 4, 7, 7,
        9, 5, 1, 8, 6, 6, 4, 5, 7, 5, 4, 4, 9, 1, 6, 7, 4, 1, 6, 7, 3, 1, 8, 7,
        4, 8, 4, 6, 2, 8, 7, 6, 1, 7, 8, 9, 6, 6, 6, 9, 5, 2, 6, 2, 6, 2, 5, 6,
        1, 6, 4, 7, 3, 5, 1, 6, 5, 6, 2, 5, 6, 9, 1, 5, 4, 2, 9, 8, 5, 2, 2, 5,
        6, 9, 1, 4, 9, 5, 2, 6, 6, 3, 1, 6, 5, 5, 9, 7, 6, 4, 8, 4, 3, 3, 7, 6,
        8, 9, 6, 6, 5, 4, 4, 8]) 
label[1]: ['b2s', 'g6t', 'b4s', 'b3s', 'r3j', 'b7j', 'w6t', 'b2s', 'r9t', 'w2t', 'g2t', 'w4s', 'g1t', 'g2j', 'r5j', 'b9s', 'g2s', 'w3j', 'r6j', 'w3t', 'w3t', 'w4j', 'w7t', 'w7t', 'r9s', 'r5t', 'g1s', 'g8j', 'w6t', 'w6s', 'w4t', 'w5t', 'r7s', 'w5j', 'r4s', 'g4s', 'b9t', 'r1t', 'g6j', 'b7s', 'g4j', 'r1t', 'r6j', 'b7t', 'b3j', 'b1s', 'r8t', 'r7t', 'g4j', 'w8t', 'w4s', 'b6t', 'b2j', 'w8t', 'w7t', 'b6t', 'r1s', 'b7s', 'b8s', 'b9t', 'b6j', 'r6t', 'g6s', 'r9j', 'r5t', 'b2t', 'r6t', 'b2j', 'g6t', 'g2j', 'b5t', 'b6j', 'w1s', 'g6j', 'r4t', 'w7s', 'w3s', 'g5s', 'r1j', 'w6t', 'b5j', 'w6s', 'r2s', 'r5t', 'r6t', 'w9t', 'r1j', 'w5s', 'g4s', 'b2s', 'r9j', 'w8s', 'b5s', 'w2t', 'r2j', 'b5s', 'g6t', 'g9s', 'w1j', 'b4j', 'w9t', 'w5j', 'r2s', 'w6s', 'w6t', 'g3t', 'b1s', 'b6t', 'b5t', 'g5j', 'b9s', 'r7s', 'b6j', 'b4t', 'w8s', 'b4t', 'r3j', 'b3j', 'w7j', 'r6j', 'b8s', 'r9t', 'g6s', 'r6t', 'b5j', 'b4j', 'b4t', 'b8t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.62455994 -0.20391946 -0.3621902  ... -0.58597594  0.5812581
 -0.142861  ]
Average of silhouette coef: -0.008251
---
  0   1   2
0 0.0 60.6 37.6 
1 60.6 0.0 24.5 
2 37.6 24.5 0.0 
correlation [[ 1.        -0.1571935]
 [-0.1571935  1.       ]]
---
[[], [], []] [1 2 1 ... 1 0 2] [[ 13.635403    5.3580337]
 [-20.757019  -18.564323 ]
 [-23.761477   -9.007041 ]
 ...
 [ 12.829575   -3.7076187]
 [ 46.13914   -16.81207  ]
 [-19.386415   -7.4886765]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': -0.008251, 'cluster_all': array([-0.62455994, -0.20391946, -0.3621902 , ..., -0.58597594,
        0.5812581 , -0.142861  ], dtype=float32), 'magnitude_avg': 0.15719349982704567, 'magnitude_all': -0.15719349982704567, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': -0.008251, 'magnitude_avg': 0.15719349982704567, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 1, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_1', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1', 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 8, 1, 3, 7, 5, 6, 2, 4, 2, 6, 4, 9, 8, 8, 9, 8, 3, 8, 4, 8, 3, 9, 8,
        5, 8, 4, 2, 1, 5, 4, 1, 4, 7, 3, 4, 1, 6, 4, 7, 7, 5, 5, 4, 7, 1, 9, 7,
        8, 3, 6, 1, 3, 2, 5, 1, 5, 9, 4, 7, 9, 5, 2, 9, 1, 4, 3, 2, 9, 9, 7, 6,
        9, 1, 6, 2, 9, 8, 2, 6, 5, 3, 2, 4, 4, 2, 9, 7, 8, 5, 2, 2, 1, 6, 8, 1,
        2, 9, 4, 9, 7, 6, 4, 2, 9, 3, 2, 3, 1, 4, 3, 8, 9, 6, 7, 1, 7, 7, 4, 8,
        9, 3, 7, 1, 2, 1, 6, 3]), ['b2j', 'g8t', 'b1t', 'g3j', 'w7s', 'r5s', 'b6j', 'w2j', 'b4t', 'w2t', 'b6t', 'r4t', 'w9j', 'r8j', 'w8j', 'w9s', 'g8j', 'w3j', 'r8s', 'b4j', 'b8j', 'r3t', 'g9j', 'w8j', 'g5s', 'r8s', 'g4j', 'b2s', 'g1t', 'r5s', 'b4t', 'b1s', 'g4t', 'b7s', 'g3j', 'w4s', 'w1j', 'w6t', 'b4t', 'r7t', 'g7t', 'w5t', 'g5j', 'r4j', 'b7j', 'g1s', 'w9t', 'b7j', 'b8s', 'b3t', 'b6s', 'g1t', 'w3s', 'w2j', 'g5t', 'r1s', 'r5j', 'b9j', 'g4s', 'g7s', 'g9j', 'w5s', 'g2j', 'b9t', 'w1j', 'r4t', 'r3s', 'b2t', 'w9s', 'w9j', 'b7j', 'w6j', 'r9j', 'g1t', 'b6j', 'g2s', 'b9j', 'g8j', 'r2s', 'g6s', 'r5s', 'b3s', 'g2j', 'w4j', 'b4j', 'g2s', 'g9j', 'g7j', 'g8j', 'w5s', 'r2t', 'r2s', 'g1t', 'w6s', 'w8s', 'b1t', 'g2t', 'b9j', 'w4j', 'g9s', 'w7t', 'w6t', 'r4t', 'b2j', 'g9t', 'r3j', 'r2j', 'g3t', 'w1t', 'w4s', 'g3j', 'b8j', 'r9j', 'g6s', 'w7s', 'b1t', 'w7s', 'r7s', 'b4j', 'w8s', 'g9j', 'r3s', 'w7t', 'b1j', 'r2t', 'w1s', 'g6t', 'g3t']] 
label[0]: tensor([2, 8, 1, 3, 7, 5, 6, 2, 4, 2, 6, 4, 9, 8, 8, 9, 8, 3, 8, 4, 8, 3, 9, 8,
        5, 8, 4, 2, 1, 5, 4, 1, 4, 7, 3, 4, 1, 6, 4, 7, 7, 5, 5, 4, 7, 1, 9, 7,
        8, 3, 6, 1, 3, 2, 5, 1, 5, 9, 4, 7, 9, 5, 2, 9, 1, 4, 3, 2, 9, 9, 7, 6,
        9, 1, 6, 2, 9, 8, 2, 6, 5, 3, 2, 4, 4, 2, 9, 7, 8, 5, 2, 2, 1, 6, 8, 1,
        2, 9, 4, 9, 7, 6, 4, 2, 9, 3, 2, 3, 1, 4, 3, 8, 9, 6, 7, 1, 7, 7, 4, 8,
        9, 3, 7, 1, 2, 1, 6, 3]) 
label[1]: ['b2j', 'g8t', 'b1t', 'g3j', 'w7s', 'r5s', 'b6j', 'w2j', 'b4t', 'w2t', 'b6t', 'r4t', 'w9j', 'r8j', 'w8j', 'w9s', 'g8j', 'w3j', 'r8s', 'b4j', 'b8j', 'r3t', 'g9j', 'w8j', 'g5s', 'r8s', 'g4j', 'b2s', 'g1t', 'r5s', 'b4t', 'b1s', 'g4t', 'b7s', 'g3j', 'w4s', 'w1j', 'w6t', 'b4t', 'r7t', 'g7t', 'w5t', 'g5j', 'r4j', 'b7j', 'g1s', 'w9t', 'b7j', 'b8s', 'b3t', 'b6s', 'g1t', 'w3s', 'w2j', 'g5t', 'r1s', 'r5j', 'b9j', 'g4s', 'g7s', 'g9j', 'w5s', 'g2j', 'b9t', 'w1j', 'r4t', 'r3s', 'b2t', 'w9s', 'w9j', 'b7j', 'w6j', 'r9j', 'g1t', 'b6j', 'g2s', 'b9j', 'g8j', 'r2s', 'g6s', 'r5s', 'b3s', 'g2j', 'w4j', 'b4j', 'g2s', 'g9j', 'g7j', 'g8j', 'w5s', 'r2t', 'r2s', 'g1t', 'w6s', 'w8s', 'b1t', 'g2t', 'b9j', 'w4j', 'g9s', 'w7t', 'w6t', 'r4t', 'b2j', 'g9t', 'r3j', 'r2j', 'g3t', 'w1t', 'w4s', 'g3j', 'b8j', 'r9j', 'g6s', 'w7s', 'b1t', 'w7s', 'r7s', 'b4j', 'w8s', 'g9j', 'r3s', 'w7t', 'b1j', 'r2t', 'w1s', 'g6t', 'g3t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.07252097  0.11149947  0.05914552 ... -0.06166066  0.0947753
 -0.04756402]
Average of silhouette coef: 0.064189926
---
  0   1   2   3
0 0.0 2.7 2.3 2.2 
1 2.7 0.0 3.6 3.6 
2 2.3 3.6 0.0 3.8 
3 2.2 3.6 3.8 0.0 
correlation [[ 1.         -0.61514904]
 [-0.61514904  1.        ]]
---
[[], [], [], []] [1 2 1 ... 0 2 0] [[ -1.3866692 -37.63024  ]
 [-27.275997   18.916376 ]
 [ 66.32291   -34.42325  ]
 ...
 [ 37.842747   29.712101 ]
 [ 34.490734  -19.65391  ]
 [ 28.915558   -9.158849 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.064189926, 'cluster_all': array([ 0.07252097,  0.11149947,  0.05914552, ..., -0.06166066,
        0.0947753 , -0.04756402], dtype=float32), 'magnitude_avg': 0.6151490397412321, 'magnitude_all': -0.6151490397412321, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.064189926, 'magnitude_avg': 0.6151490397412321, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([9, 6, 8, 9, 1, 4, 8, 9, 7, 6, 5, 3, 4, 2, 8, 4, 6, 9, 6, 3, 9, 9, 4, 8,
        5, 8, 9, 2, 1, 5, 8, 1, 6, 1, 6, 7, 7, 3, 1, 2, 4, 5, 8, 7, 9, 5, 9, 5,
        5, 4, 1, 9, 4, 5, 5, 5, 2, 2, 7, 2, 5, 1, 8, 2, 6, 2, 4, 9, 3, 8, 8, 5,
        9, 1, 7, 4, 2, 1, 7, 3, 9, 8, 5, 5, 4, 1, 4, 8, 2, 1, 3, 7, 1, 9, 9, 9,
        3, 7, 7, 4, 1, 8, 5, 3, 4, 7, 7, 2, 4, 8, 3, 7, 3, 8, 4, 5, 7, 5, 6, 9,
        7, 1, 8, 2, 7, 7, 2, 7]), ['g9s', 'w6s', 'w8t', 'r9j', 'g1t', 'g4s', 'b8t', 'r9s', 'r7s', 'w6t', 'r5s', 'w3j', 'w4t', 'w2t', 'r8t', 'g4t', 'r6t', 'w9j', 'g6s', 'w3j', 'r9t', 'w9s', 'b4j', 'g8j', 'b5j', 'b8j', 'b9j', 'g2s', 'r1s', 'r5j', 'g8s', 'r1t', 'b6s', 'b1t', 'r6s', 'w7j', 'r7j', 'w3j', 'b1s', 'w2s', 'w4s', 'w5s', 'w8j', 'g7j', 'r9s', 'b5t', 'w9t', 'w5t', 'g5t', 'r4t', 'g1t', 'r9s', 'w4j', 'b5s', 'b5s', 'b5t', 'b2j', 'g2j', 'r7s', 'r2t', 'r5t', 'w1s', 'g8j', 'w2s', 'g6s', 'b2j', 'w4j', 'r9t', 'b3t', 'g8t', 'b8j', 'r5s', 'r9t', 'r1j', 'b7j', 'g4s', 'b2s', 'g1s', 'b7j', 'b3t', 'w9t', 'r8s', 'r5t', 'g5s', 'b4j', 'b1t', 'b4s', 'r8j', 'r2t', 'r1s', 'r3s', 'b7s', 'w1t', 'b9j', 'r9t', 'b9j', 'r3t', 'w7j', 'b7s', 'g4s', 'b1t', 'g8t', 'b5t', 'w3s', 'g4t', 'w7t', 'w7s', 'g2t', 'r4t', 'r8s', 'w3s', 'g7j', 'w3j', 'b8s', 'w4j', 'w5t', 'b7t', 'r5s', 'w6t', 'w9t', 'b7j', 'b1t', 'g8s', 'b2t', 'r7s', 'b7j', 'b2s', 'b7s']] 
label[0]: tensor([9, 6, 8, 9, 1, 4, 8, 9, 7, 6, 5, 3, 4, 2, 8, 4, 6, 9, 6, 3, 9, 9, 4, 8,
        5, 8, 9, 2, 1, 5, 8, 1, 6, 1, 6, 7, 7, 3, 1, 2, 4, 5, 8, 7, 9, 5, 9, 5,
        5, 4, 1, 9, 4, 5, 5, 5, 2, 2, 7, 2, 5, 1, 8, 2, 6, 2, 4, 9, 3, 8, 8, 5,
        9, 1, 7, 4, 2, 1, 7, 3, 9, 8, 5, 5, 4, 1, 4, 8, 2, 1, 3, 7, 1, 9, 9, 9,
        3, 7, 7, 4, 1, 8, 5, 3, 4, 7, 7, 2, 4, 8, 3, 7, 3, 8, 4, 5, 7, 5, 6, 9,
        7, 1, 8, 2, 7, 7, 2, 7]) 
label[1]: ['g9s', 'w6s', 'w8t', 'r9j', 'g1t', 'g4s', 'b8t', 'r9s', 'r7s', 'w6t', 'r5s', 'w3j', 'w4t', 'w2t', 'r8t', 'g4t', 'r6t', 'w9j', 'g6s', 'w3j', 'r9t', 'w9s', 'b4j', 'g8j', 'b5j', 'b8j', 'b9j', 'g2s', 'r1s', 'r5j', 'g8s', 'r1t', 'b6s', 'b1t', 'r6s', 'w7j', 'r7j', 'w3j', 'b1s', 'w2s', 'w4s', 'w5s', 'w8j', 'g7j', 'r9s', 'b5t', 'w9t', 'w5t', 'g5t', 'r4t', 'g1t', 'r9s', 'w4j', 'b5s', 'b5s', 'b5t', 'b2j', 'g2j', 'r7s', 'r2t', 'r5t', 'w1s', 'g8j', 'w2s', 'g6s', 'b2j', 'w4j', 'r9t', 'b3t', 'g8t', 'b8j', 'r5s', 'r9t', 'r1j', 'b7j', 'g4s', 'b2s', 'g1s', 'b7j', 'b3t', 'w9t', 'r8s', 'r5t', 'g5s', 'b4j', 'b1t', 'b4s', 'r8j', 'r2t', 'r1s', 'r3s', 'b7s', 'w1t', 'b9j', 'r9t', 'b9j', 'r3t', 'w7j', 'b7s', 'g4s', 'b1t', 'g8t', 'b5t', 'w3s', 'g4t', 'w7t', 'w7s', 'g2t', 'r4t', 'r8s', 'w3s', 'g7j', 'w3j', 'b8s', 'w4j', 'w5t', 'b7t', 'r5s', 'w6t', 'w9t', 'b7j', 'b1t', 'g8s', 'b2t', 'r7s', 'b7j', 'b2s', 'b7s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([7, 8, 4, 1, 6, 6, 6, 7, 8, 3, 4, 5, 2, 2, 1, 2, 5, 3, 9, 5, 3, 1, 5, 9,
        5, 5, 6, 8, 4, 2, 9, 9, 7, 9, 2, 2, 7, 3, 4, 7, 8, 8, 1, 2, 1, 7, 4, 3,
        3, 6, 9, 2, 6, 4, 7, 6, 5, 3, 7, 5, 4, 2, 7, 2, 8, 3, 5, 9, 2, 8, 2, 4,
        3, 8, 2, 6, 4, 9, 4, 9, 9, 5, 3, 2, 2, 7, 1, 3, 2, 6, 2, 7, 7, 4, 2, 5,
        8, 7, 4, 4, 4, 5, 1, 9, 8, 8, 1, 4, 9, 5, 9, 6, 1, 1, 2, 5, 4, 7, 8, 6,
        4, 7, 9, 2, 1, 9, 1, 1])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[13  0  0  0  0  0  0  0  0]
 [20  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]
 [17  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]
 [15  0  0  0  0  0  0  0  0]
 [12  0  0  0  0  0  0  0  0]
 [15  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([1, 7, 2, 4, 2, 1, 6, 8, 2, 1, 4, 2, 9, 7, 2, 5, 1, 4, 1, 7, 3, 2, 4, 2,
        5, 4, 1, 7, 1, 9, 2, 1, 1, 4, 4, 2, 7, 3, 5, 2, 9, 4, 7, 6, 8, 1, 7, 4,
        5, 3, 4, 4, 9, 3, 7, 4, 6, 8, 3, 6, 5, 8, 2, 9, 9, 9, 4, 6, 2, 1, 2, 6,
        1, 9, 8, 5, 2, 5, 7, 1, 7, 2, 3, 4, 9, 8, 8, 1, 4, 1, 3, 1, 9, 5, 8, 1,
        6, 7, 2, 8, 1, 1, 2, 9, 1, 6, 5, 2, 8, 7, 6, 4, 7, 2, 5, 9, 4, 2, 3, 4,
        1, 7, 5, 3, 8, 2, 1, 6])
Accuracy (count): tensor(22) 
Accuracy (ratio) tensor(0.1719)
Accuracy:
 [[22  0  0  0  0  0  0  0  0]
 [21  0  0  0  0  0  0  0  0]
 [ 9  0  0  0  0  0  0  0  0]
 [18  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]
 [10  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]
 [12  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [ 0.05771805  0.17154099  0.08040939 ... -0.07027671 -0.10022703
  0.19488263]
Average of silhouette coef: 0.056819536
---
  1   2   3   4   5   6   7   8   9
1 0.0 3.5 6.5 6.4 5.9 4.9 5.9 5.9 6.9 
2 3.5 0.0 5.5 4.5 4.7 2.8 5.0 3.9 5.1 
3 6.5 5.5 0.0 6.0 2.8 5.5 6.4 3.9 5.8 
4 6.4 4.5 6.0 0.0 4.0 3.4 3.9 3.2 1.7 
5 5.9 4.7 2.8 4.0 0.0 4.1 5.0 2.9 4.1 
6 4.9 2.8 5.5 3.4 4.1 0.0 5.5 3.2 4.4 
7 5.9 5.0 6.4 3.9 5.0 5.5 0.0 5.3 3.2 
8 5.9 3.9 3.9 3.2 2.9 3.2 5.3 0.0 3.3 
9 6.9 5.1 5.8 1.7 4.1 4.4 3.2 3.3 0.0 
correlation [[1.         0.22966651]
 [0.22966651 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [9 6 8 ... 7 9 6] [[ -6.690189   32.834843 ]
 [  2.888959  -20.60721  ]
 [-24.434683   18.698586 ]
 ...
 [  8.934053   16.234907 ]
 [ -5.2425094  69.23362  ]
 [  3.4444706 -24.191317 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[45  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [61  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [45  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.1  0.1 -0.0  0.1  0.4 -0.0 -0.1  0.1  0.3 -0.2  0.2  0.0 -0.2  0.0 -0.0  0.1 -0.3  0.3  0.4  0.0 
 0.6 -0.1 -0.0  0.4 -0.0 -0.1  0.3  0.3  0.1 -0.9  0.2 -1.1  0.4 -0.1 -1.6 -0.3  0.1 -0.0  0.4  0.2 
-1.5 -0.0  0.0 -0.2  0.4 -0.0 -4.5  0.2  0.3 -0.3 -0.7 -0.9 -0.3 -0.2 -0.7 -0.1 -0.1  0.2  0.5 -0.2 
-3.0 -0.0  0.1  1.2 -0.1  0.0  0.9  0.0  0.1 -0.1 -0.4  0.1  0.4 -0.1  0.4 -0.0 -0.1 -0.0  0.5  0.1 
-2.0 -0.1  0.0  0.2  0.3  0.0 -2.5  0.1  0.3 -0.3 -1.3 -0.3  0.9 -0.2  0.4 -0.3 -0.1  0.0  0.6 -0.1 
-0.8 -0.2 -0.1  1.1 -0.1 -0.1  0.5  0.0  0.2 -1.0 -1.5 -1.8 -0.2 -0.2 -0.5  0.1 -0.0 -0.1  0.3  0.1 
-1.9  0.1  0.0 -0.8 -0.0 -0.0  0.5  0.1  0.2 -0.7  0.3  2.9 -0.0 -0.1 -0.4 -0.1 -0.0  0.1  0.4  0.0 
-2.4  0.0 -0.0  0.3  0.1 -0.0 -1.1  0.2  0.3  0.0  0.2 -1.9  0.1 -0.3 -0.1 -0.2 -0.1  0.1  0.4 -0.1 
-3.7  0.1 -0.0  0.1 -0.1 -0.0  0.6 -0.1  0.1  0.1 -0.0  0.6 -0.2 -0.1  0.0  0.0 -0.1 -0.0  0.6  0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 3.3469083
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.1  0.1 -0.0  0.1  0.4 -0.0 -0.1  0.1  0.3 -0.2  0.2  0.0 -0.2  0.0 -0.0  0.1 -0.3  0.3  0.4  0.0 
 0.6 -0.1 -0.0  0.4 -0.0 -0.1  0.3  0.3  0.1 -0.9  0.2 -1.1  0.4 -0.1 -1.6 -0.3  0.1 -0.0  0.4  0.2 
-1.5 -0.0  0.0 -0.2  0.4 -0.0 -4.5  0.2  0.3 -0.3 -0.7 -0.9 -0.3 -0.2 -0.7 -0.1 -0.1  0.2  0.5 -0.2 
-3.0 -0.0  0.1  1.2 -0.1  0.0  0.9  0.0  0.1 -0.1 -0.4  0.1  0.4 -0.1  0.4 -0.0 -0.1 -0.0  0.5  0.1 
-2.0 -0.1  0.0  0.2  0.3  0.0 -2.5  0.1  0.3 -0.3 -1.3 -0.3  0.9 -0.2  0.4 -0.3 -0.1  0.0  0.6 -0.1 
-0.8 -0.2 -0.1  1.1 -0.1 -0.1  0.5  0.0  0.2 -1.0 -1.5 -1.8 -0.2 -0.2 -0.5  0.1 -0.0 -0.1  0.3  0.1 
-1.9  0.1  0.0 -0.8 -0.0 -0.0  0.5  0.1  0.2 -0.7  0.3  2.9 -0.0 -0.1 -0.4 -0.1 -0.0  0.1  0.4  0.0 
-2.4  0.0 -0.0  0.3  0.1 -0.0 -1.1  0.2  0.3  0.0  0.2 -1.9  0.1 -0.3 -0.1 -0.2 -0.1  0.1  0.4 -0.1 
-3.7  0.1 -0.0  0.1 -0.1 -0.0  0.6 -0.1  0.1  0.1 -0.0  0.6 -0.2 -0.1  0.0  0.0 -0.1 -0.0  0.6  0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 3.1807506
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.1  0.1 -0.0  0.1  0.4 -0.0 -0.1  0.1  0.3 -0.2  0.2  0.0 -0.2  0.0 -0.0  0.1 -0.3  0.3  0.4  0.0 
 0.6 -0.1 -0.0  0.4 -0.0 -0.1  0.3  0.3  0.1 -0.9  0.2 -1.1  0.4 -0.1 -1.6 -0.3  0.1 -0.0  0.4  0.2 
-1.5 -0.0  0.0 -0.2  0.4 -0.0 -4.5  0.2  0.3 -0.3 -0.7 -0.9 -0.3 -0.2 -0.7 -0.1 -0.1  0.2  0.5 -0.2 
-3.0 -0.0  0.1  1.2 -0.1  0.0  0.9  0.0  0.1 -0.1 -0.4  0.1  0.4 -0.1  0.4 -0.0 -0.1 -0.0  0.5  0.1 
-2.0 -0.1  0.0  0.2  0.3  0.0 -2.5  0.1  0.3 -0.3 -1.3 -0.3  0.9 -0.2  0.4 -0.3 -0.1  0.0  0.6 -0.1 
-0.8 -0.2 -0.1  1.1 -0.1 -0.1  0.5  0.0  0.2 -1.0 -1.5 -1.8 -0.2 -0.2 -0.5  0.1 -0.0 -0.1  0.3  0.1 
-1.9  0.1  0.0 -0.8 -0.0 -0.0  0.5  0.1  0.2 -0.7  0.3  2.9 -0.0 -0.1 -0.4 -0.1 -0.0  0.1  0.4  0.0 
-2.4  0.0 -0.0  0.3  0.1 -0.0 -1.1  0.2  0.3  0.0  0.2 -1.9  0.1 -0.3 -0.1 -0.2 -0.1  0.1  0.4 -0.1 
-3.7  0.1 -0.0  0.1 -0.1 -0.0  0.6 -0.1  0.1  0.1 -0.0  0.6 -0.2 -0.1  0.0  0.0 -0.1 -0.0  0.6  0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 9 
distance: 3.2540364
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.1  0.1 -0.0  0.1  0.4 -0.0 -0.1  0.1  0.3 -0.2  0.2  0.0 -0.2  0.0 -0.0  0.1 -0.3  0.3  0.4  0.0 
 0.6 -0.1 -0.0  0.4 -0.0 -0.1  0.3  0.3  0.1 -0.9  0.2 -1.1  0.4 -0.1 -1.6 -0.3  0.1 -0.0  0.4  0.2 
-1.5 -0.0  0.0 -0.2  0.4 -0.0 -4.5  0.2  0.3 -0.3 -0.7 -0.9 -0.3 -0.2 -0.7 -0.1 -0.1  0.2  0.5 -0.2 
-3.0 -0.0  0.1  1.2 -0.1  0.0  0.9  0.0  0.1 -0.1 -0.4  0.1  0.4 -0.1  0.4 -0.0 -0.1 -0.0  0.5  0.1 
-2.0 -0.1  0.0  0.2  0.3  0.0 -2.5  0.1  0.3 -0.3 -1.3 -0.3  0.9 -0.2  0.4 -0.3 -0.1  0.0  0.6 -0.1 
-0.8 -0.2 -0.1  1.1 -0.1 -0.1  0.5  0.0  0.2 -1.0 -1.5 -1.8 -0.2 -0.2 -0.5  0.1 -0.0 -0.1  0.3  0.1 
-1.9  0.1  0.0 -0.8 -0.0 -0.0  0.5  0.1  0.2 -0.7  0.3  2.9 -0.0 -0.1 -0.4 -0.1 -0.0  0.1  0.4  0.0 
-2.4  0.0 -0.0  0.3  0.1 -0.0 -1.1  0.2  0.3  0.0  0.2 -1.9  0.1 -0.3 -0.1 -0.2 -0.1  0.1  0.4 -0.1 
-3.7  0.1 -0.0  0.1 -0.1 -0.0  0.6 -0.1  0.1  0.1 -0.0  0.6 -0.2 -0.1  0.0  0.0 -0.1 -0.0  0.6  0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 3 
distance: 4.7431917
results (all): {'reconst_0x0_avg': 0.1015625, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.171875, 'cross_1x0_all': nan, 'cluster_avg': 0.056819536, 'cluster_all': array([ 0.05771805,  0.17154099,  0.08040939, ..., -0.07027671,
       -0.10022703,  0.19488263], dtype=float32), 'magnitude_avg': 0.22966651212253067, 'magnitude_all': 0.22966651212253067, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1015625, 'cross_1x0_avg': 0.171875, 'cluster_avg': 0.056819536, 'magnitude_avg': 0.22966651212253067, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([4, 7, 1, 8, 3, 4, 5, 9, 9, 4, 7, 8, 2, 7, 6, 6, 4, 2, 5, 7, 3, 2, 2, 9,
        4, 1, 4, 6, 4, 7, 7, 4, 7, 1, 5, 4, 1, 9, 2, 6, 7, 1, 1, 7, 3, 9, 7, 7,
        3, 6, 7, 6, 9, 1, 6, 2, 3, 9, 9, 3, 2, 7, 5, 4, 6, 3, 9, 5, 9, 5, 7, 5,
        2, 1, 1, 8, 9, 3, 7, 6, 7, 9, 9, 8, 1, 4, 2, 9, 3, 9, 3, 1, 9, 7, 7, 8,
        7, 2, 8, 2, 6, 9, 5, 5, 6, 3, 7, 4, 9, 9, 6, 5, 8, 1, 2, 9, 6, 1, 2, 2,
        9, 8, 4, 8, 7, 7, 1, 3]), ['g4t', 'r7s', 'b1j', 'r8s', 'w3s', 'g4j', 'w5t', 'g9s', 'g9s', 'b4t', 'w7s', 'w8s', 'g2s', 'w7t', 'g6j', 'b6s', 'r4t', 'w2s', 'b5s', 'g7t', 'r3j', 'w2t', 'w2s', 'b9s', 'g4s', 'g1t', 'b4s', 'b6s', 'b4j', 'w7t', 'b7s', 'r4j', 'b7s', 'g1t', 'w5t', 'r4j', 'r1s', 'g9s', 'b2t', 'g6t', 'b7s', 'w1t', 'r1s', 'r7t', 'r3j', 'b9j', 'g7j', 'g7s', 'g3j', 'r6j', 'b7s', 'r6j', 'g9t', 'b1s', 'r6j', 'b2s', 'g3j', 'r9t', 'g9s', 'w3t', 'w2t', 'r7t', 'r5t', 'r4t', 'r6j', 'g3t', 'b9s', 'w5j', 'w9t', 'b5t', 'r7j', 'r5s', 'r2t', 'b1s', 'b1j', 'g8s', 'g9t', 'r3t', 'r7j', 'b6j', 'b7t', 'r9j', 'w9j', 'w8t', 'r1t', 'r4t', 'r2j', 'g9t', 'g3s', 'w9j', 'g3t', 'w1t', 'w9j', 'r7t', 'w7s', 'g8j', 'w7s', 'b2j', 'r8t', 'w2j', 'b6t', 'b9t', 'w5j', 'w5j', 'b6t', 'b3s', 'g7j', 'b4j', 'g9t', 'b9j', 'w6t', 'w5s', 'r8j', 'b1s', 'g2j', 'w9s', 'b6t', 'b1j', 'w2s', 'b2s', 'r9j', 'r8s', 'b4j', 'r8j', 'g7s', 'w7j', 'r1j', 'b3j']] 
label[0]: tensor([4, 7, 1, 8, 3, 4, 5, 9, 9, 4, 7, 8, 2, 7, 6, 6, 4, 2, 5, 7, 3, 2, 2, 9,
        4, 1, 4, 6, 4, 7, 7, 4, 7, 1, 5, 4, 1, 9, 2, 6, 7, 1, 1, 7, 3, 9, 7, 7,
        3, 6, 7, 6, 9, 1, 6, 2, 3, 9, 9, 3, 2, 7, 5, 4, 6, 3, 9, 5, 9, 5, 7, 5,
        2, 1, 1, 8, 9, 3, 7, 6, 7, 9, 9, 8, 1, 4, 2, 9, 3, 9, 3, 1, 9, 7, 7, 8,
        7, 2, 8, 2, 6, 9, 5, 5, 6, 3, 7, 4, 9, 9, 6, 5, 8, 1, 2, 9, 6, 1, 2, 2,
        9, 8, 4, 8, 7, 7, 1, 3]) 
label[1]: ['g4t', 'r7s', 'b1j', 'r8s', 'w3s', 'g4j', 'w5t', 'g9s', 'g9s', 'b4t', 'w7s', 'w8s', 'g2s', 'w7t', 'g6j', 'b6s', 'r4t', 'w2s', 'b5s', 'g7t', 'r3j', 'w2t', 'w2s', 'b9s', 'g4s', 'g1t', 'b4s', 'b6s', 'b4j', 'w7t', 'b7s', 'r4j', 'b7s', 'g1t', 'w5t', 'r4j', 'r1s', 'g9s', 'b2t', 'g6t', 'b7s', 'w1t', 'r1s', 'r7t', 'r3j', 'b9j', 'g7j', 'g7s', 'g3j', 'r6j', 'b7s', 'r6j', 'g9t', 'b1s', 'r6j', 'b2s', 'g3j', 'r9t', 'g9s', 'w3t', 'w2t', 'r7t', 'r5t', 'r4t', 'r6j', 'g3t', 'b9s', 'w5j', 'w9t', 'b5t', 'r7j', 'r5s', 'r2t', 'b1s', 'b1j', 'g8s', 'g9t', 'r3t', 'r7j', 'b6j', 'b7t', 'r9j', 'w9j', 'w8t', 'r1t', 'r4t', 'r2j', 'g9t', 'g3s', 'w9j', 'g3t', 'w1t', 'w9j', 'r7t', 'w7s', 'g8j', 'w7s', 'b2j', 'r8t', 'w2j', 'b6t', 'b9t', 'w5j', 'w5j', 'b6t', 'b3s', 'g7j', 'b4j', 'g9t', 'b9j', 'w6t', 'w5s', 'r8j', 'b1s', 'g2j', 'w9s', 'b6t', 'b1j', 'w2s', 'b2s', 'r9j', 'r8s', 'b4j', 'r8j', 'g7s', 'w7j', 'r1j', 'b3j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.01382616 -0.01931386  0.00594509 ... -0.00258791 -0.01522056
 -0.0188203 ]
Average of silhouette coef: -0.0058249095
---
  0   1   2
0 0.0 0.3 0.3 
1 0.3 0.0 0.4 
2 0.3 0.4 0.0 
correlation [[ 1.         -0.92340723]
 [-0.92340723  1.        ]]
---
[[], [], []] [2 1 0 ... 2 2 1] [[ 11.77455   52.211903]
 [ -5.700719  54.491905]
 [-16.178913 -52.063873]
 ...
 [-16.67701   58.302814]
 [ -6.679115  -5.402719]
 [ 23.539616 -34.69563 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.0058249095, 'cluster_all': array([-0.01382616, -0.01931386,  0.00594509, ..., -0.00258791,
       -0.01522056, -0.0188203 ], dtype=float32), 'magnitude_avg': 0.9234072281126454, 'magnitude_all': -0.9234072281126454, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.0058249095, 'magnitude_avg': 0.9234072281126454, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 1, 3, 7, 8, 3, 6, 3, 3, 9, 4, 1, 1, 4, 4, 2, 7, 9, 1, 7, 8, 5, 6, 9,
        7, 2, 7, 9, 3, 7, 7, 4, 1, 9, 3, 7, 4, 2, 3, 4, 5, 8, 5, 9, 7, 4, 2, 7,
        3, 2, 6, 9, 5, 5, 2, 8, 1, 7, 2, 1, 5, 1, 7, 5, 9, 2, 2, 1, 2, 7, 5, 8,
        2, 9, 7, 5, 8, 3, 9, 4, 2, 9, 2, 4, 1, 7, 3, 8, 1, 8, 1, 7, 2, 8, 4, 7,
        3, 9, 9, 4, 4, 5, 5, 9, 1, 9, 5, 8, 4, 9, 4, 3, 7, 4, 4, 5, 9, 4, 9, 1,
        6, 3, 7, 1, 1, 1, 1, 1]), ['w6s', 'w1s', 'g3s', 'w7s', 'r8t', 'w3j', 'b6j', 'g3t', 'b3j', 'w9t', 'w4t', 'r1t', 'w1t', 'w4s', 'b4j', 'g2t', 'b7s', 'w9j', 'g1t', 'g7s', 'w8j', 'w5s', 'b6s', 'g9t', 'g7s', 'g2s', 'r7t', 'r9t', 'b3t', 'w7j', 'g7s', 'b4s', 'b1s', 'g9j', 'w3s', 'b7j', 'w4s', 'w2j', 'g3t', 'b4t', 'b5t', 'r8j', 'r5s', 'g9j', 'w7j', 'w4t', 'w2t', 'g7t', 'g3s', 'r2s', 'w6s', 'w9s', 'r5j', 'g5s', 'g2t', 'b8j', 'w1t', 'w7t', 'b2s', 'g1t', 'g5s', 'w1s', 'w7t', 'b5s', 'g9t', 'r2s', 'r2t', 'b1t', 'g2t', 'g7s', 'r5j', 'b8s', 'w2j', 'g9s', 'w7t', 'g5s', 'b8s', 'r3t', 'r9t', 'w4j', 'g2t', 'b9t', 'w2s', 'w4t', 'b1s', 'w7j', 'r3t', 'r8j', 'r1j', 'b8j', 'g1j', 'g7j', 'b2j', 'b8t', 'w4t', 'g7t', 'b3s', 'w9t', 'b9j', 'w4t', 'g4t', 'b5j', 'b5t', 'b9j', 'g1s', 'b9s', 'g5s', 'w8j', 'w4s', 'g9s', 'g4s', 'b3s', 'b7t', 'r4s', 'w4t', 'w5j', 'w9j', 'w4t', 'r9t', 'r1t', 'r6t', 'r3s', 'g7s', 'r1s', 'r1t', 'r1s', 'w1t', 'g1s']] 
label[0]: tensor([6, 1, 3, 7, 8, 3, 6, 3, 3, 9, 4, 1, 1, 4, 4, 2, 7, 9, 1, 7, 8, 5, 6, 9,
        7, 2, 7, 9, 3, 7, 7, 4, 1, 9, 3, 7, 4, 2, 3, 4, 5, 8, 5, 9, 7, 4, 2, 7,
        3, 2, 6, 9, 5, 5, 2, 8, 1, 7, 2, 1, 5, 1, 7, 5, 9, 2, 2, 1, 2, 7, 5, 8,
        2, 9, 7, 5, 8, 3, 9, 4, 2, 9, 2, 4, 1, 7, 3, 8, 1, 8, 1, 7, 2, 8, 4, 7,
        3, 9, 9, 4, 4, 5, 5, 9, 1, 9, 5, 8, 4, 9, 4, 3, 7, 4, 4, 5, 9, 4, 9, 1,
        6, 3, 7, 1, 1, 1, 1, 1]) 
label[1]: ['w6s', 'w1s', 'g3s', 'w7s', 'r8t', 'w3j', 'b6j', 'g3t', 'b3j', 'w9t', 'w4t', 'r1t', 'w1t', 'w4s', 'b4j', 'g2t', 'b7s', 'w9j', 'g1t', 'g7s', 'w8j', 'w5s', 'b6s', 'g9t', 'g7s', 'g2s', 'r7t', 'r9t', 'b3t', 'w7j', 'g7s', 'b4s', 'b1s', 'g9j', 'w3s', 'b7j', 'w4s', 'w2j', 'g3t', 'b4t', 'b5t', 'r8j', 'r5s', 'g9j', 'w7j', 'w4t', 'w2t', 'g7t', 'g3s', 'r2s', 'w6s', 'w9s', 'r5j', 'g5s', 'g2t', 'b8j', 'w1t', 'w7t', 'b2s', 'g1t', 'g5s', 'w1s', 'w7t', 'b5s', 'g9t', 'r2s', 'r2t', 'b1t', 'g2t', 'g7s', 'r5j', 'b8s', 'w2j', 'g9s', 'w7t', 'g5s', 'b8s', 'r3t', 'r9t', 'w4j', 'g2t', 'b9t', 'w2s', 'w4t', 'b1s', 'w7j', 'r3t', 'r8j', 'r1j', 'b8j', 'g1j', 'g7j', 'b2j', 'b8t', 'w4t', 'g7t', 'b3s', 'w9t', 'b9j', 'w4t', 'g4t', 'b5j', 'b5t', 'b9j', 'g1s', 'b9s', 'g5s', 'w8j', 'w4s', 'g9s', 'g4s', 'b3s', 'b7t', 'r4s', 'w4t', 'w5j', 'w9j', 'w4t', 'r9t', 'r1t', 'r6t', 'r3s', 'g7s', 'r1s', 'r1t', 'r1s', 'w1t', 'g1s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.01423513 0.02635773 0.07085316 ... 0.08391931 0.01737265 0.07630295]
Average of silhouette coef: 0.080488294
---
  0   1   2   3
0 0.0 2.7 2.0 2.0 
1 2.7 0.0 3.7 3.7 
2 2.0 3.7 0.0 3.8 
3 2.0 3.7 3.8 0.0 
correlation [[ 1.         -0.63579917]
 [-0.63579917  1.        ]]
---
[[], [], [], []] [0 0 2 ... 3 0 2] [[ 29.81735    34.299316 ]
 [-22.018707  -54.245472 ]
 [-51.378433  -22.39709  ]
 ...
 [-50.254734    2.1504402]
 [ 26.660976   15.531773 ]
 [  7.390312   15.898661 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': 0.080488294, 'cluster_all': array([0.01423513, 0.02635773, 0.07085316, ..., 0.08391931, 0.01737265,
       0.07630295], dtype=float32), 'magnitude_avg': 0.6357991736271614, 'magnitude_all': -0.6357991736271614, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': 0.080488294, 'magnitude_avg': 0.6357991736271614, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([3, 6, 5, 7, 7, 2, 2, 7, 9, 6, 8, 2, 9, 7, 4, 3, 1, 3, 4, 1, 7, 3, 4, 6,
        8, 6, 2, 8, 8, 2, 8, 4, 3, 7, 3, 3, 3, 3, 6, 2, 4, 1, 2, 5, 6, 1, 2, 6,
        2, 3, 7, 8, 7, 1, 3, 1, 9, 4, 5, 4, 6, 7, 6, 2, 1, 1, 3, 2, 5, 8, 7, 3,
        4, 6, 9, 7, 6, 6, 2, 8, 7, 3, 9, 3, 6, 5, 7, 9, 1, 1, 9, 2, 5, 6, 7, 2,
        6, 5, 7, 9, 7, 4, 6, 6, 6, 1, 9, 5, 4, 7, 1, 8, 4, 8, 1, 4, 3, 9, 1, 9,
        4, 5, 3, 6, 7, 8, 7, 8]), ['r3t', 'r6s', 'g5j', 'w7j', 'g7s', 'w2t', 'w2s', 'r7t', 'w9t', 'g6t', 'g8s', 'w2t', 'r9j', 'w7j', 'r4s', 'g3j', 'w1s', 'r3t', 'w4t', 'b1s', 'w7t', 'w3j', 'w4j', 'g6s', 'g8t', 'b6s', 'r2j', 'r8j', 'b8s', 'g2s', 'r8t', 'b4j', 'r3t', 'r7t', 'r3s', 'g3s', 'b3j', 'b3j', 'r6j', 'w2s', 'b4j', 'b1t', 'w2j', 'g5t', 'b6j', 'w1j', 'b2s', 'w6s', 'b2s', 'g3t', 'r7t', 'b8t', 'r7s', 'w1t', 'r3t', 'r1j', 'g9j', 'r4j', 'g5j', 'w4j', 'r6t', 'r7t', 'r6s', 'r2t', 'w1s', 'b1j', 'r3s', 'w2s', 'w5t', 'b8j', 'w7t', 'w3t', 'b4t', 'g6s', 'b9s', 'g7s', 'w6s', 'r6s', 'w2s', 'g8j', 'g7t', 'g3t', 'b9s', 'g3t', 'r6j', 'g5t', 'w7t', 'r9j', 'b1j', 'b1t', 'w9s', 'w2s', 'r5j', 'g6j', 'w7s', 'b2j', 'r6s', 'b5j', 'r7t', 'g9j', 'w7s', 'w4s', 'g6s', 'b6t', 'r6j', 'g1t', 'b9t', 'w5t', 'w4j', 'r7s', 'g1t', 'w8s', 'w4t', 'r8t', 'w1t', 'g4s', 'b3t', 'g9j', 'w1t', 'b9j', 'r4j', 'g5j', 'g3t', 'g6s', 'w7j', 'b8j', 'w7t', 'w8t']] 
label[0]: tensor([3, 6, 5, 7, 7, 2, 2, 7, 9, 6, 8, 2, 9, 7, 4, 3, 1, 3, 4, 1, 7, 3, 4, 6,
        8, 6, 2, 8, 8, 2, 8, 4, 3, 7, 3, 3, 3, 3, 6, 2, 4, 1, 2, 5, 6, 1, 2, 6,
        2, 3, 7, 8, 7, 1, 3, 1, 9, 4, 5, 4, 6, 7, 6, 2, 1, 1, 3, 2, 5, 8, 7, 3,
        4, 6, 9, 7, 6, 6, 2, 8, 7, 3, 9, 3, 6, 5, 7, 9, 1, 1, 9, 2, 5, 6, 7, 2,
        6, 5, 7, 9, 7, 4, 6, 6, 6, 1, 9, 5, 4, 7, 1, 8, 4, 8, 1, 4, 3, 9, 1, 9,
        4, 5, 3, 6, 7, 8, 7, 8]) 
label[1]: ['r3t', 'r6s', 'g5j', 'w7j', 'g7s', 'w2t', 'w2s', 'r7t', 'w9t', 'g6t', 'g8s', 'w2t', 'r9j', 'w7j', 'r4s', 'g3j', 'w1s', 'r3t', 'w4t', 'b1s', 'w7t', 'w3j', 'w4j', 'g6s', 'g8t', 'b6s', 'r2j', 'r8j', 'b8s', 'g2s', 'r8t', 'b4j', 'r3t', 'r7t', 'r3s', 'g3s', 'b3j', 'b3j', 'r6j', 'w2s', 'b4j', 'b1t', 'w2j', 'g5t', 'b6j', 'w1j', 'b2s', 'w6s', 'b2s', 'g3t', 'r7t', 'b8t', 'r7s', 'w1t', 'r3t', 'r1j', 'g9j', 'r4j', 'g5j', 'w4j', 'r6t', 'r7t', 'r6s', 'r2t', 'w1s', 'b1j', 'r3s', 'w2s', 'w5t', 'b8j', 'w7t', 'w3t', 'b4t', 'g6s', 'b9s', 'g7s', 'w6s', 'r6s', 'w2s', 'g8j', 'g7t', 'g3t', 'b9s', 'g3t', 'r6j', 'g5t', 'w7t', 'r9j', 'b1j', 'b1t', 'w9s', 'w2s', 'r5j', 'g6j', 'w7s', 'b2j', 'r6s', 'b5j', 'r7t', 'g9j', 'w7s', 'w4s', 'g6s', 'b6t', 'r6j', 'g1t', 'b9t', 'w5t', 'w4j', 'r7s', 'g1t', 'w8s', 'w4t', 'r8t', 'w1t', 'g4s', 'b3t', 'g9j', 'w1t', 'b9j', 'r4j', 'g5j', 'g3t', 'g6s', 'w7j', 'b8j', 'w7t', 'w8t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([7, 3, 1, 9, 7, 2, 3, 5, 2, 7, 1, 3, 3, 4, 2, 4, 7, 8, 9, 9, 1, 7, 6, 2,
        7, 1, 5, 4, 3, 1, 4, 1, 5, 1, 9, 9, 4, 6, 4, 1, 1, 5, 9, 1, 4, 5, 9, 4,
        2, 4, 3, 4, 9, 1, 7, 4, 1, 2, 3, 7, 6, 4, 3, 3, 8, 1, 4, 8, 6, 1, 4, 5,
        1, 1, 9, 5, 8, 6, 7, 5, 9, 9, 5, 1, 6, 2, 1, 5, 1, 6, 5, 9, 7, 7, 8, 6,
        4, 8, 5, 8, 5, 2, 6, 3, 9, 4, 4, 5, 3, 4, 2, 6, 9, 7, 5, 2, 4, 4, 8, 9,
        6, 3, 2, 1, 6, 2, 7, 7])
Accuracy (count): tensor(15) 
Accuracy (ratio) tensor(0.1172)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 20]
 [ 0  0  0  0  0  0  0  0 12]
 [ 0  0  0  0  0  0  0  0 12]
 [ 0  0  0  0  0  0  0  0 20]
 [ 0  0  0  0  0  0  0  0 15]
 [ 0  0  0  0  0  0  0  0 12]
 [ 0  0  0  0  0  0  0  0 14]
 [ 0  0  0  0  0  0  0  0  8]
 [ 0  0  0  0  0  0  0  0 15]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([6, 1, 3, 4, 8, 9, 3, 1, 3, 6, 1, 3, 1, 8, 4, 1, 3, 9, 3, 8, 5, 2, 4, 7,
        4, 7, 1, 4, 2, 8, 7, 6, 2, 1, 3, 2, 5, 1, 4, 4, 7, 9, 5, 6, 4, 7, 2, 2,
        8, 5, 5, 1, 9, 8, 1, 6, 3, 4, 4, 5, 2, 9, 8, 5, 7, 9, 1, 1, 3, 2, 3, 4,
        7, 4, 6, 5, 8, 9, 2, 2, 2, 2, 5, 6, 6, 9, 4, 9, 2, 8, 5, 5, 1, 8, 8, 3,
        2, 4, 1, 8, 5, 9, 9, 5, 7, 5, 7, 7, 3, 5, 2, 3, 2, 7, 3, 4, 1, 3, 2, 3,
        9, 5, 2, 5, 4, 9, 5, 1])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 16]
 [ 0  0  0  0  0  0  0  0 18]
 [ 0  0  0  0  0  0  0  0 16]
 [ 0  0  0  0  0  0  0  0 16]
 [ 0  0  0  0  0  0  0  0 18]
 [ 0  0  0  0  0  0  0  0  8]
 [ 0  0  0  0  0  0  0  0 11]
 [ 0  0  0  0  0  0  0  0 12]
 [ 0  0  0  0  0  0  0  0 13]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
---
Silhouette values: [-0.05469993 -0.08421902 -0.0767717  ... -0.06688951 -0.11698713
 -0.04615977]
Average of silhouette coef: -0.0153958425
---
  1   2   3   4   5   6   7   8   9
1 0.0 1.1 1.8 2.4 3.0 3.6 4.3 5.1 6.0 
2 1.1 0.0 0.9 1.4 2.1 2.7 3.4 4.2 5.1 
3 1.8 0.9 0.0 0.8 1.3 1.9 2.6 3.4 4.4 
4 2.4 1.4 0.8 0.0 0.8 1.3 2.0 2.8 3.8 
5 3.0 2.1 1.3 0.8 0.0 0.7 1.4 2.2 3.3 
6 3.6 2.7 1.9 1.3 0.7 0.0 0.9 1.7 2.7 
7 4.3 3.4 2.6 2.0 1.4 0.9 0.0 1.0 2.0 
8 5.1 4.2 3.4 2.8 2.2 1.7 1.0 0.0 1.2 
9 6.0 5.1 4.4 3.8 3.3 2.7 2.0 1.2 0.0 
correlation [[1.         0.98781135]
 [0.98781135 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [3 6 5 ... 4 6 5] [[ 39.789307    4.148869 ]
 [ -5.223776  -23.516596 ]
 [ 23.522972  -31.836084 ]
 ...
 [-10.194247   35.87266  ]
 [ 32.85866    -1.4280841]
 [ 33.65962   -31.965954 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 45]
 [ 0  0  0  0  0  0  0  0 52]
 [ 0  0  0  0  0  0  0  0 57]
 [ 0  0  0  0  0  0  0  0 60]
 [ 0  0  0  0  0  0  0  0 61]
 [ 0  0  0  0  0  0  0  0 60]
 [ 0  0  0  0  0  0  0  0 57]
 [ 0  0  0  0  0  0  0  0 52]
 [ 0  0  0  0  0  0  0  0 45]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.0 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.0  0.1  0.0  0.0 -0.0 -0.0 -0.1  0.7 -0.3 
 2.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.0 -0.1  0.0  0.0  0.1 -0.0 -0.1 -0.2 -0.0 -0.1  0.4 -0.2 
 1.2 -0.1  0.0  0.0  0.1  0.0  0.0 -0.0  0.1  0.1  0.0  0.0  0.1 -0.1 -0.1  0.1 -0.1 -0.1  0.4 -0.2 
 0.6  0.1  0.0  0.0  0.0  0.0  0.1  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.0 -0.1 -0.0 -0.2  0.4 -0.0 
 0.0 -0.1 -0.0 -0.0  0.1  0.0  0.1 -0.1 -0.2  0.0  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1  0.0  0.4 -0.1 
-0.6 -0.1 -0.0 -0.0  0.2  0.0  0.1 -0.1  0.0  0.1  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1 -0.1  0.2 -0.2 
-1.3 -0.2 -0.0 -0.0 -0.1  0.0  0.0 -0.1 -0.1  0.1  0.0 -0.2  0.0  0.0  0.1 -0.0 -0.2  0.0  0.1 -0.0 
-2.0 -0.2  0.0 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.2  0.0 -0.7  0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.4  0.0 
-2.8 -0.0  0.0 -0.1 -0.0 -0.0 -0.4 -0.0 -0.1  0.1 -0.0 -1.4  0.1 -0.1  0.2 -0.0 -0.1  0.1  0.3  0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 2 
distance: 0.9219457
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.0 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.0  0.1  0.0  0.0 -0.0 -0.0 -0.1  0.7 -0.3 
 2.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.0 -0.1  0.0  0.0  0.1 -0.0 -0.1 -0.2 -0.0 -0.1  0.4 -0.2 
 1.2 -0.1  0.0  0.0  0.1  0.0  0.0 -0.0  0.1  0.1  0.0  0.0  0.1 -0.1 -0.1  0.1 -0.1 -0.1  0.4 -0.2 
 0.6  0.1  0.0  0.0  0.0  0.0  0.1  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.0 -0.1 -0.0 -0.2  0.4 -0.0 
 0.0 -0.1 -0.0 -0.0  0.1  0.0  0.1 -0.1 -0.2  0.0  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1  0.0  0.4 -0.1 
-0.6 -0.1 -0.0 -0.0  0.2  0.0  0.1 -0.1  0.0  0.1  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1 -0.1  0.2 -0.2 
-1.3 -0.2 -0.0 -0.0 -0.1  0.0  0.0 -0.1 -0.1  0.1  0.0 -0.2  0.0  0.0  0.1 -0.0 -0.2  0.0  0.1 -0.0 
-2.0 -0.2  0.0 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.2  0.0 -0.7  0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.4  0.0 
-2.8 -0.0  0.0 -0.1 -0.0 -0.0 -0.4 -0.0 -0.1  0.1 -0.0 -1.4  0.1 -0.1  0.2 -0.0 -0.1  0.1  0.3  0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 5 
distance: 1.1080322
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.0 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.0  0.1  0.0  0.0 -0.0 -0.0 -0.1  0.7 -0.3 
 2.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.0 -0.1  0.0  0.0  0.1 -0.0 -0.1 -0.2 -0.0 -0.1  0.4 -0.2 
 1.2 -0.1  0.0  0.0  0.1  0.0  0.0 -0.0  0.1  0.1  0.0  0.0  0.1 -0.1 -0.1  0.1 -0.1 -0.1  0.4 -0.2 
 0.6  0.1  0.0  0.0  0.0  0.0  0.1  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.0 -0.1 -0.0 -0.2  0.4 -0.0 
 0.0 -0.1 -0.0 -0.0  0.1  0.0  0.1 -0.1 -0.2  0.0  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1  0.0  0.4 -0.1 
-0.6 -0.1 -0.0 -0.0  0.2  0.0  0.1 -0.1  0.0  0.1  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1 -0.1  0.2 -0.2 
-1.3 -0.2 -0.0 -0.0 -0.1  0.0  0.0 -0.1 -0.1  0.1  0.0 -0.2  0.0  0.0  0.1 -0.0 -0.2  0.0  0.1 -0.0 
-2.0 -0.2  0.0 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.2  0.0 -0.7  0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.4  0.0 
-2.8 -0.0  0.0 -0.1 -0.0 -0.0 -0.4 -0.0 -0.1  0.1 -0.0 -1.4  0.1 -0.1  0.2 -0.0 -0.1  0.1  0.3  0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 8 
distance: 0.96780473
pred: tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 3.0 -0.0  0.0  0.0  0.1  0.0 -0.0 -0.1  0.0  0.1 -0.0  0.0  0.1  0.0  0.0 -0.0 -0.0 -0.1  0.7 -0.3 
 2.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.0 -0.1  0.0  0.0  0.1 -0.0 -0.1 -0.2 -0.0 -0.1  0.4 -0.2 
 1.2 -0.1  0.0  0.0  0.1  0.0  0.0 -0.0  0.1  0.1  0.0  0.0  0.1 -0.1 -0.1  0.1 -0.1 -0.1  0.4 -0.2 
 0.6  0.1  0.0  0.0  0.0  0.0  0.1  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.0 -0.1 -0.0 -0.2  0.4 -0.0 
 0.0 -0.1 -0.0 -0.0  0.1  0.0  0.1 -0.1 -0.2  0.0  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1  0.0  0.4 -0.1 
-0.6 -0.1 -0.0 -0.0  0.2  0.0  0.1 -0.1  0.0  0.1  0.0  0.1 -0.0 -0.0  0.0 -0.1 -0.1 -0.1  0.2 -0.2 
-1.3 -0.2 -0.0 -0.0 -0.1  0.0  0.0 -0.1 -0.1  0.1  0.0 -0.2  0.0  0.0  0.1 -0.0 -0.2  0.0  0.1 -0.0 
-2.0 -0.2  0.0 -0.1  0.2 -0.0 -0.1 -0.2 -0.2  0.2  0.0 -0.7  0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.4  0.0 
-2.8 -0.0  0.0 -0.1 -0.0 -0.0 -0.4 -0.0 -0.1  0.1 -0.0 -1.4  0.1 -0.1  0.2 -0.0 -0.1  0.1  0.3  0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 0.46057972
results (all): {'reconst_1x1_avg': 0.1171875, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.1015625, 'cross_0x1_all': nan, 'cluster_avg': -0.0153958425, 'cluster_all': array([-0.05469993, -0.08421902, -0.0767717 , ..., -0.06688951,
       -0.11698713, -0.04615977], dtype=float32), 'magnitude_avg': 0.9878113498554555, 'magnitude_all': 0.9878113498554555, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.1171875, 'cross_0x1_avg': 0.1015625, 'cluster_avg': -0.0153958425, 'magnitude_avg': 0.9878113498554555, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([3, 5, 2, 3, 8, 4, 2, 2, 5, 4, 4, 7, 3, 1, 5, 5, 6, 9, 2, 8, 8, 6, 4, 9,
        8, 3, 8, 2, 4, 6, 9, 5, 6, 1, 1, 2, 5, 1, 5, 1, 7, 8, 1, 7, 8, 6, 5, 8,
        3, 7, 1, 6, 1, 6, 7, 5, 5, 9, 3, 4, 7, 2, 4, 6, 4, 2, 5, 4, 2, 1, 7, 1,
        5, 6, 6, 2, 4, 7, 7, 7, 8, 5, 8, 3, 6, 3, 3, 7, 5, 1, 7, 2, 9, 9, 9, 6,
        8, 5, 1, 2, 3, 1, 7, 6, 1, 3, 5, 8, 9, 8, 7, 7, 9, 1, 4, 5, 3, 5, 3, 4,
        4, 4, 8, 7, 5, 1, 8, 7]), ['r3s', 'w5s', 'w2j', 'w3j', 'r8s', 'w4j', 'r2j', 'g2s', 'b5t', 'g4t', 'w4j', 'g7s', 'g3s', 'g1t', 'b5t', 'r5s', 'w6t', 'w9j', 'b2t', 'g8s', 'g8s', 'w6j', 'b4s', 'r9j', 'w8j', 'r3s', 'w8j', 'g2t', 'w4t', 'b6t', 'w9s', 'w5j', 'g6t', 'g1j', 'b1s', 'g2j', 'b5s', 'g1s', 'w5s', 'b1j', 'b7s', 'r8j', 'b1s', 'g7s', 'b8s', 'b6s', 'r5s', 'b8j', 'g3j', 'b7s', 'w1s', 'b6s', 'r1t', 'b6s', 'b7s', 'r5s', 'r5t', 'r9j', 'b3s', 'g4s', 'w7s', 'g2j', 'g4j', 'w6j', 'b4s', 'r2s', 'g5t', 'r4j', 'r2j', 'g1j', 'w7t', 'w1s', 'b5t', 'b6s', 'b6j', 'r2s', 'g4j', 'w7t', 'b7j', 'b7j', 'b8s', 'g5t', 'r8s', 'g3j', 'r6j', 'r3t', 'b3t', 'g7t', 'b5t', 'g1s', 'b7t', 'w2j', 'b9t', 'g9j', 'b9t', 'g6t', 'b8s', 'g5t', 'b1s', 'g2t', 'w3j', 'b1j', 'r7j', 'g6t', 'g1s', 'r3s', 'g5j', 'g8j', 'w9s', 'r8t', 'r7j', 'w7s', 'r9t', 'g1s', 'w4s', 'w5t', 'r3j', 'b5s', 'w3j', 'g4s', 'b4t', 'g4t', 'w8j', 'w7j', 'b5t', 'w1t', 'b8t', 'b7t']] 
label[0]: tensor([3, 5, 2, 3, 8, 4, 2, 2, 5, 4, 4, 7, 3, 1, 5, 5, 6, 9, 2, 8, 8, 6, 4, 9,
        8, 3, 8, 2, 4, 6, 9, 5, 6, 1, 1, 2, 5, 1, 5, 1, 7, 8, 1, 7, 8, 6, 5, 8,
        3, 7, 1, 6, 1, 6, 7, 5, 5, 9, 3, 4, 7, 2, 4, 6, 4, 2, 5, 4, 2, 1, 7, 1,
        5, 6, 6, 2, 4, 7, 7, 7, 8, 5, 8, 3, 6, 3, 3, 7, 5, 1, 7, 2, 9, 9, 9, 6,
        8, 5, 1, 2, 3, 1, 7, 6, 1, 3, 5, 8, 9, 8, 7, 7, 9, 1, 4, 5, 3, 5, 3, 4,
        4, 4, 8, 7, 5, 1, 8, 7]) 
label[1]: ['r3s', 'w5s', 'w2j', 'w3j', 'r8s', 'w4j', 'r2j', 'g2s', 'b5t', 'g4t', 'w4j', 'g7s', 'g3s', 'g1t', 'b5t', 'r5s', 'w6t', 'w9j', 'b2t', 'g8s', 'g8s', 'w6j', 'b4s', 'r9j', 'w8j', 'r3s', 'w8j', 'g2t', 'w4t', 'b6t', 'w9s', 'w5j', 'g6t', 'g1j', 'b1s', 'g2j', 'b5s', 'g1s', 'w5s', 'b1j', 'b7s', 'r8j', 'b1s', 'g7s', 'b8s', 'b6s', 'r5s', 'b8j', 'g3j', 'b7s', 'w1s', 'b6s', 'r1t', 'b6s', 'b7s', 'r5s', 'r5t', 'r9j', 'b3s', 'g4s', 'w7s', 'g2j', 'g4j', 'w6j', 'b4s', 'r2s', 'g5t', 'r4j', 'r2j', 'g1j', 'w7t', 'w1s', 'b5t', 'b6s', 'b6j', 'r2s', 'g4j', 'w7t', 'b7j', 'b7j', 'b8s', 'g5t', 'r8s', 'g3j', 'r6j', 'r3t', 'b3t', 'g7t', 'b5t', 'g1s', 'b7t', 'w2j', 'b9t', 'g9j', 'b9t', 'g6t', 'b8s', 'g5t', 'b1s', 'g2t', 'w3j', 'b1j', 'r7j', 'g6t', 'g1s', 'r3s', 'g5j', 'g8j', 'w9s', 'r8t', 'r7j', 'w7s', 'r9t', 'g1s', 'w4s', 'w5t', 'r3j', 'b5s', 'w3j', 'g4s', 'b4t', 'g4t', 'w8j', 'w7j', 'b5t', 'w1t', 'b8t', 'b7t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.01811439 0.01511459 0.05006611 ... 0.00441346 0.03652762 0.0190723 ]
Average of silhouette coef: 0.03812994
---
  0   1   2
0 0.0 1.6 3.2 
1 1.6 0.0 1.7 
2 3.2 1.7 0.0 
correlation [[1.         0.99913291]
 [0.99913291 1.        ]]
---
[[], [], []] [1 1 0 ... 1 0 1] [[-22.346292   -48.996407  ]
 [-18.736513   -35.201946  ]
 [ 26.670115    -0.06601333]
 ...
 [  5.5822287  -57.422153  ]
 [  1.5259411  -31.525192  ]
 [ 39.005447    42.186462  ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': 0.03812994, 'cluster_all': array([0.01811439, 0.01511459, 0.05006611, ..., 0.00441346, 0.03652762,
       0.0190723 ], dtype=float32), 'magnitude_avg': 0.999132905060818, 'magnitude_all': 0.999132905060818, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': 0.03812994, 'magnitude_avg': 0.999132905060818, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 2, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_2', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2', 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([1, 8, 1, 7, 3, 8, 4, 1, 7, 2, 5, 5, 1, 3, 4, 4, 7, 9, 6, 1, 3, 1, 1, 9,
        7, 7, 3, 3, 8, 4, 4, 2, 6, 5, 7, 3, 6, 7, 7, 1, 4, 6, 1, 4, 7, 8, 1, 5,
        1, 4, 2, 2, 9, 5, 9, 8, 3, 1, 9, 2, 5, 6, 9, 1, 3, 3, 2, 5, 9, 3, 2, 1,
        5, 3, 3, 3, 4, 4, 5, 1, 6, 9, 4, 9, 2, 4, 8, 1, 1, 4, 6, 6, 4, 9, 3, 3,
        9, 1, 7, 1, 3, 8, 1, 4, 9, 8, 1, 2, 3, 3, 1, 8, 9, 6, 3, 9, 3, 7, 4, 6,
        4, 3, 9, 4, 1, 7, 5, 6]), ['b1t', 'r8s', 'r1j', 'r7s', 'w3j', 'w8s', 'b4t', 'b1s', 'b7t', 'g2j', 'w5s', 'r5j', 'w1s', 'b3t', 'r4j', 'g4s', 'w7t', 'r9t', 'r6s', 'b1j', 'r3j', 'r1j', 'w1j', 'r9t', 'r7j', 'w7s', 'r3j', 'g3s', 'b8t', 'w4s', 'b4s', 'g2s', 'b6s', 'w5j', 'r7t', 'w3j', 'w6s', 'b7j', 'g7s', 'w1t', 'g4s', 'b6t', 'b1j', 'r4t', 'r7j', 'w8t', 'b1t', 'w5s', 'g1j', 'r4t', 'r2t', 'r2t', 'w9j', 'r5s', 'g9t', 'b8t', 'g3s', 'b1s', 'g9t', 'b2t', 'r5j', 'w6t', 'b9s', 'w1j', 'g3j', 'r3t', 'w2j', 'r5j', 'w9s', 'w3j', 'b2j', 'r1t', 'g5j', 'b3s', 'r3j', 'g3j', 'w4s', 'w4s', 'w5j', 'b1t', 'w6t', 'g9t', 'w4s', 'b9j', 'r2t', 'w4t', 'b8s', 'r1s', 'b1t', 'g4j', 'b6j', 'r6s', 'b4s', 'w9j', 'w3j', 'r3j', 'g9t', 'b1j', 'w7s', 'b1t', 'b3t', 'g8t', 'w1t', 'w4j', 'w9t', 'w8s', 'b1j', 'w2t', 'g3t', 'w3s', 'b1j', 'r8t', 'w9s', 'w6j', 'g3j', 'w9t', 'b3s', 'b7t', 'g4t', 'g6s', 'w4j', 'w3t', 'w9t', 'w4t', 'w1t', 'g7j', 'w5t', 'w6t']] 
label[0]: tensor([1, 8, 1, 7, 3, 8, 4, 1, 7, 2, 5, 5, 1, 3, 4, 4, 7, 9, 6, 1, 3, 1, 1, 9,
        7, 7, 3, 3, 8, 4, 4, 2, 6, 5, 7, 3, 6, 7, 7, 1, 4, 6, 1, 4, 7, 8, 1, 5,
        1, 4, 2, 2, 9, 5, 9, 8, 3, 1, 9, 2, 5, 6, 9, 1, 3, 3, 2, 5, 9, 3, 2, 1,
        5, 3, 3, 3, 4, 4, 5, 1, 6, 9, 4, 9, 2, 4, 8, 1, 1, 4, 6, 6, 4, 9, 3, 3,
        9, 1, 7, 1, 3, 8, 1, 4, 9, 8, 1, 2, 3, 3, 1, 8, 9, 6, 3, 9, 3, 7, 4, 6,
        4, 3, 9, 4, 1, 7, 5, 6]) 
label[1]: ['b1t', 'r8s', 'r1j', 'r7s', 'w3j', 'w8s', 'b4t', 'b1s', 'b7t', 'g2j', 'w5s', 'r5j', 'w1s', 'b3t', 'r4j', 'g4s', 'w7t', 'r9t', 'r6s', 'b1j', 'r3j', 'r1j', 'w1j', 'r9t', 'r7j', 'w7s', 'r3j', 'g3s', 'b8t', 'w4s', 'b4s', 'g2s', 'b6s', 'w5j', 'r7t', 'w3j', 'w6s', 'b7j', 'g7s', 'w1t', 'g4s', 'b6t', 'b1j', 'r4t', 'r7j', 'w8t', 'b1t', 'w5s', 'g1j', 'r4t', 'r2t', 'r2t', 'w9j', 'r5s', 'g9t', 'b8t', 'g3s', 'b1s', 'g9t', 'b2t', 'r5j', 'w6t', 'b9s', 'w1j', 'g3j', 'r3t', 'w2j', 'r5j', 'w9s', 'w3j', 'b2j', 'r1t', 'g5j', 'b3s', 'r3j', 'g3j', 'w4s', 'w4s', 'w5j', 'b1t', 'w6t', 'g9t', 'w4s', 'b9j', 'r2t', 'w4t', 'b8s', 'r1s', 'b1t', 'g4j', 'b6j', 'r6s', 'b4s', 'w9j', 'w3j', 'r3j', 'g9t', 'b1j', 'w7s', 'b1t', 'b3t', 'g8t', 'w1t', 'w4j', 'w9t', 'w8s', 'b1j', 'w2t', 'g3t', 'w3s', 'b1j', 'r8t', 'w9s', 'w6j', 'g3j', 'w9t', 'b3s', 'b7t', 'g4t', 'g6s', 'w4j', 'w3t', 'w9t', 'w4t', 'w1t', 'g7j', 'w5t', 'w6t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.10972981  0.11622291  0.13058129 ... -0.09218719 -0.02874109
  0.12419887]
Average of silhouette coef: 0.065781325
---
  0   1   2   3
0 0.0 2.6 2.9 2.4 
1 2.6 0.0 4.3 4.3 
2 2.9 4.3 0.0 3.9 
3 2.4 4.3 3.9 0.0 
correlation [[ 1.        -0.4497824]
 [-0.4497824  1.       ]]
---
[[], [], [], []] [1 3 3 ... 0 0 1] [[ 61.36477  -29.903387]
 [-17.466055  -1.769624]
 [ 13.525588 -49.925888]
 ...
 [ 31.39138   13.007216]
 [  8.292604  30.354286]
 [-12.275196 -30.27975 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.065781325, 'cluster_all': array([ 0.10972981,  0.11622291,  0.13058129, ..., -0.09218719,
       -0.02874109,  0.12419887], dtype=float32), 'magnitude_avg': 0.4497824007950327, 'magnitude_all': -0.4497824007950327, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.065781325, 'magnitude_avg': 0.4497824007950327, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([5, 8, 5, 5, 5, 9, 8, 3, 9, 6, 6, 9, 3, 4, 3, 9, 4, 3, 6, 1, 9, 2, 2, 3,
        1, 7, 9, 9, 6, 5, 5, 5, 3, 3, 5, 4, 3, 3, 6, 4, 4, 7, 6, 5, 4, 2, 6, 9,
        3, 6, 3, 3, 4, 6, 4, 2, 4, 9, 9, 9, 3, 3, 5, 1, 5, 4, 4, 2, 8, 9, 8, 5,
        8, 3, 9, 9, 6, 1, 4, 8, 9, 9, 8, 3, 9, 6, 4, 4, 7, 8, 3, 6, 2, 1, 6, 3,
        5, 1, 3, 7, 4, 3, 5, 4, 1, 8, 4, 1, 3, 4, 6, 3, 3, 4, 7, 8, 6, 3, 4, 4,
        9, 6, 8, 9, 5, 1, 8, 2]), ['b5s', 'r8j', 'w5j', 'b5s', 'b5s', 'g9j', 'r8t', 'g3t', 'w9s', 'g6t', 'g6j', 'b9j', 'w3s', 'g4t', 'w3s', 'b9j', 'w4s', 'b3j', 'b6s', 'w1j', 'w9j', 'w2j', 'g2j', 'g3s', 'b1s', 'w7t', 'r9j', 'r9s', 'w6s', 'w5t', 'b5s', 'r5t', 'w3j', 'g3t', 'g5j', 'w4t', 'r3t', 'r3s', 'r6j', 'g4j', 'g4j', 'r7s', 'r6s', 'b5t', 'b4s', 'w2s', 'r6j', 'g9t', 'g3t', 'r6t', 'g3s', 'r3j', 'b4t', 'w6s', 'b4j', 'r2t', 'b4j', 'g9j', 'g9s', 'b9s', 'g3s', 'w3s', 'w5s', 'r1s', 'w5j', 'r4j', 'g4s', 'w2j', 'w8t', 'b9s', 'w8s', 'w5t', 'g8t', 'b3s', 'b9s', 'w9s', 'g6s', 'b1s', 'w4t', 'b8s', 'g9t', 'b9s', 'b8s', 'w3s', 'b9t', 'g6j', 'b4t', 'w4j', 'b7t', 'g8j', 'b3t', 'b6j', 'w2j', 'w1s', 'g6t', 'g3s', 'r5t', 'b1j', 'b3s', 'b7t', 'r4s', 'b3t', 'w5t', 'r4j', 'r1j', 'r8s', 'g4j', 'r1t', 'r3s', 'w4s', 'w6j', 'b3s', 'r3j', 'b4j', 'w7s', 'w8t', 'b6t', 'b3s', 'b4j', 'r4j', 'g9j', 'g6j', 'w8s', 'b9j', 'w5t', 'r1j', 'w8t', 'w2t']] 
label[0]: tensor([5, 8, 5, 5, 5, 9, 8, 3, 9, 6, 6, 9, 3, 4, 3, 9, 4, 3, 6, 1, 9, 2, 2, 3,
        1, 7, 9, 9, 6, 5, 5, 5, 3, 3, 5, 4, 3, 3, 6, 4, 4, 7, 6, 5, 4, 2, 6, 9,
        3, 6, 3, 3, 4, 6, 4, 2, 4, 9, 9, 9, 3, 3, 5, 1, 5, 4, 4, 2, 8, 9, 8, 5,
        8, 3, 9, 9, 6, 1, 4, 8, 9, 9, 8, 3, 9, 6, 4, 4, 7, 8, 3, 6, 2, 1, 6, 3,
        5, 1, 3, 7, 4, 3, 5, 4, 1, 8, 4, 1, 3, 4, 6, 3, 3, 4, 7, 8, 6, 3, 4, 4,
        9, 6, 8, 9, 5, 1, 8, 2]) 
label[1]: ['b5s', 'r8j', 'w5j', 'b5s', 'b5s', 'g9j', 'r8t', 'g3t', 'w9s', 'g6t', 'g6j', 'b9j', 'w3s', 'g4t', 'w3s', 'b9j', 'w4s', 'b3j', 'b6s', 'w1j', 'w9j', 'w2j', 'g2j', 'g3s', 'b1s', 'w7t', 'r9j', 'r9s', 'w6s', 'w5t', 'b5s', 'r5t', 'w3j', 'g3t', 'g5j', 'w4t', 'r3t', 'r3s', 'r6j', 'g4j', 'g4j', 'r7s', 'r6s', 'b5t', 'b4s', 'w2s', 'r6j', 'g9t', 'g3t', 'r6t', 'g3s', 'r3j', 'b4t', 'w6s', 'b4j', 'r2t', 'b4j', 'g9j', 'g9s', 'b9s', 'g3s', 'w3s', 'w5s', 'r1s', 'w5j', 'r4j', 'g4s', 'w2j', 'w8t', 'b9s', 'w8s', 'w5t', 'g8t', 'b3s', 'b9s', 'w9s', 'g6s', 'b1s', 'w4t', 'b8s', 'g9t', 'b9s', 'b8s', 'w3s', 'b9t', 'g6j', 'b4t', 'w4j', 'b7t', 'g8j', 'b3t', 'b6j', 'w2j', 'w1s', 'g6t', 'g3s', 'r5t', 'b1j', 'b3s', 'b7t', 'r4s', 'b3t', 'w5t', 'r4j', 'r1j', 'r8s', 'g4j', 'r1t', 'r3s', 'w4s', 'w6j', 'b3s', 'r3j', 'b4j', 'w7s', 'w8t', 'b6t', 'b3s', 'b4j', 'r4j', 'g9j', 'g6j', 'w8s', 'b9j', 'w5t', 'r1j', 'w8t', 'w2t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([4, 9, 2, 5, 9, 9, 2, 6, 9, 9, 2, 3, 3, 2, 6, 1, 9, 4, 4, 8, 2, 8, 7, 7,
        6, 2, 9, 4, 4, 4, 7, 5, 4, 2, 1, 3, 7, 2, 3, 1, 3, 9, 3, 7, 1, 4, 9, 9,
        8, 8, 4, 2, 9, 2, 6, 8, 5, 3, 1, 2, 7, 2, 8, 2, 4, 9, 2, 5, 7, 3, 1, 4,
        3, 1, 5, 4, 7, 4, 6, 8, 7, 5, 7, 9, 9, 3, 7, 1, 5, 3, 2, 9, 2, 7, 5, 5,
        7, 2, 8, 2, 1, 7, 7, 5, 3, 1, 3, 5, 5, 5, 1, 2, 2, 9, 2, 3, 8, 8, 2, 7,
        9, 7, 3, 7, 6, 3, 6, 6])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0  0  0  0 11  0  0  0  0]
 [ 0  0  0  0 22  0  0  0  0]
 [ 0  0  0  0 16  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0  8  0  0  0  0]
 [ 0  0  0  0 18  0  0  0  0]
 [ 0  0  0  0 10  0  0  0  0]
 [ 0  0  0  0 17  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([1, 6, 2, 3, 9, 8, 4, 3, 5, 8, 3, 1, 2, 3, 1, 9, 3, 6, 4, 6, 7, 9, 3, 6,
        2, 4, 3, 7, 1, 7, 3, 9, 9, 4, 6, 6, 9, 4, 9, 4, 8, 4, 2, 4, 6, 7, 3, 1,
        3, 5, 9, 7, 6, 2, 4, 9, 7, 2, 4, 9, 2, 6, 7, 2, 3, 3, 4, 4, 4, 3, 1, 9,
        9, 5, 4, 1, 1, 6, 6, 2, 8, 7, 1, 6, 4, 7, 9, 9, 1, 1, 7, 5, 6, 2, 7, 7,
        4, 6, 2, 3, 3, 5, 9, 3, 1, 6, 1, 9, 2, 3, 7, 5, 3, 7, 9, 3, 7, 1, 5, 8,
        1, 3, 9, 1, 9, 1, 2, 5])
Accuracy (count): tensor(8) 
Accuracy (ratio) tensor(0.0625)
Accuracy:
 [[ 0  0  0  0 17  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 20  0  0  0  0]
 [ 0  0  0  0 16  0  0  0  0]
 [ 0  0  0  0  8  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0  5  0  0  0  0]
 [ 0  0  0  0 19  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
Silhouette values: [0.08537166 0.01577985 0.03491992 ... 0.32116523 0.03992935 0.34102747]
Average of silhouette coef: 0.053295042
---
  1   2   3   4   5   6   7   8   9
1 0.0 4.1 6.2 6.2 5.7 5.9 6.8 6.3 7.4 
2 4.1 0.0 4.7 3.8 4.0 3.1 5.6 3.4 4.8 
3 6.2 4.7 0.0 6.1 2.9 4.7 5.7 3.2 5.3 
4 6.2 3.8 6.1 0.0 4.6 3.7 3.8 4.1 2.6 
5 5.7 4.0 2.9 4.6 0.0 4.0 4.7 2.6 4.1 
6 5.9 3.1 4.7 3.7 4.0 0.0 5.9 3.6 4.5 
7 6.8 5.6 5.7 3.8 4.7 5.9 0.0 5.1 3.7 
8 6.3 3.4 3.2 4.1 2.6 3.6 5.1 0.0 3.1 
9 7.4 4.8 5.3 2.6 4.1 4.5 3.7 3.1 0.0 
correlation [[1.         0.29406583]
 [0.29406583 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [5 8 5 ... 1 6 1] [[-34.8696    -17.474413 ]
 [-15.259111  -42.885845 ]
 [  2.7770023  -5.858912 ]
 ...
 [ 48.17403   -21.877998 ]
 [  2.6110377 -29.646343 ]
 [ 53.75428   -15.974347 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

Accuracy (count): tensor(61) 
Accuracy (ratio) tensor(0.1247)
Accuracy:
 [[ 0  0  0  0 45  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 61  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 45  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.1 -0.1  0.2  0.3  0.2  0.3 -0.1 -0.2 -0.0  0.0  0.3  0.1 -0.1  0.2  0.0  0.3  0.3  0.1  0.1  3.6 
-0.8 -1.3  0.1  0.6 -0.2 -0.6 -0.2  0.1 -0.7  0.0  0.2 -1.2  0.3 -0.0  0.1  0.0 -0.1 -0.2  0.2  0.3 
-0.6 -0.0 -0.1  0.1  0.1 -0.3 -0.1 -0.1 -0.7  0.1  3.7  0.9 -0.1 -0.8  0.2 -0.1  0.0 -0.1  0.2 -1.2 
-0.6  0.5  0.1 -0.5  0.5 -0.8 -0.3  0.3 -0.5  0.1 -1.9 -0.9 -0.0  0.4  0.2  0.1 -0.3 -0.1 -0.1 -1.8 
-0.2  0.6 -0.2  0.9  0.5 -0.5 -0.2  0.1 -0.5  0.0  2.1  0.4 -0.4  1.2  0.3  0.2  0.2 -0.3  0.2 -1.5 
-3.0 -0.7  0.0 -0.1  0.1 -1.0 -0.4  0.1 -0.7  0.1  0.2 -0.1 -0.1  0.5  0.2  0.2  0.0 -0.2  0.5 -1.0 
 1.2  2.8  0.1 -0.3  0.2 -1.0 -0.5  0.4 -0.2  0.1 -0.6  0.4  0.2 -1.1  0.0  0.3 -0.3  0.0 -0.0 -1.9 
-0.3 -1.2 -0.1  0.4  0.1  0.1 -0.1 -0.1 -0.5  0.0  1.4  0.1 -0.0  0.1  0.4 -0.0 -0.0 -0.1  0.2 -2.4 
-0.1  0.5 -0.1 -0.5  0.4  0.1  0.0 -0.0 -0.1  0.0 -0.6 -0.5 -0.0 -0.0  0.1 -0.0 -0.1 -0.1 -0.0 -3.6 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 3.1209352
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 0.1 -0.1  0.2  0.3  0.2  0.3 -0.1 -0.2 -0.0  0.0  0.3  0.1 -0.1  0.2  0.0  0.3  0.3  0.1  0.1  3.6 
-0.8 -1.3  0.1  0.6 -0.2 -0.6 -0.2  0.1 -0.7  0.0  0.2 -1.2  0.3 -0.0  0.1  0.0 -0.1 -0.2  0.2  0.3 
-0.6 -0.0 -0.1  0.1  0.1 -0.3 -0.1 -0.1 -0.7  0.1  3.7  0.9 -0.1 -0.8  0.2 -0.1  0.0 -0.1  0.2 -1.2 
-0.6  0.5  0.1 -0.5  0.5 -0.8 -0.3  0.3 -0.5  0.1 -1.9 -0.9 -0.0  0.4  0.2  0.1 -0.3 -0.1 -0.1 -1.8 
-0.2  0.6 -0.2  0.9  0.5 -0.5 -0.2  0.1 -0.5  0.0  2.1  0.4 -0.4  1.2  0.3  0.2  0.2 -0.3  0.2 -1.5 
-3.0 -0.7  0.0 -0.1  0.1 -1.0 -0.4  0.1 -0.7  0.1  0.2 -0.1 -0.1  0.5  0.2  0.2  0.0 -0.2  0.5 -1.0 
 1.2  2.8  0.1 -0.3  0.2 -1.0 -0.5  0.4 -0.2  0.1 -0.6  0.4  0.2 -1.1  0.0  0.3 -0.3  0.0 -0.0 -1.9 
-0.3 -1.2 -0.1  0.4  0.1  0.1 -0.1 -0.1 -0.5  0.0  1.4  0.1 -0.0  0.1  0.4 -0.0 -0.0 -0.1  0.2 -2.4 
-0.1  0.5 -0.1 -0.5  0.4  0.1  0.0 -0.0 -0.1  0.0 -0.6 -0.5 -0.0 -0.0  0.1 -0.0 -0.1 -0.1 -0.0 -3.6 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 4.0821776
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.1 -0.1  0.2  0.3  0.2  0.3 -0.1 -0.2 -0.0  0.0  0.3  0.1 -0.1  0.2  0.0  0.3  0.3  0.1  0.1  3.6 
-0.8 -1.3  0.1  0.6 -0.2 -0.6 -0.2  0.1 -0.7  0.0  0.2 -1.2  0.3 -0.0  0.1  0.0 -0.1 -0.2  0.2  0.3 
-0.6 -0.0 -0.1  0.1  0.1 -0.3 -0.1 -0.1 -0.7  0.1  3.7  0.9 -0.1 -0.8  0.2 -0.1  0.0 -0.1  0.2 -1.2 
-0.6  0.5  0.1 -0.5  0.5 -0.8 -0.3  0.3 -0.5  0.1 -1.9 -0.9 -0.0  0.4  0.2  0.1 -0.3 -0.1 -0.1 -1.8 
-0.2  0.6 -0.2  0.9  0.5 -0.5 -0.2  0.1 -0.5  0.0  2.1  0.4 -0.4  1.2  0.3  0.2  0.2 -0.3  0.2 -1.5 
-3.0 -0.7  0.0 -0.1  0.1 -1.0 -0.4  0.1 -0.7  0.1  0.2 -0.1 -0.1  0.5  0.2  0.2  0.0 -0.2  0.5 -1.0 
 1.2  2.8  0.1 -0.3  0.2 -1.0 -0.5  0.4 -0.2  0.1 -0.6  0.4  0.2 -1.1  0.0  0.3 -0.3  0.0 -0.0 -1.9 
-0.3 -1.2 -0.1  0.4  0.1  0.1 -0.1 -0.1 -0.5  0.0  1.4  0.1 -0.0  0.1  0.4 -0.0 -0.0 -0.1  0.2 -2.4 
-0.1  0.5 -0.1 -0.5  0.4  0.1  0.0 -0.0 -0.1  0.0 -0.6 -0.5 -0.0 -0.0  0.1 -0.0 -0.1 -0.1 -0.0 -3.6 
Minimum distance of calculation. 
true answer: 8 
indices: 9 
distance: 3.5517519
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.1 -0.1  0.2  0.3  0.2  0.3 -0.1 -0.2 -0.0  0.0  0.3  0.1 -0.1  0.2  0.0  0.3  0.3  0.1  0.1  3.6 
-0.8 -1.3  0.1  0.6 -0.2 -0.6 -0.2  0.1 -0.7  0.0  0.2 -1.2  0.3 -0.0  0.1  0.0 -0.1 -0.2  0.2  0.3 
-0.6 -0.0 -0.1  0.1  0.1 -0.3 -0.1 -0.1 -0.7  0.1  3.7  0.9 -0.1 -0.8  0.2 -0.1  0.0 -0.1  0.2 -1.2 
-0.6  0.5  0.1 -0.5  0.5 -0.8 -0.3  0.3 -0.5  0.1 -1.9 -0.9 -0.0  0.4  0.2  0.1 -0.3 -0.1 -0.1 -1.8 
-0.2  0.6 -0.2  0.9  0.5 -0.5 -0.2  0.1 -0.5  0.0  2.1  0.4 -0.4  1.2  0.3  0.2  0.2 -0.3  0.2 -1.5 
-3.0 -0.7  0.0 -0.1  0.1 -1.0 -0.4  0.1 -0.7  0.1  0.2 -0.1 -0.1  0.5  0.2  0.2  0.0 -0.2  0.5 -1.0 
 1.2  2.8  0.1 -0.3  0.2 -1.0 -0.5  0.4 -0.2  0.1 -0.6  0.4  0.2 -1.1  0.0  0.3 -0.3  0.0 -0.0 -1.9 
-0.3 -1.2 -0.1  0.4  0.1  0.1 -0.1 -0.1 -0.5  0.0  1.4  0.1 -0.0  0.1  0.4 -0.0 -0.0 -0.1  0.2 -2.4 
-0.1  0.5 -0.1 -0.5  0.4  0.1  0.0 -0.0 -0.1  0.0 -0.6 -0.5 -0.0 -0.0  0.1 -0.0 -0.1 -0.1 -0.0 -3.6 
Minimum distance of calculation. 
true answer: 6 
indices: 3 
distance: 3.99399
results (all): {'reconst_0x0_avg': 0.1015625, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.0625, 'cross_1x0_all': nan, 'cluster_avg': 0.053295042, 'cluster_all': array([0.08537166, 0.01577985, 0.03491992, ..., 0.32116523, 0.03992935,
       0.34102747], dtype=float32), 'magnitude_avg': 0.2940658265778501, 'magnitude_all': 0.2940658265778501, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 1.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1015625, 'cross_1x0_avg': 0.0625, 'cluster_avg': 0.053295042, 'magnitude_avg': 0.2940658265778501, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 1.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([3, 3, 1, 3, 1, 2, 7, 5, 5, 7, 2, 3, 8, 4, 3, 5, 8, 4, 5, 3, 3, 3, 3, 3,
        1, 7, 8, 7, 4, 4, 6, 8, 6, 4, 2, 5, 2, 7, 7, 9, 2, 9, 5, 5, 6, 1, 6, 8,
        8, 1, 6, 1, 9, 7, 8, 5, 9, 8, 1, 8, 7, 8, 1, 6, 1, 7, 7, 1, 1, 7, 2, 7,
        9, 6, 9, 4, 1, 3, 4, 3, 4, 5, 7, 8, 2, 3, 2, 5, 8, 8, 6, 9, 1, 9, 2, 9,
        9, 9, 6, 9, 9, 9, 7, 5, 9, 7, 3, 9, 1, 3, 8, 3, 6, 3, 1, 9, 8, 9, 9, 8,
        4, 5, 4, 7, 5, 6, 9, 8]), ['w3j', 'g3t', 'r1s', 'g3t', 'w1j', 'b2s', 'b7j', 'g5s', 'r5s', 'w7t', 'b2t', 'r3s', 'b8j', 'r4j', 'r3t', 'g5s', 'r8s', 'w4t', 'r5t', 'g3j', 'b3t', 'w3s', 'r3t', 'g3s', 'g1s', 'g7t', 'w8j', 'g7j', 'g4j', 'w4t', 'r6s', 'r8s', 'w6t', 'g4s', 'r2t', 'b5t', 'r2t', 'g7s', 'r7j', 'w9t', 'g2t', 'w9j', 'w5j', 'r5j', 'w6s', 'r1j', 'g6s', 'r8s', 'g8s', 'b1s', 'r6s', 'b1j', 'b9s', 'g7t', 'b8j', 'r5s', 'g9s', 'w8t', 'b1s', 'b8s', 'r7s', 'r8t', 'r1t', 'r6s', 'r1j', 'g7j', 'w7t', 'g1j', 'b1t', 'w7s', 'w2j', 'r7t', 'b9s', 'b6s', 'r9j', 'r4s', 'r1t', 'r3s', 'w4t', 'b3j', 'b4t', 'g5t', 'w7j', 'b8t', 'w2t', 'b3j', 'w2t', 'r5s', 'r8t', 'r8t', 'r6t', 'w9s', 'w1t', 'b9j', 'g2s', 'w9t', 'w9t', 'b9t', 'b6t', 'b9t', 'g9s', 'b9j', 'g7t', 'b5t', 'w9j', 'b7s', 'b3j', 'w9j', 'w1j', 'r3j', 'b8s', 'w3j', 'g6t', 'w3t', 'r1j', 'g9t', 'w8j', 'g9j', 'r9s', 'r8j', 'g4s', 'b5j', 'b4j', 'r7j', 'g5s', 'w6j', 'r9j', 'r8s']] 
label[0]: tensor([3, 3, 1, 3, 1, 2, 7, 5, 5, 7, 2, 3, 8, 4, 3, 5, 8, 4, 5, 3, 3, 3, 3, 3,
        1, 7, 8, 7, 4, 4, 6, 8, 6, 4, 2, 5, 2, 7, 7, 9, 2, 9, 5, 5, 6, 1, 6, 8,
        8, 1, 6, 1, 9, 7, 8, 5, 9, 8, 1, 8, 7, 8, 1, 6, 1, 7, 7, 1, 1, 7, 2, 7,
        9, 6, 9, 4, 1, 3, 4, 3, 4, 5, 7, 8, 2, 3, 2, 5, 8, 8, 6, 9, 1, 9, 2, 9,
        9, 9, 6, 9, 9, 9, 7, 5, 9, 7, 3, 9, 1, 3, 8, 3, 6, 3, 1, 9, 8, 9, 9, 8,
        4, 5, 4, 7, 5, 6, 9, 8]) 
label[1]: ['w3j', 'g3t', 'r1s', 'g3t', 'w1j', 'b2s', 'b7j', 'g5s', 'r5s', 'w7t', 'b2t', 'r3s', 'b8j', 'r4j', 'r3t', 'g5s', 'r8s', 'w4t', 'r5t', 'g3j', 'b3t', 'w3s', 'r3t', 'g3s', 'g1s', 'g7t', 'w8j', 'g7j', 'g4j', 'w4t', 'r6s', 'r8s', 'w6t', 'g4s', 'r2t', 'b5t', 'r2t', 'g7s', 'r7j', 'w9t', 'g2t', 'w9j', 'w5j', 'r5j', 'w6s', 'r1j', 'g6s', 'r8s', 'g8s', 'b1s', 'r6s', 'b1j', 'b9s', 'g7t', 'b8j', 'r5s', 'g9s', 'w8t', 'b1s', 'b8s', 'r7s', 'r8t', 'r1t', 'r6s', 'r1j', 'g7j', 'w7t', 'g1j', 'b1t', 'w7s', 'w2j', 'r7t', 'b9s', 'b6s', 'r9j', 'r4s', 'r1t', 'r3s', 'w4t', 'b3j', 'b4t', 'g5t', 'w7j', 'b8t', 'w2t', 'b3j', 'w2t', 'r5s', 'r8t', 'r8t', 'r6t', 'w9s', 'w1t', 'b9j', 'g2s', 'w9t', 'w9t', 'b9t', 'b6t', 'b9t', 'g9s', 'b9j', 'g7t', 'b5t', 'w9j', 'b7s', 'b3j', 'w9j', 'w1j', 'r3j', 'b8s', 'w3j', 'g6t', 'w3t', 'r1j', 'g9t', 'w8j', 'g9j', 'r9s', 'r8j', 'g4s', 'b5j', 'b4j', 'r7j', 'g5s', 'w6j', 'r9j', 'r8s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.01933713 -0.01250672  0.00129208 ... -0.00093929 -0.0272199
 -0.01058918]
Average of silhouette coef: -0.0056438204
---
  0   1   2
0 0.0 0.4 0.3 
1 0.4 0.0 0.3 
2 0.3 0.3 0.0 
correlation [[ 1.         -0.55159938]
 [-0.55159938  1.        ]]
---
[[], [], []] [0 2 1 ... 0 0 0] [[  1.8364854 -25.75684  ]
 [ -1.2645272  43.82354  ]
 [ 27.120169  -50.016922 ]
 ...
 [-48.773144    7.105224 ]
 [-32.06402    38.537674 ]
 [-50.463135    6.470636 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.0056438204, 'cluster_all': array([-0.01933713, -0.01250672,  0.00129208, ..., -0.00093929,
       -0.0272199 , -0.01058918], dtype=float32), 'magnitude_avg': 0.5515993787013347, 'magnitude_all': -0.5515993787013347, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.0056438204, 'magnitude_avg': 0.5515993787013347, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([8, 1, 1, 8, 5, 4, 1, 2, 1, 3, 3, 9, 1, 4, 3, 1, 4, 3, 1, 8, 6, 9, 1, 8,
        4, 4, 3, 5, 7, 9, 9, 9, 9, 8, 4, 3, 7, 4, 9, 1, 5, 1, 3, 8, 3, 9, 6, 4,
        7, 7, 9, 2, 2, 9, 7, 3, 3, 7, 8, 9, 9, 1, 8, 3, 8, 6, 7, 4, 8, 8, 2, 4,
        5, 3, 1, 6, 9, 8, 1, 6, 1, 6, 6, 8, 9, 5, 6, 6, 6, 8, 4, 2, 9, 4, 7, 2,
        7, 3, 1, 8, 2, 7, 2, 4, 6, 1, 4, 7, 1, 7, 5, 1, 1, 6, 8, 4, 7, 8, 1, 9,
        9, 7, 4, 1, 1, 9, 1, 3]), ['w8s', 'g1t', 'w1t', 'g8s', 'b5j', 'g4s', 'w1s', 'r2j', 'r1j', 'r3j', 'b3j', 'b9j', 'g1j', 'r4s', 'g3s', 'r1t', 'b4t', 'r3t', 'r1t', 'g8t', 'b6t', 'w9t', 'r1t', 'g8s', 'w4s', 'r4s', 'r3s', 'w5t', 'w7s', 'w9t', 'r9t', 'g9j', 'b9t', 'r8j', 'b4t', 'g3t', 'g7j', 'g4s', 'g9s', 'r1j', 'b5j', 'w1t', 'r3j', 'g8t', 'g3t', 'b9t', 'w6j', 'g4t', 'b7s', 'b7t', 'b9t', 'w2j', 'w2j', 'r9j', 'g7s', 'r3s', 'r3s', 'b7t', 'w8j', 'w9j', 'r9j', 'r1j', 'w8s', 'r3j', 'b8s', 'w6j', 'r7j', 'w4t', 'b8j', 'w8j', 'w2j', 'w4t', 'b5j', 'b3t', 'r1s', 'b6s', 'r9s', 'w8j', 'b1t', 'b6t', 'b1j', 'w6s', 'w6s', 'g8s', 'w9s', 'g5j', 'b6t', 'g6j', 'r6s', 'r8t', 'g4j', 'b2t', 'w9j', 'g4s', 'w7t', 'g2j', 'g7t', 'b3j', 'r1j', 'g8t', 'g2j', 'w7t', 'b2s', 'b4j', 'r6s', 'w1t', 'r4s', 'g7j', 'g1j', 'b7t', 'b5j', 'g1t', 'r1j', 'g6s', 'w8s', 'b4j', 'w7t', 'r8t', 'b1j', 'w9s', 'g9s', 'b7t', 'b4j', 'b1t', 'b1j', 'w9j', 'w1t', 'r3t']] 
label[0]: tensor([8, 1, 1, 8, 5, 4, 1, 2, 1, 3, 3, 9, 1, 4, 3, 1, 4, 3, 1, 8, 6, 9, 1, 8,
        4, 4, 3, 5, 7, 9, 9, 9, 9, 8, 4, 3, 7, 4, 9, 1, 5, 1, 3, 8, 3, 9, 6, 4,
        7, 7, 9, 2, 2, 9, 7, 3, 3, 7, 8, 9, 9, 1, 8, 3, 8, 6, 7, 4, 8, 8, 2, 4,
        5, 3, 1, 6, 9, 8, 1, 6, 1, 6, 6, 8, 9, 5, 6, 6, 6, 8, 4, 2, 9, 4, 7, 2,
        7, 3, 1, 8, 2, 7, 2, 4, 6, 1, 4, 7, 1, 7, 5, 1, 1, 6, 8, 4, 7, 8, 1, 9,
        9, 7, 4, 1, 1, 9, 1, 3]) 
label[1]: ['w8s', 'g1t', 'w1t', 'g8s', 'b5j', 'g4s', 'w1s', 'r2j', 'r1j', 'r3j', 'b3j', 'b9j', 'g1j', 'r4s', 'g3s', 'r1t', 'b4t', 'r3t', 'r1t', 'g8t', 'b6t', 'w9t', 'r1t', 'g8s', 'w4s', 'r4s', 'r3s', 'w5t', 'w7s', 'w9t', 'r9t', 'g9j', 'b9t', 'r8j', 'b4t', 'g3t', 'g7j', 'g4s', 'g9s', 'r1j', 'b5j', 'w1t', 'r3j', 'g8t', 'g3t', 'b9t', 'w6j', 'g4t', 'b7s', 'b7t', 'b9t', 'w2j', 'w2j', 'r9j', 'g7s', 'r3s', 'r3s', 'b7t', 'w8j', 'w9j', 'r9j', 'r1j', 'w8s', 'r3j', 'b8s', 'w6j', 'r7j', 'w4t', 'b8j', 'w8j', 'w2j', 'w4t', 'b5j', 'b3t', 'r1s', 'b6s', 'r9s', 'w8j', 'b1t', 'b6t', 'b1j', 'w6s', 'w6s', 'g8s', 'w9s', 'g5j', 'b6t', 'g6j', 'r6s', 'r8t', 'g4j', 'b2t', 'w9j', 'g4s', 'w7t', 'g2j', 'g7t', 'b3j', 'r1j', 'g8t', 'g2j', 'w7t', 'b2s', 'b4j', 'r6s', 'w1t', 'r4s', 'g7j', 'g1j', 'b7t', 'b5j', 'g1t', 'r1j', 'g6s', 'w8s', 'b4j', 'w7t', 'r8t', 'b1j', 'w9s', 'g9s', 'b7t', 'b4j', 'b1t', 'b1j', 'w9j', 'w1t', 'r3t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.03402703 0.09206259 0.01189634 ... 0.09897442 0.06502346 0.10821217]
Average of silhouette coef: 0.07822236
---
  0   1   2   3
0 0.0 2.0 2.7 2.1 
1 2.0 0.0 3.7 3.9 
2 2.7 3.7 0.0 3.8 
3 2.1 3.9 3.8 0.0 
correlation [[ 1.         -0.37389943]
 [-0.37389943  1.        ]]
---
[[], [], [], []] [0 2 0 ... 3 1 2] [[ 29.260788 -33.43503 ]
 [-29.05771   36.805832]
 [ 18.985958  35.458805]
 ...
 [ 40.585896  -8.630875]
 [ 17.891228 -19.405056]
 [-29.28641   41.757484]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': 0.07822236, 'cluster_all': array([0.03402703, 0.09206259, 0.01189634, ..., 0.09897442, 0.06502346,
       0.10821217], dtype=float32), 'magnitude_avg': 0.3738994340316748, 'magnitude_all': -0.3738994340316748, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': 0.07822236, 'magnitude_avg': 0.3738994340316748, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 4, 7, 4, 8, 5, 2, 2, 8, 1, 6, 4, 4, 4, 9, 5, 2, 6, 7, 6, 4, 9, 2, 4,
        4, 4, 6, 3, 8, 4, 6, 3, 1, 4, 7, 7, 1, 3, 9, 9, 7, 6, 9, 6, 1, 7, 1, 5,
        3, 5, 7, 7, 3, 1, 2, 4, 7, 2, 6, 7, 7, 9, 8, 1, 4, 2, 7, 7, 2, 8, 6, 1,
        6, 6, 9, 6, 8, 7, 2, 9, 6, 1, 5, 8, 1, 9, 5, 6, 3, 2, 1, 1, 2, 2, 5, 1,
        1, 9, 2, 5, 3, 7, 8, 9, 7, 2, 7, 3, 5, 7, 9, 3, 3, 2, 7, 1, 4, 7, 4, 5,
        4, 9, 8, 9, 2, 5, 1, 8]), ['g2t', 'r4t', 'b7t', 'r4t', 'g8j', 'r5t', 'r2t', 'r2t', 'w8j', 'w1s', 'w6t', 'w4t', 'b4j', 'b4s', 'b9s', 'w5j', 'g2t', 'g6t', 'b7j', 'r6s', 'r4s', 'g9s', 'w2t', 'w4s', 'w4j', 'g4j', 'b6j', 'r3j', 'w8j', 'g4t', 'b6t', 'w3s', 'b1s', 'r4t', 'g7s', 'w7t', 'g1t', 'w3t', 'g9t', 'b9s', 'r7t', 'r6j', 'b9j', 'w6t', 'r1j', 'r7s', 'w1t', 'w5j', 'g3t', 'w5t', 'r7s', 'r7s', 'g3s', 'r1j', 'g2j', 'w4t', 'w7t', 'w2s', 'r6j', 'w7s', 'r7j', 'w9j', 'r8t', 'w1s', 'b4t', 'g2t', 'b7t', 'w7t', 'w2j', 'r8t', 'w6j', 'w1j', 'b6s', 'g6s', 'g9t', 'w6t', 'b8j', 'w7t', 'g2s', 'b9j', 'b6j', 'g1s', 'g5s', 'w8j', 'w1j', 'w9t', 'r5j', 'b6s', 'r3s', 'b2j', 'b1t', 'b1s', 'g2t', 'w2t', 'g5j', 'g1j', 'b1j', 'r9t', 'w2t', 'g5j', 'w3j', 'g7s', 'r8t', 'w9s', 'b7j', 'g2s', 'r7t', 'g3t', 'g5t', 'b7s', 'b9t', 'r3j', 'w3t', 'w2t', 'b7t', 'w1s', 'w4s', 'g7t', 'r4s', 'g5j', 'b4s', 'w9j', 'g8t', 'g9s', 'w2s', 'b5j', 'r1t', 'b8s']] 
label[0]: tensor([2, 4, 7, 4, 8, 5, 2, 2, 8, 1, 6, 4, 4, 4, 9, 5, 2, 6, 7, 6, 4, 9, 2, 4,
        4, 4, 6, 3, 8, 4, 6, 3, 1, 4, 7, 7, 1, 3, 9, 9, 7, 6, 9, 6, 1, 7, 1, 5,
        3, 5, 7, 7, 3, 1, 2, 4, 7, 2, 6, 7, 7, 9, 8, 1, 4, 2, 7, 7, 2, 8, 6, 1,
        6, 6, 9, 6, 8, 7, 2, 9, 6, 1, 5, 8, 1, 9, 5, 6, 3, 2, 1, 1, 2, 2, 5, 1,
        1, 9, 2, 5, 3, 7, 8, 9, 7, 2, 7, 3, 5, 7, 9, 3, 3, 2, 7, 1, 4, 7, 4, 5,
        4, 9, 8, 9, 2, 5, 1, 8]) 
label[1]: ['g2t', 'r4t', 'b7t', 'r4t', 'g8j', 'r5t', 'r2t', 'r2t', 'w8j', 'w1s', 'w6t', 'w4t', 'b4j', 'b4s', 'b9s', 'w5j', 'g2t', 'g6t', 'b7j', 'r6s', 'r4s', 'g9s', 'w2t', 'w4s', 'w4j', 'g4j', 'b6j', 'r3j', 'w8j', 'g4t', 'b6t', 'w3s', 'b1s', 'r4t', 'g7s', 'w7t', 'g1t', 'w3t', 'g9t', 'b9s', 'r7t', 'r6j', 'b9j', 'w6t', 'r1j', 'r7s', 'w1t', 'w5j', 'g3t', 'w5t', 'r7s', 'r7s', 'g3s', 'r1j', 'g2j', 'w4t', 'w7t', 'w2s', 'r6j', 'w7s', 'r7j', 'w9j', 'r8t', 'w1s', 'b4t', 'g2t', 'b7t', 'w7t', 'w2j', 'r8t', 'w6j', 'w1j', 'b6s', 'g6s', 'g9t', 'w6t', 'b8j', 'w7t', 'g2s', 'b9j', 'b6j', 'g1s', 'g5s', 'w8j', 'w1j', 'w9t', 'r5j', 'b6s', 'r3s', 'b2j', 'b1t', 'b1s', 'g2t', 'w2t', 'g5j', 'g1j', 'b1j', 'r9t', 'w2t', 'g5j', 'w3j', 'g7s', 'r8t', 'w9s', 'b7j', 'g2s', 'r7t', 'g3t', 'g5t', 'b7s', 'b9t', 'r3j', 'w3t', 'w2t', 'b7t', 'w1s', 'w4s', 'g7t', 'r4s', 'g5j', 'b4s', 'w9j', 'g8t', 'g9s', 'w2s', 'b5j', 'r1t', 'b8s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([5, 7, 7, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5,
        5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([3, 2, 8, 2, 1, 1, 3, 3, 4, 3, 4, 4, 3, 5, 2, 8, 2, 8, 8, 7, 5, 2, 4, 2,
        4, 1, 2, 4, 9, 9, 2, 7, 1, 6, 2, 3, 6, 3, 1, 2, 3, 2, 9, 1, 7, 3, 4, 1,
        1, 4, 4, 3, 9, 6, 2, 6, 3, 3, 9, 9, 8, 7, 4, 7, 2, 4, 3, 8, 1, 1, 7, 6,
        8, 5, 1, 7, 7, 6, 1, 3, 2, 5, 1, 4, 3, 2, 3, 8, 7, 1, 8, 9, 9, 8, 6, 9,
        4, 1, 3, 3, 2, 7, 8, 5, 7, 2, 1, 5, 1, 6, 1, 2, 2, 6, 8, 2, 5, 1, 2, 6,
        6, 7, 6, 1, 6, 1, 4, 5])
Accuracy (count): tensor(9) 
Accuracy (ratio) tensor(0.0703)
Accuracy:
 [[ 0  0  0  0 21  0  0  0  0]
 [ 0  0  0  0 19  0  2  0  0]
 [ 0  0  0  0 17  0  1  0  0]
 [ 0  0  0  0 13  0  1  0  0]
 [ 0  0  0  0  8  0  0  0  0]
 [ 0  0  0  0 12  0  1  0  0]
 [ 0  0  0  0 11  0  1  0  0]
 [ 0  0  0  0 11  0  1  0  0]
 [ 0  0  0  0  9  0  0  0  0]]
Accuracy:
 [[0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.905 0.    0.095 0.    0.   ]
 [0.    0.    0.    0.    0.944 0.    0.056 0.    0.   ]
 [0.    0.    0.    0.    0.929 0.    0.071 0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.923 0.    0.077 0.    0.   ]
 [0.    0.    0.    0.    0.917 0.    0.083 0.    0.   ]
 [0.    0.    0.    0.    0.917 0.    0.083 0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([8, 4, 3, 8, 2, 9, 2, 9, 2, 3, 7, 2, 4, 4, 5, 8, 1, 2, 3, 5, 5, 7, 1, 9,
        1, 5, 9, 9, 7, 7, 6, 9, 1, 1, 7, 3, 7, 1, 5, 8, 5, 4, 1, 7, 3, 7, 5, 8,
        3, 6, 2, 9, 8, 3, 2, 2, 6, 2, 4, 9, 1, 5, 4, 6, 6, 6, 2, 1, 1, 9, 8, 2,
        7, 1, 8, 8, 9, 3, 3, 1, 4, 8, 6, 6, 1, 2, 5, 9, 3, 7, 4, 4, 3, 8, 9, 7,
        2, 1, 2, 6, 9, 5, 1, 4, 8, 9, 5, 2, 6, 3, 3, 1, 3, 3, 9, 4, 4, 1, 4, 2,
        3, 9, 8, 5, 4, 6, 8, 2])
Accuracy (count): tensor(12) 
Accuracy (ratio) tensor(0.0938)
Accuracy:
 [[ 0  0  0  0 17  0  0  0  0]
 [ 0  0  0  0 17  0  0  0  0]
 [ 0  0  0  0 14  0  2  0  0]
 [ 0  0  0  0 14  0  0  0  0]
 [ 0  0  0  0 12  0  0  0  0]
 [ 0  0  0  0 11  0  0  0  0]
 [ 0  0  0  0 11  0  0  0  0]
 [ 0  0  0  0 14  0  0  0  0]
 [ 0  0  0  0 16  0  0  0  0]]
Accuracy:
 [[0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.875 0.    0.125 0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    1.    0.    0.    0.    0.   ]]
---
Silhouette values: [-0.07836823 -0.04863823 -0.14743873 ... -0.13179158 -0.12093852
 -0.116198  ]
Average of silhouette coef: -0.023393067
---
  1   2   3   4   5   6   7   8   9
1 0.0 1.3 2.0 2.7 3.3 3.9 4.6 5.3 6.1 
2 1.3 0.0 1.0 1.7 2.2 2.8 3.5 4.2 5.0 
3 2.0 1.0 0.0 0.8 1.3 1.9 2.6 3.4 4.2 
4 2.7 1.7 0.8 0.0 0.8 1.3 2.0 2.8 3.6 
5 3.3 2.2 1.3 0.8 0.0 0.8 1.4 2.2 3.0 
6 3.9 2.8 1.9 1.3 0.8 0.0 0.7 1.5 2.4 
7 4.6 3.5 2.6 2.0 1.4 0.7 0.0 0.8 1.7 
8 5.3 4.2 3.4 2.8 2.2 1.5 0.8 0.0 1.0 
9 6.1 5.0 4.2 3.6 3.0 2.4 1.7 1.0 0.0 
correlation [[1.         0.98831879]
 [0.98831879 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [2 4 7 ... 7 6 6] [[-55.467686   -1.4441285]
 [ 35.277477  -21.010416 ]
 [  2.5932772  15.849539 ]
 ...
 [  7.8421807  17.949886 ]
 [-28.06001    32.692226 ]
 [-12.870374   28.10773  ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 7, 7, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 7, 5, 7, 5,
        5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(57) 
Accuracy (ratio) tensor(0.1166)
Accuracy:
 [[ 0  0  0  0 44  0  1  0  0]
 [ 0  0  0  0 49  0  3  0  0]
 [ 0  0  0  0 52  0  5  0  0]
 [ 0  0  0  0 58  0  2  0  0]
 [ 0  0  0  0 54  0  7  0  0]
 [ 0  0  0  0 56  0  4  0  0]
 [ 0  0  0  0 54  0  3  0  0]
 [ 0  0  0  0 51  0  1  0  0]
 [ 0  0  0  0 44  0  1  0  0]]
Accuracy:
 [[0.    0.    0.    0.    0.978 0.    0.022 0.    0.   ]
 [0.    0.    0.    0.    0.942 0.    0.058 0.    0.   ]
 [0.    0.    0.    0.    0.912 0.    0.088 0.    0.   ]
 [0.    0.    0.    0.    0.967 0.    0.033 0.    0.   ]
 [0.    0.    0.    0.    0.885 0.    0.115 0.    0.   ]
 [0.    0.    0.    0.    0.933 0.    0.067 0.    0.   ]
 [0.    0.    0.    0.    0.947 0.    0.053 0.    0.   ]
 [0.    0.    0.    0.    0.981 0.    0.019 0.    0.   ]
 [0.    0.    0.    0.    0.978 0.    0.022 0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.1  0.0  0.4  0.0 -0.2 -0.1  0.1  0.0  0.0  0.0  0.0 -0.1 -0.0 -0.1 -0.0  0.1  0.0  3.2 
 0.0  0.0 -0.1  0.1  0.7 -0.1  0.3  0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.3 -0.1 -0.1 -0.1  2.1 
 0.0  0.1  0.0  0.1  0.4  0.0 -0.0  0.1  0.1  0.0 -0.1 -0.1  0.0  0.1  0.1 -0.1 -0.1  0.0 -0.2  1.3 
 0.0  0.1  0.1  0.0  0.1  0.0  0.0 -0.0  0.1  0.0 -0.1 -0.1  0.0  0.1  0.0  0.1 -0.1 -0.0  0.1  0.6 
 0.0  0.0  0.1 -0.0  0.5 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.0  0.0  0.0 -0.0 -0.1 -0.0  0.2 -0.1  0.0 
 0.0 -0.0  0.0 -0.0  0.4  0.1  0.1  0.0  0.1  0.0 -0.1  0.0  0.0  0.0 -0.0 -0.1 -0.0 -0.1 -0.2 -0.6 
-0.0 -0.2 -0.0  0.0  0.5  0.1  0.1  0.0  0.2  0.0 -0.0  0.1 -0.0 -0.0 -0.1  0.0  0.1  0.1 -0.1 -1.3 
-0.0 -0.5 -0.0  0.2  0.4 -0.1  0.1 -0.0  0.2  0.0  0.1  0.2 -0.1 -0.1 -0.0  0.1  0.1  0.0 -0.1 -2.0 
-0.1 -0.8 -0.1  0.4  0.3  0.1  0.1 -0.1  0.1  0.1  0.3  0.2 -0.1 -0.1  0.1  0.0  0.2 -0.0 -0.0 -2.8 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.95209765
pred: tensor([5, 5, 5, 7, 5, 5, 5, 7, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(59) 
Accuracy (ratio) tensor(0.9219)
Accuracy:
 [[59]]
Accuracy:
 [[1.]]
-0.0 -0.0 -0.1  0.0  0.4  0.0 -0.2 -0.1  0.1  0.0  0.0  0.0  0.0 -0.1 -0.0 -0.1 -0.0  0.1  0.0  3.2 
 0.0  0.0 -0.1  0.1  0.7 -0.1  0.3  0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.3 -0.1 -0.1 -0.1  2.1 
 0.0  0.1  0.0  0.1  0.4  0.0 -0.0  0.1  0.1  0.0 -0.1 -0.1  0.0  0.1  0.1 -0.1 -0.1  0.0 -0.2  1.3 
 0.0  0.1  0.1  0.0  0.1  0.0  0.0 -0.0  0.1  0.0 -0.1 -0.1  0.0  0.1  0.0  0.1 -0.1 -0.0  0.1  0.6 
 0.0  0.0  0.1 -0.0  0.5 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.0  0.0  0.0 -0.0 -0.1 -0.0  0.2 -0.1  0.0 
 0.0 -0.0  0.0 -0.0  0.4  0.1  0.1  0.0  0.1  0.0 -0.1  0.0  0.0  0.0 -0.0 -0.1 -0.0 -0.1 -0.2 -0.6 
-0.0 -0.2 -0.0  0.0  0.5  0.1  0.1  0.0  0.2  0.0 -0.0  0.1 -0.0 -0.0 -0.1  0.0  0.1  0.1 -0.1 -1.3 
-0.0 -0.5 -0.0  0.2  0.4 -0.1  0.1 -0.0  0.2  0.0  0.1  0.2 -0.1 -0.1 -0.0  0.1  0.1  0.0 -0.1 -2.0 
-0.1 -0.8 -0.1  0.4  0.3  0.1  0.1 -0.1  0.1  0.1  0.3  0.2 -0.1 -0.1  0.1  0.0  0.2 -0.0 -0.0 -2.8 
Minimum distance of calculation. 
true answer: 5 
indices: 5 
distance: 1.1360794
pred: tensor([5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.1  0.0  0.4  0.0 -0.2 -0.1  0.1  0.0  0.0  0.0  0.0 -0.1 -0.0 -0.1 -0.0  0.1  0.0  3.2 
 0.0  0.0 -0.1  0.1  0.7 -0.1  0.3  0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.3 -0.1 -0.1 -0.1  2.1 
 0.0  0.1  0.0  0.1  0.4  0.0 -0.0  0.1  0.1  0.0 -0.1 -0.1  0.0  0.1  0.1 -0.1 -0.1  0.0 -0.2  1.3 
 0.0  0.1  0.1  0.0  0.1  0.0  0.0 -0.0  0.1  0.0 -0.1 -0.1  0.0  0.1  0.0  0.1 -0.1 -0.0  0.1  0.6 
 0.0  0.0  0.1 -0.0  0.5 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.0  0.0  0.0 -0.0 -0.1 -0.0  0.2 -0.1  0.0 
 0.0 -0.0  0.0 -0.0  0.4  0.1  0.1  0.0  0.1  0.0 -0.1  0.0  0.0  0.0 -0.0 -0.1 -0.0 -0.1 -0.2 -0.6 
-0.0 -0.2 -0.0  0.0  0.5  0.1  0.1  0.0  0.2  0.0 -0.0  0.1 -0.0 -0.0 -0.1  0.0  0.1  0.1 -0.1 -1.3 
-0.0 -0.5 -0.0  0.2  0.4 -0.1  0.1 -0.0  0.2  0.0  0.1  0.2 -0.1 -0.1 -0.0  0.1  0.1  0.0 -0.1 -2.0 
-0.1 -0.8 -0.1  0.4  0.3  0.1  0.1 -0.1  0.1  0.1  0.3  0.2 -0.1 -0.1  0.1  0.0  0.2 -0.0 -0.0 -2.8 
Minimum distance of calculation. 
true answer: 8 
indices: 8 
distance: 0.8109434
pred: tensor([5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        7, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.1  0.0  0.4  0.0 -0.2 -0.1  0.1  0.0  0.0  0.0  0.0 -0.1 -0.0 -0.1 -0.0  0.1  0.0  3.2 
 0.0  0.0 -0.1  0.1  0.7 -0.1  0.3  0.1 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0 -0.3 -0.1 -0.1 -0.1  2.1 
 0.0  0.1  0.0  0.1  0.4  0.0 -0.0  0.1  0.1  0.0 -0.1 -0.1  0.0  0.1  0.1 -0.1 -0.1  0.0 -0.2  1.3 
 0.0  0.1  0.1  0.0  0.1  0.0  0.0 -0.0  0.1  0.0 -0.1 -0.1  0.0  0.1  0.0  0.1 -0.1 -0.0  0.1  0.6 
 0.0  0.0  0.1 -0.0  0.5 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.0  0.0  0.0 -0.0 -0.1 -0.0  0.2 -0.1  0.0 
 0.0 -0.0  0.0 -0.0  0.4  0.1  0.1  0.0  0.1  0.0 -0.1  0.0  0.0  0.0 -0.0 -0.1 -0.0 -0.1 -0.2 -0.6 
-0.0 -0.2 -0.0  0.0  0.5  0.1  0.1  0.0  0.2  0.0 -0.0  0.1 -0.0 -0.0 -0.1  0.0  0.1  0.1 -0.1 -1.3 
-0.0 -0.5 -0.0  0.2  0.4 -0.1  0.1 -0.0  0.2  0.0  0.1  0.2 -0.1 -0.1 -0.0  0.1  0.1  0.0 -0.1 -2.0 
-0.1 -0.8 -0.1  0.4  0.3  0.1  0.1 -0.1  0.1  0.1  0.3  0.2 -0.1 -0.1  0.1  0.0  0.2 -0.0 -0.0 -2.8 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 0.67020917
results (all): {'reconst_1x1_avg': 0.0703125, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.09375, 'cross_0x1_all': nan, 'cluster_avg': -0.023393067, 'cluster_all': array([-0.07836823, -0.04863823, -0.14743873, ..., -0.13179158,
       -0.12093852, -0.116198  ], dtype=float32), 'magnitude_avg': 0.9883187894308968, 'magnitude_all': 0.9883187894308968, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.921875, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.0703125, 'cross_0x1_avg': 0.09375, 'cluster_avg': -0.023393067, 'magnitude_avg': 0.9883187894308968, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.921875, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([9, 7, 7, 6, 2, 4, 5, 7, 6, 7, 6, 1, 8, 6, 6, 7, 8, 4, 7, 7, 7, 4, 9, 1,
        8, 2, 9, 2, 4, 3, 6, 9, 6, 8, 9, 6, 9, 8, 2, 3, 8, 9, 6, 4, 9, 2, 1, 9,
        7, 7, 7, 5, 1, 1, 4, 1, 4, 7, 3, 7, 8, 9, 4, 5, 9, 6, 4, 8, 2, 1, 9, 7,
        1, 4, 1, 9, 4, 1, 3, 2, 1, 3, 8, 3, 1, 3, 9, 6, 7, 1, 2, 3, 2, 6, 6, 2,
        3, 8, 8, 8, 3, 4, 5, 8, 4, 7, 8, 8, 4, 1, 8, 5, 2, 8, 6, 4, 6, 5, 1, 8,
        6, 2, 6, 7, 4, 3, 5, 9]), ['b9s', 'r7j', 'r7s', 'w6t', 'w2s', 'r4s', 'w5s', 'b7t', 'r6j', 'b7t', 'g6j', 'w1t', 'g8s', 'r6t', 'b6s', 'g7j', 'r8j', 'w4j', 'g7j', 'r7s', 'b7t', 'w4s', 'b9s', 'r1j', 'g8s', 'r2j', 'r9t', 'g2j', 'r4t', 'b3t', 'w6t', 'g9j', 'w6t', 'g8s', 'w9t', 'w6t', 'r9s', 'b8j', 'r2s', 'r3j', 'r8s', 'r9s', 'w6t', 'g4t', 'g9j', 'b2s', 'b1t', 'w9j', 'w7s', 'r7s', 'r7j', 'r5s', 'r1j', 'r1t', 'w4j', 'b1t', 'r4j', 'w7t', 'b3s', 'r7s', 'w8s', 'w9j', 'g4j', 'g5t', 'g9s', 'w6s', 'r4j', 'g8s', 'b2j', 'r1s', 'r9j', 'r7j', 'g1j', 'w4s', 'b1t', 'g9t', 'g4j', 'b1s', 'r3s', 'b2j', 'g1j', 'r3t', 'r8s', 'r3j', 'b1j', 'b3j', 'w9j', 'b6s', 'g7t', 'r1s', 'w2j', 'r3t', 'b2t', 'b6s', 'g6j', 'w2s', 'r3j', 'w8s', 'w8s', 'w8s', 'g3j', 'r4j', 'g5t', 'b8s', 'r4t', 'w7j', 'r8s', 'g8j', 'b4s', 'w1t', 'g8t', 'r5t', 'g2j', 'g8t', 'g6j', 'b4t', 'w6t', 'b5t', 'r1s', 'w8s', 'b6j', 'w2j', 'r6s', 'b7s', 'r4t', 'r3s', 'w5s', 'b9t']] 
label[0]: tensor([9, 7, 7, 6, 2, 4, 5, 7, 6, 7, 6, 1, 8, 6, 6, 7, 8, 4, 7, 7, 7, 4, 9, 1,
        8, 2, 9, 2, 4, 3, 6, 9, 6, 8, 9, 6, 9, 8, 2, 3, 8, 9, 6, 4, 9, 2, 1, 9,
        7, 7, 7, 5, 1, 1, 4, 1, 4, 7, 3, 7, 8, 9, 4, 5, 9, 6, 4, 8, 2, 1, 9, 7,
        1, 4, 1, 9, 4, 1, 3, 2, 1, 3, 8, 3, 1, 3, 9, 6, 7, 1, 2, 3, 2, 6, 6, 2,
        3, 8, 8, 8, 3, 4, 5, 8, 4, 7, 8, 8, 4, 1, 8, 5, 2, 8, 6, 4, 6, 5, 1, 8,
        6, 2, 6, 7, 4, 3, 5, 9]) 
label[1]: ['b9s', 'r7j', 'r7s', 'w6t', 'w2s', 'r4s', 'w5s', 'b7t', 'r6j', 'b7t', 'g6j', 'w1t', 'g8s', 'r6t', 'b6s', 'g7j', 'r8j', 'w4j', 'g7j', 'r7s', 'b7t', 'w4s', 'b9s', 'r1j', 'g8s', 'r2j', 'r9t', 'g2j', 'r4t', 'b3t', 'w6t', 'g9j', 'w6t', 'g8s', 'w9t', 'w6t', 'r9s', 'b8j', 'r2s', 'r3j', 'r8s', 'r9s', 'w6t', 'g4t', 'g9j', 'b2s', 'b1t', 'w9j', 'w7s', 'r7s', 'r7j', 'r5s', 'r1j', 'r1t', 'w4j', 'b1t', 'r4j', 'w7t', 'b3s', 'r7s', 'w8s', 'w9j', 'g4j', 'g5t', 'g9s', 'w6s', 'r4j', 'g8s', 'b2j', 'r1s', 'r9j', 'r7j', 'g1j', 'w4s', 'b1t', 'g9t', 'g4j', 'b1s', 'r3s', 'b2j', 'g1j', 'r3t', 'r8s', 'r3j', 'b1j', 'b3j', 'w9j', 'b6s', 'g7t', 'r1s', 'w2j', 'r3t', 'b2t', 'b6s', 'g6j', 'w2s', 'r3j', 'w8s', 'w8s', 'w8s', 'g3j', 'r4j', 'g5t', 'b8s', 'r4t', 'w7j', 'r8s', 'g8j', 'b4s', 'w1t', 'g8t', 'r5t', 'g2j', 'g8t', 'g6j', 'b4t', 'w6t', 'b5t', 'r1s', 'w8s', 'b6j', 'w2j', 'r6s', 'b7s', 'r4t', 'r3s', 'w5s', 'b9t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.10135007 0.06113204 0.06561614 ... 0.08549341 0.05891724 0.10873944]
Average of silhouette coef:Traceback (most recent call last):
  File "/home/taka/.pyenv/versions/3.6.9/lib/python3.6/multiprocessing/queues.py", line 240, in _feed
    send_bytes(obj)
  File "/home/taka/.pyenv/versions/3.6.9/lib/python3.6/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/taka/.pyenv/versions/3.6.9/lib/python3.6/multiprocessing/connection.py", line 404, in _send_bytes
    self._send(header + buf)
  File "/home/taka/.pyenv/versions/3.6.9/lib/python3.6/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
 0.075965315
---
  0   1   2
0 0.0 2.7 2.9 
1 2.7 0.0 2.8 
2 2.9 2.8 0.0 
correlation [[1.         0.99086437]
 [0.99086437 1.        ]]
---
[[], [], []] [1 0 1 ... 0 0 2] [[  3.5318823  56.252247 ]
 [  8.744322   15.960641 ]
 [-36.421246   18.444298 ]
 ...
 [-30.168459  -13.454567 ]
 [  9.1512575  38.725845 ]
 [-54.92883    16.680313 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': 0.075965315, 'cluster_all': array([0.10135007, 0.06113204, 0.06561614, ..., 0.08549341, 0.05891724,
       0.10873944], dtype=float32), 'magnitude_avg': 0.9908643727133619, 'magnitude_all': 0.9908643727133619, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': 0.075965315, 'magnitude_avg': 0.9908643727133619, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 3, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_3', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3', 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 7, 9, 2, 7, 4, 7, 7, 9, 7, 8, 2, 5, 8, 5, 7, 8, 6, 4, 2, 9, 7, 9, 2,
        4, 7, 7, 6, 4, 9, 9, 6, 2, 4, 5, 2, 8, 7, 9, 5, 6, 1, 4, 8, 9, 4, 5, 1,
        4, 2, 5, 5, 9, 6, 2, 3, 4, 3, 2, 8, 9, 5, 1, 7, 8, 5, 9, 1, 7, 8, 9, 6,
        4, 7, 2, 7, 6, 5, 4, 6, 3, 6, 3, 5, 4, 6, 9, 1, 6, 8, 2, 9, 1, 1, 8, 1,
        2, 8, 4, 4, 9, 6, 3, 7, 9, 1, 2, 8, 7, 6, 6, 2, 5, 3, 1, 1, 4, 8, 9, 4,
        3, 8, 1, 9, 1, 3, 8, 6]), ['r6t', 'b7j', 'b9j', 'r2s', 'w7j', 'w4s', 'r7t', 'b7j', 'r9s', 'b7t', 'b8s', 'b2s', 'w5t', 'w8j', 'r5s', 'w7s', 'r8j', 'g6j', 'r4s', 'w2j', 'r9t', 'b7j', 'r9s', 'w2t', 'b4s', 'g7s', 'b7t', 'g6t', 'r4j', 'b9j', 'g9j', 'b6s', 'w2s', 'r4j', 'r5j', 'r2t', 'r8t', 'g7t', 'b9s', 'b5t', 'r6j', 'g1s', 'w4j', 'b8j', 'g9j', 'r4j', 'r5t', 'w1j', 'r4t', 'b2t', 'b5s', 'w5t', 'g9s', 'g6s', 'w2j', 'r3s', 'g4j', 'b3t', 'b2s', 'b8s', 'b9t', 'w5j', 'b1s', 'g7t', 'g8j', 'g5s', 'r9s', 'r1t', 'r7j', 'r8t', 'b9s', 'g6j', 'b4t', 'g7j', 'b2s', 'w7s', 'w6t', 'g5t', 'b4s', 'g6s', 'g3j', 'b6j', 'g3t', 'w5j', 'b4j', 'g6t', 'r9s', 'r1s', 'b6j', 'r8s', 'g2s', 'r9j', 'w1t', 'w1t', 'w8j', 'w1t', 'w2t', 'b8t', 'g4t', 'r4t', 'g9j', 'g6j', 'r3s', 'w7t', 'r9s', 'r1j', 'r2j', 'r8j', 'w7j', 'r6s', 'b6j', 'w2j', 'r5s', 'r3j', 'w1s', 'b1j', 'r4j', 'w8j', 'r9j', 'w4s', 'w3t', 'r8t', 'g1s', 'g9j', 'w1s', 'b3t', 'r8t', 'w6j']] 
label[0]: tensor([6, 7, 9, 2, 7, 4, 7, 7, 9, 7, 8, 2, 5, 8, 5, 7, 8, 6, 4, 2, 9, 7, 9, 2,
        4, 7, 7, 6, 4, 9, 9, 6, 2, 4, 5, 2, 8, 7, 9, 5, 6, 1, 4, 8, 9, 4, 5, 1,
        4, 2, 5, 5, 9, 6, 2, 3, 4, 3, 2, 8, 9, 5, 1, 7, 8, 5, 9, 1, 7, 8, 9, 6,
        4, 7, 2, 7, 6, 5, 4, 6, 3, 6, 3, 5, 4, 6, 9, 1, 6, 8, 2, 9, 1, 1, 8, 1,
        2, 8, 4, 4, 9, 6, 3, 7, 9, 1, 2, 8, 7, 6, 6, 2, 5, 3, 1, 1, 4, 8, 9, 4,
        3, 8, 1, 9, 1, 3, 8, 6]) 
label[1]: ['r6t', 'b7j', 'b9j', 'r2s', 'w7j', 'w4s', 'r7t', 'b7j', 'r9s', 'b7t', 'b8s', 'b2s', 'w5t', 'w8j', 'r5s', 'w7s', 'r8j', 'g6j', 'r4s', 'w2j', 'r9t', 'b7j', 'r9s', 'w2t', 'b4s', 'g7s', 'b7t', 'g6t', 'r4j', 'b9j', 'g9j', 'b6s', 'w2s', 'r4j', 'r5j', 'r2t', 'r8t', 'g7t', 'b9s', 'b5t', 'r6j', 'g1s', 'w4j', 'b8j', 'g9j', 'r4j', 'r5t', 'w1j', 'r4t', 'b2t', 'b5s', 'w5t', 'g9s', 'g6s', 'w2j', 'r3s', 'g4j', 'b3t', 'b2s', 'b8s', 'b9t', 'w5j', 'b1s', 'g7t', 'g8j', 'g5s', 'r9s', 'r1t', 'r7j', 'r8t', 'b9s', 'g6j', 'b4t', 'g7j', 'b2s', 'w7s', 'w6t', 'g5t', 'b4s', 'g6s', 'g3j', 'b6j', 'g3t', 'w5j', 'b4j', 'g6t', 'r9s', 'r1s', 'b6j', 'r8s', 'g2s', 'r9j', 'w1t', 'w1t', 'w8j', 'w1t', 'w2t', 'b8t', 'g4t', 'r4t', 'g9j', 'g6j', 'r3s', 'w7t', 'r9s', 'r1j', 'r2j', 'r8j', 'w7j', 'r6s', 'b6j', 'w2j', 'r5s', 'r3j', 'w1s', 'b1j', 'r4j', 'w8j', 'r9j', 'w4s', 'w3t', 'r8t', 'g1s', 'g9j', 'w1s', 'b3t', 'r8t', 'w6j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.09514415 0.09837036 0.1504129  ... 0.10500187 0.16128653 0.11813088]
Average of silhouette coef: 0.076649226
---
  0   1   2   3
0 0.0 2.2 2.6 2.5 
1 2.2 0.0 4.0 3.8 
2 2.6 4.0 0.0 3.5 
3 2.5 3.8 3.5 0.0 
correlation [[ 1.         -0.35307614]
 [-0.35307614  1.        ]]
---
[[], [], [], []] [3 1 1 ... 2 2 1] [[-57.46579     42.089928  ]
 [-31.374035   -40.06466   ]
 [ -7.1602225  -47.898342  ]
 ...
 [-15.588896   -20.367535  ]
 [-10.64741     -0.11075379]
 [-24.838074    13.052456  ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.076649226, 'cluster_all': array([0.09514415, 0.09837036, 0.1504129 , ..., 0.10500187, 0.16128653,
       0.11813088], dtype=float32), 'magnitude_avg': 0.3530761423155879, 'magnitude_all': -0.3530761423155879, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.076649226, 'magnitude_avg': 0.3530761423155879, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([8, 3, 5, 2, 7, 4, 2, 3, 9, 8, 3, 4, 4, 6, 2, 6, 5, 4, 7, 1, 8, 4, 8, 9,
        1, 6, 1, 7, 9, 9, 4, 2, 3, 5, 4, 5, 7, 9, 3, 3, 5, 6, 2, 6, 1, 9, 8, 1,
        1, 1, 4, 7, 7, 3, 1, 6, 8, 3, 9, 5, 1, 9, 4, 5, 9, 6, 4, 1, 4, 3, 8, 7,
        9, 5, 9, 6, 3, 9, 6, 1, 4, 3, 1, 9, 9, 4, 3, 7, 6, 2, 6, 8, 9, 7, 3, 1,
        5, 4, 6, 4, 2, 9, 2, 1, 3, 4, 7, 3, 4, 5, 7, 2, 2, 3, 7, 8, 2, 8, 4, 3,
        8, 8, 3, 4, 3, 3, 7, 7]), ['g8j', 'w3s', 'r5t', 'b2j', 'b7t', 'w4t', 'w2j', 'w3t', 'w9s', 'b8t', 'r3s', 'g4t', 'b4j', 'w6j', 'w2j', 'r6j', 'r5t', 'w4s', 'w7j', 'w1t', 'r8t', 'b4j', 'b8s', 'b9j', 'w1s', 'w6j', 'w1s', 'b7s', 'g9t', 'r9t', 'r4t', 'b2j', 'r3s', 'b5j', 'r4j', 'w5s', 'r7j', 'b9t', 'r3s', 'g3t', 'r5t', 'g6t', 'r2s', 'g6s', 'r1s', 'w9s', 'w8s', 'b1s', 'w1t', 'g1t', 'g4t', 'w7s', 'g7s', 'w3s', 'w1s', 'b6s', 'w8s', 'b3t', 'b9t', 'w5j', 'b1t', 'g9s', 'g4j', 'b5t', 'b9t', 'r6s', 'r4t', 'g1s', 'w4j', 'r3t', 'g8t', 'g7s', 'r9t', 'r5j', 'b9s', 'w6s', 'w3t', 'w9j', 'g6s', 'g1j', 'g4s', 'r3j', 'g1t', 'g9j', 'w9j', 'r4s', 'g3t', 'w7t', 'w6s', 'r2s', 'g6t', 'w8t', 'b9s', 'g7j', 'w3j', 'r1j', 'g5j', 'b4j', 'g6t', 'w4t', 'b2t', 'g9s', 'r2t', 'b1t', 'g3j', 'b4j', 'g7t', 'g3t', 'b4s', 'r5s', 'g7s', 'r2j', 'g2t', 'b3s', 'g7s', 'b8j', 'b2j', 'b8t', 'b4j', 'b3j', 'w8j', 'b8t', 'w3t', 'b4j', 'r3s', 'w3t', 'g7s', 'r7j']] 
label[0]: tensor([8, 3, 5, 2, 7, 4, 2, 3, 9, 8, 3, 4, 4, 6, 2, 6, 5, 4, 7, 1, 8, 4, 8, 9,
        1, 6, 1, 7, 9, 9, 4, 2, 3, 5, 4, 5, 7, 9, 3, 3, 5, 6, 2, 6, 1, 9, 8, 1,
        1, 1, 4, 7, 7, 3, 1, 6, 8, 3, 9, 5, 1, 9, 4, 5, 9, 6, 4, 1, 4, 3, 8, 7,
        9, 5, 9, 6, 3, 9, 6, 1, 4, 3, 1, 9, 9, 4, 3, 7, 6, 2, 6, 8, 9, 7, 3, 1,
        5, 4, 6, 4, 2, 9, 2, 1, 3, 4, 7, 3, 4, 5, 7, 2, 2, 3, 7, 8, 2, 8, 4, 3,
        8, 8, 3, 4, 3, 3, 7, 7]) 
label[1]: ['g8j', 'w3s', 'r5t', 'b2j', 'b7t', 'w4t', 'w2j', 'w3t', 'w9s', 'b8t', 'r3s', 'g4t', 'b4j', 'w6j', 'w2j', 'r6j', 'r5t', 'w4s', 'w7j', 'w1t', 'r8t', 'b4j', 'b8s', 'b9j', 'w1s', 'w6j', 'w1s', 'b7s', 'g9t', 'r9t', 'r4t', 'b2j', 'r3s', 'b5j', 'r4j', 'w5s', 'r7j', 'b9t', 'r3s', 'g3t', 'r5t', 'g6t', 'r2s', 'g6s', 'r1s', 'w9s', 'w8s', 'b1s', 'w1t', 'g1t', 'g4t', 'w7s', 'g7s', 'w3s', 'w1s', 'b6s', 'w8s', 'b3t', 'b9t', 'w5j', 'b1t', 'g9s', 'g4j', 'b5t', 'b9t', 'r6s', 'r4t', 'g1s', 'w4j', 'r3t', 'g8t', 'g7s', 'r9t', 'r5j', 'b9s', 'w6s', 'w3t', 'w9j', 'g6s', 'g1j', 'g4s', 'r3j', 'g1t', 'g9j', 'w9j', 'r4s', 'g3t', 'w7t', 'w6s', 'r2s', 'g6t', 'w8t', 'b9s', 'g7j', 'w3j', 'r1j', 'g5j', 'b4j', 'g6t', 'w4t', 'b2t', 'g9s', 'r2t', 'b1t', 'g3j', 'b4j', 'g7t', 'g3t', 'b4s', 'r5s', 'g7s', 'r2j', 'g2t', 'b3s', 'g7s', 'b8j', 'b2j', 'b8t', 'b4j', 'b3j', 'w8j', 'b8t', 'w3t', 'b4j', 'r3s', 'w3t', 'g7s', 'r7j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([9, 1, 4, 9, 8, 3, 7, 3, 9, 3, 2, 5, 6, 3, 1, 6, 1, 8, 9, 8, 4, 2, 3, 9,
        1, 5, 7, 1, 3, 9, 3, 9, 2, 3, 3, 6, 9, 9, 2, 2, 3, 7, 1, 4, 6, 8, 3, 1,
        3, 3, 2, 9, 2, 3, 1, 2, 5, 7, 5, 1, 7, 5, 5, 1, 4, 9, 6, 1, 7, 9, 9, 7,
        5, 6, 2, 3, 3, 2, 8, 7, 9, 3, 5, 5, 2, 9, 3, 9, 3, 5, 3, 5, 3, 6, 7, 1,
        7, 5, 9, 1, 9, 9, 8, 8, 5, 2, 8, 1, 6, 6, 3, 7, 4, 6, 6, 1, 4, 4, 4, 3,
        3, 2, 1, 1, 2, 4, 3, 9])
Accuracy (count): tensor(8) 
Accuracy (ratio) tensor(0.0625)
Accuracy:
 [[ 0  0  0  0  0  0  0 17  0]
 [ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0 25  0]
 [ 0  0  0  0  0  0  0  9  0]
 [ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  0  0  0 11  0]
 [ 0  0  0  0  0  0  0 11  0]
 [ 0  0  0  0  0  0  0  8  0]
 [ 0  0  0  0  0  0  0 20  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([8, 3, 2, 1, 1, 4, 6, 8, 4, 2, 7, 1, 1, 7, 2, 1, 8, 6, 1, 6, 8, 4, 4, 1,
        9, 3, 6, 3, 2, 8, 7, 9, 2, 1, 4, 5, 2, 4, 1, 3, 7, 2, 4, 4, 5, 4, 1, 2,
        4, 6, 1, 5, 3, 5, 6, 1, 2, 2, 4, 9, 4, 3, 9, 1, 2, 4, 9, 7, 1, 2, 1, 1,
        8, 5, 1, 4, 5, 1, 3, 4, 6, 2, 3, 5, 9, 1, 7, 5, 3, 1, 3, 3, 2, 3, 9, 9,
        9, 2, 9, 2, 5, 3, 5, 6, 8, 7, 2, 6, 2, 2, 6, 4, 3, 7, 1, 2, 9, 9, 1, 7,
        9, 9, 2, 1, 1, 1, 2, 1])
Accuracy (count): tensor(7) 
Accuracy (ratio) tensor(0.0547)
Accuracy:
 [[ 0  0  0  0  0  0  0 26  0]
 [ 0  0  0  0  0  0  0 22  0]
 [ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 10  0]
 [ 0  0  0  0  0  0  0 10  0]
 [ 0  0  0  0  0  0  0  9  0]
 [ 0  0  0  0  0  0  0  7  0]
 [ 0  0  0  0  0  0  0 14  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
Silhouette values: [0.1763505  0.1154741  0.09154596 ... 0.32696044 0.17192441 0.05862412]
Average of silhouette coef: 0.1063364
---
  1   2   3   4   5   6   7   8   9
1 0.0 4.0 4.2 4.8 4.5 5.0 4.8 4.4 4.6 
2 4.0 0.0 3.7 4.1 4.3 4.5 4.8 4.4 4.9 
3 4.2 3.7 0.0 4.4 3.2 5.1 4.6 4.3 4.7 
4 4.8 4.1 4.4 0.0 4.2 4.6 4.4 4.9 3.9 
5 4.5 4.3 3.2 4.2 0.0 3.9 4.1 3.6 4.0 
6 5.0 4.5 5.1 4.6 3.9 0.0 5.2 4.5 4.4 
7 4.8 4.8 4.6 4.4 4.1 5.2 0.0 4.7 3.3 
8 4.4 4.4 4.3 4.9 3.6 4.5 4.7 0.0 3.1 
9 4.6 4.9 4.7 3.9 4.0 4.4 3.3 3.1 0.0 
correlation [[1.         0.39786939]
 [0.39786939 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [8 3 5 ... 1 7 9] [[-19.7091      8.062464 ]
 [ -2.3893638  28.756342 ]
 [-22.094309   19.893663 ]
 ...
 [ 80.62679    -4.7425437]
 [-35.313156  -50.962334 ]
 [ -5.823373  -24.765694 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized

Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0  0  0  0  0  0  0 45  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 61  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 45  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.3 -0.0 -3.0 -0.2  0.0  0.1  0.2  0.2 -0.1 -0.7 -0.0  0.1 -0.1 -0.1  0.2 -0.2 -0.1  0.2  0.2 -0.1 
-0.3 -0.7  0.1 -0.4  0.3 -0.1  0.3 -0.0 -0.2 -0.8 -0.2  0.1 -0.3  0.0  0.5  0.2 -0.1  2.5  0.1  0.0 
-0.1 -0.0 -0.2 -0.1 -2.7 -0.5  0.4  0.7 -0.2  0.0 -0.3  0.2 -0.0 -0.1 -0.0  0.2  0.1  0.9  0.1  0.6 
-0.2  0.1  0.8 -1.0  0.5 -2.4  0.3 -0.1 -0.3  0.4  0.3  0.1 -0.6 -0.2 -0.0 -0.1  0.1 -0.3 -0.0 -0.2 
 0.3 -0.2  0.4 -0.1 -1.8  0.3  0.0 -0.6 -0.2 -0.2 -0.4  0.1 -0.0 -0.2 -0.9 -0.2  0.1 -0.4  0.0 -1.4 
-0.1  0.5  0.2  0.0 -0.0  0.0  0.3 -3.5 -0.2  0.1 -0.3 -0.0 -0.1 -0.0 -0.4 -0.0  0.1  0.3  0.1 -0.2 
 0.1 -0.3  0.4 -0.6  0.3  0.3 -0.0  0.5 -0.2  0.5 -0.0  0.0  2.7 -0.1 -0.4 -0.1  0.1 -0.8  0.1  0.1 
-0.7  0.4  0.3  2.5 -0.2  0.7  0.3  0.0 -0.2 -0.4 -0.4  0.0 -0.3  0.1 -0.3 -0.2 -0.0 -0.3  0.0 -0.2 
-0.3  0.5  0.7  0.5  0.6  0.6  0.0  0.0 -0.2  0.8 -0.5 -0.0  0.1 -0.1  0.3  0.2  0.1 -1.8  0.1  0.5 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 3.1148467
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.3 -0.0 -3.0 -0.2  0.0  0.1  0.2  0.2 -0.1 -0.7 -0.0  0.1 -0.1 -0.1  0.2 -0.2 -0.1  0.2  0.2 -0.1 
-0.3 -0.7  0.1 -0.4  0.3 -0.1  0.3 -0.0 -0.2 -0.8 -0.2  0.1 -0.3  0.0  0.5  0.2 -0.1  2.5  0.1  0.0 
-0.1 -0.0 -0.2 -0.1 -2.7 -0.5  0.4  0.7 -0.2  0.0 -0.3  0.2 -0.0 -0.1 -0.0  0.2  0.1  0.9  0.1  0.6 
-0.2  0.1  0.8 -1.0  0.5 -2.4  0.3 -0.1 -0.3  0.4  0.3  0.1 -0.6 -0.2 -0.0 -0.1  0.1 -0.3 -0.0 -0.2 
 0.3 -0.2  0.4 -0.1 -1.8  0.3  0.0 -0.6 -0.2 -0.2 -0.4  0.1 -0.0 -0.2 -0.9 -0.2  0.1 -0.4  0.0 -1.4 
-0.1  0.5  0.2  0.0 -0.0  0.0  0.3 -3.5 -0.2  0.1 -0.3 -0.0 -0.1 -0.0 -0.4 -0.0  0.1  0.3  0.1 -0.2 
 0.1 -0.3  0.4 -0.6  0.3  0.3 -0.0  0.5 -0.2  0.5 -0.0  0.0  2.7 -0.1 -0.4 -0.1  0.1 -0.8  0.1  0.1 
-0.7  0.4  0.3  2.5 -0.2  0.7  0.3  0.0 -0.2 -0.4 -0.4  0.0 -0.3  0.1 -0.3 -0.2 -0.0 -0.3  0.0 -0.2 
-0.3  0.5  0.7  0.5  0.6  0.6  0.0  0.0 -0.2  0.8 -0.5 -0.0  0.1 -0.1  0.3  0.2  0.1 -1.8  0.1  0.5 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 4.8297453
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
-0.3 -0.0 -3.0 -0.2  0.0  0.1  0.2  0.2 -0.1 -0.7 -0.0  0.1 -0.1 -0.1  0.2 -0.2 -0.1  0.2  0.2 -0.1 
-0.3 -0.7  0.1 -0.4  0.3 -0.1  0.3 -0.0 -0.2 -0.8 -0.2  0.1 -0.3  0.0  0.5  0.2 -0.1  2.5  0.1  0.0 
-0.1 -0.0 -0.2 -0.1 -2.7 -0.5  0.4  0.7 -0.2  0.0 -0.3  0.2 -0.0 -0.1 -0.0  0.2  0.1  0.9  0.1  0.6 
-0.2  0.1  0.8 -1.0  0.5 -2.4  0.3 -0.1 -0.3  0.4  0.3  0.1 -0.6 -0.2 -0.0 -0.1  0.1 -0.3 -0.0 -0.2 
 0.3 -0.2  0.4 -0.1 -1.8  0.3  0.0 -0.6 -0.2 -0.2 -0.4  0.1 -0.0 -0.2 -0.9 -0.2  0.1 -0.4  0.0 -1.4 
-0.1  0.5  0.2  0.0 -0.0  0.0  0.3 -3.5 -0.2  0.1 -0.3 -0.0 -0.1 -0.0 -0.4 -0.0  0.1  0.3  0.1 -0.2 
 0.1 -0.3  0.4 -0.6  0.3  0.3 -0.0  0.5 -0.2  0.5 -0.0  0.0  2.7 -0.1 -0.4 -0.1  0.1 -0.8  0.1  0.1 
-0.7  0.4  0.3  2.5 -0.2  0.7  0.3  0.0 -0.2 -0.4 -0.4  0.0 -0.3  0.1 -0.3 -0.2 -0.0 -0.3  0.0 -0.2 
-0.3  0.5  0.7  0.5  0.6  0.6  0.0  0.0 -0.2  0.8 -0.5 -0.0  0.1 -0.1  0.3  0.2  0.1 -1.8  0.1  0.5 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 3.994596
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.3 -0.0 -3.0 -0.2  0.0  0.1  0.2  0.2 -0.1 -0.7 -0.0  0.1 -0.1 -0.1  0.2 -0.2 -0.1  0.2  0.2 -0.1 
-0.3 -0.7  0.1 -0.4  0.3 -0.1  0.3 -0.0 -0.2 -0.8 -0.2  0.1 -0.3  0.0  0.5  0.2 -0.1  2.5  0.1  0.0 
-0.1 -0.0 -0.2 -0.1 -2.7 -0.5  0.4  0.7 -0.2  0.0 -0.3  0.2 -0.0 -0.1 -0.0  0.2  0.1  0.9  0.1  0.6 
-0.2  0.1  0.8 -1.0  0.5 -2.4  0.3 -0.1 -0.3  0.4  0.3  0.1 -0.6 -0.2 -0.0 -0.1  0.1 -0.3 -0.0 -0.2 
 0.3 -0.2  0.4 -0.1 -1.8  0.3  0.0 -0.6 -0.2 -0.2 -0.4  0.1 -0.0 -0.2 -0.9 -0.2  0.1 -0.4  0.0 -1.4 
-0.1  0.5  0.2  0.0 -0.0  0.0  0.3 -3.5 -0.2  0.1 -0.3 -0.0 -0.1 -0.0 -0.4 -0.0  0.1  0.3  0.1 -0.2 
 0.1 -0.3  0.4 -0.6  0.3  0.3 -0.0  0.5 -0.2  0.5 -0.0  0.0  2.7 -0.1 -0.4 -0.1  0.1 -0.8  0.1  0.1 
-0.7  0.4  0.3  2.5 -0.2  0.7  0.3  0.0 -0.2 -0.4 -0.4  0.0 -0.3  0.1 -0.3 -0.2 -0.0 -0.3  0.0 -0.2 
-0.3  0.5  0.7  0.5  0.6  0.6  0.0  0.0 -0.2  0.8 -0.5 -0.0  0.1 -0.1  0.3  0.2  0.1 -1.8  0.1  0.5 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 3.692969
results (all): {'reconst_0x0_avg': 0.0625, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.0546875, 'cross_1x0_all': nan, 'cluster_avg': 0.1063364, 'cluster_all': array([0.1763505 , 0.1154741 , 0.09154596, ..., 0.32696044, 0.17192441,
       0.05862412], dtype=float32), 'magnitude_avg': 0.3978693934266984, 'magnitude_all': 0.3978693934266984, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 1.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.0625, 'cross_1x0_avg': 0.0546875, 'cluster_avg': 0.1063364, 'magnitude_avg': 0.3978693934266984, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 1.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 1, 1, 6, 5, 8, 1, 5, 4, 3, 2, 4, 6, 5, 3, 2, 7, 8, 3, 3, 5, 5, 7, 7,
        9, 7, 9, 6, 5, 3, 8, 8, 2, 4, 7, 6, 9, 6, 7, 2, 9, 1, 1, 9, 1, 6, 3, 7,
        3, 3, 3, 5, 5, 8, 2, 8, 3, 5, 5, 2, 3, 8, 8, 2, 3, 1, 6, 4, 7, 6, 8, 6,
        4, 7, 5, 6, 7, 8, 2, 6, 9, 5, 1, 9, 2, 2, 1, 4, 8, 9, 9, 8, 7, 4, 6, 9,
        2, 5, 1, 2, 8, 1, 2, 8, 5, 8, 1, 2, 1, 1, 5, 5, 3, 7, 8, 1, 2, 9, 8, 5,
        8, 1, 5, 9, 2, 6, 7, 1]), ['w6s', 'g1j', 'b1t', 'g6t', 'g5s', 'r8s', 'b1s', 'g5s', 'r4j', 'w3j', 'r2t', 'w4s', 'g6j', 'g5j', 'b3s', 'b2j', 'g7j', 'r8j', 'w3t', 'w3t', 'w5s', 'b5s', 'b7j', 'b7t', 'r9t', 'b7t', 'w9j', 'r6s', 'g5s', 'g3j', 'r8j', 'b8s', 'r2s', 'g4s', 'g7t', 'b6s', 'w9j', 'w6t', 'b7s', 'b2j', 'b9t', 'b1j', 'b1s', 'g9j', 'g1t', 'b6j', 'w3t', 'w7s', 'g3t', 'r3s', 'r3s', 'b5t', 'w5j', 'g8s', 'g2s', 'g8t', 'b3j', 'w5s', 'r5s', 'w2t', 'w3t', 'w8t', 'w8s', 'b2s', 'w3s', 'b1t', 'g6s', 'w4j', 'r7j', 'w6s', 'b8t', 'w6s', 'r4j', 'b7t', 'g5j', 'w6s', 'r7s', 'w8t', 'w2j', 'w6j', 'b9t', 'w5s', 'w1s', 'g9t', 'r2j', 'b2j', 'w1t', 'w4t', 'w8s', 'w9j', 'b9s', 'w8j', 'b7t', 'r4t', 'g6s', 'g9t', 'w2s', 'b5j', 'b1t', 'b2j', 'r8j', 'g1t', 'g2s', 'b8s', 'r5j', 'b8t', 'b1s', 'g2j', 'b1j', 'g1t', 'g5j', 'w5j', 'r3j', 'g7s', 'g8j', 'w1s', 'w2t', 'w9j', 'w8j', 'b5t', 'w8j', 'r1t', 'g5j', 'b9j', 'b2j', 'b6t', 'g7t', 'b1t']] 
label[0]: tensor([6, 1, 1, 6, 5, 8, 1, 5, 4, 3, 2, 4, 6, 5, 3, 2, 7, 8, 3, 3, 5, 5, 7, 7,
        9, 7, 9, 6, 5, 3, 8, 8, 2, 4, 7, 6, 9, 6, 7, 2, 9, 1, 1, 9, 1, 6, 3, 7,
        3, 3, 3, 5, 5, 8, 2, 8, 3, 5, 5, 2, 3, 8, 8, 2, 3, 1, 6, 4, 7, 6, 8, 6,
        4, 7, 5, 6, 7, 8, 2, 6, 9, 5, 1, 9, 2, 2, 1, 4, 8, 9, 9, 8, 7, 4, 6, 9,
        2, 5, 1, 2, 8, 1, 2, 8, 5, 8, 1, 2, 1, 1, 5, 5, 3, 7, 8, 1, 2, 9, 8, 5,
        8, 1, 5, 9, 2, 6, 7, 1]) 
label[1]: ['w6s', 'g1j', 'b1t', 'g6t', 'g5s', 'r8s', 'b1s', 'g5s', 'r4j', 'w3j', 'r2t', 'w4s', 'g6j', 'g5j', 'b3s', 'b2j', 'g7j', 'r8j', 'w3t', 'w3t', 'w5s', 'b5s', 'b7j', 'b7t', 'r9t', 'b7t', 'w9j', 'r6s', 'g5s', 'g3j', 'r8j', 'b8s', 'r2s', 'g4s', 'g7t', 'b6s', 'w9j', 'w6t', 'b7s', 'b2j', 'b9t', 'b1j', 'b1s', 'g9j', 'g1t', 'b6j', 'w3t', 'w7s', 'g3t', 'r3s', 'r3s', 'b5t', 'w5j', 'g8s', 'g2s', 'g8t', 'b3j', 'w5s', 'r5s', 'w2t', 'w3t', 'w8t', 'w8s', 'b2s', 'w3s', 'b1t', 'g6s', 'w4j', 'r7j', 'w6s', 'b8t', 'w6s', 'r4j', 'b7t', 'g5j', 'w6s', 'r7s', 'w8t', 'w2j', 'w6j', 'b9t', 'w5s', 'w1s', 'g9t', 'r2j', 'b2j', 'w1t', 'w4t', 'w8s', 'w9j', 'b9s', 'w8j', 'b7t', 'r4t', 'g6s', 'g9t', 'w2s', 'b5j', 'b1t', 'b2j', 'r8j', 'g1t', 'g2s', 'b8s', 'r5j', 'b8t', 'b1s', 'g2j', 'b1j', 'g1t', 'g5j', 'w5j', 'r3j', 'g7s', 'g8j', 'w1s', 'w2t', 'w9j', 'w8j', 'b5t', 'w8j', 'r1t', 'g5j', 'b9j', 'b2j', 'b6t', 'g7t', 'b1t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.00242569 -0.00506252 -0.01327806 ...  0.00283233 -0.00046295
 -0.0101543 ]
Average of silhouette coef: -0.0040270225
---
  0   1   2
0 0.0 0.3 0.3 
1 0.3 0.0 0.2 
2 0.3 0.2 0.0 
correlation [[1.         0.85888708]
 [0.85888708 1.        ]]
---
[[], [], []] [1 0 2 ... 0 0 2] [[ 33.367867   14.794566 ]
 [-43.056454   32.482716 ]
 [-61.639774   -6.548321 ]
 ...
 [ -9.123735   43.482777 ]
 [-71.14915    16.789537 ]
 [  4.8678446 -61.940727 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.0040270225, 'cluster_all': array([ 0.00242569, -0.00506252, -0.01327806, ...,  0.00283233,
       -0.00046295, -0.0101543 ], dtype=float32), 'magnitude_avg': 0.858887078751514, 'magnitude_all': 0.858887078751514, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.0040270225, 'magnitude_avg': 0.858887078751514, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 7, 6, 2, 3, 6, 4, 7, 1, 2, 7, 5, 5, 6, 1, 3, 8, 4, 4, 4, 5, 2, 8, 1,
        1, 5, 5, 2, 6, 5, 2, 6, 6, 8, 4, 2, 3, 9, 2, 7, 5, 2, 4, 1, 2, 1, 4, 9,
        7, 7, 6, 2, 5, 7, 8, 5, 3, 7, 7, 4, 1, 5, 9, 4, 6, 5, 8, 6, 6, 8, 7, 7,
        3, 2, 6, 4, 5, 6, 7, 7, 8, 1, 5, 7, 8, 5, 3, 6, 8, 8, 5, 5, 6, 1, 4, 6,
        6, 5, 9, 8, 7, 5, 1, 1, 4, 2, 2, 7, 5, 7, 9, 6, 6, 7, 7, 3, 3, 3, 5, 5,
        9, 7, 1, 7, 8, 4, 8, 3]), ['g2s', 'g7t', 'w6t', 'r2s', 'b3s', 'w6j', 'b4s', 'w7j', 'r1j', 'r2t', 'r7s', 'g5t', 'r5s', 'g6t', 'r1s', 'w3s', 'w8t', 'r4j', 'w4t', 'w4j', 'b5t', 'w2j', 'g8j', 'b1t', 'r1t', 'w5j', 'w5t', 'w2t', 'g6j', 'w5t', 'b2j', 'b6j', 'b6t', 'g8t', 'r4s', 'w2s', 'g3s', 'g9s', 'b2t', 'g7t', 'b5j', 'g2s', 'r4s', 'w1j', 'w2t', 'g1j', 'g4t', 'b9t', 'b7j', 'b7t', 'r6t', 'w2t', 'w5j', 'r7j', 'r8s', 'w5j', 'g3t', 'w7j', 'b7s', 'w4t', 'w1t', 'w5s', 'w9t', 'b4t', 'b6s', 'r5j', 'w8t', 'b6t', 'b6s', 'b8t', 'g7j', 'w7s', 'b3s', 'r2j', 'r6j', 'b4s', 'r5s', 'g6t', 'b7j', 'g7s', 'w8s', 'b1j', 'r5j', 'w7s', 'g8s', 'r5j', 'g3s', 'w6j', 'g8t', 'w8j', 'w5j', 'g5s', 'w6t', 'g1j', 'r4j', 'b6j', 'r6t', 'w5s', 'r9j', 'b8j', 'g7s', 'g5j', 'w1s', 'b1t', 'w4s', 'b2j', 'r2t', 'r7t', 'b5t', 'w7s', 'b9t', 'b6j', 'r6t', 'g7j', 'b7j', 'w3s', 'r3s', 'r3s', 'r5t', 'r5s', 'g9t', 'b7j', 'r1j', 'r7j', 'b8t', 'b4t', 'g8s', 'g3t']] 
label[0]: tensor([2, 7, 6, 2, 3, 6, 4, 7, 1, 2, 7, 5, 5, 6, 1, 3, 8, 4, 4, 4, 5, 2, 8, 1,
        1, 5, 5, 2, 6, 5, 2, 6, 6, 8, 4, 2, 3, 9, 2, 7, 5, 2, 4, 1, 2, 1, 4, 9,
        7, 7, 6, 2, 5, 7, 8, 5, 3, 7, 7, 4, 1, 5, 9, 4, 6, 5, 8, 6, 6, 8, 7, 7,
        3, 2, 6, 4, 5, 6, 7, 7, 8, 1, 5, 7, 8, 5, 3, 6, 8, 8, 5, 5, 6, 1, 4, 6,
        6, 5, 9, 8, 7, 5, 1, 1, 4, 2, 2, 7, 5, 7, 9, 6, 6, 7, 7, 3, 3, 3, 5, 5,
        9, 7, 1, 7, 8, 4, 8, 3]) 
label[1]: ['g2s', 'g7t', 'w6t', 'r2s', 'b3s', 'w6j', 'b4s', 'w7j', 'r1j', 'r2t', 'r7s', 'g5t', 'r5s', 'g6t', 'r1s', 'w3s', 'w8t', 'r4j', 'w4t', 'w4j', 'b5t', 'w2j', 'g8j', 'b1t', 'r1t', 'w5j', 'w5t', 'w2t', 'g6j', 'w5t', 'b2j', 'b6j', 'b6t', 'g8t', 'r4s', 'w2s', 'g3s', 'g9s', 'b2t', 'g7t', 'b5j', 'g2s', 'r4s', 'w1j', 'w2t', 'g1j', 'g4t', 'b9t', 'b7j', 'b7t', 'r6t', 'w2t', 'w5j', 'r7j', 'r8s', 'w5j', 'g3t', 'w7j', 'b7s', 'w4t', 'w1t', 'w5s', 'w9t', 'b4t', 'b6s', 'r5j', 'w8t', 'b6t', 'b6s', 'b8t', 'g7j', 'w7s', 'b3s', 'r2j', 'r6j', 'b4s', 'r5s', 'g6t', 'b7j', 'g7s', 'w8s', 'b1j', 'r5j', 'w7s', 'g8s', 'r5j', 'g3s', 'w6j', 'g8t', 'w8j', 'w5j', 'g5s', 'w6t', 'g1j', 'r4j', 'b6j', 'r6t', 'w5s', 'r9j', 'b8j', 'g7s', 'g5j', 'w1s', 'b1t', 'w4s', 'b2j', 'r2t', 'r7t', 'b5t', 'w7s', 'b9t', 'b6j', 'r6t', 'g7j', 'b7j', 'w3s', 'r3s', 'r3s', 'r5t', 'r5s', 'g9t', 'b7j', 'r1j', 'r7j', 'b8t', 'b4t', 'g8s', 'g3t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.6600662   0.06052241  0.15620099 ...  0.8123308  -0.19817176
  0.8220792 ]
Average of silhouette coef: -0.01844525
---
  0   1   2   3
0 0.0 35.1 41.6 72.5 
1 35.1 0.0 6.5 37.4 
2 41.6 6.5 0.0 30.9 
3 72.5 37.4 30.9 0.0 
correlation [[1.         0.86195133]
 [0.86195133 1.        ]]
---
[[], [], [], []] [2 2 0 ... 3 2 3] [[ 58.947445    25.540232  ]
 [-14.278579    -0.60011846]
 [-11.139757   -60.47873   ]
 ...
 [ 22.217054   -15.92156   ]
 [ -7.2514877  -61.233856  ]
 [-33.812748   -22.554071  ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': -0.01844525, 'cluster_all': array([-0.6600662 ,  0.06052241,  0.15620099, ...,  0.8123308 ,
       -0.19817176,  0.8220792 ], dtype=float32), 'magnitude_avg': 0.8619513263594284, 'magnitude_all': 0.8619513263594284, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': -0.01844525, 'magnitude_avg': 0.8619513263594284, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 1, 3, 3, 1, 9, 5, 7, 2, 3, 4, 5, 3, 7, 7, 1, 8, 6, 2, 5, 3, 6, 2, 7,
        1, 4, 7, 9, 3, 3, 2, 1, 4, 1, 4, 9, 4, 6, 3, 2, 8, 6, 8, 6, 9, 7, 3, 3,
        2, 8, 2, 7, 9, 7, 5, 8, 9, 6, 1, 2, 3, 1, 3, 7, 7, 7, 1, 9, 4, 6, 1, 3,
        6, 3, 9, 8, 6, 5, 6, 9, 3, 3, 8, 2, 7, 8, 4, 3, 2, 9, 5, 9, 7, 5, 6, 5,
        1, 4, 2, 8, 2, 1, 3, 8, 8, 1, 6, 9, 5, 5, 2, 6, 7, 3, 3, 1, 9, 7, 3, 5,
        1, 6, 1, 7, 3, 7, 2, 9]), ['b7j', 'b1t', 'b3t', 'r3t', 'g1j', 'w9s', 'b5t', 'w7s', 'b2s', 'w3s', 'w4j', 'r5j', 'b3s', 'r7s', 'b7s', 'b1s', 'g8j', 'g6s', 'g2j', 'r5s', 'w3t', 'b6t', 'r2j', 'w7t', 'w1t', 'w4j', 'w7j', 'b9t', 'g3s', 'g3j', 'b2j', 'r1j', 'g4j', 'r1j', 'w4j', 'g9s', 'b4t', 'b6j', 'b3t', 'r2s', 'g8j', 'r6s', 'g8s', 'g6t', 'b9t', 'r7s', 'w3s', 'r3j', 'b2j', 'r8s', 'w2s', 'w7s', 'w9j', 'g7s', 'b5s', 'g8t', 'b9t', 'b6s', 'r1j', 'r2s', 'b3s', 'b1t', 'g3s', 'b7s', 'b7j', 'w7j', 'g1t', 'b9s', 'r4j', 'r6j', 'g1t', 'b3j', 'r6j', 'r3s', 'w9t', 'b8t', 'w6j', 'b5s', 'w6t', 'g9t', 'r3s', 'w3t', 'g8t', 'r2s', 'r7s', 'g8s', 'r4s', 'g3s', 'g2j', 'b9j', 'g5j', 'w9j', 'r7j', 'g5t', 'r6j', 'r5t', 'r1t', 'b4s', 'g2s', 'w8j', 'b2j', 'w1t', 'g3s', 'g8s', 'r8s', 'w1s', 'w6t', 'g9j', 'g5t', 'w5t', 'g2t', 'r6s', 'w7t', 'b3j', 'g3t', 'r1j', 'g9t', 'w7s', 'r3t', 'w5t', 'r1s', 'w6j', 'r1t', 'r7t', 'b3s', 'r7t', 'w2t', 'r9t']] 
label[0]: tensor([7, 1, 3, 3, 1, 9, 5, 7, 2, 3, 4, 5, 3, 7, 7, 1, 8, 6, 2, 5, 3, 6, 2, 7,
        1, 4, 7, 9, 3, 3, 2, 1, 4, 1, 4, 9, 4, 6, 3, 2, 8, 6, 8, 6, 9, 7, 3, 3,
        2, 8, 2, 7, 9, 7, 5, 8, 9, 6, 1, 2, 3, 1, 3, 7, 7, 7, 1, 9, 4, 6, 1, 3,
        6, 3, 9, 8, 6, 5, 6, 9, 3, 3, 8, 2, 7, 8, 4, 3, 2, 9, 5, 9, 7, 5, 6, 5,
        1, 4, 2, 8, 2, 1, 3, 8, 8, 1, 6, 9, 5, 5, 2, 6, 7, 3, 3, 1, 9, 7, 3, 5,
        1, 6, 1, 7, 3, 7, 2, 9]) 
label[1]: ['b7j', 'b1t', 'b3t', 'r3t', 'g1j', 'w9s', 'b5t', 'w7s', 'b2s', 'w3s', 'w4j', 'r5j', 'b3s', 'r7s', 'b7s', 'b1s', 'g8j', 'g6s', 'g2j', 'r5s', 'w3t', 'b6t', 'r2j', 'w7t', 'w1t', 'w4j', 'w7j', 'b9t', 'g3s', 'g3j', 'b2j', 'r1j', 'g4j', 'r1j', 'w4j', 'g9s', 'b4t', 'b6j', 'b3t', 'r2s', 'g8j', 'r6s', 'g8s', 'g6t', 'b9t', 'r7s', 'w3s', 'r3j', 'b2j', 'r8s', 'w2s', 'w7s', 'w9j', 'g7s', 'b5s', 'g8t', 'b9t', 'b6s', 'r1j', 'r2s', 'b3s', 'b1t', 'g3s', 'b7s', 'b7j', 'w7j', 'g1t', 'b9s', 'r4j', 'r6j', 'g1t', 'b3j', 'r6j', 'r3s', 'w9t', 'b8t', 'w6j', 'b5s', 'w6t', 'g9t', 'r3s', 'w3t', 'g8t', 'r2s', 'r7s', 'g8s', 'r4s', 'g3s', 'g2j', 'b9j', 'g5j', 'w9j', 'r7j', 'g5t', 'r6j', 'r5t', 'r1t', 'b4s', 'g2s', 'w8j', 'b2j', 'w1t', 'g3s', 'g8s', 'r8s', 'w1s', 'w6t', 'g9j', 'g5t', 'w5t', 'g2t', 'r6s', 'w7t', 'b3j', 'g3t', 'r1j', 'g9t', 'w7s', 'r3t', 'w5t', 'r1s', 'w6j', 'r1t', 'r7t', 'b3s', 'r7t', 'w2t', 'r9t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([2, 4, 4, 9, 1, 5, 6, 8, 2, 6, 3, 7, 1, 1, 5, 2, 3, 3, 6, 3, 4, 1, 6, 7,
        1, 2, 1, 8, 9, 7, 6, 5, 2, 5, 3, 6, 7, 2, 7, 9, 9, 6, 4, 3, 2, 5, 2, 8,
        7, 1, 3, 6, 5, 7, 5, 9, 1, 1, 1, 1, 9, 7, 2, 7, 3, 7, 1, 5, 4, 2, 2, 3,
        1, 8, 1, 9, 3, 1, 2, 8, 7, 1, 2, 8, 5, 9, 6, 2, 6, 6, 4, 8, 3, 5, 7, 3,
        3, 4, 5, 9, 1, 6, 5, 7, 4, 2, 8, 7, 8, 3, 4, 9, 5, 1, 8, 4, 5, 1, 3, 1,
        5, 8, 7, 9, 7, 5, 2, 9])
Accuracy (count): tensor(10) 
Accuracy (ratio) tensor(0.0781)
Accuracy:
 [[ 0  0  0 20  0  0  0  0  0]
 [ 0  0  0 16  0  0  0  0  0]
 [ 0  0  0 15  0  0  0  0  0]
 [ 0  0  0 10  0  0  0  0  0]
 [ 0  0  0 16  0  0  0  0  0]
 [ 0  0  0 12  0  0  0  0  0]
 [ 0  0  0 16  0  0  0  0  0]
 [ 0  0  0 11  0  0  0  0  0]
 [ 0  0  0 12  0  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 8, 4, 4, 8, 4, 4, 4, 4, 4,
        8, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4,
        4, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 8, 4]) 
tar: tensor([7, 1, 4, 6, 2, 4, 5, 7, 1, 2, 8, 9, 1, 5, 2, 3, 5, 9, 1, 6, 7, 3, 1, 4,
        6, 3, 1, 9, 8, 2, 5, 1, 2, 5, 7, 4, 6, 6, 3, 7, 3, 4, 9, 4, 2, 2, 8, 7,
        9, 5, 9, 2, 4, 7, 2, 3, 4, 8, 2, 3, 5, 5, 5, 5, 4, 5, 8, 8, 2, 8, 5, 3,
        1, 2, 3, 7, 7, 1, 8, 2, 7, 7, 5, 4, 1, 2, 4, 8, 2, 6, 4, 7, 2, 9, 2, 4,
        4, 7, 5, 6, 5, 9, 5, 6, 2, 7, 4, 8, 4, 4, 7, 3, 2, 4, 6, 6, 4, 1, 4, 4,
        7, 9, 1, 4, 1, 4, 8, 3])
Accuracy (count): tensor(26) 
Accuracy (ratio) tensor(0.2031)
Accuracy:
 [[ 0  0  0 13  0  0  0  0  0]
 [ 0  0  0 18  0  0  0  1  0]
 [ 0  0  0 11  0  0  0  0  0]
 [ 0  0  0 23  0  0  0  0  0]
 [ 0  0  0 15  0  0  0  1  0]
 [ 0  0  0 10  0  0  0  0  0]
 [ 0  0  0 12  0  0  0  4  0]
 [ 0  0  0  8  0  0  0  3  0]
 [ 0  0  0  6  0  0  0  3  0]]
Accuracy:
 [[0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.947 0.    0.    0.    0.053 0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.938 0.    0.    0.    0.062 0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.75  0.    0.    0.    0.25  0.   ]
 [0.    0.    0.    0.727 0.    0.    0.    0.273 0.   ]
 [0.    0.    0.    0.667 0.    0.    0.    0.333 0.   ]]
---
Silhouette values: [-0.6239171   0.29475135  0.08015665 ... -0.13096887 -0.06596962
 -0.11134423]
Average of silhouette coef: -0.23707707
---
  1   2   3   4   5   6   7   8   9
1 0.0 8.6 19.8 26.9 34.7 48.9 57.5 61.7 72.4 
2 8.6 0.0 11.2 18.3 26.1 40.3 48.9 53.0 63.8 
3 19.8 11.2 0.0 7.1 14.8 29.0 37.7 41.8 52.6 
4 26.9 18.3 7.1 0.0 7.7 21.9 30.6 34.7 45.5 
5 34.7 26.1 14.8 7.7 0.0 14.2 22.8 27.0 37.7 
6 48.9 40.3 29.0 21.9 14.2 0.0 8.6 12.8 23.5 
7 57.5 48.9 37.7 30.6 22.8 8.6 0.0 4.1 14.9 
8 61.7 53.0 41.8 34.7 27.0 12.8 4.1 0.0 10.7 
9 72.4 63.8 52.6 45.5 37.7 23.5 14.9 10.7 0.0 
correlation [[1.         0.98879089]
 [0.98879089 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 1 3 ... 4 8 8] [[-72.66993    -7.0676355]
 [ 41.643257    6.8908024]
 [-59.60622   -16.819736 ]
 ...
 [-68.38814    -8.5869055]
 [ 11.2202425  64.3123   ]
 [ 28.915024   53.927776 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(60) 
Accuracy (ratio) tensor(0.1227)
Accuracy:
 [[ 0  0  0 45  0  0  0  0  0]
 [ 0  0  0 52  0  0  0  0  0]
 [ 0  0  0 57  0  0  0  0  0]
 [ 0  0  0 60  0  0  0  0  0]
 [ 0  0  0 61  0  0  0  0  0]
 [ 0  0  0 60  0  0  0  0  0]
 [ 0  0  0 57  0  0  0  0  0]
 [ 0  0  0 52  0  0  0  0  0]
 [ 0  0  0 45  0  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-3.2 -22.6  2.1 -0.5 -10.0  12.9  22.6  9.5  14.3  3.8 -15.4  0.3  2.9 -2.2  16.2  10.3 -17.2 -22.3  14.8 -9.4 
-3.7 -25.9  2.5 -0.6 -11.5  14.8  25.9  10.8  16.5  4.4 -17.7  0.4  3.3 -2.5  18.6  11.8 -19.8 -25.7  17.0 -10.8 
-4.3 -30.4  2.9 -0.7 -13.4  17.3  30.3  12.7  19.3  5.1 -20.7  0.5  3.9 -2.9  21.7  13.8 -23.2 -30.0  19.9 -12.6 
-4.7 -33.2  3.2 -0.8 -14.6  18.9  33.1  13.8  21.1  5.6 -22.5  0.6  4.3 -3.2  23.7  15.1 -25.3 -32.7  21.7 -13.8 
-5.1 -36.2  3.5 -0.9 -16.0  20.6  36.1  15.1  23.0  6.1 -24.6  0.6  4.6 -3.5  25.9  16.4 -27.6 -35.7  23.7 -15.1 
-5.9 -41.7  4.0 -1.0 -18.3  23.7  41.7  17.4  26.6  7.1 -28.4  0.7  5.3 -4.1  29.9  18.9 -31.9 -41.2  27.4 -17.4 
-6.4 -45.1  4.3 -1.1 -19.9  25.7  45.1  18.9  28.7  7.7 -30.6  0.7  5.8 -4.4  32.3  20.5 -34.6 -44.5  29.5 -18.8 
-6.6 -46.7  4.5 -1.2 -20.6  26.6  46.7  19.6  29.7  8.0 -31.7  0.7  6.0 -4.5  33.4  21.2 -35.9 -46.1  30.6 -19.5 
-7.2 -50.9  4.9 -1.3 -22.4  29.0  50.9  21.3  32.5  8.7 -34.6  0.8  6.6 -4.9  36.4  23.1 -39.1 -50.3  33.4 -21.2 
Minimum distance of calculation. 
true answer: 2 
indices: 2 
distance: 2.1396554
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-3.2 -22.6  2.1 -0.5 -10.0  12.9  22.6  9.5  14.3  3.8 -15.4  0.3  2.9 -2.2  16.2  10.3 -17.2 -22.3  14.8 -9.4 
-3.7 -25.9  2.5 -0.6 -11.5  14.8  25.9  10.8  16.5  4.4 -17.7  0.4  3.3 -2.5  18.6  11.8 -19.8 -25.7  17.0 -10.8 
-4.3 -30.4  2.9 -0.7 -13.4  17.3  30.3  12.7  19.3  5.1 -20.7  0.5  3.9 -2.9  21.7  13.8 -23.2 -30.0  19.9 -12.6 
-4.7 -33.2  3.2 -0.8 -14.6  18.9  33.1  13.8  21.1  5.6 -22.5  0.6  4.3 -3.2  23.7  15.1 -25.3 -32.7  21.7 -13.8 
-5.1 -36.2  3.5 -0.9 -16.0  20.6  36.1  15.1  23.0  6.1 -24.6  0.6  4.6 -3.5  25.9  16.4 -27.6 -35.7  23.7 -15.1 
-5.9 -41.7  4.0 -1.0 -18.3  23.7  41.7  17.4  26.6  7.1 -28.4  0.7  5.3 -4.1  29.9  18.9 -31.9 -41.2  27.4 -17.4 
-6.4 -45.1  4.3 -1.1 -19.9  25.7  45.1  18.9  28.7  7.7 -30.6  0.7  5.8 -4.4  32.3  20.5 -34.6 -44.5  29.5 -18.8 
-6.6 -46.7  4.5 -1.2 -20.6  26.6  46.7  19.6  29.7  8.0 -31.7  0.7  6.0 -4.5  33.4  21.2 -35.9 -46.1  30.6 -19.5 
-7.2 -50.9  4.9 -1.3 -22.4  29.0  50.9  21.3  32.5  8.7 -34.6  0.8  6.6 -4.9  36.4  23.1 -39.1 -50.3  33.4 -21.2 
Minimum distance of calculation. 
true answer: 5 
indices: 5 
distance: 0.22852233
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-3.2 -22.6  2.1 -0.5 -10.0  12.9  22.6  9.5  14.3  3.8 -15.4  0.3  2.9 -2.2  16.2  10.3 -17.2 -22.3  14.8 -9.4 
-3.7 -25.9  2.5 -0.6 -11.5  14.8  25.9  10.8  16.5  4.4 -17.7  0.4  3.3 -2.5  18.6  11.8 -19.8 -25.7  17.0 -10.8 
-4.3 -30.4  2.9 -0.7 -13.4  17.3  30.3  12.7  19.3  5.1 -20.7  0.5  3.9 -2.9  21.7  13.8 -23.2 -30.0  19.9 -12.6 
-4.7 -33.2  3.2 -0.8 -14.6  18.9  33.1  13.8  21.1  5.6 -22.5  0.6  4.3 -3.2  23.7  15.1 -25.3 -32.7  21.7 -13.8 
-5.1 -36.2  3.5 -0.9 -16.0  20.6  36.1  15.1  23.0  6.1 -24.6  0.6  4.6 -3.5  25.9  16.4 -27.6 -35.7  23.7 -15.1 
-5.9 -41.7  4.0 -1.0 -18.3  23.7  41.7  17.4  26.6  7.1 -28.4  0.7  5.3 -4.1  29.9  18.9 -31.9 -41.2  27.4 -17.4 
-6.4 -45.1  4.3 -1.1 -19.9  25.7  45.1  18.9  28.7  7.7 -30.6  0.7  5.8 -4.4  32.3  20.5 -34.6 -44.5  29.5 -18.8 
-6.6 -46.7  4.5 -1.2 -20.6  26.6  46.7  19.6  29.7  8.0 -31.7  0.7  6.0 -4.5  33.4  21.2 -35.9 -46.1  30.6 -19.5 
-7.2 -50.9  4.9 -1.3 -22.4  29.0  50.9  21.3  32.5  8.7 -34.6  0.8  6.6 -4.9  36.4  23.1 -39.1 -50.3  33.4 -21.2 
Minimum distance of calculation. 
true answer: 8 
indices: 8 
distance: 4.4760013
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-3.2 -22.6  2.1 -0.5 -10.0  12.9  22.6  9.5  14.3  3.8 -15.4  0.3  2.9 -2.2  16.2  10.3 -17.2 -22.3  14.8 -9.4 
-3.7 -25.9  2.5 -0.6 -11.5  14.8  25.9  10.8  16.5  4.4 -17.7  0.4  3.3 -2.5  18.6  11.8 -19.8 -25.7  17.0 -10.8 
-4.3 -30.4  2.9 -0.7 -13.4  17.3  30.3  12.7  19.3  5.1 -20.7  0.5  3.9 -2.9  21.7  13.8 -23.2 -30.0  19.9 -12.6 
-4.7 -33.2  3.2 -0.8 -14.6  18.9  33.1  13.8  21.1  5.6 -22.5  0.6  4.3 -3.2  23.7  15.1 -25.3 -32.7  21.7 -13.8 
-5.1 -36.2  3.5 -0.9 -16.0  20.6  36.1  15.1  23.0  6.1 -24.6  0.6  4.6 -3.5  25.9  16.4 -27.6 -35.7  23.7 -15.1 
-5.9 -41.7  4.0 -1.0 -18.3  23.7  41.7  17.4  26.6  7.1 -28.4  0.7  5.3 -4.1  29.9  18.9 -31.9 -41.2  27.4 -17.4 
-6.4 -45.1  4.3 -1.1 -19.9  25.7  45.1  18.9  28.7  7.7 -30.6  0.7  5.8 -4.4  32.3  20.5 -34.6 -44.5  29.5 -18.8 
-6.6 -46.7  4.5 -1.2 -20.6  26.6  46.7  19.6  29.7  8.0 -31.7  0.7  6.0 -4.5  33.4  21.2 -35.9 -46.1  30.6 -19.5 
-7.2 -50.9  4.9 -1.3 -22.4  29.0  50.9  21.3  32.5  8.7 -34.6  0.8  6.6 -4.9  36.4  23.1 -39.1 -50.3  33.4 -21.2 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 2.9629488
results (all): {'reconst_1x1_avg': 0.078125, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.203125, 'cross_0x1_all': nan, 'cluster_avg': -0.23707707, 'cluster_all': array([-0.6239171 ,  0.29475135,  0.08015665, ..., -0.13096887,
       -0.06596962, -0.11134423], dtype=float32), 'magnitude_avg': 0.988790890844297, 'magnitude_all': 0.988790890844297, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.078125, 'cross_0x1_avg': 0.203125, 'cluster_avg': -0.23707707, 'magnitude_avg': 0.988790890844297, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([5, 9, 7, 9, 9, 1, 4, 2, 7, 4, 9, 1, 2, 5, 8, 1, 9, 3, 2, 5, 9, 9, 5, 2,
        9, 3, 4, 9, 1, 2, 8, 8, 4, 7, 1, 7, 9, 5, 1, 6, 7, 8, 7, 9, 3, 8, 4, 7,
        2, 8, 9, 6, 8, 3, 5, 8, 8, 9, 3, 3, 9, 6, 3, 9, 8, 8, 6, 9, 6, 6, 4, 2,
        9, 1, 8, 1, 8, 1, 4, 8, 2, 3, 9, 5, 6, 3, 2, 8, 3, 3, 9, 1, 6, 3, 9, 8,
        9, 5, 8, 1, 4, 7, 4, 4, 4, 5, 7, 7, 9, 9, 1, 8, 8, 4, 7, 9, 7, 9, 1, 9,
        7, 4, 2, 3, 5, 9, 7, 9]), ['g5j', 'w9s', 'g7s', 'w9s', 'w9j', 'b1s', 'b4t', 'g2t', 'g7t', 'w4j', 'g9j', 'b1t', 'g2j', 'b5s', 'b8s', 'g1j', 'g9j', 'w3j', 'g2t', 'r5t', 'w9t', 'r9s', 'w5s', 'r2s', 'w9t', 'r3s', 'r4t', 'w9j', 'w1j', 'r2j', 'r8s', 'r8s', 'w4s', 'r7t', 'r1t', 'r7t', 'r9j', 'g5j', 'g1s', 'b6j', 'r7t', 'r8j', 'g7s', 'r9j', 'g3t', 'b8t', 'w4j', 'b7t', 'g2s', 'w8s', 'r9j', 'r6t', 'g8j', 'b3t', 'r5j', 'b8s', 'b8t', 'w9s', 'g3t', 'b3s', 'w9s', 'b6t', 'w3j', 'g9j', 'w8j', 'g8j', 'r6t', 'w9j', 'g6t', 'b6s', 'r4t', 'g2s', 'r9t', 'g1j', 'g8j', 'r1j', 'g8s', 'w1j', 'g4t', 'r8t', 'b2s', 'r3s', 'r9t', 'g5j', 'r6s', 'r3j', 'w2j', 'r8s', 'g3s', 'r3s', 'r9j', 'g1s', 'b6s', 'w3t', 'r9j', 'w8t', 'w9t', 'g5s', 'r8t', 'g1s', 'g4j', 'b7j', 'r4s', 'w4j', 'w4j', 'w5j', 'g7j', 'b7j', 'w9t', 'b9j', 'w1t', 'b8j', 'b8j', 'b4j', 'r7t', 'r9t', 'w7s', 'g9j', 'b1j', 'b9j', 'b7j', 'b4s', 'b2j', 'w3s', 'g5j', 'w9t', 'r7j', 'g9s']] 
label[0]: tensor([5, 9, 7, 9, 9, 1, 4, 2, 7, 4, 9, 1, 2, 5, 8, 1, 9, 3, 2, 5, 9, 9, 5, 2,
        9, 3, 4, 9, 1, 2, 8, 8, 4, 7, 1, 7, 9, 5, 1, 6, 7, 8, 7, 9, 3, 8, 4, 7,
        2, 8, 9, 6, 8, 3, 5, 8, 8, 9, 3, 3, 9, 6, 3, 9, 8, 8, 6, 9, 6, 6, 4, 2,
        9, 1, 8, 1, 8, 1, 4, 8, 2, 3, 9, 5, 6, 3, 2, 8, 3, 3, 9, 1, 6, 3, 9, 8,
        9, 5, 8, 1, 4, 7, 4, 4, 4, 5, 7, 7, 9, 9, 1, 8, 8, 4, 7, 9, 7, 9, 1, 9,
        7, 4, 2, 3, 5, 9, 7, 9]) 
label[1]: ['g5j', 'w9s', 'g7s', 'w9s', 'w9j', 'b1s', 'b4t', 'g2t', 'g7t', 'w4j', 'g9j', 'b1t', 'g2j', 'b5s', 'b8s', 'g1j', 'g9j', 'w3j', 'g2t', 'r5t', 'w9t', 'r9s', 'w5s', 'r2s', 'w9t', 'r3s', 'r4t', 'w9j', 'w1j', 'r2j', 'r8s', 'r8s', 'w4s', 'r7t', 'r1t', 'r7t', 'r9j', 'g5j', 'g1s', 'b6j', 'r7t', 'r8j', 'g7s', 'r9j', 'g3t', 'b8t', 'w4j', 'b7t', 'g2s', 'w8s', 'r9j', 'r6t', 'g8j', 'b3t', 'r5j', 'b8s', 'b8t', 'w9s', 'g3t', 'b3s', 'w9s', 'b6t', 'w3j', 'g9j', 'w8j', 'g8j', 'r6t', 'w9j', 'g6t', 'b6s', 'r4t', 'g2s', 'r9t', 'g1j', 'g8j', 'r1j', 'g8s', 'w1j', 'g4t', 'r8t', 'b2s', 'r3s', 'r9t', 'g5j', 'r6s', 'r3j', 'w2j', 'r8s', 'g3s', 'r3s', 'r9j', 'g1s', 'b6s', 'w3t', 'r9j', 'w8t', 'w9t', 'g5s', 'r8t', 'g1s', 'g4j', 'b7j', 'r4s', 'w4j', 'w4j', 'w5j', 'g7j', 'b7j', 'w9t', 'b9j', 'w1t', 'b8j', 'b8j', 'b4j', 'r7t', 'r9t', 'w7s', 'g9j', 'b1j', 'b9j', 'b7j', 'b4s', 'b2j', 'w3s', 'g5j', 'w9t', 'r7j', 'g9s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.59470767  0.10418098 -0.06274947 ... -0.69354737  0.07251986
  0.05762096]
Average of silhouette coef: -0.061296552
---
  0   1   2
0 0.0 39.6 22.7 
1 39.6 0.0 17.0 
2 22.7 17.0 0.0 
correlation [[ 1.         -0.27647115]
 [-0.27647115  1.        ]]
---
[[], [], []] [0 1 1 ... 1 2 2] [[  9.006503  38.949276]
 [-64.80125   41.726803]
 [ 29.084784  25.637634]
 ...
 [  5.261888  53.238445]
 [ 24.835247  24.292538]
 [ 28.820793  30.020302]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': -0.061296552, 'cluster_all': array([ 0.59470767,  0.10418098, -0.06274947, ..., -0.69354737,
        0.07251986,  0.05762096], dtype=float32), 'magnitude_avg': 0.276471145555679, 'magnitude_all': -0.276471145555679, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': -0.061296552, 'magnitude_avg': 0.276471145555679, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4', 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([9, 8, 6, 2, 9, 2, 1, 9, 5, 9, 9, 6, 3, 9, 2, 3, 6, 5, 1, 5, 5, 5, 8, 2,
        6, 9, 5, 5, 4, 7, 9, 2, 1, 4, 3, 1, 7, 2, 3, 4, 2, 6, 3, 4, 7, 3, 3, 1,
        1, 6, 8, 2, 9, 3, 4, 4, 5, 3, 4, 9, 1, 8, 1, 4, 5, 9, 7, 8, 2, 7, 7, 2,
        1, 1, 4, 9, 3, 2, 9, 3, 8, 7, 2, 7, 5, 7, 3, 2, 7, 3, 2, 4, 2, 9, 7, 3,
        4, 7, 5, 3, 4, 1, 4, 9, 5, 1, 7, 3, 3, 1, 1, 8, 1, 9, 1, 3, 7, 1, 5, 4,
        7, 4, 8, 8, 9, 3, 9, 8]), ['r9s', 'g8t', 'w6t', 'r2t', 'b9j', 'r2j', 'r1s', 'w9t', 'w5j', 'g9s', 'r9j', 'b6s', 'r3s', 'g9j', 'g2t', 'g3t', 'g6t', 'g5s', 'w1t', 'w5s', 'w5j', 'r5t', 'g8s', 'r2s', 'g6t', 'g9t', 'r5s', 'b5t', 'b4t', 'b7s', 'b9t', 'b2s', 'w1j', 'w4s', 'w3t', 'w1s', 'r7t', 'b2j', 'b3s', 'w4t', 'r2s', 'r6t', 'g3j', 'g4s', 'b7j', 'r3j', 'r3t', 'w1s', 'r1s', 'r6t', 'b8t', 'g2s', 'w9s', 'r3t', 'g4j', 'w4s', 'r5j', 'g3t', 'r4s', 'g9t', 'w1j', 'r8j', 'g1j', 'w4j', 'g5t', 'w9t', 'r7j', 'b8j', 'r2j', 'r7j', 'g7j', 'b2t', 'w1j', 'r1t', 'w4t', 'r9s', 'g3t', 'w2s', 'b9j', 'r3j', 'b8s', 'w7s', 'b2j', 'w7j', 'b5s', 'r7s', 'w3t', 'w2j', 'w7j', 'g3s', 'r2j', 'b4j', 'g2t', 'r9j', 'w7s', 'g3s', 'g4j', 'b7s', 'r5j', 'g3t', 'g4s', 'w1t', 'g4s', 'b9j', 'r5s', 'g1s', 'r7s', 'g3j', 'w3s', 'g1j', 'g1s', 'r8t', 'w1j', 'r9t', 'b1s', 'w3s', 'r7j', 'g1t', 'r5t', 'b4j', 'g7s', 'w4s', 'b8j', 'b8j', 'w9s', 'g3s', 'w9j', 'b8t']] 
label[0]: tensor([9, 8, 6, 2, 9, 2, 1, 9, 5, 9, 9, 6, 3, 9, 2, 3, 6, 5, 1, 5, 5, 5, 8, 2,
        6, 9, 5, 5, 4, 7, 9, 2, 1, 4, 3, 1, 7, 2, 3, 4, 2, 6, 3, 4, 7, 3, 3, 1,
        1, 6, 8, 2, 9, 3, 4, 4, 5, 3, 4, 9, 1, 8, 1, 4, 5, 9, 7, 8, 2, 7, 7, 2,
        1, 1, 4, 9, 3, 2, 9, 3, 8, 7, 2, 7, 5, 7, 3, 2, 7, 3, 2, 4, 2, 9, 7, 3,
        4, 7, 5, 3, 4, 1, 4, 9, 5, 1, 7, 3, 3, 1, 1, 8, 1, 9, 1, 3, 7, 1, 5, 4,
        7, 4, 8, 8, 9, 3, 9, 8]) 
label[1]: ['r9s', 'g8t', 'w6t', 'r2t', 'b9j', 'r2j', 'r1s', 'w9t', 'w5j', 'g9s', 'r9j', 'b6s', 'r3s', 'g9j', 'g2t', 'g3t', 'g6t', 'g5s', 'w1t', 'w5s', 'w5j', 'r5t', 'g8s', 'r2s', 'g6t', 'g9t', 'r5s', 'b5t', 'b4t', 'b7s', 'b9t', 'b2s', 'w1j', 'w4s', 'w3t', 'w1s', 'r7t', 'b2j', 'b3s', 'w4t', 'r2s', 'r6t', 'g3j', 'g4s', 'b7j', 'r3j', 'r3t', 'w1s', 'r1s', 'r6t', 'b8t', 'g2s', 'w9s', 'r3t', 'g4j', 'w4s', 'r5j', 'g3t', 'r4s', 'g9t', 'w1j', 'r8j', 'g1j', 'w4j', 'g5t', 'w9t', 'r7j', 'b8j', 'r2j', 'r7j', 'g7j', 'b2t', 'w1j', 'r1t', 'w4t', 'r9s', 'g3t', 'w2s', 'b9j', 'r3j', 'b8s', 'w7s', 'b2j', 'w7j', 'b5s', 'r7s', 'w3t', 'w2j', 'w7j', 'g3s', 'r2j', 'b4j', 'g2t', 'r9j', 'w7s', 'g3s', 'g4j', 'b7s', 'r5j', 'g3t', 'g4s', 'w1t', 'g4s', 'b9j', 'r5s', 'g1s', 'r7s', 'g3j', 'w3s', 'g1j', 'g1s', 'r8t', 'w1j', 'r9t', 'b1s', 'w3s', 'r7j', 'g1t', 'r5t', 'b4j', 'g7s', 'w4s', 'b8j', 'b8j', 'w9s', 'g3s', 'w9j', 'b8t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.16271003  0.17094195 -0.08122269 ...  0.12578355  0.08762556
  0.16576762]
Average of silhouette coef: 0.08585814
---
  0   1   2   3
0 0.0 2.7 2.7 2.7 
1 2.7 0.0 3.6 3.7 
2 2.7 3.6 0.0 4.1 
3 2.7 3.7 4.1 0.0 
correlation [[ 1.         -0.47981828]
 [-0.47981828  1.        ]]
---
[[], [], [], []] [3 2 0 ... 2 1 1] [[ 13.641426     9.501409  ]
 [ -4.20141     -2.0467272 ]
 [ 54.498825    -1.1057194 ]
 ...
 [-39.97028    -18.138445  ]
 [  0.09051796  44.17804   ]
 [ 22.15198    -24.360449  ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.08585814, 'cluster_all': array([ 0.16271003,  0.17094195, -0.08122269, ...,  0.12578355,
        0.08762556,  0.16576762], dtype=float32), 'magnitude_avg': 0.47981828345411937, 'magnitude_all': -0.47981828345411937, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.08585814, 'magnitude_avg': 0.47981828345411937, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 7, 3, 1, 8, 4, 4, 7, 4, 8, 5, 6, 1, 4, 2, 3, 5, 5, 8, 7, 4, 3, 4, 5,
        4, 5, 4, 2, 8, 3, 7, 7, 8, 8, 6, 1, 3, 1, 3, 5, 3, 1, 1, 4, 2, 1, 6, 7,
        9, 1, 3, 5, 7, 1, 1, 7, 5, 1, 2, 8, 3, 1, 9, 8, 5, 2, 5, 4, 6, 1, 6, 9,
        9, 5, 9, 2, 6, 1, 6, 9, 5, 5, 8, 3, 5, 1, 7, 1, 9, 5, 9, 8, 2, 3, 9, 7,
        5, 2, 6, 5, 1, 9, 4, 1, 1, 8, 7, 5, 5, 8, 1, 9, 3, 9, 4, 5, 8, 8, 2, 1,
        3, 5, 3, 5, 6, 4, 1, 3]), ['w6s', 'w7t', 'g3s', 'w1t', 'b8s', 'b4j', 'b4j', 'b7s', 'r4t', 'b8t', 'g5t', 'g6j', 'g1j', 'b4s', 'w2j', 'w3s', 'r5t', 'w5t', 'r8s', 'r7s', 'g4s', 'g3j', 'w4s', 'w5j', 'b4t', 'g5j', 'b4s', 'g2t', 'w8t', 'g3t', 'g7j', 'g7s', 'g8s', 'b8j', 'w6t', 'r1t', 'r3j', 'r1t', 'b3t', 'b5s', 'b3s', 'r1s', 'r1t', 'r4j', 'g2j', 'r1t', 'w6j', 'g7s', 'r9s', 'r1t', 'g3t', 'r5j', 'b7j', 'g1j', 'g1t', 'w7j', 'w5j', 'b1j', 'g2s', 'w8s', 'r3s', 'g1j', 'b9j', 'g8j', 'g5s', 'w2t', 'r5t', 'g4s', 'r6j', 'r1t', 'r6j', 'w9s', 'b9s', 'r5s', 'w9t', 'w2t', 'g6j', 'w1j', 'g6j', 'w9s', 'w5s', 'b5t', 'w8s', 'g3s', 'g5j', 'g1j', 'g7j', 'b1t', 'b9s', 'b5t', 'b9s', 'g8j', 'g2j', 'r3s', 'g9t', 'b7j', 'w5s', 'b2t', 'g6j', 'g5s', 'r1t', 'g9j', 'b4j', 'b1t', 'w1s', 'g8j', 'w7j', 'r5s', 'r5t', 'w8s', 'g1t', 'r9t', 'b3t', 'w9t', 'r4j', 'r5s', 'b8t', 'g8s', 'g2j', 'b1s', 'g3s', 'g5s', 'b3t', 'g5s', 'w6j', 'b4s', 'b1j', 'w3s']] 
label[0]: tensor([6, 7, 3, 1, 8, 4, 4, 7, 4, 8, 5, 6, 1, 4, 2, 3, 5, 5, 8, 7, 4, 3, 4, 5,
        4, 5, 4, 2, 8, 3, 7, 7, 8, 8, 6, 1, 3, 1, 3, 5, 3, 1, 1, 4, 2, 1, 6, 7,
        9, 1, 3, 5, 7, 1, 1, 7, 5, 1, 2, 8, 3, 1, 9, 8, 5, 2, 5, 4, 6, 1, 6, 9,
        9, 5, 9, 2, 6, 1, 6, 9, 5, 5, 8, 3, 5, 1, 7, 1, 9, 5, 9, 8, 2, 3, 9, 7,
        5, 2, 6, 5, 1, 9, 4, 1, 1, 8, 7, 5, 5, 8, 1, 9, 3, 9, 4, 5, 8, 8, 2, 1,
        3, 5, 3, 5, 6, 4, 1, 3]) 
label[1]: ['w6s', 'w7t', 'g3s', 'w1t', 'b8s', 'b4j', 'b4j', 'b7s', 'r4t', 'b8t', 'g5t', 'g6j', 'g1j', 'b4s', 'w2j', 'w3s', 'r5t', 'w5t', 'r8s', 'r7s', 'g4s', 'g3j', 'w4s', 'w5j', 'b4t', 'g5j', 'b4s', 'g2t', 'w8t', 'g3t', 'g7j', 'g7s', 'g8s', 'b8j', 'w6t', 'r1t', 'r3j', 'r1t', 'b3t', 'b5s', 'b3s', 'r1s', 'r1t', 'r4j', 'g2j', 'r1t', 'w6j', 'g7s', 'r9s', 'r1t', 'g3t', 'r5j', 'b7j', 'g1j', 'g1t', 'w7j', 'w5j', 'b1j', 'g2s', 'w8s', 'r3s', 'g1j', 'b9j', 'g8j', 'g5s', 'w2t', 'r5t', 'g4s', 'r6j', 'r1t', 'r6j', 'w9s', 'b9s', 'r5s', 'w9t', 'w2t', 'g6j', 'w1j', 'g6j', 'w9s', 'w5s', 'b5t', 'w8s', 'g3s', 'g5j', 'g1j', 'g7j', 'b1t', 'b9s', 'b5t', 'b9s', 'g8j', 'g2j', 'r3s', 'g9t', 'b7j', 'w5s', 'b2t', 'g6j', 'g5s', 'r1t', 'g9j', 'b4j', 'b1t', 'w1s', 'g8j', 'w7j', 'r5s', 'r5t', 'w8s', 'g1t', 'r9t', 'b3t', 'w9t', 'r4j', 'r5s', 'b8t', 'g8s', 'g2j', 'b1s', 'g3s', 'g5s', 'b3t', 'g5s', 'w6j', 'b4s', 'b1j', 'w3s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([4, 6, 3, 8, 2, 6, 2, 2, 2, 1, 2, 3, 2, 8, 8, 8, 4, 5, 5, 4, 8, 7, 1, 9,
        8, 5, 5, 4, 4, 1, 6, 8, 4, 3, 2, 4, 7, 2, 7, 6, 5, 9, 1, 4, 9, 4, 6, 1,
        7, 3, 4, 2, 5, 4, 4, 9, 8, 1, 2, 5, 4, 3, 8, 4, 1, 9, 3, 4, 3, 2, 5, 7,
        5, 5, 4, 2, 4, 5, 9, 4, 7, 5, 1, 2, 4, 6, 5, 9, 5, 7, 3, 4, 9, 5, 5, 8,
        1, 1, 1, 1, 9, 5, 8, 6, 3, 8, 5, 9, 4, 3, 5, 2, 6, 9, 8, 5, 2, 4, 8, 5,
        8, 5, 7, 6, 9, 1, 6, 2])
Accuracy (count): tensor(14) 
Accuracy (ratio) tensor(0.1094)
Accuracy:
 [[ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 10  0]
 [ 0  0  0  0  0  0  0 22  0]
 [ 0  0  0  0  0  0  0 22  0]
 [ 0  0  0  0  0  0  0 10  0]
 [ 0  0  0  0  0  0  0  8  0]
 [ 0  0  0  0  1  0  0 14  0]
 [ 0  0  0  0  0  0  0 12  0]]
Accuracy:
 [[0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.067 0.    0.    0.933 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([3, 4, 7, 4, 6, 1, 8, 4, 6, 5, 5, 9, 5, 8, 8, 5, 9, 1, 2, 7, 2, 3, 3, 3,
        2, 8, 9, 3, 4, 7, 6, 3, 7, 3, 6, 3, 5, 2, 8, 1, 8, 1, 8, 4, 4, 6, 7, 1,
        2, 7, 3, 7, 1, 9, 5, 2, 6, 2, 2, 2, 6, 3, 1, 5, 1, 5, 4, 7, 4, 6, 7, 5,
        5, 8, 3, 7, 4, 3, 7, 1, 8, 1, 5, 4, 2, 9, 6, 4, 1, 3, 4, 1, 8, 5, 9, 6,
        8, 1, 3, 3, 1, 9, 7, 8, 4, 2, 8, 6, 7, 6, 3, 7, 8, 5, 2, 7, 5, 8, 5, 9,
        7, 5, 5, 4, 9, 5, 6, 9])
Accuracy (count): tensor(15) 
Accuracy (ratio) tensor(0.1172)
Accuracy:
 [[ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0 12  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0 18  0]
 [ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 15  0]
 [ 0  0  0  0  0  0  0 10  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
Silhouette values: [ 0.20173849  0.00674882  0.11831142 ...  0.06408154 -0.11139648
 -0.01302986]
Average of silhouette coef: 0.0925086
---
  1   2   3   4   5   6   7   8   9
1 0.0 3.6 4.0 4.9 4.4 4.5 4.5 4.1 4.7 
2 3.6 0.0 3.3 4.4 4.1 4.2 3.9 4.0 4.4 
3 4.0 3.3 0.0 4.5 3.0 4.7 3.8 3.6 4.2 
4 4.9 4.4 4.5 0.0 4.4 4.5 4.5 5.0 3.8 
5 4.4 4.1 3.0 4.4 0.0 3.7 3.7 3.1 3.8 
6 4.5 4.2 4.7 4.5 3.7 0.0 5.1 4.0 4.1 
7 4.5 3.9 3.8 4.5 3.7 5.1 0.0 3.8 2.9 
8 4.1 4.0 3.6 5.0 3.1 4.0 3.8 0.0 2.7 
9 4.7 4.4 4.2 3.8 3.8 4.1 2.9 2.7 0.0 
correlation [[1.         0.28468831]
 [0.28468831 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [6 7 3 ... 8 6 6] [[-18.647383  -72.10584  ]
 [-12.286062    9.930568 ]
 [  1.0696518 -18.833277 ]
 ...
 [  5.053158   -1.4813116]
 [-19.287996  -14.62266  ]
 [-15.911034  -65.275246 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8,
        8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(49) 
Accuracy (ratio) tensor(0.1002)
Accuracy:
 [[ 0  0  0  0  0  0  0 45  0]
 [ 0  0  0  0  1  0  0 51  0]
 [ 0  0  0  0  1  0  0 56  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 61  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  2  0  0 55  0]
 [ 0  0  0  0  3  0  0 49  0]
 [ 0  0  0  0  2  0  0 43  0]]
Accuracy:
 [[0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.019 0.    0.    0.981 0.   ]
 [0.    0.    0.    0.    0.018 0.    0.    0.982 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.035 0.    0.    0.965 0.   ]
 [0.    0.    0.    0.    0.058 0.    0.    0.942 0.   ]
 [0.    0.    0.    0.    0.044 0.    0.    0.956 0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2  1.9  0.0  2.0  0.0  0.1  0.2  0.1 -0.0  0.1  0.1 -0.2  0.0 -0.2  0.0 -0.0 -0.8 -0.0  0.0 -0.0 
 0.2  1.3 -0.1 -1.0 -0.8  0.2  0.2  0.1  0.7  0.1  0.2  0.1 -0.0  0.1 -0.7 -1.2 -0.6  0.2 -0.1  0.7 
 0.2  0.1 -0.2 -1.0  1.4  0.0  0.2  0.1  0.4  0.1  0.4  1.0  0.0  0.1  0.2  0.2 -1.1  0.2  0.2  0.1 
 0.4 -0.7 -0.2 -0.1 -0.4  3.4  0.0  0.0 -0.2  0.1 -0.2 -0.1 -0.0 -0.0 -0.1 -0.1  0.3  0.1  0.3 -0.1 
 0.0 -0.4 -0.2 -0.9  1.1 -0.4 -0.2  0.0 -0.7 -0.1 -0.4 -1.0 -0.0  0.0 -0.3  0.6 -0.0  0.2  0.1 -0.8 
-0.3  0.1 -0.3 -0.4 -1.7 -0.0 -0.1  0.0 -2.5 -0.0  0.1  0.0 -0.0 -0.0 -0.2  0.4 -0.1  0.0  0.2 -0.2 
 0.2 -1.5 -0.3  0.5 -0.1 -0.4 -0.2 -0.0  1.7 -0.3 -0.0  0.2 -0.0  0.1 -0.8 -0.2  0.6  0.2  0.1 -0.0 
 0.3  0.2 -0.0 -0.4  0.1 -0.9 -0.4  0.1 -0.0  0.0  0.7 -0.4  0.0  0.0  1.6  0.4  1.1  0.1  0.5 -0.0 
-0.2 -1.4 -0.0  0.4 -0.3  0.3 -0.5  0.0 -0.0 -0.1  0.4  0.2  0.0  0.0  0.7 -0.2  1.7  0.2  0.5  0.6 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 2.7465515
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2  1.9  0.0  2.0  0.0  0.1  0.2  0.1 -0.0  0.1  0.1 -0.2  0.0 -0.2  0.0 -0.0 -0.8 -0.0  0.0 -0.0 
 0.2  1.3 -0.1 -1.0 -0.8  0.2  0.2  0.1  0.7  0.1  0.2  0.1 -0.0  0.1 -0.7 -1.2 -0.6  0.2 -0.1  0.7 
 0.2  0.1 -0.2 -1.0  1.4  0.0  0.2  0.1  0.4  0.1  0.4  1.0  0.0  0.1  0.2  0.2 -1.1  0.2  0.2  0.1 
 0.4 -0.7 -0.2 -0.1 -0.4  3.4  0.0  0.0 -0.2  0.1 -0.2 -0.1 -0.0 -0.0 -0.1 -0.1  0.3  0.1  0.3 -0.1 
 0.0 -0.4 -0.2 -0.9  1.1 -0.4 -0.2  0.0 -0.7 -0.1 -0.4 -1.0 -0.0  0.0 -0.3  0.6 -0.0  0.2  0.1 -0.8 
-0.3  0.1 -0.3 -0.4 -1.7 -0.0 -0.1  0.0 -2.5 -0.0  0.1  0.0 -0.0 -0.0 -0.2  0.4 -0.1  0.0  0.2 -0.2 
 0.2 -1.5 -0.3  0.5 -0.1 -0.4 -0.2 -0.0  1.7 -0.3 -0.0  0.2 -0.0  0.1 -0.8 -0.2  0.6  0.2  0.1 -0.0 
 0.3  0.2 -0.0 -0.4  0.1 -0.9 -0.4  0.1 -0.0  0.0  0.7 -0.4  0.0  0.0  1.6  0.4  1.1  0.1  0.5 -0.0 
-0.2 -1.4 -0.0  0.4 -0.3  0.3 -0.5  0.0 -0.0 -0.1  0.4  0.2  0.0  0.0  0.7 -0.2  1.7  0.2  0.5  0.6 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 4.903969
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 0.2  1.9  0.0  2.0  0.0  0.1  0.2  0.1 -0.0  0.1  0.1 -0.2  0.0 -0.2  0.0 -0.0 -0.8 -0.0  0.0 -0.0 
 0.2  1.3 -0.1 -1.0 -0.8  0.2  0.2  0.1  0.7  0.1  0.2  0.1 -0.0  0.1 -0.7 -1.2 -0.6  0.2 -0.1  0.7 
 0.2  0.1 -0.2 -1.0  1.4  0.0  0.2  0.1  0.4  0.1  0.4  1.0  0.0  0.1  0.2  0.2 -1.1  0.2  0.2  0.1 
 0.4 -0.7 -0.2 -0.1 -0.4  3.4  0.0  0.0 -0.2  0.1 -0.2 -0.1 -0.0 -0.0 -0.1 -0.1  0.3  0.1  0.3 -0.1 
 0.0 -0.4 -0.2 -0.9  1.1 -0.4 -0.2  0.0 -0.7 -0.1 -0.4 -1.0 -0.0  0.0 -0.3  0.6 -0.0  0.2  0.1 -0.8 
-0.3  0.1 -0.3 -0.4 -1.7 -0.0 -0.1  0.0 -2.5 -0.0  0.1  0.0 -0.0 -0.0 -0.2  0.4 -0.1  0.0  0.2 -0.2 
 0.2 -1.5 -0.3  0.5 -0.1 -0.4 -0.2 -0.0  1.7 -0.3 -0.0  0.2 -0.0  0.1 -0.8 -0.2  0.6  0.2  0.1 -0.0 
 0.3  0.2 -0.0 -0.4  0.1 -0.9 -0.4  0.1 -0.0  0.0  0.7 -0.4  0.0  0.0  1.6  0.4  1.1  0.1  0.5 -0.0 
-0.2 -1.4 -0.0  0.4 -0.3  0.3 -0.5  0.0 -0.0 -0.1  0.4  0.2  0.0  0.0  0.7 -0.2  1.7  0.2  0.5  0.6 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 3.5975907
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.2  1.9  0.0  2.0  0.0  0.1  0.2  0.1 -0.0  0.1  0.1 -0.2  0.0 -0.2  0.0 -0.0 -0.8 -0.0  0.0 -0.0 
 0.2  1.3 -0.1 -1.0 -0.8  0.2  0.2  0.1  0.7  0.1  0.2  0.1 -0.0  0.1 -0.7 -1.2 -0.6  0.2 -0.1  0.7 
 0.2  0.1 -0.2 -1.0  1.4  0.0  0.2  0.1  0.4  0.1  0.4  1.0  0.0  0.1  0.2  0.2 -1.1  0.2  0.2  0.1 
 0.4 -0.7 -0.2 -0.1 -0.4  3.4  0.0  0.0 -0.2  0.1 -0.2 -0.1 -0.0 -0.0 -0.1 -0.1  0.3  0.1  0.3 -0.1 
 0.0 -0.4 -0.2 -0.9  1.1 -0.4 -0.2  0.0 -0.7 -0.1 -0.4 -1.0 -0.0  0.0 -0.3  0.6 -0.0  0.2  0.1 -0.8 
-0.3  0.1 -0.3 -0.4 -1.7 -0.0 -0.1  0.0 -2.5 -0.0  0.1  0.0 -0.0 -0.0 -0.2  0.4 -0.1  0.0  0.2 -0.2 
 0.2 -1.5 -0.3  0.5 -0.1 -0.4 -0.2 -0.0  1.7 -0.3 -0.0  0.2 -0.0  0.1 -0.8 -0.2  0.6  0.2  0.1 -0.0 
 0.3  0.2 -0.0 -0.4  0.1 -0.9 -0.4  0.1 -0.0  0.0  0.7 -0.4  0.0  0.0  1.6  0.4  1.1  0.1  0.5 -0.0 
-0.2 -1.4 -0.0  0.4 -0.3  0.3 -0.5  0.0 -0.0 -0.1  0.4  0.2  0.0  0.0  0.7 -0.2  1.7  0.2  0.5  0.6 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 3.2550519
results (all): {'reconst_0x0_avg': 0.109375, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.1171875, 'cross_1x0_all': nan, 'cluster_avg': 0.0925086, 'cluster_all': array([ 0.20173849,  0.00674882,  0.11831142, ...,  0.06408154,
       -0.11139648, -0.01302986], dtype=float32), 'magnitude_avg': 0.2846883118531789, 'magnitude_all': 0.2846883118531789, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.10020449757575989, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 1.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.109375, 'cross_1x0_avg': 0.1171875, 'cluster_avg': 0.0925086, 'magnitude_avg': 0.2846883118531789, 'tsne-2d_avg': nan, 'mathematics_avg': 0.10020449757575989, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 1.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([1, 9, 1, 5, 8, 4, 4, 1, 4, 2, 8, 1, 7, 7, 6, 2, 3, 4, 7, 1, 9, 5, 5, 8,
        4, 1, 3, 3, 9, 2, 7, 8, 2, 1, 9, 1, 9, 9, 7, 7, 3, 5, 8, 1, 5, 5, 8, 4,
        2, 1, 9, 6, 3, 7, 5, 9, 4, 1, 2, 6, 7, 3, 8, 7, 9, 9, 5, 4, 6, 5, 7, 2,
        1, 2, 8, 2, 2, 6, 1, 1, 7, 3, 9, 3, 1, 8, 2, 2, 1, 3, 4, 7, 5, 5, 5, 6,
        4, 6, 8, 1, 8, 9, 5, 2, 8, 2, 8, 8, 6, 7, 1, 6, 2, 2, 5, 4, 4, 8, 2, 9,
        7, 2, 5, 5, 1, 2, 3, 1]), ['b1t', 'r9s', 'w1j', 'b5s', 'g8t', 'w4s', 'b4t', 'g1s', 'w4t', 'b2s', 'g8j', 'g1j', 'w7t', 'r7s', 'b6s', 'g2t', 'r3s', 'r4s', 'w7j', 'w1j', 'r9t', 'w5t', 'r5t', 'g8j', 'g4t', 'w1t', 'b3s', 'w3j', 'r9s', 'w2s', 'b7s', 'r8s', 'r2t', 'r1j', 'b9t', 'g1t', 'g9t', 'b9j', 'w7t', 'w7t', 'b3j', 'r5j', 'r8j', 'g1j', 'r5s', 'w5j', 'b8t', 'g4j', 'g2j', 'g1s', 'r9t', 'w6s', 'b3t', 'r7t', 'w5j', 'g9t', 'w4j', 'b1j', 'b2j', 'w6t', 'g7s', 'w3t', 'b8t', 'g7t', 'r9s', 'b9j', 'w5j', 'b4s', 'g6s', 'r5s', 'r7t', 'w2j', 'g1t', 'r2s', 'b8t', 'b2j', 'b2s', 'w6j', 'r1t', 'w1j', 'b7s', 'r3t', 'b9j', 'r3j', 'g1s', 'g8t', 'g2t', 'r2t', 'r1s', 'b3j', 'r4j', 'r7j', 'w5j', 'g5s', 'b5s', 'g6t', 'g4t', 'b6s', 'w8t', 'r1s', 'b8j', 'r9s', 'g5j', 'b2j', 'g8t', 'w2j', 'g8j', 'g8j', 'r6j', 'w7t', 'b1t', 'b6j', 'g2j', 'b2s', 'b5j', 'w4t', 'b4s', 'w8s', 'w2t', 'g9s', 'w7j', 'r2t', 'g5j', 'g5s', 'b1j', 'g2t', 'g3s', 'g1s']] 
label[0]: tensor([1, 9, 1, 5, 8, 4, 4, 1, 4, 2, 8, 1, 7, 7, 6, 2, 3, 4, 7, 1, 9, 5, 5, 8,
        4, 1, 3, 3, 9, 2, 7, 8, 2, 1, 9, 1, 9, 9, 7, 7, 3, 5, 8, 1, 5, 5, 8, 4,
        2, 1, 9, 6, 3, 7, 5, 9, 4, 1, 2, 6, 7, 3, 8, 7, 9, 9, 5, 4, 6, 5, 7, 2,
        1, 2, 8, 2, 2, 6, 1, 1, 7, 3, 9, 3, 1, 8, 2, 2, 1, 3, 4, 7, 5, 5, 5, 6,
        4, 6, 8, 1, 8, 9, 5, 2, 8, 2, 8, 8, 6, 7, 1, 6, 2, 2, 5, 4, 4, 8, 2, 9,
        7, 2, 5, 5, 1, 2, 3, 1]) 
label[1]: ['b1t', 'r9s', 'w1j', 'b5s', 'g8t', 'w4s', 'b4t', 'g1s', 'w4t', 'b2s', 'g8j', 'g1j', 'w7t', 'r7s', 'b6s', 'g2t', 'r3s', 'r4s', 'w7j', 'w1j', 'r9t', 'w5t', 'r5t', 'g8j', 'g4t', 'w1t', 'b3s', 'w3j', 'r9s', 'w2s', 'b7s', 'r8s', 'r2t', 'r1j', 'b9t', 'g1t', 'g9t', 'b9j', 'w7t', 'w7t', 'b3j', 'r5j', 'r8j', 'g1j', 'r5s', 'w5j', 'b8t', 'g4j', 'g2j', 'g1s', 'r9t', 'w6s', 'b3t', 'r7t', 'w5j', 'g9t', 'w4j', 'b1j', 'b2j', 'w6t', 'g7s', 'w3t', 'b8t', 'g7t', 'r9s', 'b9j', 'w5j', 'b4s', 'g6s', 'r5s', 'r7t', 'w2j', 'g1t', 'r2s', 'b8t', 'b2j', 'b2s', 'w6j', 'r1t', 'w1j', 'b7s', 'r3t', 'b9j', 'r3j', 'g1s', 'g8t', 'g2t', 'r2t', 'r1s', 'b3j', 'r4j', 'r7j', 'w5j', 'g5s', 'b5s', 'g6t', 'g4t', 'b6s', 'w8t', 'r1s', 'b8j', 'r9s', 'g5j', 'b2j', 'g8t', 'w2j', 'g8j', 'g8j', 'r6j', 'w7t', 'b1t', 'b6j', 'g2j', 'b2s', 'b5j', 'w4t', 'b4s', 'w8s', 'w2t', 'g9s', 'w7j', 'r2t', 'g5j', 'g5s', 'b1j', 'g2t', 'g3s', 'g1s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.00766338 -0.01368576  0.01533638 ...  0.01105731 -0.00236751
 -0.0115697 ]
Average of silhouette coef: -0.005064854
---
  0   1   2
0 0.0 0.3 0.3 
1 0.3 0.0 0.4 
2 0.3 0.4 0.0 
correlation [[ 1.         -0.36494576]
 [-0.36494576  1.        ]]
---
[[], [], []] [2 1 0 ... 0 2 1] [[ -2.148788  -70.49554  ]
 [ 32.33824    38.26382  ]
 [-13.384043  -69.70754  ]
 ...
 [-13.155502  -65.839905 ]
 [-26.228762  -35.474514 ]
 [ 21.598986   -2.4984868]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.005064854, 'cluster_all': array([-0.00766338, -0.01368576,  0.01533638, ...,  0.01105731,
       -0.00236751, -0.0115697 ], dtype=float32), 'magnitude_avg': 0.3649457633430043, 'magnitude_all': -0.3649457633430043, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.005064854, 'magnitude_avg': 0.3649457633430043, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([9, 3, 2, 9, 7, 4, 1, 9, 5, 2, 3, 3, 6, 3, 6, 7, 3, 9, 4, 3, 8, 4, 8, 2,
        4, 4, 4, 1, 7, 7, 3, 8, 9, 8, 9, 9, 2, 2, 2, 7, 7, 1, 7, 1, 4, 6, 1, 2,
        2, 9, 2, 2, 4, 8, 4, 6, 7, 4, 7, 2, 6, 1, 6, 6, 2, 1, 5, 4, 4, 5, 7, 3,
        9, 8, 8, 6, 4, 8, 7, 5, 1, 9, 1, 9, 3, 3, 3, 2, 7, 1, 8, 3, 8, 2, 7, 7,
        6, 1, 3, 6, 4, 5, 3, 4, 1, 1, 4, 1, 5, 5, 2, 7, 7, 5, 8, 1, 3, 2, 5, 1,
        8, 7, 1, 9, 4, 3, 6, 1]), ['b9s', 'b3t', 'w2j', 'g9s', 'r7j', 'b4j', 'r1s', 'r9s', 'g5t', 'r2j', 'w3t', 'w3j', 'w6t', 'w3j', 'g6s', 'b7t', 'r3j', 'w9t', 'g4j', 'b3t', 'b8s', 'r4s', 'r8t', 'w2s', 'r4j', 'w4t', 'r4j', 'w1t', 'r7j', 'r7t', 'b3j', 'b8t', 'r9t', 'r8s', 'g9j', 'b9j', 'b2j', 'r2s', 'r2j', 'b7s', 'w7j', 'w1t', 'b7s', 'w1s', 'w4j', 'b6t', 'b1t', 'b2s', 'r2t', 'w9t', 'r2s', 'r2t', 'r4j', 'g8j', 'w4s', 'w6t', 'g7t', 'b4t', 'w7s', 'w2j', 'b6t', 'b1j', 'b6s', 'w6j', 'g2t', 'w1s', 'g5t', 'g4j', 'w4t', 'g5t', 'r7j', 'g3s', 'w9s', 'r8j', 'r8j', 'b6t', 'r4j', 'r8s', 'g7s', 'g5t', 'b1t', 'b9s', 'b1t', 'b9j', 'r3s', 'r3s', 'r3s', 'r2s', 'b7t', 'r1j', 'r8t', 'w3j', 'w8j', 'b2j', 'r7t', 'r7t', 'r6j', 'b1t', 'g3s', 'b6s', 'w4j', 'r5t', 'w3s', 'r4j', 'g1j', 'b1j', 'g4t', 'g1s', 'w5j', 'g5j', 'w2j', 'w7j', 'b7s', 'g5t', 'w8t', 'g1t', 'r3s', 'g2t', 'r5s', 'r1t', 'g8t', 'b7s', 'w1t', 'r9j', 'b4s', 'b3t', 'r6j', 'w1s']] 
label[0]: tensor([9, 3, 2, 9, 7, 4, 1, 9, 5, 2, 3, 3, 6, 3, 6, 7, 3, 9, 4, 3, 8, 4, 8, 2,
        4, 4, 4, 1, 7, 7, 3, 8, 9, 8, 9, 9, 2, 2, 2, 7, 7, 1, 7, 1, 4, 6, 1, 2,
        2, 9, 2, 2, 4, 8, 4, 6, 7, 4, 7, 2, 6, 1, 6, 6, 2, 1, 5, 4, 4, 5, 7, 3,
        9, 8, 8, 6, 4, 8, 7, 5, 1, 9, 1, 9, 3, 3, 3, 2, 7, 1, 8, 3, 8, 2, 7, 7,
        6, 1, 3, 6, 4, 5, 3, 4, 1, 1, 4, 1, 5, 5, 2, 7, 7, 5, 8, 1, 3, 2, 5, 1,
        8, 7, 1, 9, 4, 3, 6, 1]) 
label[1]: ['b9s', 'b3t', 'w2j', 'g9s', 'r7j', 'b4j', 'r1s', 'r9s', 'g5t', 'r2j', 'w3t', 'w3j', 'w6t', 'w3j', 'g6s', 'b7t', 'r3j', 'w9t', 'g4j', 'b3t', 'b8s', 'r4s', 'r8t', 'w2s', 'r4j', 'w4t', 'r4j', 'w1t', 'r7j', 'r7t', 'b3j', 'b8t', 'r9t', 'r8s', 'g9j', 'b9j', 'b2j', 'r2s', 'r2j', 'b7s', 'w7j', 'w1t', 'b7s', 'w1s', 'w4j', 'b6t', 'b1t', 'b2s', 'r2t', 'w9t', 'r2s', 'r2t', 'r4j', 'g8j', 'w4s', 'w6t', 'g7t', 'b4t', 'w7s', 'w2j', 'b6t', 'b1j', 'b6s', 'w6j', 'g2t', 'w1s', 'g5t', 'g4j', 'w4t', 'g5t', 'r7j', 'g3s', 'w9s', 'r8j', 'r8j', 'b6t', 'r4j', 'r8s', 'g7s', 'g5t', 'b1t', 'b9s', 'b1t', 'b9j', 'r3s', 'r3s', 'r3s', 'r2s', 'b7t', 'r1j', 'r8t', 'w3j', 'w8j', 'b2j', 'r7t', 'r7t', 'r6j', 'b1t', 'g3s', 'b6s', 'w4j', 'r5t', 'w3s', 'r4j', 'g1j', 'b1j', 'g4t', 'g1s', 'w5j', 'g5j', 'w2j', 'w7j', 'b7s', 'g5t', 'w8t', 'g1t', 'r3s', 'g2t', 'r5s', 'r1t', 'g8t', 'b7s', 'w1t', 'r9j', 'b4s', 'b3t', 'r6j', 'w1s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.18265028 -0.48874822 -0.85172695 ...  0.607715   -0.13240963
 -0.6046272 ]
Average of silhouette coef: -0.061822224
---
  0   1   2   3
0 0.0 34.3 59.6 35.6 
1 34.3 0.0 25.2 1.3 
2 59.6 25.2 0.0 24.0 
3 35.6 1.3 24.0 0.0 
correlation [[1.         0.15532443]
 [0.15532443 1.        ]]
---
[[], [], [], []] [1 1 0 ... 2 1 3] [[  6.133993  -49.10557  ]
 [ 47.77574    24.656986 ]
 [ 50.594868  -24.73121  ]
 ...
 [ 59.082752   -6.3211794]
 [ 13.095549   60.11831  ]
 [ 55.86947    -5.5353794]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': -0.061822224, 'cluster_all': array([-0.18265028, -0.48874822, -0.85172695, ...,  0.607715  ,
       -0.13240963, -0.6046272 ], dtype=float32), 'magnitude_avg': 0.1553244322645049, 'magnitude_all': 0.1553244322645049, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': -0.061822224, 'magnitude_avg': 0.1553244322645049, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([4, 3, 3, 2, 8, 6, 5, 2, 6, 9, 4, 1, 8, 8, 3, 8, 9, 6, 1, 4, 8, 4, 2, 8,
        1, 5, 7, 3, 8, 8, 6, 1, 4, 4, 6, 8, 3, 6, 2, 1, 3, 6, 6, 1, 9, 7, 6, 7,
        1, 4, 2, 3, 9, 5, 9, 1, 3, 3, 5, 3, 9, 1, 1, 4, 6, 8, 9, 2, 7, 4, 8, 7,
        2, 4, 9, 2, 6, 3, 4, 1, 4, 5, 4, 2, 9, 3, 3, 7, 2, 5, 9, 2, 8, 4, 2, 1,
        3, 7, 7, 1, 3, 1, 6, 5, 6, 1, 2, 4, 2, 8, 3, 4, 4, 7, 9, 6, 6, 3, 7, 3,
        8, 3, 2, 9, 7, 4, 2, 1]), ['w4j', 'r3j', 'w3s', 'g2j', 'b8j', 'b6s', 'r5s', 'g2s', 'b6j', 'b9t', 'b4s', 'b1t', 'g8s', 'b8t', 'r3j', 'g8s', 'g9j', 'w6t', 'r1s', 'g4s', 'w8j', 'r4t', 'w2j', 'w8s', 'g1j', 'w5j', 'g7j', 'r3t', 'w8s', 'b8s', 'g6t', 'w1j', 'w4j', 'r4s', 'b6j', 'r8s', 'g3t', 'w6s', 'r2j', 'r1j', 'w3j', 'b6s', 'b6j', 'g1s', 'r9s', 'b7t', 'g6j', 'w7t', 'g1j', 'r4t', 'b2s', 'w3s', 'b9t', 'w5t', 'g9t', 'g1s', 'r3t', 'w3t', 'r5s', 'g3t', 'w9t', 'r1j', 'r1j', 'r4s', 'r6t', 'w8s', 'r9j', 'w2s', 'r7s', 'w4t', 'g8s', 'g7s', 'w2s', 'g4t', 'w9s', 'g2t', 'r6j', 'b3t', 'g4j', 'b1j', 'b4j', 'w5j', 'b4j', 'g2j', 'w9s', 'g3s', 'w3j', 'r7t', 'r2s', 'b5s', 'r9t', 'b2t', 'r8t', 'w4s', 'g2s', 'r1t', 'g3j', 'w7t', 'w7s', 'w1s', 'w3t', 'b1s', 'w6s', 'r5t', 'w6s', 'r1t', 'g2s', 'r4s', 'b2t', 'r8s', 'g3t', 'g4s', 'r4j', 'w7j', 'r9j', 'b6s', 'w6j', 'b3j', 'w7s', 'r3t', 'b8t', 'b3t', 'w2j', 'w9t', 'g7t', 'g4s', 'r2t', 'g1j']] 
label[0]: tensor([4, 3, 3, 2, 8, 6, 5, 2, 6, 9, 4, 1, 8, 8, 3, 8, 9, 6, 1, 4, 8, 4, 2, 8,
        1, 5, 7, 3, 8, 8, 6, 1, 4, 4, 6, 8, 3, 6, 2, 1, 3, 6, 6, 1, 9, 7, 6, 7,
        1, 4, 2, 3, 9, 5, 9, 1, 3, 3, 5, 3, 9, 1, 1, 4, 6, 8, 9, 2, 7, 4, 8, 7,
        2, 4, 9, 2, 6, 3, 4, 1, 4, 5, 4, 2, 9, 3, 3, 7, 2, 5, 9, 2, 8, 4, 2, 1,
        3, 7, 7, 1, 3, 1, 6, 5, 6, 1, 2, 4, 2, 8, 3, 4, 4, 7, 9, 6, 6, 3, 7, 3,
        8, 3, 2, 9, 7, 4, 2, 1]) 
label[1]: ['w4j', 'r3j', 'w3s', 'g2j', 'b8j', 'b6s', 'r5s', 'g2s', 'b6j', 'b9t', 'b4s', 'b1t', 'g8s', 'b8t', 'r3j', 'g8s', 'g9j', 'w6t', 'r1s', 'g4s', 'w8j', 'r4t', 'w2j', 'w8s', 'g1j', 'w5j', 'g7j', 'r3t', 'w8s', 'b8s', 'g6t', 'w1j', 'w4j', 'r4s', 'b6j', 'r8s', 'g3t', 'w6s', 'r2j', 'r1j', 'w3j', 'b6s', 'b6j', 'g1s', 'r9s', 'b7t', 'g6j', 'w7t', 'g1j', 'r4t', 'b2s', 'w3s', 'b9t', 'w5t', 'g9t', 'g1s', 'r3t', 'w3t', 'r5s', 'g3t', 'w9t', 'r1j', 'r1j', 'r4s', 'r6t', 'w8s', 'r9j', 'w2s', 'r7s', 'w4t', 'g8s', 'g7s', 'w2s', 'g4t', 'w9s', 'g2t', 'r6j', 'b3t', 'g4j', 'b1j', 'b4j', 'w5j', 'b4j', 'g2j', 'w9s', 'g3s', 'w3j', 'r7t', 'r2s', 'b5s', 'r9t', 'b2t', 'r8t', 'w4s', 'g2s', 'r1t', 'g3j', 'w7t', 'w7s', 'w1s', 'w3t', 'b1s', 'w6s', 'r5t', 'w6s', 'r1t', 'g2s', 'r4s', 'b2t', 'r8s', 'g3t', 'g4s', 'r4j', 'w7j', 'r9j', 'b6s', 'w6j', 'b3j', 'w7s', 'r3t', 'b8t', 'b3t', 'w2j', 'w9t', 'g7t', 'g4s', 'r2t', 'g1j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([3, 1, 5, 3, 1, 6, 5, 3, 7, 9, 2, 7, 8, 2, 3, 8, 1, 1, 1, 3, 1, 5, 1, 3,
        6, 7, 7, 8, 2, 5, 1, 7, 1, 3, 2, 1, 4, 4, 9, 4, 1, 2, 5, 8, 9, 3, 8, 7,
        5, 2, 3, 3, 8, 8, 6, 8, 5, 9, 9, 7, 1, 3, 2, 5, 1, 6, 8, 8, 9, 6, 9, 9,
        1, 3, 6, 5, 4, 8, 6, 3, 1, 2, 8, 1, 3, 3, 4, 8, 7, 7, 1, 8, 8, 3, 3, 2,
        9, 3, 6, 8, 1, 4, 5, 2, 4, 2, 7, 8, 4, 3, 9, 3, 6, 9, 9, 6, 9, 4, 9, 2,
        7, 1, 4, 8, 2, 1, 4, 7])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0 20  0  0  0  0  0  0  0]
 [ 0 13  0  0  0  0  0  0  0]
 [ 0 20  0  0  0  0  0  0  0]
 [ 0 11  0  0  0  0  0  0  0]
 [ 0 10  0  0  0  0  0  0  0]
 [ 0 10  0  0  0  0  0  0  0]
 [ 0 12  0  0  0  0  0  0  0]
 [ 0 18  0  0  0  0  0  0  0]
 [ 0 14  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([6, 9, 4, 8, 4, 8, 6, 7, 9, 8, 2, 1, 1, 3, 5, 3, 2, 2, 5, 2, 4, 8, 3, 1,
        1, 9, 7, 6, 4, 9, 8, 4, 1, 6, 7, 4, 4, 3, 3, 2, 3, 1, 5, 5, 2, 8, 5, 4,
        2, 1, 7, 3, 6, 3, 1, 5, 4, 5, 8, 1, 5, 4, 7, 4, 5, 4, 8, 2, 4, 5, 4, 9,
        4, 3, 5, 5, 2, 9, 5, 3, 1, 2, 9, 1, 4, 9, 4, 2, 5, 3, 2, 5, 8, 8, 2, 1,
        8, 4, 4, 6, 4, 9, 1, 6, 6, 5, 2, 7, 7, 9, 4, 1, 7, 2, 9, 9, 1, 4, 7, 8,
        4, 3, 8, 3, 3, 3, 4, 2])
Accuracy (count): tensor(18) 
Accuracy (ratio) tensor(0.1406)
Accuracy:
 [[ 0 15  0  0  0  0  0  0  0]
 [ 0 16  0  0  0  0  0  0  0]
 [ 0 15  0  0  0  0  0  0  0]
 [ 0 24  0  0  0  0  0  0  0]
 [ 0 16  0  0  0  0  0  0  0]
 [ 0  8  0  0  0  0  0  0  0]
 [ 0  8  0  0  0  0  0  0  1]
 [ 0 13  0  0  0  0  0  0  0]
 [ 0 10  0  0  0  0  0  0  2]]
Accuracy:
 [[0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.889 0.    0.    0.    0.    0.    0.    0.111]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.833 0.    0.    0.    0.    0.    0.    0.167]]
---
Silhouette values: [-0.10010746 -0.5937346  -0.24981476 ... -0.05182981 -0.66146004
  0.01914282]
Average of silhouette coef: -0.19773392
---
  1   2   3   4   5   6   7   8   9
1 0.0 9.7 16.7 26.2 35.2 41.0 55.1 60.7 71.7 
2 9.7 0.0 7.0 16.5 25.5 31.3 45.4 51.0 62.0 
3 16.7 7.0 0.0 9.5 18.5 24.3 38.4 44.0 55.0 
4 26.2 16.5 9.5 0.0 9.0 14.8 28.9 34.5 45.6 
5 35.2 25.5 18.5 9.0 0.0 5.8 19.9 25.5 36.5 
6 41.0 31.3 24.3 14.8 5.8 0.0 14.1 19.7 30.7 
7 55.1 45.4 38.4 28.9 19.9 14.1 0.0 5.6 16.6 
8 60.7 51.0 44.0 34.5 25.5 19.7 5.6 0.0 11.0 
9 71.7 62.0 55.0 45.6 36.5 30.7 16.6 11.0 0.0 
correlation [[1.         0.99259145]
 [0.99259145 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [4 3 3 ... 8 2 2] [[ 32.61105    -6.5451856]
 [-57.22122     6.8993964]
 [-14.781535  -42.209354 ]
 ...
 [-12.197451  -49.829826 ]
 [  1.427156   65.23878  ]
 [  5.6838384  19.760046 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0 45  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 61  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 45  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 12.4  13.9  1.2  8.8  3.4 -0.0 -3.2 -3.5  14.7 -0.6  16.5  15.8  11.6 -1.7  14.9 -22.2  6.1  7.8  14.4 -3.7 
 14.8  16.6  1.4  10.6  4.1  0.0 -3.8 -4.2  17.6 -0.8  19.8  19.0  14.0 -2.1  17.9 -26.7  7.3  9.4  17.2 -4.4 
 16.6  18.6  1.6  11.8  4.6 -0.0 -4.3 -4.7  19.7 -0.9  22.2  21.3  15.6 -2.3  20.1 -29.9  8.2  10.5  19.3 -5.0 
 19.0  21.3  1.8  13.5  5.2 -0.1 -4.9 -5.3  22.6 -1.1  25.4  24.3  17.9 -2.6  23.0 -34.2  9.4  12.0  22.1 -5.7 
 21.2  23.9  1.9  15.3  5.8 -0.0 -5.4 -6.0  25.3 -1.1  28.4  27.2  20.1 -2.9  25.8 -38.4  10.6  13.5  24.7 -6.3 
 22.7  25.5  2.2  16.2  6.3 -0.1 -5.9 -6.4  27.0 -1.3  30.5  29.2  21.4 -3.1  27.5 -41.0  11.2  14.4  26.5 -6.8 
 26.3  29.5  2.4  18.9  7.2  0.0 -6.7 -7.4  31.3 -1.4  35.2  33.7  24.8 -3.6  31.9 -47.4  13.0  16.6  30.6 -7.8 
 27.7  31.1  2.6  19.8  7.6 -0.0 -7.1 -7.8  32.9 -1.5  37.1  35.6  26.2 -3.8  33.6 -50.0  13.7  17.5  32.3 -8.3 
 30.5  34.3  2.8  21.8  8.4 -0.0 -7.8 -8.6  36.3 -1.6  40.9  39.2  28.8 -4.1  36.9 -55.0  15.1  19.3  35.5 -9.1 
Minimum distance of calculation. 
true answer: 2 
indices: 2 
distance: 1.3169483
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 12.4  13.9  1.2  8.8  3.4 -0.0 -3.2 -3.5  14.7 -0.6  16.5  15.8  11.6 -1.7  14.9 -22.2  6.1  7.8  14.4 -3.7 
 14.8  16.6  1.4  10.6  4.1  0.0 -3.8 -4.2  17.6 -0.8  19.8  19.0  14.0 -2.1  17.9 -26.7  7.3  9.4  17.2 -4.4 
 16.6  18.6  1.6  11.8  4.6 -0.0 -4.3 -4.7  19.7 -0.9  22.2  21.3  15.6 -2.3  20.1 -29.9  8.2  10.5  19.3 -5.0 
 19.0  21.3  1.8  13.5  5.2 -0.1 -4.9 -5.3  22.6 -1.1  25.4  24.3  17.9 -2.6  23.0 -34.2  9.4  12.0  22.1 -5.7 
 21.2  23.9  1.9  15.3  5.8 -0.0 -5.4 -6.0  25.3 -1.1  28.4  27.2  20.1 -2.9  25.8 -38.4  10.6  13.5  24.7 -6.3 
 22.7  25.5  2.2  16.2  6.3 -0.1 -5.9 -6.4  27.0 -1.3  30.5  29.2  21.4 -3.1  27.5 -41.0  11.2  14.4  26.5 -6.8 
 26.3  29.5  2.4  18.9  7.2  0.0 -6.7 -7.4  31.3 -1.4  35.2  33.7  24.8 -3.6  31.9 -47.4  13.0  16.6  30.6 -7.8 
 27.7  31.1  2.6  19.8  7.6 -0.0 -7.1 -7.8  32.9 -1.5  37.1  35.6  26.2 -3.8  33.6 -50.0  13.7  17.5  32.3 -8.3 
 30.5  34.3  2.8  21.8  8.4 -0.0 -7.8 -8.6  36.3 -1.6  40.9  39.2  28.8 -4.1  36.9 -55.0  15.1  19.3  35.5 -9.1 
Minimum distance of calculation. 
true answer: 5 
indices: 5 
distance: 0.7186734
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 12.4  13.9  1.2  8.8  3.4 -0.0 -3.2 -3.5  14.7 -0.6  16.5  15.8  11.6 -1.7  14.9 -22.2  6.1  7.8  14.4 -3.7 
 14.8  16.6  1.4  10.6  4.1  0.0 -3.8 -4.2  17.6 -0.8  19.8  19.0  14.0 -2.1  17.9 -26.7  7.3  9.4  17.2 -4.4 
 16.6  18.6  1.6  11.8  4.6 -0.0 -4.3 -4.7  19.7 -0.9  22.2  21.3  15.6 -2.3  20.1 -29.9  8.2  10.5  19.3 -5.0 
 19.0  21.3  1.8  13.5  5.2 -0.1 -4.9 -5.3  22.6 -1.1  25.4  24.3  17.9 -2.6  23.0 -34.2  9.4  12.0  22.1 -5.7 
 21.2  23.9  1.9  15.3  5.8 -0.0 -5.4 -6.0  25.3 -1.1  28.4  27.2  20.1 -2.9  25.8 -38.4  10.6  13.5  24.7 -6.3 
 22.7  25.5  2.2  16.2  6.3 -0.1 -5.9 -6.4  27.0 -1.3  30.5  29.2  21.4 -3.1  27.5 -41.0  11.2  14.4  26.5 -6.8 
 26.3  29.5  2.4  18.9  7.2  0.0 -6.7 -7.4  31.3 -1.4  35.2  33.7  24.8 -3.6  31.9 -47.4  13.0  16.6  30.6 -7.8 
 27.7  31.1  2.6  19.8  7.6 -0.0 -7.1 -7.8  32.9 -1.5  37.1  35.6  26.2 -3.8  33.6 -50.0  13.7  17.5  32.3 -8.3 
 30.5  34.3  2.8  21.8  8.4 -0.0 -7.8 -8.6  36.3 -1.6  40.9  39.2  28.8 -4.1  36.9 -55.0  15.1  19.3  35.5 -9.1 
Minimum distance of calculation. 
true answer: 8 
indices: 8 
distance: 4.0992556
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 12.4  13.9  1.2  8.8  3.4 -0.0 -3.2 -3.5  14.7 -0.6  16.5  15.8  11.6 -1.7  14.9 -22.2  6.1  7.8  14.4 -3.7 
 14.8  16.6  1.4  10.6  4.1  0.0 -3.8 -4.2  17.6 -0.8  19.8  19.0  14.0 -2.1  17.9 -26.7  7.3  9.4  17.2 -4.4 
 16.6  18.6  1.6  11.8  4.6 -0.0 -4.3 -4.7  19.7 -0.9  22.2  21.3  15.6 -2.3  20.1 -29.9  8.2  10.5  19.3 -5.0 
 19.0  21.3  1.8  13.5  5.2 -0.1 -4.9 -5.3  22.6 -1.1  25.4  24.3  17.9 -2.6  23.0 -34.2  9.4  12.0  22.1 -5.7 
 21.2  23.9  1.9  15.3  5.8 -0.0 -5.4 -6.0  25.3 -1.1  28.4  27.2  20.1 -2.9  25.8 -38.4  10.6  13.5  24.7 -6.3 
 22.7  25.5  2.2  16.2  6.3 -0.1 -5.9 -6.4  27.0 -1.3  30.5  29.2  21.4 -3.1  27.5 -41.0  11.2  14.4  26.5 -6.8 
 26.3  29.5  2.4  18.9  7.2  0.0 -6.7 -7.4  31.3 -1.4  35.2  33.7  24.8 -3.6  31.9 -47.4  13.0  16.6  30.6 -7.8 
 27.7  31.1  2.6  19.8  7.6 -0.0 -7.1 -7.8  32.9 -1.5  37.1  35.6  26.2 -3.8  33.6 -50.0  13.7  17.5  32.3 -8.3 
 30.5  34.3  2.8  21.8  8.4 -0.0 -7.8 -8.6  36.3 -1.6  40.9  39.2  28.8 -4.1  36.9 -55.0  15.1  19.3  35.5 -9.1 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 1.2560283
results (all): {'reconst_1x1_avg': 0.1015625, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.140625, 'cross_0x1_all': nan, 'cluster_avg': -0.19773392, 'cluster_all': array([-0.10010746, -0.5937346 , -0.24981476, ..., -0.05182981,
       -0.66146004,  0.01914282], dtype=float32), 'magnitude_avg': 0.9925914502000018, 'magnitude_all': 0.9925914502000018, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.1015625, 'cross_0x1_avg': 0.140625, 'cluster_avg': -0.19773392, 'magnitude_avg': 0.9925914502000018, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  # normalized



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([1, 1, 2, 6, 2, 7, 4, 1, 7, 4, 3, 1, 9, 6, 6, 5, 8, 7, 6, 9, 4, 5, 7, 8,
        4, 8, 1, 2, 7, 9, 5, 7, 8, 6, 5, 5, 9, 7, 6, 2, 3, 3, 8, 1, 6, 1, 7, 9,
        4, 5, 9, 8, 8, 9, 5, 3, 6, 5, 8, 6, 6, 6, 2, 1, 5, 4, 4, 7, 5, 2, 4, 5,
        2, 1, 1, 8, 6, 1, 7, 6, 9, 2, 9, 3, 9, 4, 4, 5, 3, 3, 1, 9, 3, 6, 3, 5,
        6, 6, 2, 8, 1, 2, 2, 7, 1, 2, 2, 6, 9, 4, 8, 3, 9, 5, 5, 3, 9, 7, 2, 4,
        3, 7, 3, 1, 9, 6, 7, 5]), ['g1s', 'w1t', 'w2s', 'r6s', 'r2s', 'b7s', 'r4s', 'b1j', 'w7j', 'w4s', 'b3t', 'w1t', 'g9j', 'g6s', 'w6t', 'w5s', 'g8j', 'b7t', 'b6s', 'g9j', 'w4j', 'w5t', 'r7t', 'w8s', 'w4j', 'b8s', 'b1t', 'r2t', 'b7j', 'b9s', 'r5t', 'b7t', 'r8s', 'w6s', 'g5s', 'g5t', 'b9t', 'r7t', 'w6t', 'w2j', 'b3t', 'r3t', 'g8j', 'r1j', 'w6j', 'b1t', 'b7s', 'b9s', 'w4t', 'b5s', 'r9t', 'g8s', 'g8j', 'g9s', 'w5t', 'r3t', 'w6s', 'g5t', 'r8j', 'r6t', 'b6s', 'w6t', 'g2s', 'r1s', 'w5j', 'g4s', 'r4s', 'r7t', 'w5t', 'r2s', 'w4j', 'b5t', 'w2s', 'r1t', 'r1s', 'g8s', 'w6s', 'g1j', 'w7j', 'w6j', 'g9s', 'g2t', 'w9t', 'w3t', 'b9j', 'b4j', 'g4t', 'r5s', 'w3s', 'g3t', 'g1s', 'r9j', 'b3t', 'w6t', 'r3t', 'b5s', 'b6j', 'b6j', 'g2t', 'r8s', 'w1t', 'b2j', 'g2j', 'w7j', 'r1s', 'g2s', 'r2t', 'r6j', 'g9j', 'w4j', 'w8j', 'r3j', 'r9s', 'w5j', 'g5s', 'b3j', 'r9t', 'r7j', 'g2s', 'g4t', 'w3s', 'w7s', 'w3t', 'r1j', 'g9s', 'g6s', 'b7s', 'w5t']] 
label[0]: tensor([1, 1, 2, 6, 2, 7, 4, 1, 7, 4, 3, 1, 9, 6, 6, 5, 8, 7, 6, 9, 4, 5, 7, 8,
        4, 8, 1, 2, 7, 9, 5, 7, 8, 6, 5, 5, 9, 7, 6, 2, 3, 3, 8, 1, 6, 1, 7, 9,
        4, 5, 9, 8, 8, 9, 5, 3, 6, 5, 8, 6, 6, 6, 2, 1, 5, 4, 4, 7, 5, 2, 4, 5,
        2, 1, 1, 8, 6, 1, 7, 6, 9, 2, 9, 3, 9, 4, 4, 5, 3, 3, 1, 9, 3, 6, 3, 5,
        6, 6, 2, 8, 1, 2, 2, 7, 1, 2, 2, 6, 9, 4, 8, 3, 9, 5, 5, 3, 9, 7, 2, 4,
        3, 7, 3, 1, 9, 6, 7, 5]) 
label[1]: ['g1s', 'w1t', 'w2s', 'r6s', 'r2s', 'b7s', 'r4s', 'b1j', 'w7j', 'w4s', 'b3t', 'w1t', 'g9j', 'g6s', 'w6t', 'w5s', 'g8j', 'b7t', 'b6s', 'g9j', 'w4j', 'w5t', 'r7t', 'w8s', 'w4j', 'b8s', 'b1t', 'r2t', 'b7j', 'b9s', 'r5t', 'b7t', 'r8s', 'w6s', 'g5s', 'g5t', 'b9t', 'r7t', 'w6t', 'w2j', 'b3t', 'r3t', 'g8j', 'r1j', 'w6j', 'b1t', 'b7s', 'b9s', 'w4t', 'b5s', 'r9t', 'g8s', 'g8j', 'g9s', 'w5t', 'r3t', 'w6s', 'g5t', 'r8j', 'r6t', 'b6s', 'w6t', 'g2s', 'r1s', 'w5j', 'g4s', 'r4s', 'r7t', 'w5t', 'r2s', 'w4j', 'b5t', 'w2s', 'r1t', 'r1s', 'g8s', 'w6s', 'g1j', 'w7j', 'w6j', 'g9s', 'g2t', 'w9t', 'w3t', 'b9j', 'b4j', 'g4t', 'r5s', 'w3s', 'g3t', 'g1s', 'r9j', 'b3t', 'w6t', 'r3t', 'b5s', 'b6j', 'b6j', 'g2t', 'r8s', 'w1t', 'b2j', 'g2j', 'w7j', 'r1s', 'g2s', 'r2t', 'r6j', 'g9j', 'w4j', 'w8j', 'r3j', 'r9s', 'w5j', 'g5s', 'b3j', 'r9t', 'r7j', 'g2s', 'g4t', 'w3s', 'w7s', 'w3t', 'r1j', 'g9s', 'g6s', 'b7s', 'w5t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.6414817  -0.52096015 -0.25039896 ... -0.63203406  0.0649749
 -0.67133987]
Average of silhouette coef: -0.072004154
---
  0   1   2
0 0.0 35.8 19.0 
1 35.8 0.0 16.9 
2 19.0 16.9 0.0 
correlation [[ 1.         -0.40929032]
 [-0.40929032  1.        ]]
---
[[], [], []] [1 2 1 ... 1 2 1] [[ 51.832283   47.88977  ]
 [ 28.468346   33.94132  ]
 [  3.1480904 -39.443687 ]
 ...
 [-45.845524    2.4722466]
 [ 25.819847   -9.766068 ]
 [-19.401945   19.209812 ]]
saved ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': -0.072004154, 'cluster_all': array([-0.6414817 , -0.52096015, -0.25039896, ..., -0.63203406,
        0.0649749 , -0.67133987], dtype=float32), 'magnitude_avg': 0.4092903209035173, 'magnitude_all': -0.4092903209035173, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': -0.072004154, 'magnitude_avg': 0.4092903209035173, 'tsne-2d_avg': nan}
Arguments (initial):
{'device': 'cuda',
 'experiment': 'test-pbs-2',
 'no_cuda': False,
 'pretrained_path': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': '2023-12-04T12:37:09.612504',
 'run_type': 'synthesize'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-2', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4', 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 2023-12-04T12:37:09.612504
Run Directory:
 ./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/2023-12-04T12:37:09.612504
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'test-pbs-2',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
run_ids_dict was automatically constructed.
run_ids_dict for synthesized: {'VAE_CMNIST': ['vae_cmnist_seed_0', 'vae_cmnist_seed_1', 'vae_cmnist_seed_2', 'vae_cmnist_seed_3', 'vae_cmnist_seed_4'], 'VAE_OSCN': ['vae_oscn_seed_4', 'vae_oscn_seed_3', 'vae_oscn_seed_0', 'vae_oscn_seed_2', 'vae_oscn_seed_1'], 'MMVAE_CMNIST_OSCN': ['mmvae_cmnist_oscn_seed_0', 'mmvae_cmnist_oscn_seed_3', 'mmvae_cmnist_oscn_seed_2', 'mmvae_cmnist_oscn_seed_1', 'mmvae_cmnist_oscn_seed_4']}
{'VAE_CMNIST': ['vae_cmnist_seed_0', 'vae_cmnist_seed_1', 'vae_cmnist_seed_2', 'vae_cmnist_seed_3', 'vae_cmnist_seed_4'], 'VAE_OSCN': ['vae_oscn_seed_4', 'vae_oscn_seed_3', 'vae_oscn_seed_0', 'vae_oscn_seed_2', 'vae_oscn_seed_1'], 'MMVAE_CMNIST_OSCN': ['mmvae_cmnist_oscn_seed_0', 'mmvae_cmnist_oscn_seed_3', 'mmvae_cmnist_oscn_seed_2', 'mmvae_cmnist_oscn_seed_1', 'mmvae_cmnist_oscn_seed_4']}
./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_4/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_3/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_0/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_2/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/VAE_OSCN/vae_oscn_seed_1/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-2/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_1/analyse_result.csv was loaded.
---
analyzed data (all):
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
10  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...              NaN            NaN
11  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...         0.093750       0.117188
12  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...              NaN            NaN
13  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...         0.078125       0.203125
14  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...              NaN            NaN
15  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...         0.070312       0.093750
16  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...              NaN            NaN
17  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...         0.117188       0.101562
18  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...              NaN            NaN
19  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...         0.101562       0.140625

[20 rows x 16 columns]
---
analyzed data (selected):
(target_modality == @target_modality and model_name in ["MMVAE_CMNIST_OSCN"]) or (not model_name in ["MMVAE_CMNIST_OSCN"])
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
10  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...              NaN            NaN
12  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...              NaN            NaN
14  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...              NaN            NaN
16  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...              NaN            NaN
18  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...              NaN            NaN

[15 rows x 16 columns]
---
Start ANOVA. 
target_modality: 0 
target_column: magnitude_avg
((model_name in [@mmvae_name]) and (target_modality == @target_modality)) or (model_name in [@vae_name])
        id         model_name  magnitude_avg
0   seed_0         VAE_CMNIST       0.065113
1   seed_1         VAE_CMNIST       0.094998
2   seed_2         VAE_CMNIST       0.145899
3   seed_3         VAE_CMNIST       0.006439
4   seed_4         VAE_CMNIST       0.053052
10  seed_0  MMVAE_CMNIST_OSCN       0.285772
12  seed_3  MMVAE_CMNIST_OSCN       0.397869
14  seed_2  MMVAE_CMNIST_OSCN       0.294066
16  seed_1  MMVAE_CMNIST_OSCN       0.229667
18  seed_4  MMVAE_CMNIST_OSCN       0.284688
model_name  MMVAE_CMNIST_OSCN  VAE_CMNIST
id                                       
seed_0               0.285772    0.065113
seed_1               0.229667    0.094998
seed_2               0.294066    0.145899
seed_3               0.397869    0.006439
seed_4               0.284688    0.053052
F-value: 39.54739431207965 
p-value: 0.00023564776275406682
                 sum_sq   df          F    PR(>F)
C(model_name)  0.126914  1.0  39.547394  0.000236
Residual       0.025673  8.0        NaN       NaN
Start ANOVA. 
target_modality: 0 
target_column: cluster_avg
((model_name in [@mmvae_name]) and (target_modality == @target_modality)) or (model_name in [@vae_name])
        id         model_name  cluster_avg
0   seed_0         VAE_CMNIST     0.069906
1   seed_1         VAE_CMNIST     0.068738
2   seed_2         VAE_CMNIST     0.069813
3   seed_3         VAE_CMNIST     0.070397
4   seed_4         VAE_CMNIST     0.071545
10  seed_0  MMVAE_CMNIST_OSCN     0.102921
12  seed_3  MMVAE_CMNIST_OSCN     0.106336
14  seed_2  MMVAE_CMNIST_OSCN     0.053295
16  seed_1  MMVAE_CMNIST_OSCN     0.056820
18  seed_4  MMVAE_CMNIST_OSCN     0.092509
model_name  MMVAE_CMNIST_OSCN  VAE_CMNIST
id                                       
seed_0               0.102921    0.069906
seed_1               0.056820    0.068738
seed_2               0.053295    0.069813
seed_3               0.106336    0.070397
seed_4               0.092509    0.071545
F-value: 1.1622438931800896 
p-value: 0.31243104558939083
                 sum_sq   df         F    PR(>F)
C(model_name)  0.000378  1.0  1.162244  0.312431
Residual       0.002602  8.0       NaN       NaN
Start ANOVA. 
target_modality: 0 
target_column: mathematics_avg
((model_name in [@mmvae_name]) and (target_modality == @target_modality)) or (model_name in [@vae_name])
        id         model_name  mathematics_avg
0   seed_0         VAE_CMNIST         0.122699
1   seed_1         VAE_CMNIST         0.092025
2   seed_2         VAE_CMNIST         0.130879
3   seed_3         VAE_CMNIST         0.118609
4   seed_4         VAE_CMNIST         0.120654
10  seed_0  MMVAE_CMNIST_OSCN         0.116564
12  seed_3  MMVAE_CMNIST_OSCN         0.106339
14  seed_2  MMVAE_CMNIST_OSCN         0.124744
16  seed_1  MMVAE_CMNIST_OSCN         0.092025
18  seed_4  MMVAE_CMNIST_OSCN         0.100204
model_name  MMVAE_CMNIST_OSCN  VAE_CMNIST
id                                       
seed_0               0.116564    0.122699
seed_1               0.092025    0.092025
seed_2               0.124744    0.130879
seed_3               0.106339    0.118609
seed_4               0.100204    0.120654
F-value: 1.053318851383057 
p-value: 0.33477259278280114
                 sum_sq   df         F    PR(>F)
C(model_name)  0.000202  1.0  1.053319  0.334773
Residual       0.001537  8.0       NaN       NaN
---
pairplot
---
analyzed data (all):
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
10  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...              NaN            NaN
11  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...         0.093750       0.117188
12  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...              NaN            NaN
13  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...         0.078125       0.203125
14  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...              NaN            NaN
15  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...         0.070312       0.093750
16  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...              NaN            NaN
17  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...         0.117188       0.101562
18  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...              NaN            NaN
19  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...         0.101562       0.140625

[20 rows x 16 columns]
---
analyzed data (selected):
(target_modality == @target_modality and model_name in ["MMVAE_CMNIST_OSCN"]) or (not model_name in ["MMVAE_CMNIST_OSCN"])
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
11  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...         0.093750       0.117188
13  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...         0.078125       0.203125
15  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...         0.070312       0.093750
17  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...         0.117188       0.101562
19  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...         0.101562       0.140625

[15 rows x 16 columns]
---
Start ANOVA. 
target_modality: 1 
target_column: magnitude_avg
((model_name in [@mmvae_name]) and (target_modality == @target_modality)) or (model_name in [@vae_name])
        id         model_name  magnitude_avg
5   seed_4           VAE_OSCN       0.574245
6   seed_3           VAE_OSCN       0.687026
7   seed_0           VAE_OSCN       0.903745
8   seed_2           VAE_OSCN       0.917863
9   seed_1           VAE_OSCN       0.933010
11  seed_0  MMVAE_CMNIST_OSCN       0.959979
13  seed_3  MMVAE_CMNIST_OSCN       0.988791
15  seed_2  MMVAE_CMNIST_OSCN       0.988319
17  seed_1  MMVAE_CMNIST_OSCN       0.987811
19  seed_4  MMVAE_CMNIST_OSCN       0.992591
model_name  MMVAE_CMNIST_OSCN  VAE_OSCN
id                                     
seed_0               0.959979  0.903745
seed_1               0.987811  0.933010
seed_2               0.988319  0.917863
seed_3               0.988791  0.687026
seed_4               0.992591  0.574245
F-value: 6.0930305353394445 
p-value: 0.03880674495120639
                 sum_sq   df         F    PR(>F)
C(model_name)  0.081289  1.0  6.093031  0.038807
Residual       0.106730  8.0       NaN       NaN
Start ANOVA. 
target_modality: 1 
target_column: cluster_avg
((model_name in [@mmvae_name]) and (target_modality == @target_modality)) or (model_name in [@vae_name])
        id         model_name  cluster_avg
5   seed_4           VAE_OSCN    -0.120066
6   seed_3           VAE_OSCN    -0.102639
7   seed_0           VAE_OSCN    -0.092765
8   seed_2           VAE_OSCN    -0.098705
9   seed_1           VAE_OSCN    -0.090186
11  seed_0  MMVAE_CMNIST_OSCN    -0.155390
13  seed_3  MMVAE_CMNIST_OSCN    -0.237077
15  seed_2  MMVAE_CMNIST_OSCN    -0.023393
17  seed_1  MMVAE_CMNIST_OSCN    -0.015396
19  seed_4  MMVAE_CMNIST_OSCN    -0.197734
model_name  MMVAE_CMNIST_OSCN  VAE_OSCN
id                                     
seed_0              -0.155390 -0.092765
seed_1              -0.015396 -0.090186
seed_2              -0.023393 -0.098705
seed_3              -0.237077 -0.102639
seed_4              -0.197734 -0.120066
F-value: 0.2982281733256543 
p-value: 0.5998891711590758
                 sum_sq   df         F    PR(>F)
C(model_name)  0.001553  1.0  0.298228  0.599889
Residual       0.041665  8.0       NaN       NaN
Start ANOVA. 
target_modality: 1 
target_column: mathematics_avg
((model_name in [@mmvae_name]) and (target_modality == @target_modality)) or (model_name in [@vae_name])
        id         model_name  mathematics_avg
5   seed_4           VAE_OSCN         0.122699
6   seed_3           VAE_OSCN         0.106339
7   seed_0           VAE_OSCN         0.116564
8   seed_2           VAE_OSCN         0.092025
9   seed_1           VAE_OSCN         0.124744
11  seed_0  MMVAE_CMNIST_OSCN         0.106339
13  seed_3  MMVAE_CMNIST_OSCN         0.122699
15  seed_2  MMVAE_CMNIST_OSCN         0.116564
17  seed_1  MMVAE_CMNIST_OSCN         0.092025
19  seed_4  MMVAE_CMNIST_OSCN         0.106339
model_name  MMVAE_CMNIST_OSCN  VAE_OSCN
id                                     
seed_0               0.106339  0.116564
seed_1               0.092025  0.124744
seed_2               0.116564  0.092025
seed_3               0.122699  0.106339
seed_4               0.106339  0.122699
F-value: 0.21259846551770994 
p-value: 0.6570148268400572
                 sum_sq   df         F    PR(>F)
C(model_name)  0.000034  1.0  0.212598  0.657015
Residual       0.001275  8.0       NaN       NaN
---
pairplot
2023/12/06 AM 09:46:50
start time:  1701777116
end time:  1701823610
run time:  46494
