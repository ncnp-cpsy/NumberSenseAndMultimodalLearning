working directory:  /home/taka/MMVAE/NumberSenseAndMultimodalLearning
omp thread num:  2
ncpus:  2
cuda visible devices:  GPU-092c06ec-7745-e924-1b6d-ed9b638cb79a
2023/12/18 PM 01:09:16
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'cross',
 'pretrained_path': './rslt/parameter-search-32/Classifier_CMNIST/classifier-cmnist',
 'print_freq': 100,
 'run_id': 'classifier-cmnist',
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  4
Parameters were loaded for classifier
Run ID:
 classifier-cmnist
Run Directory:
 ./rslt/parameter-search-32/Classifier_CMNIST/classifier-cmnist
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'cross',
 'pretrained_path': './rslt/parameter-search-32/Classifier_CMNIST/classifier-cmnist',
 'print_freq': 100,
 'run_id': 'classifier-cmnist',
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'cross',
 'pretrained_path': './rslt/parameter-search-32/Classifier_CMNIST/classifier-cmnist',
 'print_freq': 100,
 'run_id': 'classifier-cmnist',
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12                    [-1, 9]             189
================================================================
Total params: 1,438,629
Trainable params: 1,438,629
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.02
Params size (MB): 5.49
Estimated Total Size (MB): 5.52
----------------------------------------------------------------
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross
iteration 0000: loss:  0.017
iteration 0100: loss:  0.004
iteration 0200: loss:  0.003
iteration 0300: loss:  0.002
iteration 0400: loss:  0.001
iteration 0500: loss:  0.001
iteration 0600: loss:  0.001
iteration 0700: loss:  0.002
iteration 0800: loss:  0.001
iteration 0900: loss:  0.000
====> Epoch: 001 Train loss: 0.0021  took : 2.992185592651367
====> Test loss: -0.0011, Test accuracy: 0.9620
iteration 0000: loss:  0.001
iteration 0100: loss:  0.000
iteration 0200: loss:  0.001
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.001
iteration 0600: loss:  0.001
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.001
====> Epoch: 002 Train loss: 0.0006  took : 3.0115809440612793
====> Test loss: -0.0010, Test accuracy: 0.9635
iteration 0000: loss:  0.000
iteration 0100: loss:  0.001
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.001
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 003 Train loss: 0.0004  took : 3.016930341720581
====> Test loss: -0.0008, Test accuracy: 0.9715
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 004 Train loss: 0.0003  took : 2.9776008129119873
====> Test loss: -0.0009, Test accuracy: 0.9712
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 005 Train loss: 0.0002  took : 3.0082640647888184
====> Test loss: -0.0013, Test accuracy: 0.9660
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 006 Train loss: 0.0001  took : 2.99821138381958
====> Test loss: -0.0009, Test accuracy: 0.9715
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 007 Train loss: 0.0001  took : 2.9443020820617676
====> Test loss: -0.0013, Test accuracy: 0.9688
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.001
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 008 Train loss: 0.0001  took : 2.9703683853149414
====> Test loss: -0.0013, Test accuracy: 0.9698
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 009 Train loss: 0.0001  took : 2.9348580837249756
====> Test loss: -0.0012, Test accuracy: 0.9735
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 010 Train loss: 0.0001  took : 3.0109570026397705
====> Test loss: -0.0015, Test accuracy: 0.9695
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.001
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 011 Train loss: 0.0001  took : 2.9489965438842773
====> Test loss: -0.0016, Test accuracy: 0.9702
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 012 Train loss: 0.0000  took : 2.9873220920562744
====> Test loss: -0.0015, Test accuracy: 0.9715
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 013 Train loss: 0.0000  took : 3.0843586921691895
====> Test loss: -0.0016, Test accuracy: 0.9695
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 014 Train loss: 0.0000  took : 3.0177829265594482
====> Test loss: -0.0015, Test accuracy: 0.9755
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 015 Train loss: 0.0000  took : 3.0091283321380615
====> Test loss: -0.0014, Test accuracy: 0.9768
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 016 Train loss: 0.0000  took : 2.9558706283569336
====> Test loss: -0.0016, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 017 Train loss: 0.0000  took : 2.9705164432525635
====> Test loss: -0.0014, Test accuracy: 0.9765
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 018 Train loss: 0.0000  took : 2.943554639816284
====> Test loss: -0.0017, Test accuracy: 0.9745
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.001
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 019 Train loss: 0.0000  took : 2.986961603164673
====> Test loss: -0.0016, Test accuracy: 0.9718
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 020 Train loss: 0.0000  took : 2.9810879230499268
====> Test loss: -0.0014, Test accuracy: 0.9755
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 021 Train loss: 0.0000  took : 2.9666006565093994
====> Test loss: -0.0013, Test accuracy: 0.9778
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 022 Train loss: 0.0000  took : 2.940059185028076
====> Test loss: -0.0014, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 023 Train loss: 0.0000  took : 2.917187452316284
====> Test loss: -0.0015, Test accuracy: 0.9775
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 024 Train loss: 0.0000  took : 2.931140899658203
====> Test loss: -0.0016, Test accuracy: 0.9772
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 025 Train loss: 0.0000  took : 3.013406753540039
====> Test loss: -0.0015, Test accuracy: 0.9772
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 026 Train loss: 0.0000  took : 2.9278552532196045
====> Test loss: -0.0016, Test accuracy: 0.9772
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 027 Train loss: 0.0000  took : 2.9992055892944336
====> Test loss: -0.0016, Test accuracy: 0.9772
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 028 Train loss: 0.0000  took : 2.9807207584381104
====> Test loss: -0.0017, Test accuracy: 0.9772
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 029 Train loss: 0.0000  took : 3.0276107788085938
====> Test loss: -0.0017, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 030 Train loss: 0.0000  took : 3.022106885910034
====> Test loss: -0.0019, Test accuracy: 0.9772
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 031 Train loss: 0.0000  took : 3.0223097801208496
====> Test loss: -0.0018, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 032 Train loss: 0.0000  took : 2.943880319595337
====> Test loss: -0.0021, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 033 Train loss: 0.0000  took : 2.9415595531463623
====> Test loss: -0.0019, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 034 Train loss: 0.0000  took : 2.9302237033843994
====> Test loss: -0.0021, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 035 Train loss: 0.0000  took : 2.98795485496521
====> Test loss: -0.0020, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 036 Train loss: 0.0000  took : 2.94522762298584
====> Test loss: -0.0020, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 037 Train loss: 0.0000  took : 2.929978370666504
====> Test loss: -0.0021, Test accuracy: 0.9768
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 038 Train loss: 0.0000  took : 2.9987082481384277
====> Test loss: -0.0021, Test accuracy: 0.9768
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 039 Train loss: 0.0000  took : 2.9677648544311523
====> Test loss: -0.0021, Test accuracy: 0.9768
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 040 Train loss: 0.0000  took : 3.0001721382141113
====> Test loss: -0.0022, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 041 Train loss: 0.0000  took : 2.931551933288574
====> Test loss: -0.0023, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 042 Train loss: 0.0000  took : 3.028346538543701
====> Test loss: -0.0022, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 043 Train loss: 0.0000  took : 3.0573925971984863
====> Test loss: -0.0022, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 044 Train loss: 0.0000  took : 2.9427106380462646
====> Test loss: -0.0022, Test accuracy: 0.9768
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 045 Train loss: 0.0000  took : 2.967116117477417
====> Test loss: -0.0022, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 046 Train loss: 0.0000  took : 3.0568692684173584
====> Test loss: -0.0022, Test accuracy: 0.9768
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 047 Train loss: 0.0000  took : 2.9275588989257812
====> Test loss: -0.0023, Test accuracy: 0.9768
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 048 Train loss: 0.0000  took : 2.965400457382202
====> Test loss: -0.0022, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 049 Train loss: 0.0000  took : 3.0037221908569336
====> Test loss: -0.0023, Test accuracy: 0.9770
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 050 Train loss: 0.0000  took : 3.0051004886627197
====> Test loss: -0.0024, Test accuracy: 0.9768
====> [MM-VAE] Time: 172.481s or 00:02:52
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'cross',
 'pretrained_path': './rslt/parameter-search-32/Classifier_OSCN/classifier-oscn',
 'print_freq': 100,
 'run_id': 'classifier-oscn',
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  4
Parameters were loaded for classifier
Run ID:
 classifier-oscn
Run Directory:
 ./rslt/parameter-search-32/Classifier_OSCN/classifier-oscn
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'cross',
 'pretrained_path': './rslt/parameter-search-32/Classifier_OSCN/classifier-oscn',
 'print_freq': 100,
 'run_id': 'classifier-oscn',
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'cross',
 'pretrained_path': './rslt/parameter-search-32/Classifier_OSCN/classifier-oscn',
 'print_freq': 100,
 'run_id': 'classifier-oscn',
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12                    [-1, 9]             189
================================================================
Total params: 1,726,629
Trainable params: 1,726,629
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.02
Params size (MB): 6.59
Estimated Total Size (MB): 6.62
----------------------------------------------------------------
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross
iteration 0000: loss:  0.017
iteration 0100: loss:  0.002
iteration 0200: loss:  0.001
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 001 Train loss: 0.0012  took : 2.926997423171997
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 002 Train loss: 0.0000  took : 2.973456382751465
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 003 Train loss: 0.0000  took : 2.8908798694610596
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 004 Train loss: 0.0000  took : 2.967287302017212
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 005 Train loss: 0.0000  took : 2.894516706466675
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 006 Train loss: 0.0000  took : 2.898801326751709
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 007 Train loss: 0.0000  took : 2.927534818649292
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 008 Train loss: 0.0000  took : 2.905517578125
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 009 Train loss: 0.0000  took : 2.950587272644043
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 010 Train loss: 0.0000  took : 2.915879011154175
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 011 Train loss: 0.0000  took : 2.9804701805114746
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 012 Train loss: 0.0000  took : 2.9515228271484375
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 013 Train loss: 0.0000  took : 2.958245038986206
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 014 Train loss: 0.0000  took : 2.9673564434051514
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 015 Train loss: 0.0000  took : 2.9054136276245117
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 016 Train loss: 0.0000  took : 2.930241346359253
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 017 Train loss: 0.0000  took : 2.901369333267212
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 018 Train loss: 0.0000  took : 2.9371259212493896
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 019 Train loss: 0.0000  took : 2.9956023693084717
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 020 Train loss: 0.0000  took : 2.9796924591064453
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 021 Train loss: 0.0000  took : 2.927483320236206
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 022 Train loss: 0.0000  took : 2.9085395336151123
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 023 Train loss: 0.0000  took : 2.9328410625457764
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 024 Train loss: 0.0000  took : 2.940706253051758
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 025 Train loss: 0.0000  took : 2.9674670696258545
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 026 Train loss: 0.0000  took : 2.9738376140594482
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 027 Train loss: 0.0000  took : 2.9617085456848145
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 028 Train loss: 0.0000  took : 2.8862273693084717
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 029 Train loss: 0.0000  took : 2.9030444622039795
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 030 Train loss: 0.0000  took : 2.911405563354492
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 031 Train loss: 0.0000  took : 2.9925878047943115
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 032 Train loss: 0.0000  took : 2.9026854038238525
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 033 Train loss: 0.0000  took : 2.920229196548462
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 034 Train loss: 0.0000  took : 2.916022777557373
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 035 Train loss: 0.0000  took : 2.911752223968506
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 036 Train loss: 0.0000  took : 2.934504985809326
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 037 Train loss: 0.0000  took : 2.8941586017608643
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 038 Train loss: 0.0000  took : 2.9446327686309814
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 039 Train loss: 0.0000  took : 2.9117445945739746
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 040 Train loss: 0.0000  took : 2.868053674697876
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 041 Train loss: 0.0000  took : 2.9277234077453613
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 042 Train loss: 0.0000  took : 2.9460549354553223
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 043 Train loss: 0.0000  took : 2.962466239929199
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 044 Train loss: 0.0000  took : 2.9003472328186035
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 045 Train loss: 0.0000  took : 2.9438838958740234
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 046 Train loss: 0.0000  took : 2.9453201293945312
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 047 Train loss: 0.0000  took : 2.977327585220337
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 048 Train loss: 0.0000  took : 2.909473180770874
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 049 Train loss: 0.0000  took : 2.919520854949951
====> Test loss: -0.0000, Test accuracy: 1.0000
iteration 0000: loss:  0.000
iteration 0100: loss:  0.000
iteration 0200: loss:  0.000
iteration 0300: loss:  0.000
iteration 0400: loss:  0.000
iteration 0500: loss:  0.000
iteration 0600: loss:  0.000
iteration 0700: loss:  0.000
iteration 0800: loss:  0.000
iteration 0900: loss:  0.000
====> Epoch: 050 Train loss: 0.0000  took : 2.9241156578063965
====> Test loss: -0.0000, Test accuracy: 1.0000
====> [MM-VAE] Time: 171.687s or 00:02:51
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  0
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_0
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1987.625
iteration 0100: loss: 1573.352
iteration 0200: loss: 1569.001
iteration 0300: loss: 1567.013
iteration 0400: loss: 1550.360
iteration 0500: loss: 1549.505
iteration 0600: loss: 1540.167
iteration 0700: loss: 1543.619
iteration 0800: loss: 1537.186
iteration 0900: loss: 1537.998
====> Epoch: 001 Train loss: 1555.5915  took : 8.607061624526978
====> Test loss: 1538.4857
iteration 0000: loss: 1536.776
iteration 0100: loss: 1540.416
iteration 0200: loss: 1532.755
iteration 0300: loss: 1531.300
iteration 0400: loss: 1532.412
iteration 0500: loss: 1529.591
iteration 0600: loss: 1529.157
iteration 0700: loss: 1531.731
iteration 0800: loss: 1530.167
iteration 0900: loss: 1526.870
====> Epoch: 002 Train loss: 1532.0930  took : 8.600801467895508
====> Test loss: 1530.4561
iteration 0000: loss: 1526.345
iteration 0100: loss: 1527.110
iteration 0200: loss: 1525.257
iteration 0300: loss: 1526.214
iteration 0400: loss: 1525.274
iteration 0500: loss: 1528.295
iteration 0600: loss: 1525.076
iteration 0700: loss: 1527.564
iteration 0800: loss: 1527.819
iteration 0900: loss: 1524.706
====> Epoch: 003 Train loss: 1526.8236  took : 8.574337244033813
====> Test loss: 1527.5385
iteration 0000: loss: 1524.546
iteration 0100: loss: 1523.262
iteration 0200: loss: 1522.996
iteration 0300: loss: 1525.646
iteration 0400: loss: 1524.390
iteration 0500: loss: 1525.620
iteration 0600: loss: 1522.108
iteration 0700: loss: 1524.034
iteration 0800: loss: 1521.161
iteration 0900: loss: 1524.111
====> Epoch: 004 Train loss: 1523.9078  took : 8.616637468338013
====> Test loss: 1525.1298
iteration 0000: loss: 1522.813
iteration 0100: loss: 1521.967
iteration 0200: loss: 1523.189
iteration 0300: loss: 1519.803
iteration 0400: loss: 1521.024
iteration 0500: loss: 1520.842
iteration 0600: loss: 1522.935
iteration 0700: loss: 1523.247
iteration 0800: loss: 1520.752
iteration 0900: loss: 1523.814
====> Epoch: 005 Train loss: 1522.1990  took : 8.60125994682312
====> Test loss: 1524.0102
iteration 0000: loss: 1523.980
iteration 0100: loss: 1519.101
iteration 0200: loss: 1523.750
iteration 0300: loss: 1518.272
iteration 0400: loss: 1522.957
iteration 0500: loss: 1520.073
iteration 0600: loss: 1524.613
iteration 0700: loss: 1520.973
iteration 0800: loss: 1522.639
iteration 0900: loss: 1520.241
====> Epoch: 006 Train loss: 1520.9812  took : 8.559184074401855
====> Test loss: 1522.9736
iteration 0000: loss: 1518.691
iteration 0100: loss: 1516.040
iteration 0200: loss: 1519.826
iteration 0300: loss: 1518.451
iteration 0400: loss: 1517.847
iteration 0500: loss: 1521.520
iteration 0600: loss: 1520.323
iteration 0700: loss: 1520.067
iteration 0800: loss: 1520.363
iteration 0900: loss: 1521.414
====> Epoch: 007 Train loss: 1520.0465  took : 8.625754356384277
====> Test loss: 1522.3599
iteration 0000: loss: 1518.712
iteration 0100: loss: 1517.709
iteration 0200: loss: 1518.650
iteration 0300: loss: 1518.580
iteration 0400: loss: 1518.470
iteration 0500: loss: 1518.818
iteration 0600: loss: 1521.399
iteration 0700: loss: 1517.530
iteration 0800: loss: 1519.260
iteration 0900: loss: 1519.632
====> Epoch: 008 Train loss: 1519.3263  took : 8.574482917785645
====> Test loss: 1521.6869
iteration 0000: loss: 1520.106
iteration 0100: loss: 1516.732
iteration 0200: loss: 1517.616
iteration 0300: loss: 1517.642
iteration 0400: loss: 1517.903
iteration 0500: loss: 1514.445
iteration 0600: loss: 1517.921
iteration 0700: loss: 1522.042
iteration 0800: loss: 1518.248
iteration 0900: loss: 1518.567
====> Epoch: 009 Train loss: 1518.7433  took : 8.577418804168701
====> Test loss: 1521.0710
iteration 0000: loss: 1517.265
iteration 0100: loss: 1518.119
iteration 0200: loss: 1517.370
iteration 0300: loss: 1516.292
iteration 0400: loss: 1517.968
iteration 0500: loss: 1518.370
iteration 0600: loss: 1519.262
iteration 0700: loss: 1519.163
iteration 0800: loss: 1519.109
iteration 0900: loss: 1520.424
====> Epoch: 010 Train loss: 1518.2761  took : 8.45350980758667
====> Test loss: 1520.7742
iteration 0000: loss: 1515.649
iteration 0100: loss: 1516.978
iteration 0200: loss: 1524.522
iteration 0300: loss: 1516.977
iteration 0400: loss: 1516.475
iteration 0500: loss: 1517.156
iteration 0600: loss: 1517.864
iteration 0700: loss: 1519.454
iteration 0800: loss: 1516.060
iteration 0900: loss: 1519.665
====> Epoch: 011 Train loss: 1517.9078  took : 8.55970287322998
====> Test loss: 1520.3807
iteration 0000: loss: 1518.886
iteration 0100: loss: 1521.249
iteration 0200: loss: 1518.540
iteration 0300: loss: 1518.882
iteration 0400: loss: 1519.462
iteration 0500: loss: 1519.086
iteration 0600: loss: 1520.934
iteration 0700: loss: 1517.276
iteration 0800: loss: 1517.699
iteration 0900: loss: 1515.848
====> Epoch: 012 Train loss: 1517.5902  took : 8.62329363822937
====> Test loss: 1520.2172
iteration 0000: loss: 1517.207
iteration 0100: loss: 1517.103
iteration 0200: loss: 1518.374
iteration 0300: loss: 1514.699
iteration 0400: loss: 1519.106
iteration 0500: loss: 1516.130
iteration 0600: loss: 1519.753
iteration 0700: loss: 1518.553
iteration 0800: loss: 1516.301
iteration 0900: loss: 1516.205
====> Epoch: 013 Train loss: 1517.2946  took : 8.51051139831543
====> Test loss: 1519.9066
iteration 0000: loss: 1516.395
iteration 0100: loss: 1517.262
iteration 0200: loss: 1521.314
iteration 0300: loss: 1516.990
iteration 0400: loss: 1514.874
iteration 0500: loss: 1513.654
iteration 0600: loss: 1515.986
iteration 0700: loss: 1517.735
iteration 0800: loss: 1521.294
iteration 0900: loss: 1517.249
====> Epoch: 014 Train loss: 1517.0710  took : 8.61815071105957
====> Test loss: 1519.9072
iteration 0000: loss: 1518.819
iteration 0100: loss: 1518.915
iteration 0200: loss: 1516.642
iteration 0300: loss: 1518.520
iteration 0400: loss: 1518.293
iteration 0500: loss: 1516.157
iteration 0600: loss: 1517.945
iteration 0700: loss: 1516.855
iteration 0800: loss: 1514.632
iteration 0900: loss: 1516.664
====> Epoch: 015 Train loss: 1516.7943  took : 8.553014278411865
====> Test loss: 1519.6870
iteration 0000: loss: 1516.170
iteration 0100: loss: 1514.484
iteration 0200: loss: 1516.260
iteration 0300: loss: 1516.636
iteration 0400: loss: 1516.475
iteration 0500: loss: 1516.708
iteration 0600: loss: 1517.098
iteration 0700: loss: 1515.264
iteration 0800: loss: 1517.588
iteration 0900: loss: 1516.841
====> Epoch: 016 Train loss: 1516.6029  took : 8.62404179573059
====> Test loss: 1519.4896
iteration 0000: loss: 1516.252
iteration 0100: loss: 1516.441
iteration 0200: loss: 1516.722
iteration 0300: loss: 1515.639
iteration 0400: loss: 1516.093
iteration 0500: loss: 1515.117
iteration 0600: loss: 1516.360
iteration 0700: loss: 1516.880
iteration 0800: loss: 1515.220
iteration 0900: loss: 1513.152
====> Epoch: 017 Train loss: 1516.4407  took : 8.611627101898193
====> Test loss: 1520.4970
iteration 0000: loss: 1519.120
iteration 0100: loss: 1516.919
iteration 0200: loss: 1516.170
iteration 0300: loss: 1516.876
iteration 0400: loss: 1517.338
iteration 0500: loss: 1518.477
iteration 0600: loss: 1516.804
iteration 0700: loss: 1518.058
iteration 0800: loss: 1515.041
iteration 0900: loss: 1515.719
====> Epoch: 018 Train loss: 1516.2924  took : 8.47198224067688
====> Test loss: 1519.2455
iteration 0000: loss: 1515.004
iteration 0100: loss: 1516.109
iteration 0200: loss: 1517.859
iteration 0300: loss: 1513.333
iteration 0400: loss: 1517.168
iteration 0500: loss: 1519.310
iteration 0600: loss: 1517.186
iteration 0700: loss: 1517.060
iteration 0800: loss: 1516.472
iteration 0900: loss: 1514.590
====> Epoch: 019 Train loss: 1516.0963  took : 8.46924352645874
====> Test loss: 1519.1642
iteration 0000: loss: 1517.369
iteration 0100: loss: 1516.681
iteration 0200: loss: 1519.901
iteration 0300: loss: 1515.315
iteration 0400: loss: 1518.274
iteration 0500: loss: 1518.203
iteration 0600: loss: 1516.842
iteration 0700: loss: 1516.183
iteration 0800: loss: 1516.712
iteration 0900: loss: 1516.703
====> Epoch: 020 Train loss: 1515.9017  took : 8.661972761154175
====> Test loss: 1519.0584
iteration 0000: loss: 1516.426
iteration 0100: loss: 1515.339
iteration 0200: loss: 1515.629
iteration 0300: loss: 1516.165
iteration 0400: loss: 1514.970
iteration 0500: loss: 1515.675
iteration 0600: loss: 1516.620
iteration 0700: loss: 1515.659
iteration 0800: loss: 1516.539
iteration 0900: loss: 1516.658
====> Epoch: 021 Train loss: 1515.7793  took : 8.605826616287231
====> Test loss: 1518.8816
iteration 0000: loss: 1515.380
iteration 0100: loss: 1514.313
iteration 0200: loss: 1516.980
iteration 0300: loss: 1514.675
iteration 0400: loss: 1512.446
iteration 0500: loss: 1516.112
iteration 0600: loss: 1515.805
iteration 0700: loss: 1515.071
iteration 0800: loss: 1515.844
iteration 0900: loss: 1513.985
====> Epoch: 022 Train loss: 1515.5976  took : 8.518954992294312
====> Test loss: 1518.7583
iteration 0000: loss: 1513.731
iteration 0100: loss: 1520.678
iteration 0200: loss: 1515.483
iteration 0300: loss: 1515.291
iteration 0400: loss: 1517.406
iteration 0500: loss: 1516.131
iteration 0600: loss: 1513.687
iteration 0700: loss: 1517.816
iteration 0800: loss: 1514.866
iteration 0900: loss: 1516.972
====> Epoch: 023 Train loss: 1515.6046  took : 8.636238813400269
====> Test loss: 1518.9447
iteration 0000: loss: 1515.210
iteration 0100: loss: 1515.496
iteration 0200: loss: 1513.771
iteration 0300: loss: 1514.135
iteration 0400: loss: 1515.562
iteration 0500: loss: 1516.341
iteration 0600: loss: 1518.055
iteration 0700: loss: 1515.028
iteration 0800: loss: 1513.856
iteration 0900: loss: 1516.667
====> Epoch: 024 Train loss: 1515.3866  took : 8.521857261657715
====> Test loss: 1518.6827
iteration 0000: loss: 1514.797
iteration 0100: loss: 1514.511
iteration 0200: loss: 1512.860
iteration 0300: loss: 1515.231
iteration 0400: loss: 1514.641
iteration 0500: loss: 1516.366
iteration 0600: loss: 1512.235
iteration 0700: loss: 1515.722
iteration 0800: loss: 1512.554
iteration 0900: loss: 1515.794
====> Epoch: 025 Train loss: 1515.2458  took : 8.612007141113281
====> Test loss: 1518.6193
iteration 0000: loss: 1512.533
iteration 0100: loss: 1515.517
iteration 0200: loss: 1513.577
iteration 0300: loss: 1514.650
iteration 0400: loss: 1516.951
iteration 0500: loss: 1515.047
iteration 0600: loss: 1517.047
iteration 0700: loss: 1516.401
iteration 0800: loss: 1516.505
iteration 0900: loss: 1513.886
====> Epoch: 026 Train loss: 1515.1070  took : 8.602105379104614
====> Test loss: 1518.7852
iteration 0000: loss: 1518.726
iteration 0100: loss: 1518.396
iteration 0200: loss: 1517.621
iteration 0300: loss: 1517.188
iteration 0400: loss: 1516.525
iteration 0500: loss: 1515.831
iteration 0600: loss: 1514.727
iteration 0700: loss: 1514.769
iteration 0800: loss: 1516.275
iteration 0900: loss: 1512.530
====> Epoch: 027 Train loss: 1514.9973  took : 8.60089111328125
====> Test loss: 1518.5156
iteration 0000: loss: 1514.782
iteration 0100: loss: 1516.209
iteration 0200: loss: 1517.549
iteration 0300: loss: 1514.829
iteration 0400: loss: 1513.016
iteration 0500: loss: 1512.499
iteration 0600: loss: 1512.336
iteration 0700: loss: 1514.734
iteration 0800: loss: 1513.937
iteration 0900: loss: 1516.614
====> Epoch: 028 Train loss: 1514.9304  took : 8.581789255142212
====> Test loss: 1518.4593
iteration 0000: loss: 1517.413
iteration 0100: loss: 1515.038
iteration 0200: loss: 1513.944
iteration 0300: loss: 1515.154
iteration 0400: loss: 1511.997
iteration 0500: loss: 1514.717
iteration 0600: loss: 1514.221
iteration 0700: loss: 1516.051
iteration 0800: loss: 1518.266
iteration 0900: loss: 1513.997
====> Epoch: 029 Train loss: 1514.8223  took : 8.65421462059021
====> Test loss: 1518.4213
iteration 0000: loss: 1511.830
iteration 0100: loss: 1516.171
iteration 0200: loss: 1515.530
iteration 0300: loss: 1516.097
iteration 0400: loss: 1512.516
iteration 0500: loss: 1512.810
iteration 0600: loss: 1513.344
iteration 0700: loss: 1515.229
iteration 0800: loss: 1516.752
iteration 0900: loss: 1516.535
====> Epoch: 030 Train loss: 1514.6976  took : 8.641414642333984
====> Test loss: 1518.1609
iteration 0000: loss: 1516.810
iteration 0100: loss: 1515.697
iteration 0200: loss: 1514.926
iteration 0300: loss: 1514.299
iteration 0400: loss: 1514.673
iteration 0500: loss: 1513.325
iteration 0600: loss: 1513.988
iteration 0700: loss: 1516.132
iteration 0800: loss: 1512.865
iteration 0900: loss: 1518.051
====> Epoch: 031 Train loss: 1514.6191  took : 8.575370788574219
====> Test loss: 1518.1894
iteration 0000: loss: 1514.347
iteration 0100: loss: 1513.390
iteration 0200: loss: 1513.975
iteration 0300: loss: 1515.525
iteration 0400: loss: 1518.057
iteration 0500: loss: 1514.925
iteration 0600: loss: 1515.791
iteration 0700: loss: 1514.232
iteration 0800: loss: 1514.331
iteration 0900: loss: 1513.603
====> Epoch: 032 Train loss: 1514.5456  took : 8.636424779891968
====> Test loss: 1518.2474
iteration 0000: loss: 1513.972
iteration 0100: loss: 1513.447
iteration 0200: loss: 1514.609
iteration 0300: loss: 1516.077
iteration 0400: loss: 1513.570
iteration 0500: loss: 1513.989
iteration 0600: loss: 1513.943
iteration 0700: loss: 1514.485
iteration 0800: loss: 1514.542
iteration 0900: loss: 1514.107
====> Epoch: 033 Train loss: 1514.4782  took : 8.472719192504883
====> Test loss: 1518.1419
iteration 0000: loss: 1515.412
iteration 0100: loss: 1513.337
iteration 0200: loss: 1513.143
iteration 0300: loss: 1512.213
iteration 0400: loss: 1516.492
iteration 0500: loss: 1514.479
iteration 0600: loss: 1515.330
iteration 0700: loss: 1514.148
iteration 0800: loss: 1513.426
iteration 0900: loss: 1513.745
====> Epoch: 034 Train loss: 1514.4678  took : 8.546260118484497
====> Test loss: 1517.9065
iteration 0000: loss: 1515.453
iteration 0100: loss: 1513.735
iteration 0200: loss: 1514.349
iteration 0300: loss: 1512.531
iteration 0400: loss: 1511.669
iteration 0500: loss: 1515.406
iteration 0600: loss: 1513.831
iteration 0700: loss: 1517.221
iteration 0800: loss: 1513.860
iteration 0900: loss: 1514.717
====> Epoch: 035 Train loss: 1514.3450  took : 8.638141393661499
====> Test loss: 1518.0528
iteration 0000: loss: 1513.719
iteration 0100: loss: 1513.218
iteration 0200: loss: 1512.478
iteration 0300: loss: 1513.068
iteration 0400: loss: 1514.870
iteration 0500: loss: 1514.263
iteration 0600: loss: 1515.577
iteration 0700: loss: 1514.273
iteration 0800: loss: 1512.762
iteration 0900: loss: 1514.279
====> Epoch: 036 Train loss: 1514.2745  took : 8.526979207992554
====> Test loss: 1517.9599
iteration 0000: loss: 1515.096
iteration 0100: loss: 1513.223
iteration 0200: loss: 1514.689
iteration 0300: loss: 1516.301
iteration 0400: loss: 1513.351
iteration 0500: loss: 1513.469
iteration 0600: loss: 1516.534
iteration 0700: loss: 1513.830
iteration 0800: loss: 1513.262
iteration 0900: loss: 1513.990
====> Epoch: 037 Train loss: 1514.2420  took : 8.498432159423828
====> Test loss: 1517.8466
iteration 0000: loss: 1517.735
iteration 0100: loss: 1513.152
iteration 0200: loss: 1515.507
iteration 0300: loss: 1515.602
iteration 0400: loss: 1513.290
iteration 0500: loss: 1514.192
iteration 0600: loss: 1515.724
iteration 0700: loss: 1512.445
iteration 0800: loss: 1513.708
iteration 0900: loss: 1517.038
====> Epoch: 038 Train loss: 1514.2329  took : 8.531236410140991
====> Test loss: 1518.0139
iteration 0000: loss: 1513.471
iteration 0100: loss: 1514.479
iteration 0200: loss: 1516.067
iteration 0300: loss: 1514.255
iteration 0400: loss: 1512.719
iteration 0500: loss: 1513.479
iteration 0600: loss: 1515.394
iteration 0700: loss: 1515.541
iteration 0800: loss: 1513.645
iteration 0900: loss: 1514.276
====> Epoch: 039 Train loss: 1514.1652  took : 8.608437776565552
====> Test loss: 1517.8511
iteration 0000: loss: 1516.096
iteration 0100: loss: 1517.646
iteration 0200: loss: 1515.279
iteration 0300: loss: 1514.345
iteration 0400: loss: 1513.091
iteration 0500: loss: 1512.653
iteration 0600: loss: 1514.379
iteration 0700: loss: 1513.899
iteration 0800: loss: 1515.227
iteration 0900: loss: 1514.836
====> Epoch: 040 Train loss: 1514.0603  took : 8.52052617073059
====> Test loss: 1517.9189
iteration 0000: loss: 1513.995
iteration 0100: loss: 1513.579
iteration 0200: loss: 1513.332
iteration 0300: loss: 1515.494
iteration 0400: loss: 1512.354
iteration 0500: loss: 1513.016
iteration 0600: loss: 1516.759
iteration 0700: loss: 1512.538
iteration 0800: loss: 1515.613
iteration 0900: loss: 1513.903
====> Epoch: 041 Train loss: 1514.0730  took : 8.600829839706421
====> Test loss: 1517.8462
iteration 0000: loss: 1514.680
iteration 0100: loss: 1512.451
iteration 0200: loss: 1513.376
iteration 0300: loss: 1511.732
iteration 0400: loss: 1514.353
iteration 0500: loss: 1514.281
iteration 0600: loss: 1512.964
iteration 0700: loss: 1512.577
iteration 0800: loss: 1513.026
iteration 0900: loss: 1513.248
====> Epoch: 042 Train loss: 1514.1079  took : 8.60332179069519
====> Test loss: 1517.7491
iteration 0000: loss: 1513.025
iteration 0100: loss: 1513.969
iteration 0200: loss: 1514.840
iteration 0300: loss: 1513.933
iteration 0400: loss: 1512.535
iteration 0500: loss: 1513.805
iteration 0600: loss: 1514.905
iteration 0700: loss: 1514.684
iteration 0800: loss: 1513.871
iteration 0900: loss: 1513.350
====> Epoch: 043 Train loss: 1513.9506  took : 8.612579345703125
====> Test loss: 1517.7984
iteration 0000: loss: 1512.845
iteration 0100: loss: 1515.356
iteration 0200: loss: 1513.051
iteration 0300: loss: 1515.213
iteration 0400: loss: 1514.155
iteration 0500: loss: 1514.930
iteration 0600: loss: 1511.347
iteration 0700: loss: 1514.292
iteration 0800: loss: 1513.568
iteration 0900: loss: 1515.278
====> Epoch: 044 Train loss: 1513.8215  took : 8.483363389968872
====> Test loss: 1517.7014
iteration 0000: loss: 1513.300
iteration 0100: loss: 1513.356
iteration 0200: loss: 1511.583
iteration 0300: loss: 1512.297
iteration 0400: loss: 1512.725
iteration 0500: loss: 1514.207
iteration 0600: loss: 1514.114
iteration 0700: loss: 1513.740
iteration 0800: loss: 1512.268
iteration 0900: loss: 1514.997
====> Epoch: 045 Train loss: 1513.7809  took : 8.522859811782837
====> Test loss: 1517.6516
iteration 0000: loss: 1514.180
iteration 0100: loss: 1513.222
iteration 0200: loss: 1514.327
iteration 0300: loss: 1513.931
iteration 0400: loss: 1513.169
iteration 0500: loss: 1512.746
iteration 0600: loss: 1512.976
iteration 0700: loss: 1512.953
iteration 0800: loss: 1515.023
iteration 0900: loss: 1512.407
====> Epoch: 046 Train loss: 1513.7821  took : 8.589291334152222
====> Test loss: 1517.8994
iteration 0000: loss: 1511.374
iteration 0100: loss: 1512.889
iteration 0200: loss: 1517.044
iteration 0300: loss: 1514.907
iteration 0400: loss: 1510.929
iteration 0500: loss: 1513.330
iteration 0600: loss: 1512.958
iteration 0700: loss: 1511.718
iteration 0800: loss: 1514.197
iteration 0900: loss: 1511.892
====> Epoch: 047 Train loss: 1513.6964  took : 8.607600212097168
====> Test loss: 1517.6826
iteration 0000: loss: 1513.704
iteration 0100: loss: 1515.000
iteration 0200: loss: 1517.603
iteration 0300: loss: 1514.457
iteration 0400: loss: 1512.254
iteration 0500: loss: 1512.698
iteration 0600: loss: 1513.616
iteration 0700: loss: 1513.736
iteration 0800: loss: 1513.297
iteration 0900: loss: 1513.944
====> Epoch: 048 Train loss: 1513.6456  took : 8.51008677482605
====> Test loss: 1517.5705
iteration 0000: loss: 1515.304
iteration 0100: loss: 1512.814
iteration 0200: loss: 1512.534
iteration 0300: loss: 1511.877
iteration 0400: loss: 1515.983
iteration 0500: loss: 1512.965
iteration 0600: loss: 1513.949
iteration 0700: loss: 1511.787
iteration 0800: loss: 1510.927
iteration 0900: loss: 1513.688
====> Epoch: 049 Train loss: 1513.6779  took : 8.609342575073242
====> Test loss: 1517.6804
iteration 0000: loss: 1512.921
iteration 0100: loss: 1511.301
iteration 0200: loss: 1512.013
iteration 0300: loss: 1514.801
iteration 0400: loss: 1514.233
iteration 0500: loss: 1516.407
iteration 0600: loss: 1515.257
iteration 0700: loss: 1513.543
iteration 0800: loss: 1513.110
iteration 0900: loss: 1511.170
====> Epoch: 050 Train loss: 1513.5896  took : 8.598290205001831
====> Test loss: 1517.5526
====> [MM-VAE] Time: 515.251s or 00:08:35
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  0
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_0
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.545
iteration 0100: loss: 2118.262
iteration 0200: loss: 2045.553
iteration 0300: loss: 2031.959
iteration 0400: loss: 2006.287
iteration 0500: loss: 2006.664
iteration 0600: loss: 2000.729
iteration 0700: loss: 1997.625
iteration 0800: loss: 1993.824
iteration 0900: loss: 1993.840
====> Epoch: 001 Train loss: 2026.2156  took : 13.387124061584473
====> Test loss: 1996.9403
iteration 0000: loss: 1990.620
iteration 0100: loss: 1992.287
iteration 0200: loss: 1991.788
iteration 0300: loss: 1986.750
iteration 0400: loss: 1988.275
iteration 0500: loss: 1982.878
iteration 0600: loss: 1978.899
iteration 0700: loss: 1983.978
iteration 0800: loss: 1982.447
iteration 0900: loss: 1974.038
====> Epoch: 002 Train loss: 1984.9989  took : 12.253880500793457
====> Test loss: 1978.1071
iteration 0000: loss: 1979.398
iteration 0100: loss: 1976.565
iteration 0200: loss: 1976.157
iteration 0300: loss: 1970.037
iteration 0400: loss: 1972.855
iteration 0500: loss: 1970.901
iteration 0600: loss: 1972.399
iteration 0700: loss: 1976.400
iteration 0800: loss: 1974.899
iteration 0900: loss: 1967.466
====> Epoch: 003 Train loss: 1972.0004  took : 12.963513374328613
====> Test loss: 1971.5128
iteration 0000: loss: 1969.681
iteration 0100: loss: 1964.844
iteration 0200: loss: 1968.463
iteration 0300: loss: 1962.660
iteration 0400: loss: 1967.020
iteration 0500: loss: 1964.155
iteration 0600: loss: 1964.385
iteration 0700: loss: 1962.161
iteration 0800: loss: 1966.119
iteration 0900: loss: 1960.605
====> Epoch: 004 Train loss: 1965.6719  took : 12.395004272460938
====> Test loss: 1966.0501
iteration 0000: loss: 1961.460
iteration 0100: loss: 1963.973
iteration 0200: loss: 1959.849
iteration 0300: loss: 1960.731
iteration 0400: loss: 1960.368
iteration 0500: loss: 1960.178
iteration 0600: loss: 1962.918
iteration 0700: loss: 1959.894
iteration 0800: loss: 1960.521
iteration 0900: loss: 1959.742
====> Epoch: 005 Train loss: 1961.4302  took : 13.194084167480469
====> Test loss: 1963.2257
iteration 0000: loss: 1963.444
iteration 0100: loss: 1958.199
iteration 0200: loss: 1959.642
iteration 0300: loss: 1958.711
iteration 0400: loss: 1958.284
iteration 0500: loss: 1957.279
iteration 0600: loss: 1958.471
iteration 0700: loss: 1957.445
iteration 0800: loss: 1957.709
iteration 0900: loss: 1958.409
====> Epoch: 006 Train loss: 1958.9063  took : 12.497083187103271
====> Test loss: 1959.8039
iteration 0000: loss: 1957.224
iteration 0100: loss: 1955.721
iteration 0200: loss: 1956.085
iteration 0300: loss: 1956.781
iteration 0400: loss: 1957.161
iteration 0500: loss: 1957.416
iteration 0600: loss: 1959.656
iteration 0700: loss: 1956.462
iteration 0800: loss: 1956.295
iteration 0900: loss: 1956.809
====> Epoch: 007 Train loss: 1957.1890  took : 12.969175338745117
====> Test loss: 1959.0785
iteration 0000: loss: 1955.634
iteration 0100: loss: 1958.202
iteration 0200: loss: 1954.787
iteration 0300: loss: 1953.770
iteration 0400: loss: 1957.326
iteration 0500: loss: 1953.799
iteration 0600: loss: 1955.707
iteration 0700: loss: 1956.541
iteration 0800: loss: 1956.175
iteration 0900: loss: 1955.798
====> Epoch: 008 Train loss: 1956.0829  took : 12.305427312850952
====> Test loss: 1958.5196
iteration 0000: loss: 1956.375
iteration 0100: loss: 1957.840
iteration 0200: loss: 1954.984
iteration 0300: loss: 1953.674
iteration 0400: loss: 1953.176
iteration 0500: loss: 1955.605
iteration 0600: loss: 1952.265
iteration 0700: loss: 1953.585
iteration 0800: loss: 1955.207
iteration 0900: loss: 1954.414
====> Epoch: 009 Train loss: 1954.8046  took : 12.922593355178833
====> Test loss: 1956.3509
iteration 0000: loss: 1953.610
iteration 0100: loss: 1953.256
iteration 0200: loss: 1953.829
iteration 0300: loss: 1958.244
iteration 0400: loss: 1953.899
iteration 0500: loss: 1953.641
iteration 0600: loss: 1954.508
iteration 0700: loss: 1953.601
iteration 0800: loss: 1955.791
iteration 0900: loss: 1954.221
====> Epoch: 010 Train loss: 1953.7094  took : 13.002862453460693
====> Test loss: 1955.6564
iteration 0000: loss: 1953.947
iteration 0100: loss: 1953.470
iteration 0200: loss: 1954.290
iteration 0300: loss: 1952.052
iteration 0400: loss: 1955.063
iteration 0500: loss: 1951.846
iteration 0600: loss: 1953.055
iteration 0700: loss: 1954.422
iteration 0800: loss: 1952.758
iteration 0900: loss: 1952.092
====> Epoch: 011 Train loss: 1953.3431  took : 12.37010145187378
====> Test loss: 1955.1150
iteration 0000: loss: 1952.566
iteration 0100: loss: 1952.451
iteration 0200: loss: 1951.971
iteration 0300: loss: 1954.055
iteration 0400: loss: 1953.359
iteration 0500: loss: 1952.698
iteration 0600: loss: 1952.870
iteration 0700: loss: 1952.119
iteration 0800: loss: 1951.444
iteration 0900: loss: 1953.544
====> Epoch: 012 Train loss: 1952.8742  took : 13.357048511505127
====> Test loss: 1955.8913
iteration 0000: loss: 1953.682
iteration 0100: loss: 1952.584
iteration 0200: loss: 1952.046
iteration 0300: loss: 1952.990
iteration 0400: loss: 1952.676
iteration 0500: loss: 1953.226
iteration 0600: loss: 1954.843
iteration 0700: loss: 1952.849
iteration 0800: loss: 1953.135
iteration 0900: loss: 1951.722
====> Epoch: 013 Train loss: 1952.7870  took : 11.988650798797607
====> Test loss: 1954.7200
iteration 0000: loss: 1952.298
iteration 0100: loss: 1952.652
iteration 0200: loss: 1953.121
iteration 0300: loss: 1951.890
iteration 0400: loss: 1953.434
iteration 0500: loss: 1954.772
iteration 0600: loss: 1953.001
iteration 0700: loss: 1951.552
iteration 0800: loss: 1952.353
iteration 0900: loss: 1952.999
====> Epoch: 014 Train loss: 1952.5014  took : 12.985642910003662
====> Test loss: 1954.6366
iteration 0000: loss: 1953.832
iteration 0100: loss: 1952.050
iteration 0200: loss: 1952.986
iteration 0300: loss: 1951.597
iteration 0400: loss: 1951.958
iteration 0500: loss: 1952.270
iteration 0600: loss: 1952.281
iteration 0700: loss: 1952.315
iteration 0800: loss: 1951.367
iteration 0900: loss: 1952.391
====> Epoch: 015 Train loss: 1952.4533  took : 13.110002040863037
====> Test loss: 1954.6672
iteration 0000: loss: 1951.047
iteration 0100: loss: 1951.092
iteration 0200: loss: 1951.550
iteration 0300: loss: 1951.156
iteration 0400: loss: 1951.614
iteration 0500: loss: 1951.497
iteration 0600: loss: 1952.650
iteration 0700: loss: 1952.895
iteration 0800: loss: 1954.308
iteration 0900: loss: 1950.982
====> Epoch: 016 Train loss: 1952.4151  took : 12.963934898376465
====> Test loss: 1954.2653
iteration 0000: loss: 1952.540
iteration 0100: loss: 1951.437
iteration 0200: loss: 1951.460
iteration 0300: loss: 1951.152
iteration 0400: loss: 1951.559
iteration 0500: loss: 1953.109
iteration 0600: loss: 1952.856
iteration 0700: loss: 1951.433
iteration 0800: loss: 1952.327
iteration 0900: loss: 1951.801
====> Epoch: 017 Train loss: 1952.1813  took : 12.61780571937561
====> Test loss: 1954.0454
iteration 0000: loss: 1952.377
iteration 0100: loss: 1953.347
iteration 0200: loss: 1951.556
iteration 0300: loss: 1950.547
iteration 0400: loss: 1951.627
iteration 0500: loss: 1953.673
iteration 0600: loss: 1951.386
iteration 0700: loss: 1952.992
iteration 0800: loss: 1953.014
iteration 0900: loss: 1952.254
====> Epoch: 018 Train loss: 1952.0009  took : 12.173011302947998
====> Test loss: 1954.3701
iteration 0000: loss: 1952.619
iteration 0100: loss: 1953.386
iteration 0200: loss: 1952.509
iteration 0300: loss: 1951.011
iteration 0400: loss: 1951.251
iteration 0500: loss: 1953.341
iteration 0600: loss: 1952.193
iteration 0700: loss: 1952.136
iteration 0800: loss: 1950.596
iteration 0900: loss: 1952.333
====> Epoch: 019 Train loss: 1952.0614  took : 12.988081455230713
====> Test loss: 1953.8217
iteration 0000: loss: 1951.580
iteration 0100: loss: 1951.362
iteration 0200: loss: 1951.803
iteration 0300: loss: 1953.533
iteration 0400: loss: 1954.559
iteration 0500: loss: 1953.008
iteration 0600: loss: 1952.183
iteration 0700: loss: 1952.374
iteration 0800: loss: 1951.211
iteration 0900: loss: 1951.358
====> Epoch: 020 Train loss: 1952.2109  took : 12.578001976013184
====> Test loss: 1954.0561
iteration 0000: loss: 1950.241
iteration 0100: loss: 1950.045
iteration 0200: loss: 1953.181
iteration 0300: loss: 1952.188
iteration 0400: loss: 1952.709
iteration 0500: loss: 1953.008
iteration 0600: loss: 1951.843
iteration 0700: loss: 1951.991
iteration 0800: loss: 1954.315
iteration 0900: loss: 1951.933
====> Epoch: 021 Train loss: 1952.0280  took : 13.191005945205688
====> Test loss: 1953.8425
iteration 0000: loss: 1951.691
iteration 0100: loss: 1951.088
iteration 0200: loss: 1952.836
iteration 0300: loss: 1952.700
iteration 0400: loss: 1951.430
iteration 0500: loss: 1952.513
iteration 0600: loss: 1952.818
iteration 0700: loss: 1952.744
iteration 0800: loss: 1952.039
iteration 0900: loss: 1950.818
====> Epoch: 022 Train loss: 1951.8655  took : 13.05249571800232
====> Test loss: 1953.8872
iteration 0000: loss: 1951.593
iteration 0100: loss: 1951.056
iteration 0200: loss: 1951.935
iteration 0300: loss: 1951.283
iteration 0400: loss: 1951.386
iteration 0500: loss: 1951.296
iteration 0600: loss: 1949.720
iteration 0700: loss: 1954.262
iteration 0800: loss: 1952.040
iteration 0900: loss: 1951.524
====> Epoch: 023 Train loss: 1951.8228  took : 12.352299451828003
====> Test loss: 1954.1847
iteration 0000: loss: 1951.200
iteration 0100: loss: 1950.570
iteration 0200: loss: 1951.680
iteration 0300: loss: 1950.978
iteration 0400: loss: 1951.835
iteration 0500: loss: 1950.740
iteration 0600: loss: 1951.679
iteration 0700: loss: 1951.083
iteration 0800: loss: 1950.663
iteration 0900: loss: 1950.854
====> Epoch: 024 Train loss: 1951.7988  took : 13.290328741073608
====> Test loss: 1953.8590
iteration 0000: loss: 1951.074
iteration 0100: loss: 1951.570
iteration 0200: loss: 1951.815
iteration 0300: loss: 1952.660
iteration 0400: loss: 1951.222
iteration 0500: loss: 1951.117
iteration 0600: loss: 1950.745
iteration 0700: loss: 1949.843
iteration 0800: loss: 1951.768
iteration 0900: loss: 1952.038
====> Epoch: 025 Train loss: 1951.9527  took : 12.031270503997803
====> Test loss: 1953.9979
iteration 0000: loss: 1951.089
iteration 0100: loss: 1949.757
iteration 0200: loss: 1950.407
iteration 0300: loss: 1953.536
iteration 0400: loss: 1951.988
iteration 0500: loss: 1953.085
iteration 0600: loss: 1951.550
iteration 0700: loss: 1952.235
iteration 0800: loss: 1951.955
iteration 0900: loss: 1952.447
====> Epoch: 026 Train loss: 1951.5611  took : 11.924938440322876
====> Test loss: 1953.5370
iteration 0000: loss: 1949.595
iteration 0100: loss: 1952.605
iteration 0200: loss: 1950.779
iteration 0300: loss: 1951.398
iteration 0400: loss: 1950.920
iteration 0500: loss: 1951.542
iteration 0600: loss: 1950.995
iteration 0700: loss: 1951.181
iteration 0800: loss: 1952.179
iteration 0900: loss: 1952.857
====> Epoch: 027 Train loss: 1951.4701  took : 12.815696001052856
====> Test loss: 1953.1499
iteration 0000: loss: 1952.204
iteration 0100: loss: 1950.594
iteration 0200: loss: 1952.539
iteration 0300: loss: 1952.404
iteration 0400: loss: 1950.029
iteration 0500: loss: 1950.776
iteration 0600: loss: 1949.817
iteration 0700: loss: 1950.429
iteration 0800: loss: 1951.007
iteration 0900: loss: 1950.965
====> Epoch: 028 Train loss: 1951.4814  took : 13.206892728805542
====> Test loss: 1953.4641
iteration 0000: loss: 1950.610
iteration 0100: loss: 1950.086
iteration 0200: loss: 1951.946
iteration 0300: loss: 1950.848
iteration 0400: loss: 1951.163
iteration 0500: loss: 1951.831
iteration 0600: loss: 1951.262
iteration 0700: loss: 1951.358
iteration 0800: loss: 1951.238
iteration 0900: loss: 1949.632
====> Epoch: 029 Train loss: 1951.3090  took : 12.202601432800293
====> Test loss: 1953.3800
iteration 0000: loss: 1950.396
iteration 0100: loss: 1950.392
iteration 0200: loss: 1952.888
iteration 0300: loss: 1950.814
iteration 0400: loss: 1950.461
iteration 0500: loss: 1952.656
iteration 0600: loss: 1951.656
iteration 0700: loss: 1950.672
iteration 0800: loss: 1949.690
iteration 0900: loss: 1950.203
====> Epoch: 030 Train loss: 1951.3700  took : 12.095595836639404
====> Test loss: 1953.2035
iteration 0000: loss: 1950.056
iteration 0100: loss: 1952.487
iteration 0200: loss: 1950.952
iteration 0300: loss: 1950.293
iteration 0400: loss: 1950.185
iteration 0500: loss: 1952.640
iteration 0600: loss: 1950.029
iteration 0700: loss: 1950.792
iteration 0800: loss: 1951.050
iteration 0900: loss: 1950.956
====> Epoch: 031 Train loss: 1951.2609  took : 12.03677225112915
====> Test loss: 1953.0334
iteration 0000: loss: 1950.129
iteration 0100: loss: 1952.562
iteration 0200: loss: 1951.215
iteration 0300: loss: 1951.521
iteration 0400: loss: 1951.488
iteration 0500: loss: 1950.673
iteration 0600: loss: 1950.569
iteration 0700: loss: 1951.447
iteration 0800: loss: 1951.578
iteration 0900: loss: 1952.113
====> Epoch: 032 Train loss: 1951.2649  took : 11.938626527786255
====> Test loss: 1953.2744
iteration 0000: loss: 1950.679
iteration 0100: loss: 1950.226
iteration 0200: loss: 1951.665
iteration 0300: loss: 1950.538
iteration 0400: loss: 1950.482
iteration 0500: loss: 1951.663
iteration 0600: loss: 1951.952
iteration 0700: loss: 1950.822
iteration 0800: loss: 1951.263
iteration 0900: loss: 1951.322
====> Epoch: 033 Train loss: 1951.2084  took : 12.964963674545288
====> Test loss: 1952.6945
iteration 0000: loss: 1953.585
iteration 0100: loss: 1949.312
iteration 0200: loss: 1950.902
iteration 0300: loss: 1951.336
iteration 0400: loss: 1949.336
iteration 0500: loss: 1951.128
iteration 0600: loss: 1950.587
iteration 0700: loss: 1951.180
iteration 0800: loss: 1951.448
iteration 0900: loss: 1951.586
====> Epoch: 034 Train loss: 1951.1437  took : 12.707025289535522
====> Test loss: 1953.1207
iteration 0000: loss: 1950.559
iteration 0100: loss: 1950.994
iteration 0200: loss: 1950.202
iteration 0300: loss: 1950.369
iteration 0400: loss: 1950.190
iteration 0500: loss: 1950.189
iteration 0600: loss: 1950.866
iteration 0700: loss: 1952.027
iteration 0800: loss: 1949.545
iteration 0900: loss: 1949.746
====> Epoch: 035 Train loss: 1950.8764  took : 12.463750123977661
====> Test loss: 1952.6898
iteration 0000: loss: 1951.655
iteration 0100: loss: 1951.192
iteration 0200: loss: 1951.022
iteration 0300: loss: 1950.659
iteration 0400: loss: 1951.920
iteration 0500: loss: 1950.555
iteration 0600: loss: 1952.400
iteration 0700: loss: 1950.958
iteration 0800: loss: 1950.559
iteration 0900: loss: 1950.388
====> Epoch: 036 Train loss: 1950.8939  took : 12.473161458969116
====> Test loss: 1953.2182
iteration 0000: loss: 1951.006
iteration 0100: loss: 1949.658
iteration 0200: loss: 1950.727
iteration 0300: loss: 1950.439
iteration 0400: loss: 1950.607
iteration 0500: loss: 1949.635
iteration 0600: loss: 1949.671
iteration 0700: loss: 1951.269
iteration 0800: loss: 1951.321
iteration 0900: loss: 1950.502
====> Epoch: 037 Train loss: 1950.6977  took : 11.785882711410522
====> Test loss: 1952.7838
iteration 0000: loss: 1950.636
iteration 0100: loss: 1950.619
iteration 0200: loss: 1951.056
iteration 0300: loss: 1950.348
iteration 0400: loss: 1950.609
iteration 0500: loss: 1951.117
iteration 0600: loss: 1950.222
iteration 0700: loss: 1949.991
iteration 0800: loss: 1950.337
iteration 0900: loss: 1949.963
====> Epoch: 038 Train loss: 1950.8744  took : 10.847052812576294
====> Test loss: 1952.7806
iteration 0000: loss: 1950.019
iteration 0100: loss: 1951.334
iteration 0200: loss: 1952.061
iteration 0300: loss: 1949.288
iteration 0400: loss: 1951.989
iteration 0500: loss: 1949.885
iteration 0600: loss: 1949.973
iteration 0700: loss: 1949.852
iteration 0800: loss: 1950.168
iteration 0900: loss: 1948.622
====> Epoch: 039 Train loss: 1950.7404  took : 12.131701946258545
====> Test loss: 1952.5653
iteration 0000: loss: 1949.885
iteration 0100: loss: 1949.645
iteration 0200: loss: 1950.041
iteration 0300: loss: 1951.376
iteration 0400: loss: 1952.115
iteration 0500: loss: 1951.612
iteration 0600: loss: 1947.862
iteration 0700: loss: 1952.159
iteration 0800: loss: 1950.580
iteration 0900: loss: 1950.131
====> Epoch: 040 Train loss: 1950.5316  took : 12.874180793762207
====> Test loss: 1952.3232
iteration 0000: loss: 1949.170
iteration 0100: loss: 1951.411
iteration 0200: loss: 1949.750
iteration 0300: loss: 1949.958
iteration 0400: loss: 1951.879
iteration 0500: loss: 1951.168
iteration 0600: loss: 1951.352
iteration 0700: loss: 1949.929
iteration 0800: loss: 1950.302
iteration 0900: loss: 1949.983
====> Epoch: 041 Train loss: 1950.5262  took : 12.959237575531006
====> Test loss: 1952.4789
iteration 0000: loss: 1949.763
iteration 0100: loss: 1951.413
iteration 0200: loss: 1949.999
iteration 0300: loss: 1950.789
iteration 0400: loss: 1948.946
iteration 0500: loss: 1950.001
iteration 0600: loss: 1950.302
iteration 0700: loss: 1950.803
iteration 0800: loss: 1950.074
iteration 0900: loss: 1950.393
====> Epoch: 042 Train loss: 1950.5375  took : 12.353089809417725
====> Test loss: 1952.7935
iteration 0000: loss: 1952.620
iteration 0100: loss: 1949.788
iteration 0200: loss: 1950.598
iteration 0300: loss: 1949.838
iteration 0400: loss: 1950.093
iteration 0500: loss: 1951.304
iteration 0600: loss: 1950.244
iteration 0700: loss: 1949.234
iteration 0800: loss: 1949.891
iteration 0900: loss: 1951.074
====> Epoch: 043 Train loss: 1950.4378  took : 12.20738697052002
====> Test loss: 1952.3018
iteration 0000: loss: 1949.831
iteration 0100: loss: 1950.049
iteration 0200: loss: 1951.912
iteration 0300: loss: 1949.627
iteration 0400: loss: 1950.086
iteration 0500: loss: 1950.700
iteration 0600: loss: 1949.461
iteration 0700: loss: 1950.760
iteration 0800: loss: 1950.886
iteration 0900: loss: 1949.860
====> Epoch: 044 Train loss: 1950.4776  took : 13.289063215255737
====> Test loss: 1952.5079
iteration 0000: loss: 1949.067
iteration 0100: loss: 1950.597
iteration 0200: loss: 1949.744
iteration 0300: loss: 1950.786
iteration 0400: loss: 1950.352
iteration 0500: loss: 1950.176
iteration 0600: loss: 1950.888
iteration 0700: loss: 1950.651
iteration 0800: loss: 1950.357
iteration 0900: loss: 1950.000
====> Epoch: 045 Train loss: 1950.4663  took : 12.58406400680542
====> Test loss: 1952.6226
iteration 0000: loss: 1949.495
iteration 0100: loss: 1952.038
iteration 0200: loss: 1950.480
iteration 0300: loss: 1950.359
iteration 0400: loss: 1949.952
iteration 0500: loss: 1949.330
iteration 0600: loss: 1950.561
iteration 0700: loss: 1949.764
iteration 0800: loss: 1952.257
iteration 0900: loss: 1951.159
====> Epoch: 046 Train loss: 1950.4881  took : 12.68099069595337
====> Test loss: 1952.4904
iteration 0000: loss: 1950.015
iteration 0100: loss: 1951.541
iteration 0200: loss: 1950.272
iteration 0300: loss: 1950.839
iteration 0400: loss: 1950.387
iteration 0500: loss: 1950.068
iteration 0600: loss: 1949.210
iteration 0700: loss: 1951.023
iteration 0800: loss: 1949.753
iteration 0900: loss: 1951.175
====> Epoch: 047 Train loss: 1950.4831  took : 13.768864393234253
====> Test loss: 1952.4722
iteration 0000: loss: 1949.624
iteration 0100: loss: 1948.595
iteration 0200: loss: 1951.221
iteration 0300: loss: 1951.223
iteration 0400: loss: 1948.740
iteration 0500: loss: 1951.630
iteration 0600: loss: 1949.605
iteration 0700: loss: 1950.028
iteration 0800: loss: 1949.788
iteration 0900: loss: 1949.836
====> Epoch: 048 Train loss: 1950.4166  took : 12.460179090499878
====> Test loss: 1952.4327
iteration 0000: loss: 1951.291
iteration 0100: loss: 1950.340
iteration 0200: loss: 1950.765
iteration 0300: loss: 1950.919
iteration 0400: loss: 1949.607
iteration 0500: loss: 1950.238
iteration 0600: loss: 1949.625
iteration 0700: loss: 1950.020
iteration 0800: loss: 1949.288
iteration 0900: loss: 1949.334
====> Epoch: 049 Train loss: 1950.4249  took : 12.20316767692566
====> Test loss: 1954.0129
iteration 0000: loss: 1951.979
iteration 0100: loss: 1950.883
iteration 0200: loss: 1949.283
iteration 0300: loss: 1948.962
iteration 0400: loss: 1949.879
iteration 0500: loss: 1948.499
iteration 0600: loss: 1951.165
iteration 0700: loss: 1949.526
iteration 0800: loss: 1949.539
iteration 0900: loss: 1950.183
====> Epoch: 050 Train loss: 1950.5197  took : 11.969083786010742
====> Test loss: 1952.2709
====> [MM-VAE] Time: 703.902s or 00:11:43
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  0
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_0
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5216.554
iteration 0100: loss: 4127.889
iteration 0200: loss: 4088.711
iteration 0300: loss: 4050.667
iteration 0400: loss: 4013.966
iteration 0500: loss: 4029.695
iteration 0600: loss: 4004.195
iteration 0700: loss: 4015.947
iteration 0800: loss: 4001.930
iteration 0900: loss: 3993.211
iteration 1000: loss: 4007.833
iteration 1100: loss: 4010.045
iteration 1200: loss: 4004.636
iteration 1300: loss: 3985.175
iteration 1400: loss: 3985.349
iteration 1500: loss: 3979.976
iteration 1600: loss: 3984.900
iteration 1700: loss: 3984.382
iteration 1800: loss: 3995.322
====> Epoch: 001 Train loss: 4022.3588  took : 53.42346143722534
====> Test loss: 3986.7981
iteration 0000: loss: 3992.673
iteration 0100: loss: 3997.069
iteration 0200: loss: 3982.324
iteration 0300: loss: 3976.566
iteration 0400: loss: 3991.282
iteration 0500: loss: 3991.303
iteration 0600: loss: 3973.326
iteration 0700: loss: 3966.735
iteration 0800: loss: 3980.822
iteration 0900: loss: 3970.988
iteration 1000: loss: 3972.520
iteration 1100: loss: 3980.897
iteration 1200: loss: 3969.231
iteration 1300: loss: 3969.131
iteration 1400: loss: 3969.259
iteration 1500: loss: 3970.972
iteration 1600: loss: 3969.420
iteration 1700: loss: 3964.391
iteration 1800: loss: 3961.097
====> Epoch: 002 Train loss: 3973.2962  took : 53.432941198349
====> Test loss: 3968.9934
iteration 0000: loss: 3960.928
iteration 0100: loss: 3958.749
iteration 0200: loss: 3967.747
iteration 0300: loss: 3963.135
iteration 0400: loss: 3960.879
iteration 0500: loss: 3971.328
iteration 0600: loss: 3956.252
iteration 0700: loss: 3962.601
iteration 0800: loss: 3960.017
iteration 0900: loss: 3957.213
iteration 1000: loss: 3956.638
iteration 1100: loss: 3951.255
iteration 1200: loss: 3949.583
iteration 1300: loss: 3960.340
iteration 1400: loss: 3956.583
iteration 1500: loss: 3953.198
iteration 1600: loss: 3958.061
iteration 1700: loss: 3961.724
iteration 1800: loss: 3959.862
====> Epoch: 003 Train loss: 3959.6032  took : 53.24553871154785
====> Test loss: 3957.4764
iteration 0000: loss: 3949.891
iteration 0100: loss: 3947.071
iteration 0200: loss: 3955.099
iteration 0300: loss: 3945.923
iteration 0400: loss: 3951.855
iteration 0500: loss: 3947.660
iteration 0600: loss: 3951.534
iteration 0700: loss: 3948.986
iteration 0800: loss: 3950.535
iteration 0900: loss: 3952.722
iteration 1000: loss: 3951.110
iteration 1100: loss: 3951.617
iteration 1200: loss: 3948.883
iteration 1300: loss: 3947.799
iteration 1400: loss: 3948.647
iteration 1500: loss: 3948.846
iteration 1600: loss: 3950.316
iteration 1700: loss: 3947.456
iteration 1800: loss: 3950.823
====> Epoch: 004 Train loss: 3950.8778  took : 53.034674882888794
====> Test loss: 3951.0567
iteration 0000: loss: 3951.265
iteration 0100: loss: 3944.232
iteration 0200: loss: 3950.254
iteration 0300: loss: 3949.784
iteration 0400: loss: 3946.558
iteration 0500: loss: 3957.135
iteration 0600: loss: 3954.305
iteration 0700: loss: 3952.785
iteration 0800: loss: 3942.148
iteration 0900: loss: 3950.058
iteration 1000: loss: 3949.491
iteration 1100: loss: 3942.927
iteration 1200: loss: 3952.878
iteration 1300: loss: 3942.811
iteration 1400: loss: 3943.652
iteration 1500: loss: 3946.163
iteration 1600: loss: 3948.036
iteration 1700: loss: 3946.425
iteration 1800: loss: 3947.998
====> Epoch: 005 Train loss: 3947.0079  took : 52.98694181442261
====> Test loss: 3949.3014
iteration 0000: loss: 3948.451
iteration 0100: loss: 3948.778
iteration 0200: loss: 3943.611
iteration 0300: loss: 3945.427
iteration 0400: loss: 3945.938
iteration 0500: loss: 3943.920
iteration 0600: loss: 3941.305
iteration 0700: loss: 3940.919
iteration 0800: loss: 3944.546
iteration 0900: loss: 3947.641
iteration 1000: loss: 3947.974
iteration 1100: loss: 3951.750
iteration 1200: loss: 3936.331
iteration 1300: loss: 3943.559
iteration 1400: loss: 3946.400
iteration 1500: loss: 3947.641
iteration 1600: loss: 3943.369
iteration 1700: loss: 3948.914
iteration 1800: loss: 3939.050
====> Epoch: 006 Train loss: 3944.7453  took : 53.17995548248291
====> Test loss: 3947.2863
iteration 0000: loss: 3948.839
iteration 0100: loss: 3939.784
iteration 0200: loss: 3944.111
iteration 0300: loss: 3941.167
iteration 0400: loss: 3945.129
iteration 0500: loss: 3936.375
iteration 0600: loss: 3942.291
iteration 0700: loss: 3949.470
iteration 0800: loss: 3945.606
iteration 0900: loss: 3937.912
iteration 1000: loss: 3946.242
iteration 1100: loss: 3942.570
iteration 1200: loss: 3941.635
iteration 1300: loss: 3949.814
iteration 1400: loss: 3946.078
iteration 1500: loss: 3941.507
iteration 1600: loss: 3946.360
iteration 1700: loss: 3945.881
iteration 1800: loss: 3941.735
====> Epoch: 007 Train loss: 3943.9446  took : 53.179022550582886
====> Test loss: 3946.9766
iteration 0000: loss: 3942.910
iteration 0100: loss: 3949.080
iteration 0200: loss: 3942.772
iteration 0300: loss: 3947.946
iteration 0400: loss: 3942.954
iteration 0500: loss: 3941.942
iteration 0600: loss: 3944.501
iteration 0700: loss: 3945.871
iteration 0800: loss: 3944.269
iteration 0900: loss: 3942.440
iteration 1000: loss: 3940.825
iteration 1100: loss: 3936.741
iteration 1200: loss: 3944.399
iteration 1300: loss: 3940.189
iteration 1400: loss: 3936.291
iteration 1500: loss: 3939.121
iteration 1600: loss: 3948.183
iteration 1700: loss: 3941.583
iteration 1800: loss: 3940.173
====> Epoch: 008 Train loss: 3943.0644  took : 53.02711725234985
====> Test loss: 3945.9160
iteration 0000: loss: 3944.265
iteration 0100: loss: 3937.798
iteration 0200: loss: 3939.155
iteration 0300: loss: 3938.432
iteration 0400: loss: 3942.545
iteration 0500: loss: 3942.091
iteration 0600: loss: 3939.294
iteration 0700: loss: 3939.327
iteration 0800: loss: 3941.519
iteration 0900: loss: 3947.402
iteration 1000: loss: 3948.990
iteration 1100: loss: 3938.200
iteration 1200: loss: 3940.961
iteration 1300: loss: 3949.406
iteration 1400: loss: 3946.939
iteration 1500: loss: 3953.360
iteration 1600: loss: 3949.557
iteration 1700: loss: 3943.748
iteration 1800: loss: 3945.457
====> Epoch: 009 Train loss: 3942.3394  took : 52.71470808982849
====> Test loss: 3946.0777
iteration 0000: loss: 3939.175
iteration 0100: loss: 3940.167
iteration 0200: loss: 3945.108
iteration 0300: loss: 3943.912
iteration 0400: loss: 3941.108
iteration 0500: loss: 3941.693
iteration 0600: loss: 3939.494
iteration 0700: loss: 3939.356
iteration 0800: loss: 3940.805
iteration 0900: loss: 3949.390
iteration 1000: loss: 3943.890
iteration 1100: loss: 3936.882
iteration 1200: loss: 3945.858
iteration 1300: loss: 3943.799
iteration 1400: loss: 3946.137
iteration 1500: loss: 3947.434
iteration 1600: loss: 3936.331
iteration 1700: loss: 3936.252
iteration 1800: loss: 3937.066
====> Epoch: 010 Train loss: 3942.0689  took : 52.95959782600403
====> Test loss: 3945.2574
iteration 0000: loss: 3945.979
iteration 0100: loss: 3940.753
iteration 0200: loss: 3940.117
iteration 0300: loss: 3948.267
iteration 0400: loss: 3946.230
iteration 0500: loss: 3940.530
iteration 0600: loss: 3942.381
iteration 0700: loss: 3948.857
iteration 0800: loss: 3939.584
iteration 0900: loss: 3943.117
iteration 1000: loss: 3934.870
iteration 1100: loss: 3940.151
iteration 1200: loss: 3936.277
iteration 1300: loss: 3936.262
iteration 1400: loss: 3938.531
iteration 1500: loss: 3935.215
iteration 1600: loss: 3944.071
iteration 1700: loss: 3939.546
iteration 1800: loss: 3936.851
====> Epoch: 011 Train loss: 3941.6640  took : 53.23523497581482
====> Test loss: 3945.7395
iteration 0000: loss: 3953.772
iteration 0100: loss: 3936.828
iteration 0200: loss: 3939.170
iteration 0300: loss: 3945.032
iteration 0400: loss: 3941.229
iteration 0500: loss: 3943.360
iteration 0600: loss: 3944.614
iteration 0700: loss: 3941.432
iteration 0800: loss: 3946.229
iteration 0900: loss: 3939.900
iteration 1000: loss: 3935.750
iteration 1100: loss: 3937.421
iteration 1200: loss: 3943.224
iteration 1300: loss: 3940.436
iteration 1400: loss: 3946.551
iteration 1500: loss: 3942.801
iteration 1600: loss: 3933.477
iteration 1700: loss: 3942.537
iteration 1800: loss: 3940.669
====> Epoch: 012 Train loss: 3941.4243  took : 52.85904097557068
====> Test loss: 3944.9880
iteration 0000: loss: 3947.459
iteration 0100: loss: 3942.988
iteration 0200: loss: 3942.024
iteration 0300: loss: 3936.107
iteration 0400: loss: 3944.022
iteration 0500: loss: 3939.431
iteration 0600: loss: 3943.978
iteration 0700: loss: 3948.331
iteration 0800: loss: 3943.425
iteration 0900: loss: 3938.511
iteration 1000: loss: 3939.690
iteration 1100: loss: 3944.121
iteration 1200: loss: 3937.735
iteration 1300: loss: 3939.470
iteration 1400: loss: 3941.584
iteration 1500: loss: 3936.956
iteration 1600: loss: 3935.820
iteration 1700: loss: 3937.952
iteration 1800: loss: 3945.394
====> Epoch: 013 Train loss: 3941.0290  took : 52.92386507987976
====> Test loss: 3944.3018
iteration 0000: loss: 3945.383
iteration 0100: loss: 3936.515
iteration 0200: loss: 3935.999
iteration 0300: loss: 3942.976
iteration 0400: loss: 3937.715
iteration 0500: loss: 3934.007
iteration 0600: loss: 3937.078
iteration 0700: loss: 3939.051
iteration 0800: loss: 3939.974
iteration 0900: loss: 3936.681
iteration 1000: loss: 3935.631
iteration 1100: loss: 3940.437
iteration 1200: loss: 3943.019
iteration 1300: loss: 3946.099
iteration 1400: loss: 3938.087
iteration 1500: loss: 3934.800
iteration 1600: loss: 3938.025
iteration 1700: loss: 3939.735
iteration 1800: loss: 3944.801
====> Epoch: 014 Train loss: 3940.8081  took : 53.0848114490509
====> Test loss: 3943.7558
iteration 0000: loss: 3937.721
iteration 0100: loss: 3939.181
iteration 0200: loss: 3947.989
iteration 0300: loss: 3943.316
iteration 0400: loss: 3939.078
iteration 0500: loss: 3943.386
iteration 0600: loss: 3939.900
iteration 0700: loss: 3935.761
iteration 0800: loss: 3943.877
iteration 0900: loss: 3939.992
iteration 1000: loss: 3937.711
iteration 1100: loss: 3935.055
iteration 1200: loss: 3936.278
iteration 1300: loss: 3940.194
iteration 1400: loss: 3942.262
iteration 1500: loss: 3939.963
iteration 1600: loss: 3938.829
iteration 1700: loss: 3944.626
iteration 1800: loss: 3943.825
====> Epoch: 015 Train loss: 3940.5475  took : 52.8911406993866
====> Test loss: 3943.9685
iteration 0000: loss: 3940.797
iteration 0100: loss: 3938.271
iteration 0200: loss: 3939.305
iteration 0300: loss: 3942.743
iteration 0400: loss: 3934.091
iteration 0500: loss: 3946.564
iteration 0600: loss: 3938.605
iteration 0700: loss: 3934.665
iteration 0800: loss: 3947.425
iteration 0900: loss: 3937.228
iteration 1000: loss: 3938.790
iteration 1100: loss: 3936.917
iteration 1200: loss: 3936.369
iteration 1300: loss: 3941.136
iteration 1400: loss: 3930.180
iteration 1500: loss: 3940.366
iteration 1600: loss: 3936.845
iteration 1700: loss: 3940.650
iteration 1800: loss: 3946.729
====> Epoch: 016 Train loss: 3940.6241  took : 52.97433114051819
====> Test loss: 3944.0632
iteration 0000: loss: 3942.299
iteration 0100: loss: 3941.913
iteration 0200: loss: 3936.678
iteration 0300: loss: 3937.445
iteration 0400: loss: 3943.125
iteration 0500: loss: 3940.480
iteration 0600: loss: 3947.033
iteration 0700: loss: 3943.793
iteration 0800: loss: 3942.757
iteration 0900: loss: 3946.173
iteration 1000: loss: 3938.675
iteration 1100: loss: 3939.910
iteration 1200: loss: 3938.232
iteration 1300: loss: 3942.285
iteration 1400: loss: 3937.852
iteration 1500: loss: 3937.348
iteration 1600: loss: 3942.369
iteration 1700: loss: 3939.330
iteration 1800: loss: 3937.427
====> Epoch: 017 Train loss: 3940.3606  took : 52.98212003707886
====> Test loss: 3944.1364
iteration 0000: loss: 3940.860
iteration 0100: loss: 3939.919
iteration 0200: loss: 3941.922
iteration 0300: loss: 3934.412
iteration 0400: loss: 3940.503
iteration 0500: loss: 3940.572
iteration 0600: loss: 3938.617
iteration 0700: loss: 3936.190
iteration 0800: loss: 3934.950
iteration 0900: loss: 3942.632
iteration 1000: loss: 3942.034
iteration 1100: loss: 3940.657
iteration 1200: loss: 3939.134
iteration 1300: loss: 3940.656
iteration 1400: loss: 3941.578
iteration 1500: loss: 3942.895
iteration 1600: loss: 3942.259
iteration 1700: loss: 3936.849
iteration 1800: loss: 3944.557
====> Epoch: 018 Train loss: 3940.1665  took : 52.930007219314575
====> Test loss: 3944.0541
iteration 0000: loss: 3940.814
iteration 0100: loss: 3934.429
iteration 0200: loss: 3943.685
iteration 0300: loss: 3935.594
iteration 0400: loss: 3935.551
iteration 0500: loss: 3937.117
iteration 0600: loss: 3939.083
iteration 0700: loss: 3935.584
iteration 0800: loss: 3934.642
iteration 0900: loss: 3933.920
iteration 1000: loss: 3937.354
iteration 1100: loss: 3936.817
iteration 1200: loss: 3934.296
iteration 1300: loss: 3937.399
iteration 1400: loss: 3940.354
iteration 1500: loss: 3939.864
iteration 1600: loss: 3940.569
iteration 1700: loss: 3939.463
iteration 1800: loss: 3939.580
====> Epoch: 019 Train loss: 3939.9234  took : 52.94718885421753
====> Test loss: 3943.4733
iteration 0000: loss: 3942.988
iteration 0100: loss: 3939.869
iteration 0200: loss: 3934.130
iteration 0300: loss: 3938.196
iteration 0400: loss: 3940.229
iteration 0500: loss: 3941.255
iteration 0600: loss: 3942.422
iteration 0700: loss: 3940.763
iteration 0800: loss: 3939.433
iteration 0900: loss: 3948.392
iteration 1000: loss: 3939.299
iteration 1100: loss: 3941.685
iteration 1200: loss: 3939.245
iteration 1300: loss: 3932.803
iteration 1400: loss: 3938.225
iteration 1500: loss: 3936.136
iteration 1600: loss: 3941.676
iteration 1700: loss: 3935.697
iteration 1800: loss: 3934.967
====> Epoch: 020 Train loss: 3940.2931  took : 52.81326770782471
====> Test loss: 3943.1976
iteration 0000: loss: 3936.964
iteration 0100: loss: 3937.849
iteration 0200: loss: 3943.351
iteration 0300: loss: 3944.522
iteration 0400: loss: 3941.311
iteration 0500: loss: 3939.297
iteration 0600: loss: 3939.088
iteration 0700: loss: 3937.440
iteration 0800: loss: 3937.364
iteration 0900: loss: 3945.936
iteration 1000: loss: 3939.009
iteration 1100: loss: 3945.634
iteration 1200: loss: 3937.417
iteration 1300: loss: 3939.899
iteration 1400: loss: 3935.955
iteration 1500: loss: 3940.432
iteration 1600: loss: 3937.958
iteration 1700: loss: 3939.868
iteration 1800: loss: 3942.512
====> Epoch: 021 Train loss: 3939.7341  took : 52.935524702072144
====> Test loss: 3943.0423
iteration 0000: loss: 3941.572
iteration 0100: loss: 3939.333
iteration 0200: loss: 3932.818
iteration 0300: loss: 3938.621
iteration 0400: loss: 3941.694
iteration 0500: loss: 3934.647
iteration 0600: loss: 3938.146
iteration 0700: loss: 3936.511
iteration 0800: loss: 3940.244
iteration 0900: loss: 3934.503
iteration 1000: loss: 3939.233
iteration 1100: loss: 3940.808
iteration 1200: loss: 3941.326
iteration 1300: loss: 3934.628
iteration 1400: loss: 3937.380
iteration 1500: loss: 3942.377
iteration 1600: loss: 3945.694
iteration 1700: loss: 3943.038
iteration 1800: loss: 3939.624
====> Epoch: 022 Train loss: 3939.5182  took : 53.029454469680786
====> Test loss: 3944.1131
iteration 0000: loss: 3941.703
iteration 0100: loss: 3935.605
iteration 0200: loss: 3937.649
iteration 0300: loss: 3947.116
iteration 0400: loss: 3939.457
iteration 0500: loss: 3940.627
iteration 0600: loss: 3938.535
iteration 0700: loss: 3935.983
iteration 0800: loss: 3931.843
iteration 0900: loss: 3937.157
iteration 1000: loss: 3944.271
iteration 1100: loss: 3935.479
iteration 1200: loss: 3940.015
iteration 1300: loss: 3930.169
iteration 1400: loss: 3935.091
iteration 1500: loss: 3936.462
iteration 1600: loss: 3931.970
iteration 1700: loss: 3943.562
iteration 1800: loss: 3937.734
====> Epoch: 023 Train loss: 3939.2666  took : 53.025282859802246
====> Test loss: 3942.8096
iteration 0000: loss: 3938.103
iteration 0100: loss: 3935.075
iteration 0200: loss: 3944.321
iteration 0300: loss: 3937.749
iteration 0400: loss: 3944.356
iteration 0500: loss: 3937.584
iteration 0600: loss: 3939.114
iteration 0700: loss: 3943.874
iteration 0800: loss: 3933.843
iteration 0900: loss: 3940.339
iteration 1000: loss: 3939.320
iteration 1100: loss: 3941.839
iteration 1200: loss: 3936.503
iteration 1300: loss: 3938.306
iteration 1400: loss: 3947.089
iteration 1500: loss: 3940.701
iteration 1600: loss: 3942.425
iteration 1700: loss: 3930.890
iteration 1800: loss: 3936.862
====> Epoch: 024 Train loss: 3939.0348  took : 52.8917293548584
====> Test loss: 3943.4278
iteration 0000: loss: 3939.393
iteration 0100: loss: 3945.580
iteration 0200: loss: 3940.675
iteration 0300: loss: 3939.186
iteration 0400: loss: 3937.427
iteration 0500: loss: 3939.792
iteration 0600: loss: 3943.468
iteration 0700: loss: 3941.576
iteration 0800: loss: 3936.419
iteration 0900: loss: 3940.806
iteration 1000: loss: 3946.592
iteration 1100: loss: 3933.292
iteration 1200: loss: 3937.212
iteration 1300: loss: 3933.819
iteration 1400: loss: 3940.642
iteration 1500: loss: 3934.501
iteration 1600: loss: 3940.966
iteration 1700: loss: 3931.990
iteration 1800: loss: 3944.405
====> Epoch: 025 Train loss: 3938.8709  took : 52.94379234313965
====> Test loss: 3942.4771
iteration 0000: loss: 3937.189
iteration 0100: loss: 3939.902
iteration 0200: loss: 3942.121
iteration 0300: loss: 3945.001
iteration 0400: loss: 3943.003
iteration 0500: loss: 3940.105
iteration 0600: loss: 3941.030
iteration 0700: loss: 3939.456
iteration 0800: loss: 3941.368
iteration 0900: loss: 3935.218
iteration 1000: loss: 3942.379
iteration 1100: loss: 3939.477
iteration 1200: loss: 3939.045
iteration 1300: loss: 3939.174
iteration 1400: loss: 3950.846
iteration 1500: loss: 3934.769
iteration 1600: loss: 3938.934
iteration 1700: loss: 3941.741
iteration 1800: loss: 3940.941
====> Epoch: 026 Train loss: 3938.8150  took : 52.791449546813965
====> Test loss: 3942.9788
iteration 0000: loss: 3935.865
iteration 0100: loss: 3935.756
iteration 0200: loss: 3938.941
iteration 0300: loss: 3940.497
iteration 0400: loss: 3936.469
iteration 0500: loss: 3931.955
iteration 0600: loss: 3936.020
iteration 0700: loss: 3940.288
iteration 0800: loss: 3935.565
iteration 0900: loss: 3936.493
iteration 1000: loss: 3937.302
iteration 1100: loss: 3940.081
iteration 1200: loss: 3937.934
iteration 1300: loss: 3936.894
iteration 1400: loss: 3939.694
iteration 1500: loss: 3937.217
iteration 1600: loss: 3945.762
iteration 1700: loss: 3938.092
iteration 1800: loss: 3941.517
====> Epoch: 027 Train loss: 3938.9691  took : 52.948442459106445
====> Test loss: 3943.0602
iteration 0000: loss: 3939.534
iteration 0100: loss: 3939.298
iteration 0200: loss: 3936.288
iteration 0300: loss: 3938.802
iteration 0400: loss: 3935.035
iteration 0500: loss: 3939.797
iteration 0600: loss: 3933.658
iteration 0700: loss: 3941.174
iteration 0800: loss: 3936.388
iteration 0900: loss: 3942.833
iteration 1000: loss: 3935.005
iteration 1100: loss: 3939.693
iteration 1200: loss: 3939.402
iteration 1300: loss: 3937.820
iteration 1400: loss: 3939.392
iteration 1500: loss: 3943.606
iteration 1600: loss: 3941.552
iteration 1700: loss: 3939.017
iteration 1800: loss: 3939.880
====> Epoch: 028 Train loss: 3939.0706  took : 52.839887619018555
====> Test loss: 3942.4304
iteration 0000: loss: 3934.190
iteration 0100: loss: 3938.049
iteration 0200: loss: 3940.201
iteration 0300: loss: 3942.597
iteration 0400: loss: 3939.260
iteration 0500: loss: 3938.106
iteration 0600: loss: 3942.886
iteration 0700: loss: 3943.063
iteration 0800: loss: 3937.922
iteration 0900: loss: 3937.321
iteration 1000: loss: 3936.076
iteration 1100: loss: 3938.203
iteration 1200: loss: 3939.531
iteration 1300: loss: 3937.298
iteration 1400: loss: 3938.134
iteration 1500: loss: 3937.499
iteration 1600: loss: 3935.292
iteration 1700: loss: 3944.897
iteration 1800: loss: 3936.767
====> Epoch: 029 Train loss: 3938.8300  took : 52.926809310913086
====> Test loss: 3942.2516
iteration 0000: loss: 3937.224
iteration 0100: loss: 3932.443
iteration 0200: loss: 3940.674
iteration 0300: loss: 3936.049
iteration 0400: loss: 3937.004
iteration 0500: loss: 3944.561
iteration 0600: loss: 3939.412
iteration 0700: loss: 3938.091
iteration 0800: loss: 3936.582
iteration 0900: loss: 3943.348
iteration 1000: loss: 3939.879
iteration 1100: loss: 3944.860
iteration 1200: loss: 3937.277
iteration 1300: loss: 3943.060
iteration 1400: loss: 3942.493
iteration 1500: loss: 3940.680
iteration 1600: loss: 3936.147
iteration 1700: loss: 3937.773
iteration 1800: loss: 3938.639
====> Epoch: 030 Train loss: 3938.6649  took : 52.8945746421814
====> Test loss: 3942.6998
iteration 0000: loss: 3936.282
iteration 0100: loss: 3941.622
iteration 0200: loss: 3941.104
iteration 0300: loss: 3937.019
iteration 0400: loss: 3940.102
iteration 0500: loss: 3932.479
iteration 0600: loss: 3934.450
iteration 0700: loss: 3945.347
iteration 0800: loss: 3940.609
iteration 0900: loss: 3935.617
iteration 1000: loss: 3935.329
iteration 1100: loss: 3936.207
iteration 1200: loss: 3937.377
iteration 1300: loss: 3940.569
iteration 1400: loss: 3938.010
iteration 1500: loss: 3940.480
iteration 1600: loss: 3941.423
iteration 1700: loss: 3933.637
iteration 1800: loss: 3932.694
====> Epoch: 031 Train loss: 3938.5899  took : 52.8943817615509
====> Test loss: 3942.5591
iteration 0000: loss: 3938.223
iteration 0100: loss: 3934.587
iteration 0200: loss: 3940.077
iteration 0300: loss: 3939.552
iteration 0400: loss: 3930.418
iteration 0500: loss: 3938.068
iteration 0600: loss: 3935.710
iteration 0700: loss: 3939.693
iteration 0800: loss: 3940.312
iteration 0900: loss: 3936.207
iteration 1000: loss: 3937.858
iteration 1100: loss: 3931.483
iteration 1200: loss: 3938.097
iteration 1300: loss: 3943.196
iteration 1400: loss: 3941.399
iteration 1500: loss: 3939.118
iteration 1600: loss: 3937.755
iteration 1700: loss: 3938.178
iteration 1800: loss: 3938.222
====> Epoch: 032 Train loss: 3938.4628  took : 53.010568618774414
====> Test loss: 3942.1841
iteration 0000: loss: 3937.990
iteration 0100: loss: 3937.860
iteration 0200: loss: 3939.942
iteration 0300: loss: 3941.979
iteration 0400: loss: 3942.258
iteration 0500: loss: 3935.833
iteration 0600: loss: 3933.634
iteration 0700: loss: 3932.243
iteration 0800: loss: 3938.537
iteration 0900: loss: 3942.165
iteration 1000: loss: 3942.750
iteration 1100: loss: 3935.784
iteration 1200: loss: 3938.729
iteration 1300: loss: 3939.970
iteration 1400: loss: 3940.218
iteration 1500: loss: 3938.720
iteration 1600: loss: 3938.960
iteration 1700: loss: 3941.587
iteration 1800: loss: 3933.564
====> Epoch: 033 Train loss: 3938.4894  took : 53.02068018913269
====> Test loss: 3942.2886
iteration 0000: loss: 3936.680
iteration 0100: loss: 3943.245
iteration 0200: loss: 3938.247
iteration 0300: loss: 3935.696
iteration 0400: loss: 3941.571
iteration 0500: loss: 3937.967
iteration 0600: loss: 3941.645
iteration 0700: loss: 3940.521
iteration 0800: loss: 3938.036
iteration 0900: loss: 3939.071
iteration 1000: loss: 3941.705
iteration 1100: loss: 3946.752
iteration 1200: loss: 3936.635
iteration 1300: loss: 3936.408
iteration 1400: loss: 3941.352
iteration 1500: loss: 3937.050
iteration 1600: loss: 3936.417
iteration 1700: loss: 3936.156
iteration 1800: loss: 3938.950
====> Epoch: 034 Train loss: 3938.3303  took : 52.76138210296631
====> Test loss: 3942.2647
iteration 0000: loss: 3940.144
iteration 0100: loss: 3937.363
iteration 0200: loss: 3936.529
iteration 0300: loss: 3937.150
iteration 0400: loss: 3934.946
iteration 0500: loss: 3939.067
iteration 0600: loss: 3934.684
iteration 0700: loss: 3941.132
iteration 0800: loss: 3939.078
iteration 0900: loss: 3936.157
iteration 1000: loss: 3939.132
iteration 1100: loss: 3948.879
iteration 1200: loss: 3938.855
iteration 1300: loss: 3934.658
iteration 1400: loss: 3935.696
iteration 1500: loss: 3935.031
iteration 1600: loss: 3940.571
iteration 1700: loss: 3939.155
iteration 1800: loss: 3944.689
====> Epoch: 035 Train loss: 3938.3099  took : 52.80331516265869
====> Test loss: 3942.3847
iteration 0000: loss: 3945.802
iteration 0100: loss: 3937.365
iteration 0200: loss: 3939.568
iteration 0300: loss: 3941.653
iteration 0400: loss: 3942.324
iteration 0500: loss: 3937.226
iteration 0600: loss: 3939.272
iteration 0700: loss: 3938.490
iteration 0800: loss: 3935.485
iteration 0900: loss: 3937.520
iteration 1000: loss: 3934.389
iteration 1100: loss: 3939.732
iteration 1200: loss: 3943.479
iteration 1300: loss: 3943.937
iteration 1400: loss: 3937.179
iteration 1500: loss: 3935.397
iteration 1600: loss: 3942.276
iteration 1700: loss: 3938.158
iteration 1800: loss: 3937.815
====> Epoch: 036 Train loss: 3938.3662  took : 53.018697023391724
====> Test loss: 3942.0380
iteration 0000: loss: 3940.359
iteration 0100: loss: 3940.340
iteration 0200: loss: 3935.061
iteration 0300: loss: 3942.412
iteration 0400: loss: 3938.597
iteration 0500: loss: 3934.771
iteration 0600: loss: 3944.875
iteration 0700: loss: 3935.894
iteration 0800: loss: 3939.633
iteration 0900: loss: 3941.532
iteration 1000: loss: 3939.376
iteration 1100: loss: 3939.968
iteration 1200: loss: 3943.323
iteration 1300: loss: 3939.599
iteration 1400: loss: 3941.282
iteration 1500: loss: 3941.417
iteration 1600: loss: 3936.984
iteration 1700: loss: 3937.022
iteration 1800: loss: 3935.056
====> Epoch: 037 Train loss: 3938.2895  took : 53.07626152038574
====> Test loss: 3942.4284
iteration 0000: loss: 3934.512
iteration 0100: loss: 3934.815
iteration 0200: loss: 3936.252
iteration 0300: loss: 3937.870
iteration 0400: loss: 3937.664
iteration 0500: loss: 3936.196
iteration 0600: loss: 3941.356
iteration 0700: loss: 3941.009
iteration 0800: loss: 3943.142
iteration 0900: loss: 3933.104
iteration 1000: loss: 3935.296
iteration 1100: loss: 3944.604
iteration 1200: loss: 3945.007
iteration 1300: loss: 3938.586
iteration 1400: loss: 3932.940
iteration 1500: loss: 3939.218
iteration 1600: loss: 3938.510
iteration 1700: loss: 3933.614
iteration 1800: loss: 3936.845
====> Epoch: 038 Train loss: 3938.2777  took : 53.225597858428955
====> Test loss: 3942.1242
iteration 0000: loss: 3947.272
iteration 0100: loss: 3928.886
iteration 0200: loss: 3939.992
iteration 0300: loss: 3941.760
iteration 0400: loss: 3941.277
iteration 0500: loss: 3942.463
iteration 0600: loss: 3941.091
iteration 0700: loss: 3933.672
iteration 0800: loss: 3939.190
iteration 0900: loss: 3937.087
iteration 1000: loss: 3942.210
iteration 1100: loss: 3935.427
iteration 1200: loss: 3941.064
iteration 1300: loss: 3936.746
iteration 1400: loss: 3942.127
iteration 1500: loss: 3942.645
iteration 1600: loss: 3934.120
iteration 1700: loss: 3944.294
iteration 1800: loss: 3941.655
====> Epoch: 039 Train loss: 3938.2361  took : 52.89512825012207
====> Test loss: 3942.3292
iteration 0000: loss: 3934.714
iteration 0100: loss: 3942.513
iteration 0200: loss: 3935.387
iteration 0300: loss: 3939.451
iteration 0400: loss: 3931.821
iteration 0500: loss: 3935.697
iteration 0600: loss: 3941.350
iteration 0700: loss: 3940.826
iteration 0800: loss: 3934.455
iteration 0900: loss: 3938.426
iteration 1000: loss: 3938.888
iteration 1100: loss: 3935.614
iteration 1200: loss: 3937.467
iteration 1300: loss: 3934.590
iteration 1400: loss: 3936.766
iteration 1500: loss: 3934.489
iteration 1600: loss: 3941.826
iteration 1700: loss: 3941.276
iteration 1800: loss: 3937.655
====> Epoch: 040 Train loss: 3938.1118  took : 52.95572519302368
====> Test loss: 3942.8836
iteration 0000: loss: 3941.456
iteration 0100: loss: 3945.661
iteration 0200: loss: 3941.343
iteration 0300: loss: 3944.307
iteration 0400: loss: 3931.836
iteration 0500: loss: 3935.444
iteration 0600: loss: 3932.784
iteration 0700: loss: 3934.950
iteration 0800: loss: 3933.513
iteration 0900: loss: 3937.418
iteration 1000: loss: 3937.263
iteration 1100: loss: 3935.264
iteration 1200: loss: 3938.130
iteration 1300: loss: 3945.604
iteration 1400: loss: 3936.452
iteration 1500: loss: 3942.327
iteration 1600: loss: 3945.982
iteration 1700: loss: 3944.727
iteration 1800: loss: 3939.173
====> Epoch: 041 Train loss: 3938.2632  took : 52.90552282333374
====> Test loss: 3942.2278
iteration 0000: loss: 3937.474
iteration 0100: loss: 3931.249
iteration 0200: loss: 3942.233
iteration 0300: loss: 3940.349
iteration 0400: loss: 3933.748
iteration 0500: loss: 3942.717
iteration 0600: loss: 3944.712
iteration 0700: loss: 3937.637
iteration 0800: loss: 3936.777
iteration 0900: loss: 3933.859
iteration 1000: loss: 3940.224
iteration 1100: loss: 3932.280
iteration 1200: loss: 3935.372
iteration 1300: loss: 3936.911
iteration 1400: loss: 3939.209
iteration 1500: loss: 3932.403
iteration 1600: loss: 3939.505
iteration 1700: loss: 3933.126
iteration 1800: loss: 3935.417
====> Epoch: 042 Train loss: 3938.0827  took : 53.079548358917236
====> Test loss: 3942.1834
iteration 0000: loss: 3940.620
iteration 0100: loss: 3940.289
iteration 0200: loss: 3932.132
iteration 0300: loss: 3938.218
iteration 0400: loss: 3935.643
iteration 0500: loss: 3935.592
iteration 0600: loss: 3940.046
iteration 0700: loss: 3935.845
iteration 0800: loss: 3946.361
iteration 0900: loss: 3935.358
iteration 1000: loss: 3932.595
iteration 1100: loss: 3930.200
iteration 1200: loss: 3937.993
iteration 1300: loss: 3934.234
iteration 1400: loss: 3935.646
iteration 1500: loss: 3937.236
iteration 1600: loss: 3944.825
iteration 1700: loss: 3935.427
iteration 1800: loss: 3936.567
====> Epoch: 043 Train loss: 3938.3926  took : 52.87197279930115
====> Test loss: 3942.1430
iteration 0000: loss: 3931.209
iteration 0100: loss: 3944.865
iteration 0200: loss: 3938.169
iteration 0300: loss: 3938.888
iteration 0400: loss: 3933.759
iteration 0500: loss: 3940.627
iteration 0600: loss: 3938.786
iteration 0700: loss: 3937.110
iteration 0800: loss: 3936.919
iteration 0900: loss: 3940.917
iteration 1000: loss: 3941.069
iteration 1100: loss: 3934.809
iteration 1200: loss: 3935.833
iteration 1300: loss: 3939.908
iteration 1400: loss: 3937.239
iteration 1500: loss: 3949.553
iteration 1600: loss: 3937.706
iteration 1700: loss: 3945.377
iteration 1800: loss: 3935.254
====> Epoch: 044 Train loss: 3938.0432  took : 52.8596510887146
====> Test loss: 3941.9530
iteration 0000: loss: 3932.691
iteration 0100: loss: 3936.804
iteration 0200: loss: 3938.199
iteration 0300: loss: 3934.497
iteration 0400: loss: 3941.202
iteration 0500: loss: 3939.385
iteration 0600: loss: 3938.519
iteration 0700: loss: 3933.381
iteration 0800: loss: 3943.443
iteration 0900: loss: 3935.334
iteration 1000: loss: 3940.312
iteration 1100: loss: 3943.559
iteration 1200: loss: 3937.219
iteration 1300: loss: 3939.790
iteration 1400: loss: 3934.422
iteration 1500: loss: 3938.701
iteration 1600: loss: 3934.756
iteration 1700: loss: 3933.982
iteration 1800: loss: 3936.869
====> Epoch: 045 Train loss: 3937.8735  took : 53.100449562072754
====> Test loss: 3942.8384
iteration 0000: loss: 3941.771
iteration 0100: loss: 3938.678
iteration 0200: loss: 3943.663
iteration 0300: loss: 3935.868
iteration 0400: loss: 3943.603
iteration 0500: loss: 3936.503
iteration 0600: loss: 3938.647
iteration 0700: loss: 3938.095
iteration 0800: loss: 3935.690
iteration 0900: loss: 3939.360
iteration 1000: loss: 3933.476
iteration 1100: loss: 3938.454
iteration 1200: loss: 3936.916
iteration 1300: loss: 3935.637
iteration 1400: loss: 3933.364
iteration 1500: loss: 3931.242
iteration 1600: loss: 3936.991
iteration 1700: loss: 3940.728
iteration 1800: loss: 3933.893
====> Epoch: 046 Train loss: 3937.8113  took : 53.10295367240906
====> Test loss: 3942.1305
iteration 0000: loss: 3942.505
iteration 0100: loss: 3941.451
iteration 0200: loss: 3937.197
iteration 0300: loss: 3938.674
iteration 0400: loss: 3935.971
iteration 0500: loss: 3934.591
iteration 0600: loss: 3942.143
iteration 0700: loss: 3934.588
iteration 0800: loss: 3937.470
iteration 0900: loss: 3933.830
iteration 1000: loss: 3934.846
iteration 1100: loss: 3932.598
iteration 1200: loss: 3938.350
iteration 1300: loss: 3937.312
iteration 1400: loss: 3937.728
iteration 1500: loss: 3936.897
iteration 1600: loss: 3930.907
iteration 1700: loss: 3940.547
iteration 1800: loss: 3936.561
====> Epoch: 047 Train loss: 3937.9922  took : 53.010879039764404
====> Test loss: 3941.8319
iteration 0000: loss: 3938.507
iteration 0100: loss: 3942.546
iteration 0200: loss: 3936.953
iteration 0300: loss: 3935.765
iteration 0400: loss: 3932.484
iteration 0500: loss: 3934.095
iteration 0600: loss: 3933.636
iteration 0700: loss: 3940.518
iteration 0800: loss: 3942.497
iteration 0900: loss: 3939.860
iteration 1000: loss: 3935.391
iteration 1100: loss: 3936.224
iteration 1200: loss: 3937.256
iteration 1300: loss: 3938.639
iteration 1400: loss: 3935.178
iteration 1500: loss: 3937.987
iteration 1600: loss: 3936.262
iteration 1700: loss: 3932.969
iteration 1800: loss: 3932.906
====> Epoch: 048 Train loss: 3937.8082  took : 53.10018515586853
====> Test loss: 3941.8034
iteration 0000: loss: 3937.946
iteration 0100: loss: 3941.036
iteration 0200: loss: 3930.938
iteration 0300: loss: 3940.736
iteration 0400: loss: 3933.742
iteration 0500: loss: 3937.557
iteration 0600: loss: 3941.813
iteration 0700: loss: 3952.502
iteration 0800: loss: 3942.796
iteration 0900: loss: 3942.980
iteration 1000: loss: 3934.327
iteration 1100: loss: 3931.124
iteration 1200: loss: 3937.384
iteration 1300: loss: 3931.131
iteration 1400: loss: 3933.508
iteration 1500: loss: 3945.624
iteration 1600: loss: 3935.381
iteration 1700: loss: 3944.967
iteration 1800: loss: 3942.604
====> Epoch: 049 Train loss: 3937.8713  took : 53.033926486968994
====> Test loss: 3942.5675
iteration 0000: loss: 3942.025
iteration 0100: loss: 3943.102
iteration 0200: loss: 3940.423
iteration 0300: loss: 3942.342
iteration 0400: loss: 3937.456
iteration 0500: loss: 3939.549
iteration 0600: loss: 3939.799
iteration 0700: loss: 3941.697
iteration 0800: loss: 3935.969
iteration 0900: loss: 3943.362
iteration 1000: loss: 3939.577
iteration 1100: loss: 3937.826
iteration 1200: loss: 3942.803
iteration 1300: loss: 3935.338
iteration 1400: loss: 3933.868
iteration 1500: loss: 3939.251
iteration 1600: loss: 3937.184
iteration 1700: loss: 3933.635
iteration 1800: loss: 3936.161
====> Epoch: 050 Train loss: 3937.7826  took : 52.86021375656128
====> Test loss: 3941.7601
====> [MM-VAE] Time: 3149.744s or 00:52:29
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  1
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_1
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1989.124
iteration 0100: loss: 1576.272
iteration 0200: loss: 1564.272
iteration 0300: loss: 1560.177
iteration 0400: loss: 1546.262
iteration 0500: loss: 1550.490
iteration 0600: loss: 1552.039
iteration 0700: loss: 1546.338
iteration 0800: loss: 1541.201
iteration 0900: loss: 1538.966
====> Epoch: 001 Train loss: 1556.1164  took : 8.480418682098389
====> Test loss: 1539.4519
iteration 0000: loss: 1540.708
iteration 0100: loss: 1539.591
iteration 0200: loss: 1532.576
iteration 0300: loss: 1529.936
iteration 0400: loss: 1531.625
iteration 0500: loss: 1526.469
iteration 0600: loss: 1528.813
iteration 0700: loss: 1531.827
iteration 0800: loss: 1527.556
iteration 0900: loss: 1532.914
====> Epoch: 002 Train loss: 1532.8780  took : 8.479758024215698
====> Test loss: 1530.4001
iteration 0000: loss: 1533.171
iteration 0100: loss: 1529.384
iteration 0200: loss: 1527.485
iteration 0300: loss: 1525.365
iteration 0400: loss: 1522.352
iteration 0500: loss: 1524.412
iteration 0600: loss: 1524.390
iteration 0700: loss: 1522.956
iteration 0800: loss: 1527.064
iteration 0900: loss: 1524.207
====> Epoch: 003 Train loss: 1525.9610  took : 8.483189344406128
====> Test loss: 1526.0434
iteration 0000: loss: 1524.039
iteration 0100: loss: 1523.503
iteration 0200: loss: 1523.720
iteration 0300: loss: 1523.078
iteration 0400: loss: 1524.127
iteration 0500: loss: 1521.606
iteration 0600: loss: 1526.009
iteration 0700: loss: 1520.981
iteration 0800: loss: 1520.010
iteration 0900: loss: 1520.845
====> Epoch: 004 Train loss: 1522.9699  took : 8.488368511199951
====> Test loss: 1524.3297
iteration 0000: loss: 1523.513
iteration 0100: loss: 1522.698
iteration 0200: loss: 1518.484
iteration 0300: loss: 1520.884
iteration 0400: loss: 1525.428
iteration 0500: loss: 1519.526
iteration 0600: loss: 1525.093
iteration 0700: loss: 1519.806
iteration 0800: loss: 1522.559
iteration 0900: loss: 1520.713
====> Epoch: 005 Train loss: 1521.2413  took : 8.441996097564697
====> Test loss: 1523.0357
iteration 0000: loss: 1520.094
iteration 0100: loss: 1519.420
iteration 0200: loss: 1519.859
iteration 0300: loss: 1519.179
iteration 0400: loss: 1519.139
iteration 0500: loss: 1521.105
iteration 0600: loss: 1520.898
iteration 0700: loss: 1519.704
iteration 0800: loss: 1521.009
iteration 0900: loss: 1521.703
====> Epoch: 006 Train loss: 1520.0686  took : 8.447029113769531
====> Test loss: 1521.9488
iteration 0000: loss: 1519.571
iteration 0100: loss: 1519.195
iteration 0200: loss: 1518.126
iteration 0300: loss: 1518.187
iteration 0400: loss: 1520.174
iteration 0500: loss: 1519.523
iteration 0600: loss: 1518.257
iteration 0700: loss: 1518.206
iteration 0800: loss: 1519.267
iteration 0900: loss: 1519.641
====> Epoch: 007 Train loss: 1519.1317  took : 8.454119205474854
====> Test loss: 1521.2919
iteration 0000: loss: 1521.886
iteration 0100: loss: 1521.681
iteration 0200: loss: 1518.717
iteration 0300: loss: 1515.511
iteration 0400: loss: 1522.182
iteration 0500: loss: 1521.546
iteration 0600: loss: 1517.426
iteration 0700: loss: 1515.867
iteration 0800: loss: 1514.841
iteration 0900: loss: 1513.395
====> Epoch: 008 Train loss: 1518.5097  took : 8.405582189559937
====> Test loss: 1520.9358
iteration 0000: loss: 1521.243
iteration 0100: loss: 1520.280
iteration 0200: loss: 1519.566
iteration 0300: loss: 1519.284
iteration 0400: loss: 1518.043
iteration 0500: loss: 1519.258
iteration 0600: loss: 1521.199
iteration 0700: loss: 1519.975
iteration 0800: loss: 1515.903
iteration 0900: loss: 1517.938
====> Epoch: 009 Train loss: 1518.0265  took : 8.548444509506226
====> Test loss: 1520.6080
iteration 0000: loss: 1516.528
iteration 0100: loss: 1518.071
iteration 0200: loss: 1516.477
iteration 0300: loss: 1517.391
iteration 0400: loss: 1518.648
iteration 0500: loss: 1516.879
iteration 0600: loss: 1516.556
iteration 0700: loss: 1519.922
iteration 0800: loss: 1516.903
iteration 0900: loss: 1519.215
====> Epoch: 010 Train loss: 1517.6289  took : 8.446655511856079
====> Test loss: 1520.2175
iteration 0000: loss: 1514.305
iteration 0100: loss: 1515.573
iteration 0200: loss: 1518.757
iteration 0300: loss: 1517.125
iteration 0400: loss: 1516.537
iteration 0500: loss: 1515.173
iteration 0600: loss: 1522.469
iteration 0700: loss: 1516.795
iteration 0800: loss: 1517.719
iteration 0900: loss: 1516.086
====> Epoch: 011 Train loss: 1517.3058  took : 8.405377864837646
====> Test loss: 1520.0646
iteration 0000: loss: 1515.837
iteration 0100: loss: 1514.906
iteration 0200: loss: 1519.294
iteration 0300: loss: 1516.044
iteration 0400: loss: 1518.431
iteration 0500: loss: 1517.832
iteration 0600: loss: 1517.603
iteration 0700: loss: 1518.082
iteration 0800: loss: 1516.583
iteration 0900: loss: 1514.984
====> Epoch: 012 Train loss: 1517.0608  took : 8.458520650863647
====> Test loss: 1519.7539
iteration 0000: loss: 1516.270
iteration 0100: loss: 1516.720
iteration 0200: loss: 1517.891
iteration 0300: loss: 1515.694
iteration 0400: loss: 1514.349
iteration 0500: loss: 1517.427
iteration 0600: loss: 1513.577
iteration 0700: loss: 1516.445
iteration 0800: loss: 1514.221
iteration 0900: loss: 1513.880
====> Epoch: 013 Train loss: 1516.7931  took : 8.426501750946045
====> Test loss: 1519.6235
iteration 0000: loss: 1516.893
iteration 0100: loss: 1517.422
iteration 0200: loss: 1515.268
iteration 0300: loss: 1515.497
iteration 0400: loss: 1515.996
iteration 0500: loss: 1518.639
iteration 0600: loss: 1515.854
iteration 0700: loss: 1515.221
iteration 0800: loss: 1516.270
iteration 0900: loss: 1521.174
====> Epoch: 014 Train loss: 1516.5934  took : 8.430580854415894
====> Test loss: 1519.5486
iteration 0000: loss: 1517.161
iteration 0100: loss: 1514.893
iteration 0200: loss: 1517.217
iteration 0300: loss: 1514.862
iteration 0400: loss: 1516.068
iteration 0500: loss: 1516.521
iteration 0600: loss: 1518.392
iteration 0700: loss: 1512.428
iteration 0800: loss: 1517.272
iteration 0900: loss: 1514.211
====> Epoch: 015 Train loss: 1516.4488  took : 8.487274169921875
====> Test loss: 1519.4045
iteration 0000: loss: 1515.911
iteration 0100: loss: 1517.613
iteration 0200: loss: 1516.731
iteration 0300: loss: 1514.948
iteration 0400: loss: 1517.493
iteration 0500: loss: 1514.885
iteration 0600: loss: 1515.144
iteration 0700: loss: 1517.139
iteration 0800: loss: 1515.628
iteration 0900: loss: 1517.269
====> Epoch: 016 Train loss: 1516.2315  took : 8.461034536361694
====> Test loss: 1519.2802
iteration 0000: loss: 1515.571
iteration 0100: loss: 1515.722
iteration 0200: loss: 1518.430
iteration 0300: loss: 1514.716
iteration 0400: loss: 1515.393
iteration 0500: loss: 1517.258
iteration 0600: loss: 1513.125
iteration 0700: loss: 1513.416
iteration 0800: loss: 1519.145
iteration 0900: loss: 1515.121
====> Epoch: 017 Train loss: 1516.0623  took : 8.48137879371643
====> Test loss: 1519.1547
iteration 0000: loss: 1516.320
iteration 0100: loss: 1513.109
iteration 0200: loss: 1517.129
iteration 0300: loss: 1516.427
iteration 0400: loss: 1517.677
iteration 0500: loss: 1515.868
iteration 0600: loss: 1516.983
iteration 0700: loss: 1514.229
iteration 0800: loss: 1517.545
iteration 0900: loss: 1515.990
====> Epoch: 018 Train loss: 1515.9265  took : 8.421421527862549
====> Test loss: 1519.0450
iteration 0000: loss: 1517.839
iteration 0100: loss: 1513.896
iteration 0200: loss: 1516.485
iteration 0300: loss: 1514.213
iteration 0400: loss: 1517.254
iteration 0500: loss: 1514.635
iteration 0600: loss: 1515.609
iteration 0700: loss: 1516.812
iteration 0800: loss: 1516.630
iteration 0900: loss: 1515.195
====> Epoch: 019 Train loss: 1515.8002  took : 8.412840604782104
====> Test loss: 1518.9085
iteration 0000: loss: 1514.969
iteration 0100: loss: 1515.685
iteration 0200: loss: 1514.322
iteration 0300: loss: 1514.230
iteration 0400: loss: 1515.566
iteration 0500: loss: 1517.712
iteration 0600: loss: 1513.512
iteration 0700: loss: 1514.904
iteration 0800: loss: 1516.015
iteration 0900: loss: 1515.713
====> Epoch: 020 Train loss: 1515.6594  took : 8.509453535079956
====> Test loss: 1518.9083
iteration 0000: loss: 1514.309
iteration 0100: loss: 1515.042
iteration 0200: loss: 1514.528
iteration 0300: loss: 1515.046
iteration 0400: loss: 1514.360
iteration 0500: loss: 1514.271
iteration 0600: loss: 1516.109
iteration 0700: loss: 1514.999
iteration 0800: loss: 1516.830
iteration 0900: loss: 1518.400
====> Epoch: 021 Train loss: 1515.5945  took : 8.453309535980225
====> Test loss: 1518.8345
iteration 0000: loss: 1515.435
iteration 0100: loss: 1515.364
iteration 0200: loss: 1514.772
iteration 0300: loss: 1514.375
iteration 0400: loss: 1515.350
iteration 0500: loss: 1518.463
iteration 0600: loss: 1516.229
iteration 0700: loss: 1517.359
iteration 0800: loss: 1516.114
iteration 0900: loss: 1514.708
====> Epoch: 022 Train loss: 1515.4498  took : 8.389889240264893
====> Test loss: 1518.7508
iteration 0000: loss: 1517.176
iteration 0100: loss: 1514.468
iteration 0200: loss: 1510.846
iteration 0300: loss: 1514.888
iteration 0400: loss: 1516.760
iteration 0500: loss: 1514.504
iteration 0600: loss: 1513.762
iteration 0700: loss: 1516.911
iteration 0800: loss: 1516.657
iteration 0900: loss: 1515.292
====> Epoch: 023 Train loss: 1515.3711  took : 8.430239915847778
====> Test loss: 1518.6264
iteration 0000: loss: 1514.966
iteration 0100: loss: 1513.693
iteration 0200: loss: 1515.189
iteration 0300: loss: 1514.934
iteration 0400: loss: 1515.548
iteration 0500: loss: 1517.422
iteration 0600: loss: 1514.995
iteration 0700: loss: 1516.187
iteration 0800: loss: 1515.320
iteration 0900: loss: 1516.710
====> Epoch: 024 Train loss: 1515.2798  took : 8.510330438613892
====> Test loss: 1518.5907
iteration 0000: loss: 1512.421
iteration 0100: loss: 1515.327
iteration 0200: loss: 1514.955
iteration 0300: loss: 1514.586
iteration 0400: loss: 1515.291
iteration 0500: loss: 1516.827
iteration 0600: loss: 1515.977
iteration 0700: loss: 1513.734
iteration 0800: loss: 1514.995
iteration 0900: loss: 1516.062
====> Epoch: 025 Train loss: 1515.1990  took : 8.523123025894165
====> Test loss: 1518.5853
iteration 0000: loss: 1516.002
iteration 0100: loss: 1514.094
iteration 0200: loss: 1513.834
iteration 0300: loss: 1513.805
iteration 0400: loss: 1514.524
iteration 0500: loss: 1518.253
iteration 0600: loss: 1514.008
iteration 0700: loss: 1516.029
iteration 0800: loss: 1515.813
iteration 0900: loss: 1512.900
====> Epoch: 026 Train loss: 1515.1195  took : 8.550961971282959
====> Test loss: 1518.4131
iteration 0000: loss: 1513.984
iteration 0100: loss: 1514.718
iteration 0200: loss: 1513.548
iteration 0300: loss: 1515.402
iteration 0400: loss: 1516.495
iteration 0500: loss: 1514.760
iteration 0600: loss: 1513.476
iteration 0700: loss: 1514.106
iteration 0800: loss: 1515.401
iteration 0900: loss: 1512.857
====> Epoch: 027 Train loss: 1515.0206  took : 8.3893563747406
====> Test loss: 1518.3775
iteration 0000: loss: 1517.024
iteration 0100: loss: 1519.753
iteration 0200: loss: 1515.691
iteration 0300: loss: 1512.188
iteration 0400: loss: 1514.922
iteration 0500: loss: 1514.108
iteration 0600: loss: 1516.058
iteration 0700: loss: 1517.104
iteration 0800: loss: 1516.117
iteration 0900: loss: 1514.137
====> Epoch: 028 Train loss: 1514.9529  took : 8.521106004714966
====> Test loss: 1518.4824
iteration 0000: loss: 1519.087
iteration 0100: loss: 1515.941
iteration 0200: loss: 1513.179
iteration 0300: loss: 1516.073
iteration 0400: loss: 1513.580
iteration 0500: loss: 1516.390
iteration 0600: loss: 1515.599
iteration 0700: loss: 1516.714
iteration 0800: loss: 1514.917
iteration 0900: loss: 1514.957
====> Epoch: 029 Train loss: 1514.9020  took : 8.44820523262024
====> Test loss: 1518.3857
iteration 0000: loss: 1511.780
iteration 0100: loss: 1515.180
iteration 0200: loss: 1515.670
iteration 0300: loss: 1515.163
iteration 0400: loss: 1515.212
iteration 0500: loss: 1517.077
iteration 0600: loss: 1516.535
iteration 0700: loss: 1515.536
iteration 0800: loss: 1514.888
iteration 0900: loss: 1515.150
====> Epoch: 030 Train loss: 1514.8638  took : 8.476933002471924
====> Test loss: 1518.3229
iteration 0000: loss: 1515.183
iteration 0100: loss: 1513.485
iteration 0200: loss: 1514.974
iteration 0300: loss: 1515.043
iteration 0400: loss: 1515.526
iteration 0500: loss: 1515.302
iteration 0600: loss: 1516.684
iteration 0700: loss: 1514.044
iteration 0800: loss: 1513.487
iteration 0900: loss: 1518.624
====> Epoch: 031 Train loss: 1514.7653  took : 8.473068952560425
====> Test loss: 1518.2963
iteration 0000: loss: 1515.079
iteration 0100: loss: 1516.840
iteration 0200: loss: 1514.385
iteration 0300: loss: 1515.417
iteration 0400: loss: 1515.428
iteration 0500: loss: 1514.394
iteration 0600: loss: 1511.955
iteration 0700: loss: 1514.402
iteration 0800: loss: 1516.486
iteration 0900: loss: 1515.278
====> Epoch: 032 Train loss: 1514.7033  took : 8.446426153182983
====> Test loss: 1518.3716
iteration 0000: loss: 1513.198
iteration 0100: loss: 1515.630
iteration 0200: loss: 1518.045
iteration 0300: loss: 1513.350
iteration 0400: loss: 1518.763
iteration 0500: loss: 1514.938
iteration 0600: loss: 1514.630
iteration 0700: loss: 1517.667
iteration 0800: loss: 1516.372
iteration 0900: loss: 1513.279
====> Epoch: 033 Train loss: 1514.6064  took : 8.452598571777344
====> Test loss: 1518.2167
iteration 0000: loss: 1514.272
iteration 0100: loss: 1514.834
iteration 0200: loss: 1514.205
iteration 0300: loss: 1514.391
iteration 0400: loss: 1513.596
iteration 0500: loss: 1515.942
iteration 0600: loss: 1512.957
iteration 0700: loss: 1513.330
iteration 0800: loss: 1514.472
iteration 0900: loss: 1514.419
====> Epoch: 034 Train loss: 1514.5521  took : 8.408672571182251
====> Test loss: 1518.2697
iteration 0000: loss: 1516.716
iteration 0100: loss: 1515.166
iteration 0200: loss: 1515.743
iteration 0300: loss: 1515.158
iteration 0400: loss: 1511.388
iteration 0500: loss: 1514.184
iteration 0600: loss: 1513.620
iteration 0700: loss: 1513.649
iteration 0800: loss: 1514.518
iteration 0900: loss: 1514.818
====> Epoch: 035 Train loss: 1514.4806  took : 8.46772313117981
====> Test loss: 1518.3661
iteration 0000: loss: 1515.513
iteration 0100: loss: 1512.038
iteration 0200: loss: 1516.553
iteration 0300: loss: 1514.596
iteration 0400: loss: 1514.429
iteration 0500: loss: 1514.545
iteration 0600: loss: 1513.354
iteration 0700: loss: 1511.799
iteration 0800: loss: 1517.233
iteration 0900: loss: 1512.508
====> Epoch: 036 Train loss: 1514.4233  took : 8.462318181991577
====> Test loss: 1518.1749
iteration 0000: loss: 1515.127
iteration 0100: loss: 1514.750
iteration 0200: loss: 1516.581
iteration 0300: loss: 1515.935
iteration 0400: loss: 1513.170
iteration 0500: loss: 1515.693
iteration 0600: loss: 1515.329
iteration 0700: loss: 1514.312
iteration 0800: loss: 1515.537
iteration 0900: loss: 1514.827
====> Epoch: 037 Train loss: 1514.4072  took : 8.381040334701538
====> Test loss: 1518.0978
iteration 0000: loss: 1514.820
iteration 0100: loss: 1513.684
iteration 0200: loss: 1514.540
iteration 0300: loss: 1514.188
iteration 0400: loss: 1515.877
iteration 0500: loss: 1515.250
iteration 0600: loss: 1513.633
iteration 0700: loss: 1514.892
iteration 0800: loss: 1513.650
iteration 0900: loss: 1516.413
====> Epoch: 038 Train loss: 1514.3210  took : 8.46679162979126
====> Test loss: 1518.0580
iteration 0000: loss: 1513.874
iteration 0100: loss: 1513.290
iteration 0200: loss: 1514.734
iteration 0300: loss: 1514.466
iteration 0400: loss: 1515.510
iteration 0500: loss: 1515.969
iteration 0600: loss: 1515.187
iteration 0700: loss: 1513.869
iteration 0800: loss: 1515.921
iteration 0900: loss: 1515.974
====> Epoch: 039 Train loss: 1514.2596  took : 8.507706642150879
====> Test loss: 1518.0151
iteration 0000: loss: 1512.160
iteration 0100: loss: 1514.670
iteration 0200: loss: 1514.954
iteration 0300: loss: 1515.010
iteration 0400: loss: 1514.381
iteration 0500: loss: 1514.330
iteration 0600: loss: 1515.741
iteration 0700: loss: 1513.873
iteration 0800: loss: 1514.548
iteration 0900: loss: 1514.870
====> Epoch: 040 Train loss: 1514.2228  took : 8.411441564559937
====> Test loss: 1518.0441
iteration 0000: loss: 1514.684
iteration 0100: loss: 1515.588
iteration 0200: loss: 1514.215
iteration 0300: loss: 1512.846
iteration 0400: loss: 1514.035
iteration 0500: loss: 1514.362
iteration 0600: loss: 1512.624
iteration 0700: loss: 1511.331
iteration 0800: loss: 1514.135
iteration 0900: loss: 1515.092
====> Epoch: 041 Train loss: 1514.1701  took : 8.455116510391235
====> Test loss: 1518.0131
iteration 0000: loss: 1517.198
iteration 0100: loss: 1511.742
iteration 0200: loss: 1514.382
iteration 0300: loss: 1512.642
iteration 0400: loss: 1511.621
iteration 0500: loss: 1514.708
iteration 0600: loss: 1513.774
iteration 0700: loss: 1512.732
iteration 0800: loss: 1514.232
iteration 0900: loss: 1511.791
====> Epoch: 042 Train loss: 1514.1391  took : 8.446985006332397
====> Test loss: 1518.0290
iteration 0000: loss: 1515.211
iteration 0100: loss: 1513.508
iteration 0200: loss: 1512.899
iteration 0300: loss: 1517.231
iteration 0400: loss: 1513.630
iteration 0500: loss: 1514.600
iteration 0600: loss: 1515.518
iteration 0700: loss: 1513.683
iteration 0800: loss: 1515.154
iteration 0900: loss: 1514.972
====> Epoch: 043 Train loss: 1514.1260  took : 8.422966957092285
====> Test loss: 1518.0266
iteration 0000: loss: 1511.482
iteration 0100: loss: 1514.248
iteration 0200: loss: 1515.051
iteration 0300: loss: 1515.173
iteration 0400: loss: 1514.468
iteration 0500: loss: 1512.509
iteration 0600: loss: 1516.051
iteration 0700: loss: 1515.239
iteration 0800: loss: 1513.793
iteration 0900: loss: 1513.824
====> Epoch: 044 Train loss: 1514.1163  took : 8.513148546218872
====> Test loss: 1518.1109
iteration 0000: loss: 1514.703
iteration 0100: loss: 1514.776
iteration 0200: loss: 1513.549
iteration 0300: loss: 1514.000
iteration 0400: loss: 1513.764
iteration 0500: loss: 1513.834
iteration 0600: loss: 1516.093
iteration 0700: loss: 1517.306
iteration 0800: loss: 1513.540
iteration 0900: loss: 1514.502
====> Epoch: 045 Train loss: 1514.0013  took : 8.493530988693237
====> Test loss: 1517.9999
iteration 0000: loss: 1513.911
iteration 0100: loss: 1513.498
iteration 0200: loss: 1516.039
iteration 0300: loss: 1511.950
iteration 0400: loss: 1516.009
iteration 0500: loss: 1514.393
iteration 0600: loss: 1515.895
iteration 0700: loss: 1513.781
iteration 0800: loss: 1514.421
iteration 0900: loss: 1513.865
====> Epoch: 046 Train loss: 1513.9238  took : 8.416917324066162
====> Test loss: 1517.8617
iteration 0000: loss: 1516.928
iteration 0100: loss: 1515.330
iteration 0200: loss: 1512.991
iteration 0300: loss: 1514.761
iteration 0400: loss: 1515.713
iteration 0500: loss: 1516.496
iteration 0600: loss: 1513.208
iteration 0700: loss: 1512.335
iteration 0800: loss: 1513.028
iteration 0900: loss: 1513.707
====> Epoch: 047 Train loss: 1513.8742  took : 8.48178243637085
====> Test loss: 1517.8472
iteration 0000: loss: 1511.662
iteration 0100: loss: 1512.623
iteration 0200: loss: 1513.444
iteration 0300: loss: 1511.379
iteration 0400: loss: 1514.300
iteration 0500: loss: 1512.875
iteration 0600: loss: 1512.786
iteration 0700: loss: 1515.806
iteration 0800: loss: 1514.221
iteration 0900: loss: 1515.615
====> Epoch: 048 Train loss: 1513.8299  took : 8.4625563621521
====> Test loss: 1517.9072
iteration 0000: loss: 1512.369
iteration 0100: loss: 1512.807
iteration 0200: loss: 1512.037
iteration 0300: loss: 1513.121
iteration 0400: loss: 1512.181
iteration 0500: loss: 1514.429
iteration 0600: loss: 1514.227
iteration 0700: loss: 1512.334
iteration 0800: loss: 1514.274
iteration 0900: loss: 1513.684
====> Epoch: 049 Train loss: 1513.8039  took : 8.437185049057007
====> Test loss: 1517.9140
iteration 0000: loss: 1513.440
iteration 0100: loss: 1514.872
iteration 0200: loss: 1515.986
iteration 0300: loss: 1512.279
iteration 0400: loss: 1515.056
iteration 0500: loss: 1513.158
iteration 0600: loss: 1513.585
iteration 0700: loss: 1514.234
iteration 0800: loss: 1514.086
iteration 0900: loss: 1514.964
====> Epoch: 050 Train loss: 1513.8110  took : 8.621267795562744
====> Test loss: 1518.0643
====> [MM-VAE] Time: 505.065s or 00:08:25
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  1
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_1
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.342
iteration 0100: loss: 2094.137
iteration 0200: loss: 2040.624
iteration 0300: loss: 2018.679
iteration 0400: loss: 2018.416
iteration 0500: loss: 2003.640
iteration 0600: loss: 2005.452
iteration 0700: loss: 1997.059
iteration 0800: loss: 2000.513
iteration 0900: loss: 1991.824
====> Epoch: 001 Train loss: 2023.2100  took : 13.46703553199768
====> Test loss: 1994.9042
iteration 0000: loss: 1986.843
iteration 0100: loss: 1993.658
iteration 0200: loss: 1990.491
iteration 0300: loss: 1985.486
iteration 0400: loss: 1981.618
iteration 0500: loss: 1985.594
iteration 0600: loss: 1983.244
iteration 0700: loss: 1988.612
iteration 0800: loss: 1981.723
iteration 0900: loss: 1981.238
====> Epoch: 002 Train loss: 1985.8189  took : 12.711925983428955
====> Test loss: 1983.8786
iteration 0000: loss: 1981.905
iteration 0100: loss: 1987.555
iteration 0200: loss: 1981.314
iteration 0300: loss: 1976.130
iteration 0400: loss: 1980.030
iteration 0500: loss: 1976.639
iteration 0600: loss: 1975.746
iteration 0700: loss: 1978.288
iteration 0800: loss: 1974.955
iteration 0900: loss: 1974.040
====> Epoch: 003 Train loss: 1976.6656  took : 13.239015817642212
====> Test loss: 1973.8108
iteration 0000: loss: 1974.452
iteration 0100: loss: 1969.203
iteration 0200: loss: 1966.227
iteration 0300: loss: 1968.894
iteration 0400: loss: 1976.631
iteration 0500: loss: 1969.758
iteration 0600: loss: 1965.711
iteration 0700: loss: 1964.216
iteration 0800: loss: 1969.935
iteration 0900: loss: 1965.779
====> Epoch: 004 Train loss: 1969.2667  took : 12.160903692245483
====> Test loss: 1970.1265
iteration 0000: loss: 1963.406
iteration 0100: loss: 1968.283
iteration 0200: loss: 1965.598
iteration 0300: loss: 1968.232
iteration 0400: loss: 1964.802
iteration 0500: loss: 1964.345
iteration 0600: loss: 1963.524
iteration 0700: loss: 1967.499
iteration 0800: loss: 1961.097
iteration 0900: loss: 1964.733
====> Epoch: 005 Train loss: 1964.7224  took : 13.458479642868042
====> Test loss: 1964.1689
iteration 0000: loss: 1961.755
iteration 0100: loss: 1962.547
iteration 0200: loss: 1961.385
iteration 0300: loss: 1965.495
iteration 0400: loss: 1957.296
iteration 0500: loss: 1959.370
iteration 0600: loss: 1960.453
iteration 0700: loss: 1962.444
iteration 0800: loss: 1965.790
iteration 0900: loss: 1959.817
====> Epoch: 006 Train loss: 1961.2994  took : 12.220608949661255
====> Test loss: 1961.5269
iteration 0000: loss: 1958.079
iteration 0100: loss: 1963.101
iteration 0200: loss: 1956.552
iteration 0300: loss: 1958.278
iteration 0400: loss: 1961.307
iteration 0500: loss: 1961.784
iteration 0600: loss: 1961.768
iteration 0700: loss: 1955.783
iteration 0800: loss: 1957.992
iteration 0900: loss: 1954.731
====> Epoch: 007 Train loss: 1958.9456  took : 12.111494064331055
====> Test loss: 1960.0099
iteration 0000: loss: 1954.431
iteration 0100: loss: 1955.741
iteration 0200: loss: 1957.015
iteration 0300: loss: 1957.962
iteration 0400: loss: 1955.754
iteration 0500: loss: 1955.790
iteration 0600: loss: 1961.834
iteration 0700: loss: 1955.599
iteration 0800: loss: 1958.854
iteration 0900: loss: 1955.406
====> Epoch: 008 Train loss: 1957.4864  took : 13.16031289100647
====> Test loss: 1958.8922
iteration 0000: loss: 1955.110
iteration 0100: loss: 1957.921
iteration 0200: loss: 1953.510
iteration 0300: loss: 1958.076
iteration 0400: loss: 1953.554
iteration 0500: loss: 1955.938
iteration 0600: loss: 1957.309
iteration 0700: loss: 1957.578
iteration 0800: loss: 1955.795
iteration 0900: loss: 1955.031
====> Epoch: 009 Train loss: 1956.7635  took : 12.032694578170776
====> Test loss: 1958.6014
iteration 0000: loss: 1958.016
iteration 0100: loss: 1955.661
iteration 0200: loss: 1956.928
iteration 0300: loss: 1959.216
iteration 0400: loss: 1955.141
iteration 0500: loss: 1955.042
iteration 0600: loss: 1956.257
iteration 0700: loss: 1955.199
iteration 0800: loss: 1961.157
iteration 0900: loss: 1953.203
====> Epoch: 010 Train loss: 1956.2308  took : 13.401797533035278
====> Test loss: 1957.9510
iteration 0000: loss: 1955.587
iteration 0100: loss: 1956.329
iteration 0200: loss: 1955.075
iteration 0300: loss: 1954.110
iteration 0400: loss: 1953.644
iteration 0500: loss: 1957.142
iteration 0600: loss: 1958.736
iteration 0700: loss: 1957.996
iteration 0800: loss: 1956.741
iteration 0900: loss: 1958.014
====> Epoch: 011 Train loss: 1955.6083  took : 12.518700361251831
====> Test loss: 1956.6563
iteration 0000: loss: 1954.316
iteration 0100: loss: 1951.312
iteration 0200: loss: 1955.523
iteration 0300: loss: 1954.578
iteration 0400: loss: 1957.629
iteration 0500: loss: 1954.848
iteration 0600: loss: 1956.502
iteration 0700: loss: 1954.518
iteration 0800: loss: 1954.073
iteration 0900: loss: 1955.574
====> Epoch: 012 Train loss: 1955.0994  took : 12.44907832145691
====> Test loss: 1956.7433
iteration 0000: loss: 1954.245
iteration 0100: loss: 1956.286
iteration 0200: loss: 1957.484
iteration 0300: loss: 1954.185
iteration 0400: loss: 1957.109
iteration 0500: loss: 1953.174
iteration 0600: loss: 1953.820
iteration 0700: loss: 1951.393
iteration 0800: loss: 1955.797
iteration 0900: loss: 1956.267
====> Epoch: 013 Train loss: 1954.9138  took : 13.13418960571289
====> Test loss: 1956.2482
iteration 0000: loss: 1959.171
iteration 0100: loss: 1955.375
iteration 0200: loss: 1958.152
iteration 0300: loss: 1953.494
iteration 0400: loss: 1955.116
iteration 0500: loss: 1955.436
iteration 0600: loss: 1951.940
iteration 0700: loss: 1955.916
iteration 0800: loss: 1957.887
iteration 0900: loss: 1955.158
====> Epoch: 014 Train loss: 1954.6197  took : 12.242504358291626
====> Test loss: 1956.7680
iteration 0000: loss: 1957.340
iteration 0100: loss: 1953.136
iteration 0200: loss: 1955.738
iteration 0300: loss: 1957.757
iteration 0400: loss: 1953.371
iteration 0500: loss: 1955.551
iteration 0600: loss: 1954.919
iteration 0700: loss: 1957.049
iteration 0800: loss: 1956.606
iteration 0900: loss: 1951.377
====> Epoch: 015 Train loss: 1954.2986  took : 12.629014492034912
====> Test loss: 1955.8121
iteration 0000: loss: 1953.185
iteration 0100: loss: 1954.324
iteration 0200: loss: 1953.995
iteration 0300: loss: 1951.163
iteration 0400: loss: 1952.469
iteration 0500: loss: 1952.453
iteration 0600: loss: 1953.910
iteration 0700: loss: 1956.285
iteration 0800: loss: 1952.896
iteration 0900: loss: 1954.040
====> Epoch: 016 Train loss: 1953.9630  took : 12.531825542449951
====> Test loss: 1955.2515
iteration 0000: loss: 1954.125
iteration 0100: loss: 1954.785
iteration 0200: loss: 1952.468
iteration 0300: loss: 1951.710
iteration 0400: loss: 1956.868
iteration 0500: loss: 1951.599
iteration 0600: loss: 1954.266
iteration 0700: loss: 1953.292
iteration 0800: loss: 1953.295
iteration 0900: loss: 1951.844
====> Epoch: 017 Train loss: 1953.6894  took : 12.583876609802246
====> Test loss: 1955.3623
iteration 0000: loss: 1954.859
iteration 0100: loss: 1952.774
iteration 0200: loss: 1953.517
iteration 0300: loss: 1956.428
iteration 0400: loss: 1953.212
iteration 0500: loss: 1951.248
iteration 0600: loss: 1956.181
iteration 0700: loss: 1954.468
iteration 0800: loss: 1953.481
iteration 0900: loss: 1958.379
====> Epoch: 018 Train loss: 1953.8401  took : 13.340076684951782
====> Test loss: 1955.7187
iteration 0000: loss: 1954.367
iteration 0100: loss: 1953.944
iteration 0200: loss: 1951.536
iteration 0300: loss: 1954.998
iteration 0400: loss: 1954.817
iteration 0500: loss: 1954.387
iteration 0600: loss: 1956.973
iteration 0700: loss: 1954.810
iteration 0800: loss: 1953.021
iteration 0900: loss: 1958.119
====> Epoch: 019 Train loss: 1953.5841  took : 13.093429565429688
====> Test loss: 1954.6958
iteration 0000: loss: 1956.227
iteration 0100: loss: 1954.344
iteration 0200: loss: 1949.038
iteration 0300: loss: 1953.322
iteration 0400: loss: 1952.469
iteration 0500: loss: 1957.257
iteration 0600: loss: 1953.485
iteration 0700: loss: 1953.057
iteration 0800: loss: 1950.720
iteration 0900: loss: 1952.254
====> Epoch: 020 Train loss: 1953.5755  took : 11.954486608505249
====> Test loss: 1954.8939
iteration 0000: loss: 1957.266
iteration 0100: loss: 1950.011
iteration 0200: loss: 1952.649
iteration 0300: loss: 1954.777
iteration 0400: loss: 1952.217
iteration 0500: loss: 1952.811
iteration 0600: loss: 1952.832
iteration 0700: loss: 1954.549
iteration 0800: loss: 1956.070
iteration 0900: loss: 1953.044
====> Epoch: 021 Train loss: 1953.7179  took : 11.656994819641113
====> Test loss: 1954.7539
iteration 0000: loss: 1951.950
iteration 0100: loss: 1953.934
iteration 0200: loss: 1956.008
iteration 0300: loss: 1955.179
iteration 0400: loss: 1953.799
iteration 0500: loss: 1953.260
iteration 0600: loss: 1954.520
iteration 0700: loss: 1952.343
iteration 0800: loss: 1951.377
iteration 0900: loss: 1951.831
====> Epoch: 022 Train loss: 1953.3158  took : 12.43360447883606
====> Test loss: 1954.6071
iteration 0000: loss: 1953.788
iteration 0100: loss: 1955.829
iteration 0200: loss: 1953.584
iteration 0300: loss: 1950.439
iteration 0400: loss: 1953.391
iteration 0500: loss: 1953.289
iteration 0600: loss: 1953.876
iteration 0700: loss: 1951.094
iteration 0800: loss: 1952.858
iteration 0900: loss: 1952.437
====> Epoch: 023 Train loss: 1953.1886  took : 12.14357304573059
====> Test loss: 1954.7344
iteration 0000: loss: 1951.237
iteration 0100: loss: 1952.229
iteration 0200: loss: 1960.494
iteration 0300: loss: 1951.432
iteration 0400: loss: 1951.519
iteration 0500: loss: 1954.590
iteration 0600: loss: 1953.342
iteration 0700: loss: 1958.319
iteration 0800: loss: 1952.054
iteration 0900: loss: 1953.145
====> Epoch: 024 Train loss: 1952.9215  took : 13.600718021392822
====> Test loss: 1954.2038
iteration 0000: loss: 1952.245
iteration 0100: loss: 1951.800
iteration 0200: loss: 1953.704
iteration 0300: loss: 1953.573
iteration 0400: loss: 1954.724
iteration 0500: loss: 1954.307
iteration 0600: loss: 1949.398
iteration 0700: loss: 1953.998
iteration 0800: loss: 1951.835
iteration 0900: loss: 1954.932
====> Epoch: 025 Train loss: 1953.2056  took : 12.186166048049927
====> Test loss: 1954.9917
iteration 0000: loss: 1952.266
iteration 0100: loss: 1950.949
iteration 0200: loss: 1954.302
iteration 0300: loss: 1952.139
iteration 0400: loss: 1956.093
iteration 0500: loss: 1952.803
iteration 0600: loss: 1953.974
iteration 0700: loss: 1953.830
iteration 0800: loss: 1950.070
iteration 0900: loss: 1952.803
====> Epoch: 026 Train loss: 1952.9082  took : 12.966186761856079
====> Test loss: 1954.6163
iteration 0000: loss: 1953.351
iteration 0100: loss: 1951.508
iteration 0200: loss: 1950.975
iteration 0300: loss: 1951.208
iteration 0400: loss: 1952.595
iteration 0500: loss: 1950.316
iteration 0600: loss: 1952.770
iteration 0700: loss: 1958.436
iteration 0800: loss: 1959.207
iteration 0900: loss: 1951.511
====> Epoch: 027 Train loss: 1952.8257  took : 12.920230865478516
====> Test loss: 1954.1646
iteration 0000: loss: 1952.843
iteration 0100: loss: 1953.958
iteration 0200: loss: 1954.715
iteration 0300: loss: 1953.148
iteration 0400: loss: 1956.880
iteration 0500: loss: 1950.979
iteration 0600: loss: 1952.548
iteration 0700: loss: 1954.437
iteration 0800: loss: 1951.892
iteration 0900: loss: 1950.115
====> Epoch: 028 Train loss: 1952.9360  took : 12.905349254608154
====> Test loss: 1954.1642
iteration 0000: loss: 1952.415
iteration 0100: loss: 1954.630
iteration 0200: loss: 1949.853
iteration 0300: loss: 1951.719
iteration 0400: loss: 1952.191
iteration 0500: loss: 1951.713
iteration 0600: loss: 1955.446
iteration 0700: loss: 1953.472
iteration 0800: loss: 1952.807
iteration 0900: loss: 1954.031
====> Epoch: 029 Train loss: 1952.8090  took : 11.991646766662598
====> Test loss: 1954.4890
iteration 0000: loss: 1952.988
iteration 0100: loss: 1950.714
iteration 0200: loss: 1955.728
iteration 0300: loss: 1951.412
iteration 0400: loss: 1950.813
iteration 0500: loss: 1954.929
iteration 0600: loss: 1953.497
iteration 0700: loss: 1950.545
iteration 0800: loss: 1955.345
iteration 0900: loss: 1954.528
====> Epoch: 030 Train loss: 1952.8434  took : 11.819123983383179
====> Test loss: 1954.1238
iteration 0000: loss: 1951.744
iteration 0100: loss: 1955.308
iteration 0200: loss: 1953.714
iteration 0300: loss: 1951.423
iteration 0400: loss: 1949.704
iteration 0500: loss: 1951.945
iteration 0600: loss: 1955.089
iteration 0700: loss: 1954.349
iteration 0800: loss: 1950.354
iteration 0900: loss: 1953.140
====> Epoch: 031 Train loss: 1952.9920  took : 12.482602834701538
====> Test loss: 1954.1559
iteration 0000: loss: 1950.888
iteration 0100: loss: 1953.963
iteration 0200: loss: 1952.253
iteration 0300: loss: 1952.363
iteration 0400: loss: 1950.579
iteration 0500: loss: 1956.979
iteration 0600: loss: 1950.191
iteration 0700: loss: 1950.405
iteration 0800: loss: 1953.420
iteration 0900: loss: 1952.215
====> Epoch: 032 Train loss: 1952.8575  took : 12.287086963653564
====> Test loss: 1954.1603
iteration 0000: loss: 1951.385
iteration 0100: loss: 1956.467
iteration 0200: loss: 1950.981
iteration 0300: loss: 1952.162
iteration 0400: loss: 1952.357
iteration 0500: loss: 1951.297
iteration 0600: loss: 1953.404
iteration 0700: loss: 1953.453
iteration 0800: loss: 1955.078
iteration 0900: loss: 1951.861
====> Epoch: 033 Train loss: 1952.8956  took : 12.89740800857544
====> Test loss: 1954.3192
iteration 0000: loss: 1951.820
iteration 0100: loss: 1954.385
iteration 0200: loss: 1953.605
iteration 0300: loss: 1953.741
iteration 0400: loss: 1949.444
iteration 0500: loss: 1950.880
iteration 0600: loss: 1953.898
iteration 0700: loss: 1955.564
iteration 0800: loss: 1951.077
iteration 0900: loss: 1951.536
====> Epoch: 034 Train loss: 1952.9068  took : 12.625377893447876
====> Test loss: 1954.1616
iteration 0000: loss: 1953.864
iteration 0100: loss: 1954.389
iteration 0200: loss: 1952.358
iteration 0300: loss: 1953.190
iteration 0400: loss: 1953.612
iteration 0500: loss: 1951.525
iteration 0600: loss: 1954.651
iteration 0700: loss: 1953.011
iteration 0800: loss: 1952.486
iteration 0900: loss: 1951.058
====> Epoch: 035 Train loss: 1952.6766  took : 12.279950380325317
====> Test loss: 1954.2537
iteration 0000: loss: 1951.655
iteration 0100: loss: 1951.390
iteration 0200: loss: 1950.662
iteration 0300: loss: 1954.460
iteration 0400: loss: 1954.529
iteration 0500: loss: 1952.545
iteration 0600: loss: 1952.166
iteration 0700: loss: 1951.047
iteration 0800: loss: 1951.638
iteration 0900: loss: 1957.461
====> Epoch: 036 Train loss: 1952.7184  took : 12.79260516166687
====> Test loss: 1953.8476
iteration 0000: loss: 1955.410
iteration 0100: loss: 1957.334
iteration 0200: loss: 1952.517
iteration 0300: loss: 1949.325
iteration 0400: loss: 1954.838
iteration 0500: loss: 1951.175
iteration 0600: loss: 1952.972
iteration 0700: loss: 1957.310
iteration 0800: loss: 1950.597
iteration 0900: loss: 1953.814
====> Epoch: 037 Train loss: 1952.8159  took : 11.717997550964355
====> Test loss: 1954.1664
iteration 0000: loss: 1952.335
iteration 0100: loss: 1952.580
iteration 0200: loss: 1950.109
iteration 0300: loss: 1955.025
iteration 0400: loss: 1954.177
iteration 0500: loss: 1953.071
iteration 0600: loss: 1952.692
iteration 0700: loss: 1956.603
iteration 0800: loss: 1950.724
iteration 0900: loss: 1951.048
====> Epoch: 038 Train loss: 1952.9185  took : 12.250925540924072
====> Test loss: 1954.1343
iteration 0000: loss: 1954.001
iteration 0100: loss: 1952.996
iteration 0200: loss: 1950.012
iteration 0300: loss: 1952.898
iteration 0400: loss: 1951.008
iteration 0500: loss: 1950.566
iteration 0600: loss: 1954.680
iteration 0700: loss: 1950.077
iteration 0800: loss: 1949.271
iteration 0900: loss: 1952.910
====> Epoch: 039 Train loss: 1952.6227  took : 12.218276023864746
====> Test loss: 1953.8746
iteration 0000: loss: 1955.026
iteration 0100: loss: 1953.033
iteration 0200: loss: 1954.418
iteration 0300: loss: 1952.068
iteration 0400: loss: 1952.502
iteration 0500: loss: 1951.104
iteration 0600: loss: 1951.266
iteration 0700: loss: 1955.200
iteration 0800: loss: 1956.444
iteration 0900: loss: 1953.244
====> Epoch: 040 Train loss: 1952.8369  took : 12.790328979492188
====> Test loss: 1954.2098
iteration 0000: loss: 1954.407
iteration 0100: loss: 1956.195
iteration 0200: loss: 1949.745
iteration 0300: loss: 1951.146
iteration 0400: loss: 1953.744
iteration 0500: loss: 1952.412
iteration 0600: loss: 1953.406
iteration 0700: loss: 1953.168
iteration 0800: loss: 1951.149
iteration 0900: loss: 1952.907
====> Epoch: 041 Train loss: 1952.7827  took : 12.146855592727661
====> Test loss: 1953.9209
iteration 0000: loss: 1951.977
iteration 0100: loss: 1954.199
iteration 0200: loss: 1954.509
iteration 0300: loss: 1953.707
iteration 0400: loss: 1955.229
iteration 0500: loss: 1953.696
iteration 0600: loss: 1952.710
iteration 0700: loss: 1952.710
iteration 0800: loss: 1955.123
iteration 0900: loss: 1952.932
====> Epoch: 042 Train loss: 1952.7495  took : 13.108877658843994
====> Test loss: 1954.0493
iteration 0000: loss: 1953.909
iteration 0100: loss: 1950.481
iteration 0200: loss: 1958.114
iteration 0300: loss: 1951.068
iteration 0400: loss: 1954.502
iteration 0500: loss: 1953.141
iteration 0600: loss: 1952.189
iteration 0700: loss: 1950.338
iteration 0800: loss: 1952.855
iteration 0900: loss: 1954.571
====> Epoch: 043 Train loss: 1952.6411  took : 12.110811710357666
====> Test loss: 1953.9160
iteration 0000: loss: 1950.595
iteration 0100: loss: 1950.707
iteration 0200: loss: 1951.007
iteration 0300: loss: 1951.887
iteration 0400: loss: 1951.490
iteration 0500: loss: 1955.458
iteration 0600: loss: 1956.706
iteration 0700: loss: 1950.724
iteration 0800: loss: 1954.007
iteration 0900: loss: 1953.290
====> Epoch: 044 Train loss: 1952.6581  took : 11.740639686584473
====> Test loss: 1954.1265
iteration 0000: loss: 1949.602
iteration 0100: loss: 1952.623
iteration 0200: loss: 1953.200
iteration 0300: loss: 1954.190
iteration 0400: loss: 1952.578
iteration 0500: loss: 1950.342
iteration 0600: loss: 1951.763
iteration 0700: loss: 1954.434
iteration 0800: loss: 1956.801
iteration 0900: loss: 1953.351
====> Epoch: 045 Train loss: 1952.7474  took : 13.185454368591309
====> Test loss: 1953.7990
iteration 0000: loss: 1952.769
iteration 0100: loss: 1954.716
iteration 0200: loss: 1952.879
iteration 0300: loss: 1952.630
iteration 0400: loss: 1953.042
iteration 0500: loss: 1952.679
iteration 0600: loss: 1953.450
iteration 0700: loss: 1953.183
iteration 0800: loss: 1952.989
iteration 0900: loss: 1950.322
====> Epoch: 046 Train loss: 1952.5671  took : 12.345020294189453
====> Test loss: 1953.7517
iteration 0000: loss: 1954.033
iteration 0100: loss: 1953.825
iteration 0200: loss: 1953.044
iteration 0300: loss: 1954.110
iteration 0400: loss: 1949.532
iteration 0500: loss: 1953.109
iteration 0600: loss: 1953.542
iteration 0700: loss: 1955.684
iteration 0800: loss: 1950.843
iteration 0900: loss: 1952.811
====> Epoch: 047 Train loss: 1952.7251  took : 12.011664628982544
====> Test loss: 1954.5102
iteration 0000: loss: 1951.996
iteration 0100: loss: 1951.612
iteration 0200: loss: 1952.052
iteration 0300: loss: 1954.240
iteration 0400: loss: 1951.694
iteration 0500: loss: 1953.382
iteration 0600: loss: 1953.912
iteration 0700: loss: 1951.166
iteration 0800: loss: 1950.981
iteration 0900: loss: 1953.456
====> Epoch: 048 Train loss: 1952.7498  took : 12.96030855178833
====> Test loss: 1953.9190
iteration 0000: loss: 1951.243
iteration 0100: loss: 1950.978
iteration 0200: loss: 1952.175
iteration 0300: loss: 1955.796
iteration 0400: loss: 1953.426
iteration 0500: loss: 1952.907
iteration 0600: loss: 1950.025
iteration 0700: loss: 1950.988
iteration 0800: loss: 1956.929
iteration 0900: loss: 1955.179
====> Epoch: 049 Train loss: 1952.7938  took : 12.743221998214722
====> Test loss: 1954.1776
iteration 0000: loss: 1950.368
iteration 0100: loss: 1952.114
iteration 0200: loss: 1951.615
iteration 0300: loss: 1951.469
iteration 0400: loss: 1950.711
iteration 0500: loss: 1952.804
iteration 0600: loss: 1952.900
iteration 0700: loss: 1951.475
iteration 0800: loss: 1955.776
iteration 0900: loss: 1951.258
====> Epoch: 050 Train loss: 1952.5102  took : 13.538747310638428
====> Test loss: 1953.9546
====> [MM-VAE] Time: 699.447s or 00:11:39
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  1
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_1
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5218.502
iteration 0100: loss: 4140.039
iteration 0200: loss: 4119.605
iteration 0300: loss: 4086.032
iteration 0400: loss: 4027.413
iteration 0500: loss: 4025.413
iteration 0600: loss: 4017.265
iteration 0700: loss: 4004.667
iteration 0800: loss: 4010.705
iteration 0900: loss: 3993.248
iteration 1000: loss: 4005.034
iteration 1100: loss: 4006.591
iteration 1200: loss: 4007.404
iteration 1300: loss: 3997.479
iteration 1400: loss: 4003.264
iteration 1500: loss: 3990.304
iteration 1600: loss: 3994.397
iteration 1700: loss: 3996.065
iteration 1800: loss: 3989.647
====> Epoch: 001 Train loss: 4029.9598  took : 53.455414056777954
====> Test loss: 3993.6719
iteration 0000: loss: 3998.947
iteration 0100: loss: 3986.548
iteration 0200: loss: 3995.475
iteration 0300: loss: 3988.809
iteration 0400: loss: 3986.754
iteration 0500: loss: 3979.557
iteration 0600: loss: 3980.913
iteration 0700: loss: 3976.800
iteration 0800: loss: 3988.485
iteration 0900: loss: 3992.141
iteration 1000: loss: 3978.999
iteration 1100: loss: 3988.281
iteration 1200: loss: 3973.343
iteration 1300: loss: 3986.126
iteration 1400: loss: 3978.801
iteration 1500: loss: 3976.545
iteration 1600: loss: 3974.880
iteration 1700: loss: 3980.585
iteration 1800: loss: 3982.057
====> Epoch: 002 Train loss: 3983.2080  took : 53.435728311538696
====> Test loss: 3982.9431
iteration 0000: loss: 3972.551
iteration 0100: loss: 3973.687
iteration 0200: loss: 3979.697
iteration 0300: loss: 3969.211
iteration 0400: loss: 3988.351
iteration 0500: loss: 3968.163
iteration 0600: loss: 3975.873
iteration 0700: loss: 3973.816
iteration 0800: loss: 3963.465
iteration 0900: loss: 3972.625
iteration 1000: loss: 3961.279
iteration 1100: loss: 3985.893
iteration 1200: loss: 3981.350
iteration 1300: loss: 3966.359
iteration 1400: loss: 3977.950
iteration 1500: loss: 3979.821
iteration 1600: loss: 3978.947
iteration 1700: loss: 3973.186
iteration 1800: loss: 3965.957
====> Epoch: 003 Train loss: 3974.5458  took : 53.221627712249756
====> Test loss: 3977.5263
iteration 0000: loss: 3971.659
iteration 0100: loss: 3958.001
iteration 0200: loss: 3958.349
iteration 0300: loss: 3974.862
iteration 0400: loss: 3971.303
iteration 0500: loss: 3974.059
iteration 0600: loss: 3963.543
iteration 0700: loss: 3972.038
iteration 0800: loss: 3965.682
iteration 0900: loss: 3963.565
iteration 1000: loss: 3969.844
iteration 1100: loss: 3971.729
iteration 1200: loss: 3974.298
iteration 1300: loss: 3969.706
iteration 1400: loss: 3978.408
iteration 1500: loss: 3972.748
iteration 1600: loss: 3969.770
iteration 1700: loss: 3969.650
iteration 1800: loss: 3964.898
====> Epoch: 004 Train loss: 3970.8461  took : 53.275187969207764
====> Test loss: 3976.6922
iteration 0000: loss: 3965.017
iteration 0100: loss: 3967.862
iteration 0200: loss: 3960.429
iteration 0300: loss: 3971.602
iteration 0400: loss: 3964.476
iteration 0500: loss: 3969.057
iteration 0600: loss: 3969.084
iteration 0700: loss: 3971.216
iteration 0800: loss: 3967.895
iteration 0900: loss: 3985.227
iteration 1000: loss: 3975.444
iteration 1100: loss: 3964.008
iteration 1200: loss: 3966.978
iteration 1300: loss: 3973.887
iteration 1400: loss: 3978.412
iteration 1500: loss: 3970.561
iteration 1600: loss: 3968.564
iteration 1700: loss: 3972.209
iteration 1800: loss: 3965.122
====> Epoch: 005 Train loss: 3970.3108  took : 53.250431537628174
====> Test loss: 3973.3417
iteration 0000: loss: 3972.889
iteration 0100: loss: 3981.551
iteration 0200: loss: 3969.404
iteration 0300: loss: 3972.489
iteration 0400: loss: 3970.396
iteration 0500: loss: 3962.802
iteration 0600: loss: 3966.303
iteration 0700: loss: 3968.588
iteration 0800: loss: 3968.771
iteration 0900: loss: 3955.562
iteration 1000: loss: 3967.469
iteration 1100: loss: 3968.384
iteration 1200: loss: 3964.407
iteration 1300: loss: 3971.217
iteration 1400: loss: 3957.700
iteration 1500: loss: 3957.169
iteration 1600: loss: 3959.798
iteration 1700: loss: 3968.662
iteration 1800: loss: 3960.239
====> Epoch: 006 Train loss: 3966.5099  took : 53.43581223487854
====> Test loss: 3970.7970
iteration 0000: loss: 3967.918
iteration 0100: loss: 3967.868
iteration 0200: loss: 3960.975
iteration 0300: loss: 3964.825
iteration 0400: loss: 3964.876
iteration 0500: loss: 3965.277
iteration 0600: loss: 3960.708
iteration 0700: loss: 3973.814
iteration 0800: loss: 3964.732
iteration 0900: loss: 3964.689
iteration 1000: loss: 3960.369
iteration 1100: loss: 3962.459
iteration 1200: loss: 3977.210
iteration 1300: loss: 3957.443
iteration 1400: loss: 3973.381
iteration 1500: loss: 3967.514
iteration 1600: loss: 3968.165
iteration 1700: loss: 3967.696
iteration 1800: loss: 3959.129
====> Epoch: 007 Train loss: 3965.6824  took : 53.44966721534729
====> Test loss: 3970.2349
iteration 0000: loss: 3963.171
iteration 0100: loss: 3958.983
iteration 0200: loss: 3962.951
iteration 0300: loss: 3967.115
iteration 0400: loss: 3969.543
iteration 0500: loss: 3968.372
iteration 0600: loss: 3966.874
iteration 0700: loss: 3959.325
iteration 0800: loss: 3962.456
iteration 0900: loss: 3968.757
iteration 1000: loss: 3958.899
iteration 1100: loss: 3967.341
iteration 1200: loss: 3961.315
iteration 1300: loss: 3967.309
iteration 1400: loss: 3955.423
iteration 1500: loss: 3965.360
iteration 1600: loss: 3963.069
iteration 1700: loss: 3963.517
iteration 1800: loss: 3968.403
====> Epoch: 008 Train loss: 3964.4035  took : 53.250473499298096
====> Test loss: 3967.8263
iteration 0000: loss: 3964.606
iteration 0100: loss: 3963.196
iteration 0200: loss: 3968.360
iteration 0300: loss: 3959.948
iteration 0400: loss: 3970.744
iteration 0500: loss: 3965.284
iteration 0600: loss: 3969.296
iteration 0700: loss: 3968.360
iteration 0800: loss: 3962.008
iteration 0900: loss: 3964.984
iteration 1000: loss: 3964.666
iteration 1100: loss: 3963.195
iteration 1200: loss: 3962.535
iteration 1300: loss: 3967.522
iteration 1400: loss: 3957.929
iteration 1500: loss: 3961.346
iteration 1600: loss: 3957.312
iteration 1700: loss: 3965.990
iteration 1800: loss: 3961.410
====> Epoch: 009 Train loss: 3962.6543  took : 53.500208616256714
====> Test loss: 3966.6416
iteration 0000: loss: 3958.852
iteration 0100: loss: 3952.538
iteration 0200: loss: 3957.510
iteration 0300: loss: 3955.492
iteration 0400: loss: 3964.031
iteration 0500: loss: 3968.725
iteration 0600: loss: 3960.747
iteration 0700: loss: 3958.517
iteration 0800: loss: 3968.790
iteration 0900: loss: 3960.927
iteration 1000: loss: 3959.613
iteration 1100: loss: 3963.491
iteration 1200: loss: 3958.143
iteration 1300: loss: 3944.439
iteration 1400: loss: 3963.055
iteration 1500: loss: 3959.773
iteration 1600: loss: 3948.528
iteration 1700: loss: 3962.314
iteration 1800: loss: 3957.768
====> Epoch: 010 Train loss: 3961.1158  took : 53.35310649871826
====> Test loss: 3966.6979
iteration 0000: loss: 3968.493
iteration 0100: loss: 3958.387
iteration 0200: loss: 3964.513
iteration 0300: loss: 3959.469
iteration 0400: loss: 3962.397
iteration 0500: loss: 3964.344
iteration 0600: loss: 3962.754
iteration 0700: loss: 3954.236
iteration 0800: loss: 3958.555
iteration 0900: loss: 3962.788
iteration 1000: loss: 3954.688
iteration 1100: loss: 3954.554
iteration 1200: loss: 3963.595
iteration 1300: loss: 3961.170
iteration 1400: loss: 3955.165
iteration 1500: loss: 3954.043
iteration 1600: loss: 3967.792
iteration 1700: loss: 3956.745
iteration 1800: loss: 3961.423
====> Epoch: 011 Train loss: 3960.2332  took : 53.12164330482483
====> Test loss: 3964.2985
iteration 0000: loss: 3954.416
iteration 0100: loss: 3959.503
iteration 0200: loss: 3955.259
iteration 0300: loss: 3963.957
iteration 0400: loss: 3955.567
iteration 0500: loss: 3964.828
iteration 0600: loss: 3962.619
iteration 0700: loss: 3960.479
iteration 0800: loss: 3962.000
iteration 0900: loss: 3967.083
iteration 1000: loss: 3958.460
iteration 1100: loss: 3964.254
iteration 1200: loss: 3959.473
iteration 1300: loss: 3963.008
iteration 1400: loss: 3952.564
iteration 1500: loss: 3946.578
iteration 1600: loss: 3958.701
iteration 1700: loss: 3962.727
iteration 1800: loss: 3952.315
====> Epoch: 012 Train loss: 3959.6837  took : 53.31794238090515
====> Test loss: 3963.3618
iteration 0000: loss: 3960.655
iteration 0100: loss: 3953.581
iteration 0200: loss: 3963.000
iteration 0300: loss: 3963.168
iteration 0400: loss: 3954.202
iteration 0500: loss: 3961.961
iteration 0600: loss: 3952.456
iteration 0700: loss: 3958.523
iteration 0800: loss: 3951.663
iteration 0900: loss: 3955.642
iteration 1000: loss: 3960.786
iteration 1100: loss: 3972.767
iteration 1200: loss: 3956.133
iteration 1300: loss: 3960.148
iteration 1400: loss: 3961.973
iteration 1500: loss: 3955.784
iteration 1600: loss: 3949.527
iteration 1700: loss: 3966.140
iteration 1800: loss: 3956.532
====> Epoch: 013 Train loss: 3958.2640  took : 53.16991353034973
====> Test loss: 3962.8701
iteration 0000: loss: 3962.960
iteration 0100: loss: 3959.098
iteration 0200: loss: 3963.436
iteration 0300: loss: 3955.484
iteration 0400: loss: 3952.598
iteration 0500: loss: 3957.133
iteration 0600: loss: 3950.952
iteration 0700: loss: 3964.345
iteration 0800: loss: 3957.173
iteration 0900: loss: 3959.068
iteration 1000: loss: 3952.437
iteration 1100: loss: 3958.346
iteration 1200: loss: 3954.087
iteration 1300: loss: 3958.539
iteration 1400: loss: 3959.789
iteration 1500: loss: 3954.483
iteration 1600: loss: 3958.309
iteration 1700: loss: 3963.255
iteration 1800: loss: 3952.906
====> Epoch: 014 Train loss: 3957.0941  took : 53.259133100509644
====> Test loss: 3960.7813
iteration 0000: loss: 3958.329
iteration 0100: loss: 3959.260
iteration 0200: loss: 3964.356
iteration 0300: loss: 3956.167
iteration 0400: loss: 3955.448
iteration 0500: loss: 3950.689
iteration 0600: loss: 3956.285
iteration 0700: loss: 3946.275
iteration 0800: loss: 3955.508
iteration 0900: loss: 3945.869
iteration 1000: loss: 3950.311
iteration 1100: loss: 3956.693
iteration 1200: loss: 3955.211
iteration 1300: loss: 3949.523
iteration 1400: loss: 3961.095
iteration 1500: loss: 3960.832
iteration 1600: loss: 3953.747
iteration 1700: loss: 3958.865
iteration 1800: loss: 3957.117
====> Epoch: 015 Train loss: 3955.6624  took : 53.39391732215881
====> Test loss: 3961.2979
iteration 0000: loss: 3953.776
iteration 0100: loss: 3954.075
iteration 0200: loss: 3951.980
iteration 0300: loss: 3955.077
iteration 0400: loss: 3961.235
iteration 0500: loss: 3948.744
iteration 0600: loss: 3960.646
iteration 0700: loss: 3957.037
iteration 0800: loss: 3956.370
iteration 0900: loss: 3960.747
iteration 1000: loss: 3960.366
iteration 1100: loss: 3956.199
iteration 1200: loss: 3952.489
iteration 1300: loss: 3950.773
iteration 1400: loss: 3952.380
iteration 1500: loss: 3951.648
iteration 1600: loss: 3950.276
iteration 1700: loss: 3950.277
iteration 1800: loss: 3953.345
====> Epoch: 016 Train loss: 3953.9070  took : 53.29090642929077
====> Test loss: 3957.8333
iteration 0000: loss: 3960.237
iteration 0100: loss: 3951.864
iteration 0200: loss: 3955.768
iteration 0300: loss: 3947.681
iteration 0400: loss: 3948.131
iteration 0500: loss: 3947.246
iteration 0600: loss: 3946.673
iteration 0700: loss: 3952.247
iteration 0800: loss: 3948.203
iteration 0900: loss: 3954.602
iteration 1000: loss: 3959.289
iteration 1100: loss: 3953.891
iteration 1200: loss: 3951.062
iteration 1300: loss: 3954.086
iteration 1400: loss: 3954.468
iteration 1500: loss: 3948.967
iteration 1600: loss: 3949.417
iteration 1700: loss: 3951.403
iteration 1800: loss: 3951.830
====> Epoch: 017 Train loss: 3951.5031  took : 53.16975426673889
====> Test loss: 3955.0928
iteration 0000: loss: 3955.211
iteration 0100: loss: 3952.659
iteration 0200: loss: 3949.718
iteration 0300: loss: 3947.714
iteration 0400: loss: 3946.792
iteration 0500: loss: 3948.664
iteration 0600: loss: 3946.937
iteration 0700: loss: 3949.325
iteration 0800: loss: 3951.553
iteration 0900: loss: 3946.134
iteration 1000: loss: 3950.921
iteration 1100: loss: 3954.143
iteration 1200: loss: 3949.220
iteration 1300: loss: 3949.229
iteration 1400: loss: 3947.479
iteration 1500: loss: 3957.793
iteration 1600: loss: 3942.503
iteration 1700: loss: 3950.275
iteration 1800: loss: 3955.969
====> Epoch: 018 Train loss: 3949.7053  took : 53.238635778427124
====> Test loss: 3953.1972
iteration 0000: loss: 3948.486
iteration 0100: loss: 3951.713
iteration 0200: loss: 3947.468
iteration 0300: loss: 3951.224
iteration 0400: loss: 3947.820
iteration 0500: loss: 3952.510
iteration 0600: loss: 3943.252
iteration 0700: loss: 3951.614
iteration 0800: loss: 3945.792
iteration 0900: loss: 3949.085
iteration 1000: loss: 3948.807
iteration 1100: loss: 3942.984
iteration 1200: loss: 3953.523
iteration 1300: loss: 3949.247
iteration 1400: loss: 3946.590
iteration 1500: loss: 3939.885
iteration 1600: loss: 3949.948
iteration 1700: loss: 3952.261
iteration 1800: loss: 3946.444
====> Epoch: 019 Train loss: 3947.7904  took : 53.053608655929565
====> Test loss: 3952.2769
iteration 0000: loss: 3948.571
iteration 0100: loss: 3946.521
iteration 0200: loss: 3944.758
iteration 0300: loss: 3947.503
iteration 0400: loss: 3945.269
iteration 0500: loss: 3949.557
iteration 0600: loss: 3947.677
iteration 0700: loss: 3944.855
iteration 0800: loss: 3944.509
iteration 0900: loss: 3944.990
iteration 1000: loss: 3944.273
iteration 1100: loss: 3945.236
iteration 1200: loss: 3944.147
iteration 1300: loss: 3953.479
iteration 1400: loss: 3947.744
iteration 1500: loss: 3940.138
iteration 1600: loss: 3953.837
iteration 1700: loss: 3940.526
iteration 1800: loss: 3948.038
====> Epoch: 020 Train loss: 3946.6098  took : 53.14708495140076
====> Test loss: 3950.8895
iteration 0000: loss: 3945.572
iteration 0100: loss: 3942.265
iteration 0200: loss: 3952.299
iteration 0300: loss: 3948.448
iteration 0400: loss: 3944.219
iteration 0500: loss: 3955.153
iteration 0600: loss: 3938.605
iteration 0700: loss: 3945.729
iteration 0800: loss: 3943.872
iteration 0900: loss: 3947.357
iteration 1000: loss: 3948.960
iteration 1100: loss: 3940.021
iteration 1200: loss: 3943.282
iteration 1300: loss: 3948.291
iteration 1400: loss: 3937.466
iteration 1500: loss: 3949.756
iteration 1600: loss: 3941.227
iteration 1700: loss: 3941.242
iteration 1800: loss: 3947.290
====> Epoch: 021 Train loss: 3945.6270  took : 53.33583354949951
====> Test loss: 3949.1610
iteration 0000: loss: 3945.817
iteration 0100: loss: 3950.742
iteration 0200: loss: 3938.606
iteration 0300: loss: 3945.771
iteration 0400: loss: 3946.312
iteration 0500: loss: 3944.267
iteration 0600: loss: 3947.740
iteration 0700: loss: 3934.905
iteration 0800: loss: 3943.239
iteration 0900: loss: 3953.620
iteration 1000: loss: 3949.391
iteration 1100: loss: 3937.275
iteration 1200: loss: 3950.243
iteration 1300: loss: 3942.325
iteration 1400: loss: 3945.415
iteration 1500: loss: 3943.449
iteration 1600: loss: 3944.514
iteration 1700: loss: 3949.041
iteration 1800: loss: 3941.538
====> Epoch: 022 Train loss: 3944.2804  took : 53.13445234298706
====> Test loss: 3948.7481
iteration 0000: loss: 3944.110
iteration 0100: loss: 3948.701
iteration 0200: loss: 3944.955
iteration 0300: loss: 3937.584
iteration 0400: loss: 3941.847
iteration 0500: loss: 3944.270
iteration 0600: loss: 3938.538
iteration 0700: loss: 3941.503
iteration 0800: loss: 3940.918
iteration 0900: loss: 3933.315
iteration 1000: loss: 3949.865
iteration 1100: loss: 3943.455
iteration 1200: loss: 3943.729
iteration 1300: loss: 3948.159
iteration 1400: loss: 3937.939
iteration 1500: loss: 3941.497
iteration 1600: loss: 3940.662
iteration 1700: loss: 3943.316
iteration 1800: loss: 3942.335
====> Epoch: 023 Train loss: 3943.6635  took : 53.3915741443634
====> Test loss: 3948.3424
iteration 0000: loss: 3940.454
iteration 0100: loss: 3946.851
iteration 0200: loss: 3941.503
iteration 0300: loss: 3946.510
iteration 0400: loss: 3940.606
iteration 0500: loss: 3944.845
iteration 0600: loss: 3941.044
iteration 0700: loss: 3938.741
iteration 0800: loss: 3935.846
iteration 0900: loss: 3944.391
iteration 1000: loss: 3947.376
iteration 1100: loss: 3941.162
iteration 1200: loss: 3939.792
iteration 1300: loss: 3949.008
iteration 1400: loss: 3941.710
iteration 1500: loss: 3944.596
iteration 1600: loss: 3942.500
iteration 1700: loss: 3948.734
iteration 1800: loss: 3944.317
====> Epoch: 024 Train loss: 3943.4272  took : 53.02009057998657
====> Test loss: 3947.3571
iteration 0000: loss: 3944.550
iteration 0100: loss: 3939.597
iteration 0200: loss: 3942.728
iteration 0300: loss: 3940.629
iteration 0400: loss: 3943.239
iteration 0500: loss: 3948.072
iteration 0600: loss: 3938.376
iteration 0700: loss: 3943.714
iteration 0800: loss: 3942.043
iteration 0900: loss: 3941.423
iteration 1000: loss: 3945.895
iteration 1100: loss: 3939.261
iteration 1200: loss: 3941.997
iteration 1300: loss: 3942.246
iteration 1400: loss: 3946.218
iteration 1500: loss: 3948.437
iteration 1600: loss: 3941.464
iteration 1700: loss: 3946.992
iteration 1800: loss: 3943.754
====> Epoch: 025 Train loss: 3942.8623  took : 53.20280432701111
====> Test loss: 3946.5004
iteration 0000: loss: 3937.591
iteration 0100: loss: 3938.932
iteration 0200: loss: 3937.869
iteration 0300: loss: 3938.386
iteration 0400: loss: 3943.762
iteration 0500: loss: 3941.757
iteration 0600: loss: 3938.791
iteration 0700: loss: 3942.207
iteration 0800: loss: 3944.323
iteration 0900: loss: 3938.868
iteration 1000: loss: 3939.503
iteration 1100: loss: 3934.964
iteration 1200: loss: 3941.688
iteration 1300: loss: 3941.704
iteration 1400: loss: 3936.313
iteration 1500: loss: 3937.845
iteration 1600: loss: 3941.316
iteration 1700: loss: 3937.028
iteration 1800: loss: 3937.322
====> Epoch: 026 Train loss: 3942.1844  took : 53.065531492233276
====> Test loss: 3945.7787
iteration 0000: loss: 3945.515
iteration 0100: loss: 3945.415
iteration 0200: loss: 3939.223
iteration 0300: loss: 3937.996
iteration 0400: loss: 3947.255
iteration 0500: loss: 3936.628
iteration 0600: loss: 3942.129
iteration 0700: loss: 3947.737
iteration 0800: loss: 3947.612
iteration 0900: loss: 3932.253
iteration 1000: loss: 3939.787
iteration 1100: loss: 3943.687
iteration 1200: loss: 3941.567
iteration 1300: loss: 3943.924
iteration 1400: loss: 3938.873
iteration 1500: loss: 3941.868
iteration 1600: loss: 3946.177
iteration 1700: loss: 3942.093
iteration 1800: loss: 3936.194
====> Epoch: 027 Train loss: 3941.5596  took : 53.01118779182434
====> Test loss: 3945.9245
iteration 0000: loss: 3941.922
iteration 0100: loss: 3939.686
iteration 0200: loss: 3936.109
iteration 0300: loss: 3936.263
iteration 0400: loss: 3936.003
iteration 0500: loss: 3940.962
iteration 0600: loss: 3943.394
iteration 0700: loss: 3944.745
iteration 0800: loss: 3937.707
iteration 0900: loss: 3936.655
iteration 1000: loss: 3940.100
iteration 1100: loss: 3939.059
iteration 1200: loss: 3942.722
iteration 1300: loss: 3936.855
iteration 1400: loss: 3941.048
iteration 1500: loss: 3944.627
iteration 1600: loss: 3939.554
iteration 1700: loss: 3945.069
iteration 1800: loss: 3946.820
====> Epoch: 028 Train loss: 3940.9792  took : 53.07439374923706
====> Test loss: 3945.7830
iteration 0000: loss: 3940.565
iteration 0100: loss: 3939.047
iteration 0200: loss: 3939.550
iteration 0300: loss: 3946.211
iteration 0400: loss: 3941.701
iteration 0500: loss: 3936.509
iteration 0600: loss: 3938.516
iteration 0700: loss: 3943.923
iteration 0800: loss: 3941.543
iteration 0900: loss: 3940.816
iteration 1000: loss: 3947.001
iteration 1100: loss: 3940.011
iteration 1200: loss: 3943.977
iteration 1300: loss: 3939.173
iteration 1400: loss: 3944.854
iteration 1500: loss: 3938.414
iteration 1600: loss: 3943.579
iteration 1700: loss: 3939.866
iteration 1800: loss: 3943.866
====> Epoch: 029 Train loss: 3940.7254  took : 53.1494402885437
====> Test loss: 3945.3745
iteration 0000: loss: 3945.121
iteration 0100: loss: 3938.880
iteration 0200: loss: 3937.026
iteration 0300: loss: 3938.396
iteration 0400: loss: 3944.811
iteration 0500: loss: 3939.689
iteration 0600: loss: 3934.842
iteration 0700: loss: 3938.190
iteration 0800: loss: 3943.196
iteration 0900: loss: 3939.453
iteration 1000: loss: 3937.563
iteration 1100: loss: 3943.627
iteration 1200: loss: 3945.439
iteration 1300: loss: 3939.150
iteration 1400: loss: 3941.539
iteration 1500: loss: 3940.193
iteration 1600: loss: 3937.380
iteration 1700: loss: 3941.816
iteration 1800: loss: 3944.401
====> Epoch: 030 Train loss: 3940.5606  took : 52.988520860672
====> Test loss: 3944.4885
iteration 0000: loss: 3933.506
iteration 0100: loss: 3943.247
iteration 0200: loss: 3931.757
iteration 0300: loss: 3934.402
iteration 0400: loss: 3938.810
iteration 0500: loss: 3940.116
iteration 0600: loss: 3939.716
iteration 0700: loss: 3942.169
iteration 0800: loss: 3944.313
iteration 0900: loss: 3938.624
iteration 1000: loss: 3946.001
iteration 1100: loss: 3938.259
iteration 1200: loss: 3936.272
iteration 1300: loss: 3941.369
iteration 1400: loss: 3939.473
iteration 1500: loss: 3940.237
iteration 1600: loss: 3945.527
iteration 1700: loss: 3939.154
iteration 1800: loss: 3938.673
====> Epoch: 031 Train loss: 3940.0733  took : 53.36228942871094
====> Test loss: 3944.6229
iteration 0000: loss: 3937.863
iteration 0100: loss: 3937.396
iteration 0200: loss: 3941.562
iteration 0300: loss: 3951.717
iteration 0400: loss: 3944.155
iteration 0500: loss: 3938.746
iteration 0600: loss: 3937.983
iteration 0700: loss: 3940.347
iteration 0800: loss: 3943.607
iteration 0900: loss: 3947.336
iteration 1000: loss: 3940.201
iteration 1100: loss: 3940.804
iteration 1200: loss: 3939.778
iteration 1300: loss: 3943.125
iteration 1400: loss: 3939.554
iteration 1500: loss: 3939.317
iteration 1600: loss: 3933.646
iteration 1700: loss: 3947.250
iteration 1800: loss: 3939.421
====> Epoch: 032 Train loss: 3940.3284  took : 53.24853777885437
====> Test loss: 3945.0512
iteration 0000: loss: 3940.218
iteration 0100: loss: 3938.764
iteration 0200: loss: 3944.985
iteration 0300: loss: 3939.332
iteration 0400: loss: 3942.360
iteration 0500: loss: 3936.930
iteration 0600: loss: 3939.081
iteration 0700: loss: 3942.420
iteration 0800: loss: 3936.408
iteration 0900: loss: 3937.550
iteration 1000: loss: 3936.111
iteration 1100: loss: 3940.153
iteration 1200: loss: 3942.958
iteration 1300: loss: 3940.777
iteration 1400: loss: 3944.256
iteration 1500: loss: 3937.055
iteration 1600: loss: 3936.084
iteration 1700: loss: 3939.912
iteration 1800: loss: 3947.726
====> Epoch: 033 Train loss: 3939.9706  took : 53.1531240940094
====> Test loss: 3944.4450
iteration 0000: loss: 3936.396
iteration 0100: loss: 3940.701
iteration 0200: loss: 3935.898
iteration 0300: loss: 3939.449
iteration 0400: loss: 3937.460
iteration 0500: loss: 3940.765
iteration 0600: loss: 3934.345
iteration 0700: loss: 3936.319
iteration 0800: loss: 3946.015
iteration 0900: loss: 3940.132
iteration 1000: loss: 3938.313
iteration 1100: loss: 3940.816
iteration 1200: loss: 3940.256
iteration 1300: loss: 3943.456
iteration 1400: loss: 3945.863
iteration 1500: loss: 3943.949
iteration 1600: loss: 3937.021
iteration 1700: loss: 3946.566
iteration 1800: loss: 3939.170
====> Epoch: 034 Train loss: 3939.6421  took : 52.92422080039978
====> Test loss: 3944.3539
iteration 0000: loss: 3944.811
iteration 0100: loss: 3942.084
iteration 0200: loss: 3938.770
iteration 0300: loss: 3940.403
iteration 0400: loss: 3936.567
iteration 0500: loss: 3943.159
iteration 0600: loss: 3932.980
iteration 0700: loss: 3941.760
iteration 0800: loss: 3940.883
iteration 0900: loss: 3942.426
iteration 1000: loss: 3933.302
iteration 1100: loss: 3936.910
iteration 1200: loss: 3944.781
iteration 1300: loss: 3939.091
iteration 1400: loss: 3942.400
iteration 1500: loss: 3939.505
iteration 1600: loss: 3936.484
iteration 1700: loss: 3941.573
iteration 1800: loss: 3937.490
====> Epoch: 035 Train loss: 3939.7451  took : 53.23916554450989
====> Test loss: 3944.3298
iteration 0000: loss: 3938.551
iteration 0100: loss: 3940.902
iteration 0200: loss: 3938.638
iteration 0300: loss: 3943.434
iteration 0400: loss: 3935.690
iteration 0500: loss: 3941.712
iteration 0600: loss: 3935.109
iteration 0700: loss: 3939.039
iteration 0800: loss: 3940.850
iteration 0900: loss: 3940.789
iteration 1000: loss: 3937.359
iteration 1100: loss: 3943.428
iteration 1200: loss: 3939.348
iteration 1300: loss: 3935.615
iteration 1400: loss: 3938.027
iteration 1500: loss: 3946.436
iteration 1600: loss: 3933.942
iteration 1700: loss: 3939.927
iteration 1800: loss: 3940.914
====> Epoch: 036 Train loss: 3939.6401  took : 53.015305519104004
====> Test loss: 3944.5495
iteration 0000: loss: 3943.259
iteration 0100: loss: 3934.095
iteration 0200: loss: 3942.422
iteration 0300: loss: 3937.392
iteration 0400: loss: 3935.022
iteration 0500: loss: 3941.949
iteration 0600: loss: 3937.631
iteration 0700: loss: 3940.365
iteration 0800: loss: 3942.387
iteration 0900: loss: 3942.590
iteration 1000: loss: 3938.558
iteration 1100: loss: 3941.562
iteration 1200: loss: 3938.743
iteration 1300: loss: 3934.084
iteration 1400: loss: 3945.265
iteration 1500: loss: 3938.191
iteration 1600: loss: 3940.530
iteration 1700: loss: 3949.090
iteration 1800: loss: 3942.997
====> Epoch: 037 Train loss: 3939.7191  took : 53.296592235565186
====> Test loss: 3944.3833
iteration 0000: loss: 3938.789
iteration 0100: loss: 3937.084
iteration 0200: loss: 3942.039
iteration 0300: loss: 3935.344
iteration 0400: loss: 3947.596
iteration 0500: loss: 3942.961
iteration 0600: loss: 3940.125
iteration 0700: loss: 3939.015
iteration 0800: loss: 3927.660
iteration 0900: loss: 3937.442
iteration 1000: loss: 3937.513
iteration 1100: loss: 3943.146
iteration 1200: loss: 3942.099
iteration 1300: loss: 3944.207
iteration 1400: loss: 3941.575
iteration 1500: loss: 3933.602
iteration 1600: loss: 3940.320
iteration 1700: loss: 3937.000
iteration 1800: loss: 3943.367
====> Epoch: 038 Train loss: 3939.4416  took : 52.93758010864258
====> Test loss: 3944.0140
iteration 0000: loss: 3934.978
iteration 0100: loss: 3943.560
iteration 0200: loss: 3937.021
iteration 0300: loss: 3944.303
iteration 0400: loss: 3938.064
iteration 0500: loss: 3945.383
iteration 0600: loss: 3934.981
iteration 0700: loss: 3934.785
iteration 0800: loss: 3942.907
iteration 0900: loss: 3941.938
iteration 1000: loss: 3939.348
iteration 1100: loss: 3938.194
iteration 1200: loss: 3935.004
iteration 1300: loss: 3943.886
iteration 1400: loss: 3935.415
iteration 1500: loss: 3939.181
iteration 1600: loss: 3945.335
iteration 1700: loss: 3941.154
iteration 1800: loss: 3935.119
====> Epoch: 039 Train loss: 3939.1798  took : 53.10704827308655
====> Test loss: 3943.6144
iteration 0000: loss: 3936.423
iteration 0100: loss: 3935.712
iteration 0200: loss: 3938.187
iteration 0300: loss: 3949.476
iteration 0400: loss: 3942.088
iteration 0500: loss: 3942.839
iteration 0600: loss: 3941.223
iteration 0700: loss: 3935.187
iteration 0800: loss: 3936.995
iteration 0900: loss: 3941.984
iteration 1000: loss: 3940.959
iteration 1100: loss: 3936.584
iteration 1200: loss: 3936.312
iteration 1300: loss: 3936.811
iteration 1400: loss: 3940.851
iteration 1500: loss: 3937.185
iteration 1600: loss: 3939.208
iteration 1700: loss: 3933.768
iteration 1800: loss: 3949.514
====> Epoch: 040 Train loss: 3939.1723  took : 52.991392374038696
====> Test loss: 3944.5609
iteration 0000: loss: 3937.260
iteration 0100: loss: 3941.423
iteration 0200: loss: 3938.681
iteration 0300: loss: 3938.073
iteration 0400: loss: 3939.885
iteration 0500: loss: 3941.756
iteration 0600: loss: 3935.690
iteration 0700: loss: 3946.559
iteration 0800: loss: 3940.952
iteration 0900: loss: 3933.801
iteration 1000: loss: 3941.378
iteration 1100: loss: 3939.380
iteration 1200: loss: 3940.227
iteration 1300: loss: 3943.515
iteration 1400: loss: 3937.662
iteration 1500: loss: 3941.405
iteration 1600: loss: 3938.490
iteration 1700: loss: 3946.625
iteration 1800: loss: 3942.378
====> Epoch: 041 Train loss: 3938.9710  took : 52.982110023498535
====> Test loss: 3943.7387
iteration 0000: loss: 3938.571
iteration 0100: loss: 3939.175
iteration 0200: loss: 3938.343
iteration 0300: loss: 3932.460
iteration 0400: loss: 3939.813
iteration 0500: loss: 3938.446
iteration 0600: loss: 3937.750
iteration 0700: loss: 3941.623
iteration 0800: loss: 3938.811
iteration 0900: loss: 3942.485
iteration 1000: loss: 3937.753
iteration 1100: loss: 3934.517
iteration 1200: loss: 3941.547
iteration 1300: loss: 3939.203
iteration 1400: loss: 3936.219
iteration 1500: loss: 3942.329
iteration 1600: loss: 3937.115
iteration 1700: loss: 3940.710
iteration 1800: loss: 3936.141
====> Epoch: 042 Train loss: 3938.9493  took : 53.359506368637085
====> Test loss: 3943.0873
iteration 0000: loss: 3935.074
iteration 0100: loss: 3935.720
iteration 0200: loss: 3933.155
iteration 0300: loss: 3938.895
iteration 0400: loss: 3930.748
iteration 0500: loss: 3938.954
iteration 0600: loss: 3940.099
iteration 0700: loss: 3936.444
iteration 0800: loss: 3937.422
iteration 0900: loss: 3941.494
iteration 1000: loss: 3940.430
iteration 1100: loss: 3942.222
iteration 1200: loss: 3942.296
iteration 1300: loss: 3939.288
iteration 1400: loss: 3936.579
iteration 1500: loss: 3934.014
iteration 1600: loss: 3947.770
iteration 1700: loss: 3947.618
iteration 1800: loss: 3939.483
====> Epoch: 043 Train loss: 3938.6004  took : 53.18950271606445
====> Test loss: 3943.5817
iteration 0000: loss: 3935.142
iteration 0100: loss: 3940.628
iteration 0200: loss: 3939.205
iteration 0300: loss: 3939.991
iteration 0400: loss: 3937.202
iteration 0500: loss: 3941.621
iteration 0600: loss: 3944.448
iteration 0700: loss: 3933.186
iteration 0800: loss: 3936.588
iteration 0900: loss: 3937.324
iteration 1000: loss: 3935.960
iteration 1100: loss: 3938.123
iteration 1200: loss: 3932.618
iteration 1300: loss: 3935.739
iteration 1400: loss: 3938.432
iteration 1500: loss: 3936.433
iteration 1600: loss: 3939.833
iteration 1700: loss: 3940.776
iteration 1800: loss: 3937.206
====> Epoch: 044 Train loss: 3938.7272  took : 52.97223687171936
====> Test loss: 3943.5352
iteration 0000: loss: 3933.182
iteration 0100: loss: 3943.783
iteration 0200: loss: 3940.240
iteration 0300: loss: 3938.946
iteration 0400: loss: 3943.107
iteration 0500: loss: 3938.401
iteration 0600: loss: 3943.439
iteration 0700: loss: 3943.424
iteration 0800: loss: 3938.591
iteration 0900: loss: 3939.663
iteration 1000: loss: 3932.282
iteration 1100: loss: 3935.009
iteration 1200: loss: 3933.229
iteration 1300: loss: 3934.039
iteration 1400: loss: 3942.209
iteration 1500: loss: 3938.106
iteration 1600: loss: 3945.718
iteration 1700: loss: 3935.779
iteration 1800: loss: 3935.985
====> Epoch: 045 Train loss: 3938.6829  took : 53.140860080718994
====> Test loss: 3943.2281
iteration 0000: loss: 3936.136
iteration 0100: loss: 3933.588
iteration 0200: loss: 3937.317
iteration 0300: loss: 3940.338
iteration 0400: loss: 3937.162
iteration 0500: loss: 3937.523
iteration 0600: loss: 3940.400
iteration 0700: loss: 3933.618
iteration 0800: loss: 3942.199
iteration 0900: loss: 3940.359
iteration 1000: loss: 3942.225
iteration 1100: loss: 3942.864
iteration 1200: loss: 3933.951
iteration 1300: loss: 3936.943
iteration 1400: loss: 3935.199
iteration 1500: loss: 3941.355
iteration 1600: loss: 3943.957
iteration 1700: loss: 3935.841
iteration 1800: loss: 3941.307
====> Epoch: 046 Train loss: 3938.6107  took : 53.260573387145996
====> Test loss: 3943.2941
iteration 0000: loss: 3931.753
iteration 0100: loss: 3935.678
iteration 0200: loss: 3929.113
iteration 0300: loss: 3940.235
iteration 0400: loss: 3932.181
iteration 0500: loss: 3935.172
iteration 0600: loss: 3941.767
iteration 0700: loss: 3935.568
iteration 0800: loss: 3945.587
iteration 0900: loss: 3936.685
iteration 1000: loss: 3938.680
iteration 1100: loss: 3934.432
iteration 1200: loss: 3936.291
iteration 1300: loss: 3938.205
iteration 1400: loss: 3937.047
iteration 1500: loss: 3934.417
iteration 1600: loss: 3941.678
iteration 1700: loss: 3935.915
iteration 1800: loss: 3940.518
====> Epoch: 047 Train loss: 3938.2059  took : 52.952228307724
====> Test loss: 3942.9507
iteration 0000: loss: 3943.432
iteration 0100: loss: 3937.377
iteration 0200: loss: 3933.436
iteration 0300: loss: 3937.148
iteration 0400: loss: 3935.837
iteration 0500: loss: 3943.273
iteration 0600: loss: 3937.851
iteration 0700: loss: 3934.406
iteration 0800: loss: 3939.759
iteration 0900: loss: 3943.128
iteration 1000: loss: 3936.952
iteration 1100: loss: 3939.047
iteration 1200: loss: 3937.840
iteration 1300: loss: 3940.260
iteration 1400: loss: 3938.036
iteration 1500: loss: 3940.338
iteration 1600: loss: 3944.738
iteration 1700: loss: 3936.801
iteration 1800: loss: 3936.897
====> Epoch: 048 Train loss: 3938.1328  took : 53.16676712036133
====> Test loss: 3943.0002
iteration 0000: loss: 3938.788
iteration 0100: loss: 3939.180
iteration 0200: loss: 3940.880
iteration 0300: loss: 3939.838
iteration 0400: loss: 3932.405
iteration 0500: loss: 3937.686
iteration 0600: loss: 3932.042
iteration 0700: loss: 3944.833
iteration 0800: loss: 3937.755
iteration 0900: loss: 3934.982
iteration 1000: loss: 3937.547
iteration 1100: loss: 3933.771
iteration 1200: loss: 3939.215
iteration 1300: loss: 3934.860
iteration 1400: loss: 3937.440
iteration 1500: loss: 3937.595
iteration 1600: loss: 3937.338
iteration 1700: loss: 3945.923
iteration 1800: loss: 3940.649
====> Epoch: 049 Train loss: 3938.1412  took : 53.20565104484558
====> Test loss: 3943.5372
iteration 0000: loss: 3935.330
iteration 0100: loss: 3938.651
iteration 0200: loss: 3934.770
iteration 0300: loss: 3943.783
iteration 0400: loss: 3937.679
iteration 0500: loss: 3944.545
iteration 0600: loss: 3932.384
iteration 0700: loss: 3940.546
iteration 0800: loss: 3940.779
iteration 0900: loss: 3944.638
iteration 1000: loss: 3944.462
iteration 1100: loss: 3935.829
iteration 1200: loss: 3937.443
iteration 1300: loss: 3944.177
iteration 1400: loss: 3937.698
iteration 1500: loss: 3934.278
iteration 1600: loss: 3934.309
iteration 1700: loss: 3941.028
iteration 1800: loss: 3941.675
====> Epoch: 050 Train loss: 3938.4320  took : 53.240861892700195
====> Test loss: 3942.7497
====> [MM-VAE] Time: 3182.212s or 00:53:02
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  2
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_2
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1987.268
iteration 0100: loss: 1567.121
iteration 0200: loss: 1563.283
iteration 0300: loss: 1564.682
iteration 0400: loss: 1547.563
iteration 0500: loss: 1545.841
iteration 0600: loss: 1537.124
iteration 0700: loss: 1539.511
iteration 0800: loss: 1535.032
iteration 0900: loss: 1537.321
====> Epoch: 001 Train loss: 1555.5197  took : 8.56789755821228
====> Test loss: 1538.3732
iteration 0000: loss: 1535.931
iteration 0100: loss: 1532.816
iteration 0200: loss: 1532.106
iteration 0300: loss: 1530.919
iteration 0400: loss: 1531.779
iteration 0500: loss: 1531.590
iteration 0600: loss: 1532.633
iteration 0700: loss: 1530.883
iteration 0800: loss: 1532.561
iteration 0900: loss: 1526.702
====> Epoch: 002 Train loss: 1532.3068  took : 8.400631666183472
====> Test loss: 1530.8993
iteration 0000: loss: 1528.278
iteration 0100: loss: 1530.999
iteration 0200: loss: 1526.993
iteration 0300: loss: 1529.304
iteration 0400: loss: 1528.860
iteration 0500: loss: 1528.118
iteration 0600: loss: 1529.703
iteration 0700: loss: 1529.079
iteration 0800: loss: 1530.958
iteration 0900: loss: 1524.310
====> Epoch: 003 Train loss: 1527.4820  took : 8.400948286056519
====> Test loss: 1527.9756
iteration 0000: loss: 1524.026
iteration 0100: loss: 1526.336
iteration 0200: loss: 1528.530
iteration 0300: loss: 1523.849
iteration 0400: loss: 1523.925
iteration 0500: loss: 1523.752
iteration 0600: loss: 1525.068
iteration 0700: loss: 1524.579
iteration 0800: loss: 1521.957
iteration 0900: loss: 1522.231
====> Epoch: 004 Train loss: 1524.8940  took : 8.447645425796509
====> Test loss: 1525.9339
iteration 0000: loss: 1523.423
iteration 0100: loss: 1526.581
iteration 0200: loss: 1519.823
iteration 0300: loss: 1524.388
iteration 0400: loss: 1522.436
iteration 0500: loss: 1523.953
iteration 0600: loss: 1520.509
iteration 0700: loss: 1525.091
iteration 0800: loss: 1522.490
iteration 0900: loss: 1519.786
====> Epoch: 005 Train loss: 1523.0421  took : 8.509521961212158
====> Test loss: 1524.7000
iteration 0000: loss: 1524.406
iteration 0100: loss: 1523.776
iteration 0200: loss: 1520.979
iteration 0300: loss: 1521.968
iteration 0400: loss: 1521.526
iteration 0500: loss: 1525.095
iteration 0600: loss: 1524.590
iteration 0700: loss: 1520.112
iteration 0800: loss: 1521.865
iteration 0900: loss: 1517.757
====> Epoch: 006 Train loss: 1521.6233  took : 8.409287452697754
====> Test loss: 1523.5768
iteration 0000: loss: 1522.381
iteration 0100: loss: 1522.365
iteration 0200: loss: 1518.222
iteration 0300: loss: 1522.268
iteration 0400: loss: 1519.027
iteration 0500: loss: 1520.603
iteration 0600: loss: 1518.389
iteration 0700: loss: 1519.274
iteration 0800: loss: 1517.442
iteration 0900: loss: 1521.343
====> Epoch: 007 Train loss: 1520.5636  took : 8.50032114982605
====> Test loss: 1522.6667
iteration 0000: loss: 1519.217
iteration 0100: loss: 1522.714
iteration 0200: loss: 1519.534
iteration 0300: loss: 1519.725
iteration 0400: loss: 1520.131
iteration 0500: loss: 1519.348
iteration 0600: loss: 1518.950
iteration 0700: loss: 1518.433
iteration 0800: loss: 1521.986
iteration 0900: loss: 1517.944
====> Epoch: 008 Train loss: 1519.7756  took : 8.524347066879272
====> Test loss: 1522.1114
iteration 0000: loss: 1517.815
iteration 0100: loss: 1516.632
iteration 0200: loss: 1517.310
iteration 0300: loss: 1522.850
iteration 0400: loss: 1520.746
iteration 0500: loss: 1519.785
iteration 0600: loss: 1519.073
iteration 0700: loss: 1522.285
iteration 0800: loss: 1517.523
iteration 0900: loss: 1520.601
====> Epoch: 009 Train loss: 1519.1422  took : 8.52272915840149
====> Test loss: 1521.6620
iteration 0000: loss: 1514.053
iteration 0100: loss: 1517.764
iteration 0200: loss: 1518.433
iteration 0300: loss: 1518.255
iteration 0400: loss: 1520.807
iteration 0500: loss: 1520.236
iteration 0600: loss: 1521.175
iteration 0700: loss: 1516.702
iteration 0800: loss: 1517.067
iteration 0900: loss: 1518.870
====> Epoch: 010 Train loss: 1518.6395  took : 8.512811422348022
====> Test loss: 1521.1528
iteration 0000: loss: 1518.848
iteration 0100: loss: 1519.499
iteration 0200: loss: 1519.089
iteration 0300: loss: 1520.100
iteration 0400: loss: 1516.872
iteration 0500: loss: 1520.535
iteration 0600: loss: 1516.950
iteration 0700: loss: 1521.398
iteration 0800: loss: 1517.769
iteration 0900: loss: 1517.601
====> Epoch: 011 Train loss: 1518.1928  took : 8.509618520736694
====> Test loss: 1520.8161
iteration 0000: loss: 1516.889
iteration 0100: loss: 1520.187
iteration 0200: loss: 1517.523
iteration 0300: loss: 1519.774
iteration 0400: loss: 1521.396
iteration 0500: loss: 1519.147
iteration 0600: loss: 1518.487
iteration 0700: loss: 1516.074
iteration 0800: loss: 1515.961
iteration 0900: loss: 1519.477
====> Epoch: 012 Train loss: 1517.8262  took : 8.532625436782837
====> Test loss: 1520.4404
iteration 0000: loss: 1517.597
iteration 0100: loss: 1516.224
iteration 0200: loss: 1518.147
iteration 0300: loss: 1516.982
iteration 0400: loss: 1518.777
iteration 0500: loss: 1518.311
iteration 0600: loss: 1520.282
iteration 0700: loss: 1518.031
iteration 0800: loss: 1518.558
iteration 0900: loss: 1515.306
====> Epoch: 013 Train loss: 1517.5344  took : 8.51440691947937
====> Test loss: 1520.2837
iteration 0000: loss: 1514.075
iteration 0100: loss: 1516.333
iteration 0200: loss: 1517.650
iteration 0300: loss: 1516.543
iteration 0400: loss: 1516.042
iteration 0500: loss: 1516.712
iteration 0600: loss: 1517.019
iteration 0700: loss: 1519.857
iteration 0800: loss: 1518.468
iteration 0900: loss: 1517.202
====> Epoch: 014 Train loss: 1517.2761  took : 8.41483211517334
====> Test loss: 1520.1825
iteration 0000: loss: 1514.462
iteration 0100: loss: 1518.835
iteration 0200: loss: 1515.866
iteration 0300: loss: 1515.450
iteration 0400: loss: 1516.653
iteration 0500: loss: 1517.838
iteration 0600: loss: 1519.085
iteration 0700: loss: 1515.199
iteration 0800: loss: 1517.247
iteration 0900: loss: 1519.651
====> Epoch: 015 Train loss: 1517.0444  took : 8.493442296981812
====> Test loss: 1519.8692
iteration 0000: loss: 1519.812
iteration 0100: loss: 1514.831
iteration 0200: loss: 1515.571
iteration 0300: loss: 1519.271
iteration 0400: loss: 1517.280
iteration 0500: loss: 1515.258
iteration 0600: loss: 1518.004
iteration 0700: loss: 1517.910
iteration 0800: loss: 1514.458
iteration 0900: loss: 1517.847
====> Epoch: 016 Train loss: 1516.7876  took : 8.44867753982544
====> Test loss: 1519.7682
iteration 0000: loss: 1516.344
iteration 0100: loss: 1519.318
iteration 0200: loss: 1518.252
iteration 0300: loss: 1517.461
iteration 0400: loss: 1515.323
iteration 0500: loss: 1515.341
iteration 0600: loss: 1516.166
iteration 0700: loss: 1516.573
iteration 0800: loss: 1515.632
iteration 0900: loss: 1518.438
====> Epoch: 017 Train loss: 1516.6606  took : 8.524725914001465
====> Test loss: 1519.7098
iteration 0000: loss: 1518.671
iteration 0100: loss: 1517.772
iteration 0200: loss: 1516.618
iteration 0300: loss: 1517.039
iteration 0400: loss: 1517.106
iteration 0500: loss: 1514.375
iteration 0600: loss: 1514.068
iteration 0700: loss: 1517.056
iteration 0800: loss: 1518.770
iteration 0900: loss: 1514.359
====> Epoch: 018 Train loss: 1516.4753  took : 8.524755239486694
====> Test loss: 1519.4703
iteration 0000: loss: 1516.822
iteration 0100: loss: 1518.848
iteration 0200: loss: 1516.345
iteration 0300: loss: 1515.994
iteration 0400: loss: 1515.077
iteration 0500: loss: 1514.954
iteration 0600: loss: 1519.059
iteration 0700: loss: 1515.802
iteration 0800: loss: 1517.683
iteration 0900: loss: 1515.674
====> Epoch: 019 Train loss: 1516.3357  took : 8.468920230865479
====> Test loss: 1519.4709
iteration 0000: loss: 1515.806
iteration 0100: loss: 1519.241
iteration 0200: loss: 1514.805
iteration 0300: loss: 1516.279
iteration 0400: loss: 1516.564
iteration 0500: loss: 1516.121
iteration 0600: loss: 1516.356
iteration 0700: loss: 1518.514
iteration 0800: loss: 1516.211
iteration 0900: loss: 1515.783
====> Epoch: 020 Train loss: 1516.1664  took : 8.520237445831299
====> Test loss: 1519.4899
iteration 0000: loss: 1515.555
iteration 0100: loss: 1516.008
iteration 0200: loss: 1513.423
iteration 0300: loss: 1512.736
iteration 0400: loss: 1514.991
iteration 0500: loss: 1513.097
iteration 0600: loss: 1515.934
iteration 0700: loss: 1514.318
iteration 0800: loss: 1514.365
iteration 0900: loss: 1517.439
====> Epoch: 021 Train loss: 1516.0578  took : 8.539404392242432
====> Test loss: 1519.2632
iteration 0000: loss: 1516.759
iteration 0100: loss: 1516.603
iteration 0200: loss: 1516.666
iteration 0300: loss: 1514.788
iteration 0400: loss: 1518.153
iteration 0500: loss: 1512.794
iteration 0600: loss: 1518.423
iteration 0700: loss: 1514.531
iteration 0800: loss: 1514.091
iteration 0900: loss: 1515.218
====> Epoch: 022 Train loss: 1515.9842  took : 8.472475051879883
====> Test loss: 1519.1753
iteration 0000: loss: 1514.176
iteration 0100: loss: 1516.635
iteration 0200: loss: 1515.964
iteration 0300: loss: 1517.949
iteration 0400: loss: 1514.285
iteration 0500: loss: 1515.594
iteration 0600: loss: 1516.767
iteration 0700: loss: 1516.510
iteration 0800: loss: 1514.016
iteration 0900: loss: 1513.962
====> Epoch: 023 Train loss: 1515.8115  took : 8.522857427597046
====> Test loss: 1519.1185
iteration 0000: loss: 1513.406
iteration 0100: loss: 1515.095
iteration 0200: loss: 1517.923
iteration 0300: loss: 1519.210
iteration 0400: loss: 1517.130
iteration 0500: loss: 1516.321
iteration 0600: loss: 1515.529
iteration 0700: loss: 1514.600
iteration 0800: loss: 1514.680
iteration 0900: loss: 1516.072
====> Epoch: 024 Train loss: 1515.7324  took : 8.519679069519043
====> Test loss: 1519.0907
iteration 0000: loss: 1513.585
iteration 0100: loss: 1514.590
iteration 0200: loss: 1516.398
iteration 0300: loss: 1516.353
iteration 0400: loss: 1518.049
iteration 0500: loss: 1513.271
iteration 0600: loss: 1517.137
iteration 0700: loss: 1518.661
iteration 0800: loss: 1516.374
iteration 0900: loss: 1517.158
====> Epoch: 025 Train loss: 1515.6253  took : 8.45996379852295
====> Test loss: 1519.4787
iteration 0000: loss: 1515.592
iteration 0100: loss: 1516.292
iteration 0200: loss: 1516.064
iteration 0300: loss: 1515.940
iteration 0400: loss: 1513.996
iteration 0500: loss: 1514.321
iteration 0600: loss: 1512.892
iteration 0700: loss: 1515.225
iteration 0800: loss: 1513.876
iteration 0900: loss: 1517.334
====> Epoch: 026 Train loss: 1515.6758  took : 8.489736557006836
====> Test loss: 1518.9793
iteration 0000: loss: 1515.271
iteration 0100: loss: 1514.540
iteration 0200: loss: 1514.346
iteration 0300: loss: 1515.857
iteration 0400: loss: 1516.232
iteration 0500: loss: 1515.761
iteration 0600: loss: 1518.039
iteration 0700: loss: 1515.384
iteration 0800: loss: 1513.585
iteration 0900: loss: 1516.025
====> Epoch: 027 Train loss: 1515.4376  took : 8.505296230316162
====> Test loss: 1518.8990
iteration 0000: loss: 1514.069
iteration 0100: loss: 1514.461
iteration 0200: loss: 1516.496
iteration 0300: loss: 1517.257
iteration 0400: loss: 1514.994
iteration 0500: loss: 1514.100
iteration 0600: loss: 1516.107
iteration 0700: loss: 1514.087
iteration 0800: loss: 1515.891
iteration 0900: loss: 1514.373
====> Epoch: 028 Train loss: 1515.3658  took : 8.404489040374756
====> Test loss: 1518.8699
iteration 0000: loss: 1517.258
iteration 0100: loss: 1516.397
iteration 0200: loss: 1517.404
iteration 0300: loss: 1513.961
iteration 0400: loss: 1515.946
iteration 0500: loss: 1516.035
iteration 0600: loss: 1512.374
iteration 0700: loss: 1516.387
iteration 0800: loss: 1516.736
iteration 0900: loss: 1514.676
====> Epoch: 029 Train loss: 1515.2596  took : 8.491381883621216
====> Test loss: 1518.7613
iteration 0000: loss: 1515.196
iteration 0100: loss: 1518.153
iteration 0200: loss: 1514.586
iteration 0300: loss: 1514.399
iteration 0400: loss: 1513.800
iteration 0500: loss: 1514.746
iteration 0600: loss: 1512.510
iteration 0700: loss: 1515.039
iteration 0800: loss: 1515.534
iteration 0900: loss: 1517.112
====> Epoch: 030 Train loss: 1515.2809  took : 8.402242660522461
====> Test loss: 1518.8798
iteration 0000: loss: 1513.518
iteration 0100: loss: 1517.258
iteration 0200: loss: 1513.355
iteration 0300: loss: 1512.609
iteration 0400: loss: 1512.964
iteration 0500: loss: 1517.709
iteration 0600: loss: 1513.785
iteration 0700: loss: 1517.842
iteration 0800: loss: 1514.604
iteration 0900: loss: 1514.855
====> Epoch: 031 Train loss: 1515.1429  took : 8.40169095993042
====> Test loss: 1518.5493
iteration 0000: loss: 1515.373
iteration 0100: loss: 1513.629
iteration 0200: loss: 1515.396
iteration 0300: loss: 1513.131
iteration 0400: loss: 1513.841
iteration 0500: loss: 1511.608
iteration 0600: loss: 1515.033
iteration 0700: loss: 1513.503
iteration 0800: loss: 1512.970
iteration 0900: loss: 1515.585
====> Epoch: 032 Train loss: 1515.0628  took : 8.453630685806274
====> Test loss: 1518.6696
iteration 0000: loss: 1514.273
iteration 0100: loss: 1516.357
iteration 0200: loss: 1514.658
iteration 0300: loss: 1514.128
iteration 0400: loss: 1513.188
iteration 0500: loss: 1517.557
iteration 0600: loss: 1514.157
iteration 0700: loss: 1513.741
iteration 0800: loss: 1515.089
iteration 0900: loss: 1517.408
====> Epoch: 033 Train loss: 1514.9843  took : 8.477718114852905
====> Test loss: 1518.5492
iteration 0000: loss: 1516.500
iteration 0100: loss: 1514.231
iteration 0200: loss: 1516.873
iteration 0300: loss: 1516.524
iteration 0400: loss: 1514.756
iteration 0500: loss: 1513.053
iteration 0600: loss: 1514.158
iteration 0700: loss: 1512.559
iteration 0800: loss: 1514.108
iteration 0900: loss: 1514.673
====> Epoch: 034 Train loss: 1514.9874  took : 8.43327522277832
====> Test loss: 1518.7307
iteration 0000: loss: 1515.197
iteration 0100: loss: 1514.419
iteration 0200: loss: 1510.610
iteration 0300: loss: 1515.307
iteration 0400: loss: 1516.118
iteration 0500: loss: 1511.423
iteration 0600: loss: 1517.239
iteration 0700: loss: 1515.866
iteration 0800: loss: 1514.725
iteration 0900: loss: 1512.341
====> Epoch: 035 Train loss: 1514.8411  took : 8.519091129302979
====> Test loss: 1518.6856
iteration 0000: loss: 1514.101
iteration 0100: loss: 1512.865
iteration 0200: loss: 1515.357
iteration 0300: loss: 1516.352
iteration 0400: loss: 1515.037
iteration 0500: loss: 1513.885
iteration 0600: loss: 1513.919
iteration 0700: loss: 1515.206
iteration 0800: loss: 1516.477
iteration 0900: loss: 1514.205
====> Epoch: 036 Train loss: 1514.8270  took : 8.460216522216797
====> Test loss: 1518.5153
iteration 0000: loss: 1513.396
iteration 0100: loss: 1515.718
iteration 0200: loss: 1513.880
iteration 0300: loss: 1514.604
iteration 0400: loss: 1515.237
iteration 0500: loss: 1511.435
iteration 0600: loss: 1516.512
iteration 0700: loss: 1515.377
iteration 0800: loss: 1515.147
iteration 0900: loss: 1514.745
====> Epoch: 037 Train loss: 1514.7425  took : 8.422394037246704
====> Test loss: 1518.4996
iteration 0000: loss: 1515.121
iteration 0100: loss: 1514.034
iteration 0200: loss: 1512.400
iteration 0300: loss: 1514.892
iteration 0400: loss: 1515.193
iteration 0500: loss: 1511.971
iteration 0600: loss: 1514.083
iteration 0700: loss: 1513.432
iteration 0800: loss: 1517.440
iteration 0900: loss: 1511.556
====> Epoch: 038 Train loss: 1514.6685  took : 8.459734439849854
====> Test loss: 1518.5094
iteration 0000: loss: 1513.483
iteration 0100: loss: 1516.554
iteration 0200: loss: 1513.191
iteration 0300: loss: 1517.423
iteration 0400: loss: 1513.320
iteration 0500: loss: 1513.491
iteration 0600: loss: 1512.633
iteration 0700: loss: 1516.774
iteration 0800: loss: 1513.567
iteration 0900: loss: 1513.636
====> Epoch: 039 Train loss: 1514.6535  took : 8.388951539993286
====> Test loss: 1518.4426
iteration 0000: loss: 1512.895
iteration 0100: loss: 1513.972
iteration 0200: loss: 1518.186
iteration 0300: loss: 1512.179
iteration 0400: loss: 1511.858
iteration 0500: loss: 1516.307
iteration 0600: loss: 1516.868
iteration 0700: loss: 1514.636
iteration 0800: loss: 1513.414
iteration 0900: loss: 1513.590
====> Epoch: 040 Train loss: 1514.6892  took : 8.533894777297974
====> Test loss: 1518.4416
iteration 0000: loss: 1514.878
iteration 0100: loss: 1515.011
iteration 0200: loss: 1515.527
iteration 0300: loss: 1514.300
iteration 0400: loss: 1513.062
iteration 0500: loss: 1512.816
iteration 0600: loss: 1512.937
iteration 0700: loss: 1514.465
iteration 0800: loss: 1518.659
iteration 0900: loss: 1515.444
====> Epoch: 041 Train loss: 1514.5507  took : 8.450377941131592
====> Test loss: 1518.5018
iteration 0000: loss: 1511.950
iteration 0100: loss: 1514.175
iteration 0200: loss: 1513.598
iteration 0300: loss: 1515.796
iteration 0400: loss: 1514.433
iteration 0500: loss: 1516.627
iteration 0600: loss: 1515.391
iteration 0700: loss: 1512.891
iteration 0800: loss: 1512.713
iteration 0900: loss: 1517.033
====> Epoch: 042 Train loss: 1514.6864  took : 8.477374076843262
====> Test loss: 1518.3478
iteration 0000: loss: 1515.349
iteration 0100: loss: 1512.500
iteration 0200: loss: 1515.503
iteration 0300: loss: 1514.941
iteration 0400: loss: 1514.327
iteration 0500: loss: 1513.557
iteration 0600: loss: 1511.203
iteration 0700: loss: 1515.043
iteration 0800: loss: 1514.479
iteration 0900: loss: 1515.106
====> Epoch: 043 Train loss: 1514.4466  took : 8.412205696105957
====> Test loss: 1518.4493
iteration 0000: loss: 1515.087
iteration 0100: loss: 1513.627
iteration 0200: loss: 1512.730
iteration 0300: loss: 1513.101
iteration 0400: loss: 1515.520
iteration 0500: loss: 1512.036
iteration 0600: loss: 1515.570
iteration 0700: loss: 1516.371
iteration 0800: loss: 1514.648
iteration 0900: loss: 1514.092
====> Epoch: 044 Train loss: 1514.4583  took : 8.401804685592651
====> Test loss: 1518.4997
iteration 0000: loss: 1513.720
iteration 0100: loss: 1513.530
iteration 0200: loss: 1515.245
iteration 0300: loss: 1515.439
iteration 0400: loss: 1512.022
iteration 0500: loss: 1514.469
iteration 0600: loss: 1514.508
iteration 0700: loss: 1513.981
iteration 0800: loss: 1514.899
iteration 0900: loss: 1513.937
====> Epoch: 045 Train loss: 1514.3767  took : 8.516355514526367
====> Test loss: 1518.4072
iteration 0000: loss: 1511.292
iteration 0100: loss: 1511.778
iteration 0200: loss: 1514.759
iteration 0300: loss: 1513.410
iteration 0400: loss: 1513.594
iteration 0500: loss: 1514.957
iteration 0600: loss: 1512.648
iteration 0700: loss: 1514.763
iteration 0800: loss: 1514.256
iteration 0900: loss: 1513.849
====> Epoch: 046 Train loss: 1514.4050  took : 8.40930724143982
====> Test loss: 1518.4329
iteration 0000: loss: 1515.397
iteration 0100: loss: 1514.865
iteration 0200: loss: 1514.581
iteration 0300: loss: 1514.379
iteration 0400: loss: 1513.895
iteration 0500: loss: 1513.484
iteration 0600: loss: 1510.072
iteration 0700: loss: 1513.449
iteration 0800: loss: 1514.616
iteration 0900: loss: 1512.710
====> Epoch: 047 Train loss: 1514.2967  took : 8.519585371017456
====> Test loss: 1518.2837
iteration 0000: loss: 1513.164
iteration 0100: loss: 1514.579
iteration 0200: loss: 1515.693
iteration 0300: loss: 1511.255
iteration 0400: loss: 1515.916
iteration 0500: loss: 1515.592
iteration 0600: loss: 1514.610
iteration 0700: loss: 1511.373
iteration 0800: loss: 1515.055
iteration 0900: loss: 1515.233
====> Epoch: 048 Train loss: 1514.2176  took : 8.521762609481812
====> Test loss: 1518.5787
iteration 0000: loss: 1515.076
iteration 0100: loss: 1514.598
iteration 0200: loss: 1515.175
iteration 0300: loss: 1512.215
iteration 0400: loss: 1512.890
iteration 0500: loss: 1515.622
iteration 0600: loss: 1516.721
iteration 0700: loss: 1511.925
iteration 0800: loss: 1517.073
iteration 0900: loss: 1514.515
====> Epoch: 049 Train loss: 1514.2188  took : 8.441531658172607
====> Test loss: 1518.5564
iteration 0000: loss: 1514.345
iteration 0100: loss: 1516.005
iteration 0200: loss: 1513.530
iteration 0300: loss: 1516.510
iteration 0400: loss: 1516.764
iteration 0500: loss: 1514.559
iteration 0600: loss: 1516.826
iteration 0700: loss: 1514.852
iteration 0800: loss: 1513.044
iteration 0900: loss: 1513.175
====> Epoch: 050 Train loss: 1514.1359  took : 8.562148571014404
====> Test loss: 1518.3841
====> [MM-VAE] Time: 505.701s or 00:08:25
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  2
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_2
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.370
iteration 0100: loss: 2061.500
iteration 0200: loss: 2036.650
iteration 0300: loss: 2008.703
iteration 0400: loss: 2005.579
iteration 0500: loss: 2002.872
iteration 0600: loss: 2001.566
iteration 0700: loss: 1997.073
iteration 0800: loss: 1999.953
iteration 0900: loss: 1995.699
====> Epoch: 001 Train loss: 2021.2288  took : 12.351354598999023
====> Test loss: 1999.6596
iteration 0000: loss: 1995.326
iteration 0100: loss: 1996.255
iteration 0200: loss: 1988.887
iteration 0300: loss: 1988.794
iteration 0400: loss: 1990.825
iteration 0500: loss: 1993.623
iteration 0600: loss: 1980.245
iteration 0700: loss: 1986.172
iteration 0800: loss: 1984.667
iteration 0900: loss: 1981.691
====> Epoch: 002 Train loss: 1989.3463  took : 13.449631214141846
====> Test loss: 1983.5037
iteration 0000: loss: 1987.052
iteration 0100: loss: 1977.964
iteration 0200: loss: 1981.155
iteration 0300: loss: 1981.276
iteration 0400: loss: 1973.750
iteration 0500: loss: 1972.836
iteration 0600: loss: 1979.112
iteration 0700: loss: 1971.417
iteration 0800: loss: 1976.171
iteration 0900: loss: 1974.580
====> Epoch: 003 Train loss: 1977.0808  took : 13.211960792541504
====> Test loss: 1976.3161
iteration 0000: loss: 1978.848
iteration 0100: loss: 1971.337
iteration 0200: loss: 1970.520
iteration 0300: loss: 1968.333
iteration 0400: loss: 1969.223
iteration 0500: loss: 1969.613
iteration 0600: loss: 1968.562
iteration 0700: loss: 1973.018
iteration 0800: loss: 1970.194
iteration 0900: loss: 1972.643
====> Epoch: 004 Train loss: 1971.0929  took : 12.68033504486084
====> Test loss: 1973.5355
iteration 0000: loss: 1970.508
iteration 0100: loss: 1970.017
iteration 0200: loss: 1970.046
iteration 0300: loss: 1968.358
iteration 0400: loss: 1967.825
iteration 0500: loss: 1970.398
iteration 0600: loss: 1966.084
iteration 0700: loss: 1971.354
iteration 0800: loss: 1968.196
iteration 0900: loss: 1962.727
====> Epoch: 005 Train loss: 1967.6035  took : 13.048694133758545
====> Test loss: 1967.9755
iteration 0000: loss: 1969.236
iteration 0100: loss: 1964.521
iteration 0200: loss: 1962.573
iteration 0300: loss: 1962.508
iteration 0400: loss: 1960.961
iteration 0500: loss: 1961.291
iteration 0600: loss: 1964.041
iteration 0700: loss: 1966.585
iteration 0800: loss: 1962.227
iteration 0900: loss: 1966.752
====> Epoch: 006 Train loss: 1964.4245  took : 11.93833303451538
====> Test loss: 1965.7435
iteration 0000: loss: 1958.038
iteration 0100: loss: 1966.176
iteration 0200: loss: 1962.460
iteration 0300: loss: 1961.093
iteration 0400: loss: 1965.206
iteration 0500: loss: 1959.378
iteration 0600: loss: 1965.996
iteration 0700: loss: 1958.013
iteration 0800: loss: 1964.485
iteration 0900: loss: 1963.505
====> Epoch: 007 Train loss: 1962.4500  took : 12.596297264099121
====> Test loss: 1963.8919
iteration 0000: loss: 1958.674
iteration 0100: loss: 1963.248
iteration 0200: loss: 1963.308
iteration 0300: loss: 1955.944
iteration 0400: loss: 1961.676
iteration 0500: loss: 1962.526
iteration 0600: loss: 1962.571
iteration 0700: loss: 1960.892
iteration 0800: loss: 1959.468
iteration 0900: loss: 1961.526
====> Epoch: 008 Train loss: 1961.1192  took : 12.636897802352905
====> Test loss: 1962.5949
iteration 0000: loss: 1965.532
iteration 0100: loss: 1962.018
iteration 0200: loss: 1961.893
iteration 0300: loss: 1960.682
iteration 0400: loss: 1959.492
iteration 0500: loss: 1960.622
iteration 0600: loss: 1956.918
iteration 0700: loss: 1963.610
iteration 0800: loss: 1964.851
iteration 0900: loss: 1959.572
====> Epoch: 009 Train loss: 1959.9622  took : 13.051426887512207
====> Test loss: 1961.7715
iteration 0000: loss: 1959.517
iteration 0100: loss: 1957.895
iteration 0200: loss: 1959.953
iteration 0300: loss: 1961.543
iteration 0400: loss: 1957.152
iteration 0500: loss: 1959.768
iteration 0600: loss: 1959.960
iteration 0700: loss: 1959.049
iteration 0800: loss: 1958.662
iteration 0900: loss: 1958.167
====> Epoch: 010 Train loss: 1959.0854  took : 12.041188955307007
====> Test loss: 1961.1168
iteration 0000: loss: 1959.016
iteration 0100: loss: 1959.947
iteration 0200: loss: 1955.255
iteration 0300: loss: 1959.536
iteration 0400: loss: 1963.396
iteration 0500: loss: 1958.593
iteration 0600: loss: 1958.552
iteration 0700: loss: 1958.803
iteration 0800: loss: 1959.997
iteration 0900: loss: 1961.869
====> Epoch: 011 Train loss: 1958.5491  took : 13.182702779769897
====> Test loss: 1960.7698
iteration 0000: loss: 1957.408
iteration 0100: loss: 1960.583
iteration 0200: loss: 1960.613
iteration 0300: loss: 1958.675
iteration 0400: loss: 1959.945
iteration 0500: loss: 1957.131
iteration 0600: loss: 1961.756
iteration 0700: loss: 1955.965
iteration 0800: loss: 1957.057
iteration 0900: loss: 1959.030
====> Epoch: 012 Train loss: 1958.2792  took : 12.466713428497314
====> Test loss: 1959.9418
iteration 0000: loss: 1954.223
iteration 0100: loss: 1957.651
iteration 0200: loss: 1956.666
iteration 0300: loss: 1959.276
iteration 0400: loss: 1958.975
iteration 0500: loss: 1958.229
iteration 0600: loss: 1957.264
iteration 0700: loss: 1955.355
iteration 0800: loss: 1956.288
iteration 0900: loss: 1959.163
====> Epoch: 013 Train loss: 1957.8169  took : 12.185686111450195
====> Test loss: 1959.6795
iteration 0000: loss: 1958.612
iteration 0100: loss: 1956.943
iteration 0200: loss: 1957.299
iteration 0300: loss: 1956.938
iteration 0400: loss: 1957.713
iteration 0500: loss: 1955.950
iteration 0600: loss: 1955.842
iteration 0700: loss: 1959.980
iteration 0800: loss: 1956.416
iteration 0900: loss: 1957.341
====> Epoch: 014 Train loss: 1957.2752  took : 13.265479803085327
====> Test loss: 1960.2955
iteration 0000: loss: 1961.427
iteration 0100: loss: 1952.847
iteration 0200: loss: 1957.250
iteration 0300: loss: 1953.021
iteration 0400: loss: 1965.416
iteration 0500: loss: 1959.726
iteration 0600: loss: 1957.068
iteration 0700: loss: 1956.266
iteration 0800: loss: 1954.685
iteration 0900: loss: 1958.880
====> Epoch: 015 Train loss: 1957.1776  took : 12.275219678878784
====> Test loss: 1959.0467
iteration 0000: loss: 1959.062
iteration 0100: loss: 1957.751
iteration 0200: loss: 1956.171
iteration 0300: loss: 1958.701
iteration 0400: loss: 1954.941
iteration 0500: loss: 1955.839
iteration 0600: loss: 1955.826
iteration 0700: loss: 1956.520
iteration 0800: loss: 1957.189
iteration 0900: loss: 1957.794
====> Epoch: 016 Train loss: 1956.8280  took : 11.740872621536255
====> Test loss: 1958.9471
iteration 0000: loss: 1954.243
iteration 0100: loss: 1955.605
iteration 0200: loss: 1955.680
iteration 0300: loss: 1955.323
iteration 0400: loss: 1955.583
iteration 0500: loss: 1957.128
iteration 0600: loss: 1954.663
iteration 0700: loss: 1956.326
iteration 0800: loss: 1956.154
iteration 0900: loss: 1955.261
====> Epoch: 017 Train loss: 1956.5068  took : 12.5380117893219
====> Test loss: 1958.1698
iteration 0000: loss: 1957.281
iteration 0100: loss: 1953.388
iteration 0200: loss: 1956.510
iteration 0300: loss: 1956.584
iteration 0400: loss: 1957.487
iteration 0500: loss: 1956.772
iteration 0600: loss: 1954.850
iteration 0700: loss: 1952.957
iteration 0800: loss: 1956.497
iteration 0900: loss: 1954.982
====> Epoch: 018 Train loss: 1956.5446  took : 11.760936498641968
====> Test loss: 1958.3468
iteration 0000: loss: 1956.699
iteration 0100: loss: 1954.193
iteration 0200: loss: 1955.405
iteration 0300: loss: 1956.942
iteration 0400: loss: 1957.251
iteration 0500: loss: 1954.251
iteration 0600: loss: 1954.086
iteration 0700: loss: 1958.041
iteration 0800: loss: 1957.917
iteration 0900: loss: 1953.697
====> Epoch: 019 Train loss: 1956.1204  took : 13.042738676071167
====> Test loss: 1957.8714
iteration 0000: loss: 1954.504
iteration 0100: loss: 1957.311
iteration 0200: loss: 1957.713
iteration 0300: loss: 1958.385
iteration 0400: loss: 1956.240
iteration 0500: loss: 1952.701
iteration 0600: loss: 1953.437
iteration 0700: loss: 1956.750
iteration 0800: loss: 1954.336
iteration 0900: loss: 1953.981
====> Epoch: 020 Train loss: 1955.7770  took : 12.327688455581665
====> Test loss: 1957.4642
iteration 0000: loss: 1956.807
iteration 0100: loss: 1953.929
iteration 0200: loss: 1954.890
iteration 0300: loss: 1953.487
iteration 0400: loss: 1957.930
iteration 0500: loss: 1952.683
iteration 0600: loss: 1957.628
iteration 0700: loss: 1955.520
iteration 0800: loss: 1954.087
iteration 0900: loss: 1956.589
====> Epoch: 021 Train loss: 1955.7519  took : 12.492668628692627
====> Test loss: 1957.2946
iteration 0000: loss: 1953.163
iteration 0100: loss: 1958.579
iteration 0200: loss: 1958.249
iteration 0300: loss: 1953.667
iteration 0400: loss: 1951.628
iteration 0500: loss: 1956.239
iteration 0600: loss: 1954.788
iteration 0700: loss: 1952.798
iteration 0800: loss: 1956.090
iteration 0900: loss: 1956.048
====> Epoch: 022 Train loss: 1955.7075  took : 12.593321084976196
====> Test loss: 1957.2172
iteration 0000: loss: 1956.160
iteration 0100: loss: 1952.967
iteration 0200: loss: 1953.224
iteration 0300: loss: 1956.090
iteration 0400: loss: 1954.773
iteration 0500: loss: 1953.924
iteration 0600: loss: 1957.007
iteration 0700: loss: 1954.598
iteration 0800: loss: 1955.052
iteration 0900: loss: 1953.112
====> Epoch: 023 Train loss: 1955.6695  took : 12.361421346664429
====> Test loss: 1956.9092
iteration 0000: loss: 1956.132
iteration 0100: loss: 1954.704
iteration 0200: loss: 1953.420
iteration 0300: loss: 1955.700
iteration 0400: loss: 1954.190
iteration 0500: loss: 1955.809
iteration 0600: loss: 1953.618
iteration 0700: loss: 1955.633
iteration 0800: loss: 1958.131
iteration 0900: loss: 1954.393
====> Epoch: 024 Train loss: 1955.1016  took : 13.0557861328125
====> Test loss: 1956.6629
iteration 0000: loss: 1954.813
iteration 0100: loss: 1955.327
iteration 0200: loss: 1957.128
iteration 0300: loss: 1954.019
iteration 0400: loss: 1957.265
iteration 0500: loss: 1956.773
iteration 0600: loss: 1954.995
iteration 0700: loss: 1952.996
iteration 0800: loss: 1955.344
iteration 0900: loss: 1956.387
====> Epoch: 025 Train loss: 1955.1039  took : 12.506953239440918
====> Test loss: 1956.6866
iteration 0000: loss: 1953.561
iteration 0100: loss: 1954.758
iteration 0200: loss: 1957.346
iteration 0300: loss: 1955.772
iteration 0400: loss: 1956.658
iteration 0500: loss: 1957.985
iteration 0600: loss: 1958.099
iteration 0700: loss: 1956.641
iteration 0800: loss: 1954.715
iteration 0900: loss: 1957.261
====> Epoch: 026 Train loss: 1955.0955  took : 12.01595950126648
====> Test loss: 1956.8455
iteration 0000: loss: 1953.881
iteration 0100: loss: 1957.021
iteration 0200: loss: 1957.623
iteration 0300: loss: 1954.536
iteration 0400: loss: 1954.317
iteration 0500: loss: 1955.281
iteration 0600: loss: 1955.390
iteration 0700: loss: 1953.371
iteration 0800: loss: 1955.870
iteration 0900: loss: 1954.466
====> Epoch: 027 Train loss: 1955.0464  took : 12.002804040908813
====> Test loss: 1956.2404
iteration 0000: loss: 1953.721
iteration 0100: loss: 1952.018
iteration 0200: loss: 1953.952
iteration 0300: loss: 1956.260
iteration 0400: loss: 1954.187
iteration 0500: loss: 1958.405
iteration 0600: loss: 1954.496
iteration 0700: loss: 1953.091
iteration 0800: loss: 1962.093
iteration 0900: loss: 1954.462
====> Epoch: 028 Train loss: 1954.8684  took : 13.076696634292603
====> Test loss: 1956.8195
iteration 0000: loss: 1951.943
iteration 0100: loss: 1954.619
iteration 0200: loss: 1953.635
iteration 0300: loss: 1955.683
iteration 0400: loss: 1957.981
iteration 0500: loss: 1956.268
iteration 0600: loss: 1956.130
iteration 0700: loss: 1954.656
iteration 0800: loss: 1956.603
iteration 0900: loss: 1957.771
====> Epoch: 029 Train loss: 1955.0006  took : 12.236325979232788
====> Test loss: 1957.0202
iteration 0000: loss: 1954.911
iteration 0100: loss: 1954.317
iteration 0200: loss: 1953.932
iteration 0300: loss: 1952.194
iteration 0400: loss: 1957.115
iteration 0500: loss: 1957.412
iteration 0600: loss: 1953.967
iteration 0700: loss: 1955.028
iteration 0800: loss: 1954.790
iteration 0900: loss: 1955.219
====> Epoch: 030 Train loss: 1954.8677  took : 13.579471588134766
====> Test loss: 1956.3432
iteration 0000: loss: 1957.851
iteration 0100: loss: 1952.856
iteration 0200: loss: 1955.585
iteration 0300: loss: 1956.615
iteration 0400: loss: 1955.030
iteration 0500: loss: 1953.271
iteration 0600: loss: 1953.058
iteration 0700: loss: 1958.466
iteration 0800: loss: 1953.549
iteration 0900: loss: 1952.881
====> Epoch: 031 Train loss: 1954.8624  took : 13.23386549949646
====> Test loss: 1956.7892
iteration 0000: loss: 1954.598
iteration 0100: loss: 1955.187
iteration 0200: loss: 1953.645
iteration 0300: loss: 1952.644
iteration 0400: loss: 1953.612
iteration 0500: loss: 1954.968
iteration 0600: loss: 1954.787
iteration 0700: loss: 1953.824
iteration 0800: loss: 1954.134
iteration 0900: loss: 1957.238
====> Epoch: 032 Train loss: 1954.8992  took : 12.408535480499268
====> Test loss: 1956.6033
iteration 0000: loss: 1955.009
iteration 0100: loss: 1954.621
iteration 0200: loss: 1954.422
iteration 0300: loss: 1953.868
iteration 0400: loss: 1953.114
iteration 0500: loss: 1954.936
iteration 0600: loss: 1953.643
iteration 0700: loss: 1952.622
iteration 0800: loss: 1953.041
iteration 0900: loss: 1954.145
====> Epoch: 033 Train loss: 1954.8413  took : 12.111854076385498
====> Test loss: 1956.1153
iteration 0000: loss: 1955.259
iteration 0100: loss: 1953.414
iteration 0200: loss: 1953.104
iteration 0300: loss: 1956.064
iteration 0400: loss: 1955.456
iteration 0500: loss: 1955.741
iteration 0600: loss: 1953.840
iteration 0700: loss: 1955.923
iteration 0800: loss: 1951.961
iteration 0900: loss: 1953.308
====> Epoch: 034 Train loss: 1954.9096  took : 12.647618532180786
====> Test loss: 1956.1203
iteration 0000: loss: 1953.233
iteration 0100: loss: 1956.213
iteration 0200: loss: 1955.721
iteration 0300: loss: 1954.059
iteration 0400: loss: 1952.324
iteration 0500: loss: 1952.329
iteration 0600: loss: 1955.379
iteration 0700: loss: 1954.628
iteration 0800: loss: 1957.001
iteration 0900: loss: 1956.530
====> Epoch: 035 Train loss: 1954.7770  took : 12.301604986190796
====> Test loss: 1956.5863
iteration 0000: loss: 1955.872
iteration 0100: loss: 1952.836
iteration 0200: loss: 1954.079
iteration 0300: loss: 1955.602
iteration 0400: loss: 1953.463
iteration 0500: loss: 1956.954
iteration 0600: loss: 1956.400
iteration 0700: loss: 1952.166
iteration 0800: loss: 1956.200
iteration 0900: loss: 1957.105
====> Epoch: 036 Train loss: 1954.6941  took : 12.053001165390015
====> Test loss: 1956.2370
iteration 0000: loss: 1955.880
iteration 0100: loss: 1955.944
iteration 0200: loss: 1955.731
iteration 0300: loss: 1954.367
iteration 0400: loss: 1956.226
iteration 0500: loss: 1955.610
iteration 0600: loss: 1952.970
iteration 0700: loss: 1955.947
iteration 0800: loss: 1957.324
iteration 0900: loss: 1954.878
====> Epoch: 037 Train loss: 1954.6939  took : 12.99484395980835
====> Test loss: 1956.2206
iteration 0000: loss: 1955.430
iteration 0100: loss: 1956.018
iteration 0200: loss: 1955.108
iteration 0300: loss: 1954.465
iteration 0400: loss: 1957.465
iteration 0500: loss: 1955.077
iteration 0600: loss: 1953.231
iteration 0700: loss: 1955.828
iteration 0800: loss: 1957.220
iteration 0900: loss: 1955.293
====> Epoch: 038 Train loss: 1954.7793  took : 13.203528642654419
====> Test loss: 1956.2928
iteration 0000: loss: 1953.924
iteration 0100: loss: 1954.476
iteration 0200: loss: 1954.557
iteration 0300: loss: 1958.014
iteration 0400: loss: 1955.279
iteration 0500: loss: 1955.348
iteration 0600: loss: 1953.534
iteration 0700: loss: 1952.608
iteration 0800: loss: 1954.416
iteration 0900: loss: 1955.274
====> Epoch: 039 Train loss: 1954.7797  took : 13.139703750610352
====> Test loss: 1956.0809
iteration 0000: loss: 1953.498
iteration 0100: loss: 1956.567
iteration 0200: loss: 1955.308
iteration 0300: loss: 1955.610
iteration 0400: loss: 1956.884
iteration 0500: loss: 1952.099
iteration 0600: loss: 1954.330
iteration 0700: loss: 1954.838
iteration 0800: loss: 1952.616
iteration 0900: loss: 1957.073
====> Epoch: 040 Train loss: 1954.6067  took : 13.209167718887329
====> Test loss: 1956.1366
iteration 0000: loss: 1955.148
iteration 0100: loss: 1956.011
iteration 0200: loss: 1956.381
iteration 0300: loss: 1952.960
iteration 0400: loss: 1952.933
iteration 0500: loss: 1956.477
iteration 0600: loss: 1954.075
iteration 0700: loss: 1954.723
iteration 0800: loss: 1952.551
iteration 0900: loss: 1953.454
====> Epoch: 041 Train loss: 1954.4166  took : 12.845620393753052
====> Test loss: 1956.1156
iteration 0000: loss: 1953.273
iteration 0100: loss: 1954.703
iteration 0200: loss: 1954.784
iteration 0300: loss: 1952.358
iteration 0400: loss: 1956.435
iteration 0500: loss: 1954.569
iteration 0600: loss: 1954.526
iteration 0700: loss: 1954.513
iteration 0800: loss: 1953.686
iteration 0900: loss: 1957.387
====> Epoch: 042 Train loss: 1954.4744  took : 11.671987533569336
====> Test loss: 1955.9097
iteration 0000: loss: 1952.618
iteration 0100: loss: 1952.709
iteration 0200: loss: 1957.090
iteration 0300: loss: 1953.334
iteration 0400: loss: 1957.166
iteration 0500: loss: 1954.703
iteration 0600: loss: 1954.500
iteration 0700: loss: 1954.132
iteration 0800: loss: 1952.688
iteration 0900: loss: 1956.702
====> Epoch: 043 Train loss: 1954.2226  took : 11.947744846343994
====> Test loss: 1955.7819
iteration 0000: loss: 1954.961
iteration 0100: loss: 1956.726
iteration 0200: loss: 1952.951
iteration 0300: loss: 1955.314
iteration 0400: loss: 1952.587
iteration 0500: loss: 1954.301
iteration 0600: loss: 1953.673
iteration 0700: loss: 1953.960
iteration 0800: loss: 1953.560
iteration 0900: loss: 1956.221
====> Epoch: 044 Train loss: 1954.5644  took : 13.160982131958008
====> Test loss: 1955.9762
iteration 0000: loss: 1954.186
iteration 0100: loss: 1953.498
iteration 0200: loss: 1953.469
iteration 0300: loss: 1953.430
iteration 0400: loss: 1952.517
iteration 0500: loss: 1955.971
iteration 0600: loss: 1954.910
iteration 0700: loss: 1954.555
iteration 0800: loss: 1953.836
iteration 0900: loss: 1956.057
====> Epoch: 045 Train loss: 1954.3743  took : 11.712958812713623
====> Test loss: 1956.3497
iteration 0000: loss: 1954.019
iteration 0100: loss: 1953.697
iteration 0200: loss: 1952.346
iteration 0300: loss: 1954.884
iteration 0400: loss: 1954.598
iteration 0500: loss: 1954.517
iteration 0600: loss: 1954.499
iteration 0700: loss: 1954.170
iteration 0800: loss: 1953.657
iteration 0900: loss: 1952.301
====> Epoch: 046 Train loss: 1954.4506  took : 13.682475090026855
====> Test loss: 1955.9564
iteration 0000: loss: 1953.057
iteration 0100: loss: 1956.555
iteration 0200: loss: 1951.705
iteration 0300: loss: 1955.336
iteration 0400: loss: 1957.038
iteration 0500: loss: 1955.492
iteration 0600: loss: 1952.569
iteration 0700: loss: 1955.312
iteration 0800: loss: 1954.929
iteration 0900: loss: 1956.368
====> Epoch: 047 Train loss: 1954.3565  took : 12.98327922821045
====> Test loss: 1955.8594
iteration 0000: loss: 1952.564
iteration 0100: loss: 1954.496
iteration 0200: loss: 1954.102
iteration 0300: loss: 1953.886
iteration 0400: loss: 1952.145
iteration 0500: loss: 1951.348
iteration 0600: loss: 1954.195
iteration 0700: loss: 1954.829
iteration 0800: loss: 1954.892
iteration 0900: loss: 1954.764
====> Epoch: 048 Train loss: 1954.2637  took : 12.912446737289429
====> Test loss: 1956.0201
iteration 0000: loss: 1954.180
iteration 0100: loss: 1957.199
iteration 0200: loss: 1953.934
iteration 0300: loss: 1956.916
iteration 0400: loss: 1954.668
iteration 0500: loss: 1951.592
iteration 0600: loss: 1952.782
iteration 0700: loss: 1954.326
iteration 0800: loss: 1953.738
iteration 0900: loss: 1954.940
====> Epoch: 049 Train loss: 1954.3037  took : 12.927798986434937
====> Test loss: 1955.8375
iteration 0000: loss: 1954.080
iteration 0100: loss: 1953.283
iteration 0200: loss: 1953.310
iteration 0300: loss: 1956.673
iteration 0400: loss: 1956.078
iteration 0500: loss: 1954.861
iteration 0600: loss: 1956.286
iteration 0700: loss: 1957.587
iteration 0800: loss: 1953.162
iteration 0900: loss: 1953.778
====> Epoch: 050 Train loss: 1954.3437  took : 12.200305938720703
====> Test loss: 1955.6342
====> [MM-VAE] Time: 703.122s or 00:11:43
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  2
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_2
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5219.068
iteration 0100: loss: 4119.885
iteration 0200: loss: 4094.655
iteration 0300: loss: 4056.988
iteration 0400: loss: 4038.420
iteration 0500: loss: 4024.139
iteration 0600: loss: 4025.189
iteration 0700: loss: 4011.437
iteration 0800: loss: 4003.046
iteration 0900: loss: 4006.315
iteration 1000: loss: 4004.428
iteration 1100: loss: 3990.218
iteration 1200: loss: 3987.077
iteration 1300: loss: 4000.639
iteration 1400: loss: 4002.676
iteration 1500: loss: 3993.815
iteration 1600: loss: 3995.290
iteration 1700: loss: 3987.733
iteration 1800: loss: 4001.199
====> Epoch: 001 Train loss: 4025.0374  took : 52.95930814743042
====> Test loss: 3992.6029
iteration 0000: loss: 3982.231
iteration 0100: loss: 3998.715
iteration 0200: loss: 3993.629
iteration 0300: loss: 3991.242
iteration 0400: loss: 3998.979
iteration 0500: loss: 3971.557
iteration 0600: loss: 3985.225
iteration 0700: loss: 3983.824
iteration 0800: loss: 3978.857
iteration 0900: loss: 3986.425
iteration 1000: loss: 3980.437
iteration 1100: loss: 3987.552
iteration 1200: loss: 3969.630
iteration 1300: loss: 3976.479
iteration 1400: loss: 3986.850
iteration 1500: loss: 3973.377
iteration 1600: loss: 3976.929
iteration 1700: loss: 3978.348
iteration 1800: loss: 3979.539
====> Epoch: 002 Train loss: 3982.9805  took : 53.050201416015625
====> Test loss: 3982.7994
iteration 0000: loss: 3965.947
iteration 0100: loss: 3977.888
iteration 0200: loss: 3976.049
iteration 0300: loss: 3968.669
iteration 0400: loss: 3989.193
iteration 0500: loss: 3983.298
iteration 0600: loss: 3979.928
iteration 0700: loss: 3971.335
iteration 0800: loss: 3980.734
iteration 0900: loss: 3982.090
iteration 1000: loss: 3983.263
iteration 1100: loss: 3970.224
iteration 1200: loss: 3978.649
iteration 1300: loss: 3971.000
iteration 1400: loss: 3971.315
iteration 1500: loss: 3974.449
iteration 1600: loss: 3981.476
iteration 1700: loss: 3974.993
iteration 1800: loss: 3972.825
====> Epoch: 003 Train loss: 3975.8056  took : 53.153005838394165
====> Test loss: 3978.9338
iteration 0000: loss: 3967.410
iteration 0100: loss: 3972.806
iteration 0200: loss: 3971.466
iteration 0300: loss: 3981.590
iteration 0400: loss: 3967.058
iteration 0500: loss: 3977.871
iteration 0600: loss: 3964.676
iteration 0700: loss: 3975.585
iteration 0800: loss: 3967.117
iteration 0900: loss: 3977.467
iteration 1000: loss: 3979.325
iteration 1100: loss: 3969.150
iteration 1200: loss: 3975.439
iteration 1300: loss: 3964.780
iteration 1400: loss: 3969.665
iteration 1500: loss: 3966.439
iteration 1600: loss: 3962.162
iteration 1700: loss: 3984.099
iteration 1800: loss: 3964.229
====> Epoch: 004 Train loss: 3972.7770  took : 52.80841040611267
====> Test loss: 3977.8511
iteration 0000: loss: 3968.798
iteration 0100: loss: 3986.200
iteration 0200: loss: 3966.866
iteration 0300: loss: 3974.391
iteration 0400: loss: 3966.916
iteration 0500: loss: 3980.969
iteration 0600: loss: 3968.505
iteration 0700: loss: 3971.036
iteration 0800: loss: 3987.640
iteration 0900: loss: 3967.628
iteration 1000: loss: 3975.612
iteration 1100: loss: 3977.402
iteration 1200: loss: 3976.606
iteration 1300: loss: 3978.778
iteration 1400: loss: 3975.003
iteration 1500: loss: 3978.141
iteration 1600: loss: 3966.342
iteration 1700: loss: 3977.295
iteration 1800: loss: 3975.599
====> Epoch: 005 Train loss: 3971.5141  took : 53.083290815353394
====> Test loss: 3976.3830
iteration 0000: loss: 3964.175
iteration 0100: loss: 3973.070
iteration 0200: loss: 3982.602
iteration 0300: loss: 3977.566
iteration 0400: loss: 3978.364
iteration 0500: loss: 3969.547
iteration 0600: loss: 3969.945
iteration 0700: loss: 3969.152
iteration 0800: loss: 3976.927
iteration 0900: loss: 3978.675
iteration 1000: loss: 3959.930
iteration 1100: loss: 3978.503
iteration 1200: loss: 3967.088
iteration 1300: loss: 3964.157
iteration 1400: loss: 3959.508
iteration 1500: loss: 3969.448
iteration 1600: loss: 3980.247
iteration 1700: loss: 3969.725
iteration 1800: loss: 3970.094
====> Epoch: 006 Train loss: 3970.4189  took : 53.08921551704407
====> Test loss: 3976.0202
iteration 0000: loss: 3975.750
iteration 0100: loss: 3976.321
iteration 0200: loss: 3976.463
iteration 0300: loss: 3964.647
iteration 0400: loss: 3964.575
iteration 0500: loss: 3978.408
iteration 0600: loss: 3980.739
iteration 0700: loss: 3963.821
iteration 0800: loss: 3972.924
iteration 0900: loss: 3971.106
iteration 1000: loss: 3970.746
iteration 1100: loss: 3970.513
iteration 1200: loss: 3967.638
iteration 1300: loss: 3957.894
iteration 1400: loss: 3964.514
iteration 1500: loss: 3967.358
iteration 1600: loss: 3962.323
iteration 1700: loss: 3968.350
iteration 1800: loss: 3972.422
====> Epoch: 007 Train loss: 3969.0876  took : 53.24514031410217
====> Test loss: 3977.1064
iteration 0000: loss: 3982.359
iteration 0100: loss: 3969.895
iteration 0200: loss: 3969.723
iteration 0300: loss: 3975.886
iteration 0400: loss: 3961.811
iteration 0500: loss: 3957.114
iteration 0600: loss: 3966.008
iteration 0700: loss: 3960.258
iteration 0800: loss: 3970.413
iteration 0900: loss: 3967.405
iteration 1000: loss: 3961.026
iteration 1100: loss: 3967.248
iteration 1200: loss: 3962.172
iteration 1300: loss: 3968.421
iteration 1400: loss: 3964.802
iteration 1500: loss: 3963.029
iteration 1600: loss: 3971.854
iteration 1700: loss: 3962.090
iteration 1800: loss: 3964.478
====> Epoch: 008 Train loss: 3967.7167  took : 53.00735950469971
====> Test loss: 3973.1874
iteration 0000: loss: 3966.385
iteration 0100: loss: 3960.753
iteration 0200: loss: 3967.541
iteration 0300: loss: 3970.920
iteration 0400: loss: 3965.969
iteration 0500: loss: 3967.820
iteration 0600: loss: 3965.966
iteration 0700: loss: 3979.385
iteration 0800: loss: 3964.230
iteration 0900: loss: 3966.734
iteration 1000: loss: 3980.395
iteration 1100: loss: 3974.094
iteration 1200: loss: 3976.222
iteration 1300: loss: 3966.134
iteration 1400: loss: 3965.233
iteration 1500: loss: 3969.738
iteration 1600: loss: 3962.921
iteration 1700: loss: 3961.125
iteration 1800: loss: 3966.249
====> Epoch: 009 Train loss: 3967.2367  took : 53.08539295196533
====> Test loss: 3973.3013
iteration 0000: loss: 3960.768
iteration 0100: loss: 3973.549
iteration 0200: loss: 3966.585
iteration 0300: loss: 3963.315
iteration 0400: loss: 3960.750
iteration 0500: loss: 3972.069
iteration 0600: loss: 3965.222
iteration 0700: loss: 3965.594
iteration 0800: loss: 3971.157
iteration 0900: loss: 3965.986
iteration 1000: loss: 3961.306
iteration 1100: loss: 3972.117
iteration 1200: loss: 3961.840
iteration 1300: loss: 3968.909
iteration 1400: loss: 3967.307
iteration 1500: loss: 3958.619
iteration 1600: loss: 3973.350
iteration 1700: loss: 3971.505
iteration 1800: loss: 3962.556
====> Epoch: 010 Train loss: 3966.6581  took : 52.97405672073364
====> Test loss: 3972.6199
iteration 0000: loss: 3962.011
iteration 0100: loss: 3963.170
iteration 0200: loss: 3956.779
iteration 0300: loss: 3965.766
iteration 0400: loss: 3965.683
iteration 0500: loss: 3962.515
iteration 0600: loss: 3967.903
iteration 0700: loss: 3975.085
iteration 0800: loss: 3975.124
iteration 0900: loss: 3973.158
iteration 1000: loss: 3958.958
iteration 1100: loss: 3964.527
iteration 1200: loss: 3962.122
iteration 1300: loss: 3976.417
iteration 1400: loss: 3962.282
iteration 1500: loss: 3967.148
iteration 1600: loss: 3957.395
iteration 1700: loss: 3950.695
iteration 1800: loss: 3967.574
====> Epoch: 011 Train loss: 3966.2350  took : 53.17597675323486
====> Test loss: 3972.2171
iteration 0000: loss: 3964.934
iteration 0100: loss: 3964.846
iteration 0200: loss: 3966.699
iteration 0300: loss: 3969.220
iteration 0400: loss: 3963.093
iteration 0500: loss: 3966.082
iteration 0600: loss: 3964.914
iteration 0700: loss: 3957.600
iteration 0800: loss: 3970.477
iteration 0900: loss: 3974.367
iteration 1000: loss: 3960.611
iteration 1100: loss: 3964.821
iteration 1200: loss: 3963.811
iteration 1300: loss: 3965.083
iteration 1400: loss: 3959.680
iteration 1500: loss: 3959.375
iteration 1600: loss: 3961.201
iteration 1700: loss: 3964.755
iteration 1800: loss: 3973.568
====> Epoch: 012 Train loss: 3965.4145  took : 53.06732988357544
====> Test loss: 3971.7897
iteration 0000: loss: 3959.699
iteration 0100: loss: 3960.583
iteration 0200: loss: 3961.442
iteration 0300: loss: 3961.187
iteration 0400: loss: 3977.828
iteration 0500: loss: 3962.646
iteration 0600: loss: 3963.642
iteration 0700: loss: 3967.404
iteration 0800: loss: 3966.084
iteration 0900: loss: 3954.144
iteration 1000: loss: 3965.953
iteration 1100: loss: 3962.002
iteration 1200: loss: 3957.949
iteration 1300: loss: 3965.491
iteration 1400: loss: 3962.085
iteration 1500: loss: 3966.848
iteration 1600: loss: 3968.049
iteration 1700: loss: 3957.019
iteration 1800: loss: 3964.312
====> Epoch: 013 Train loss: 3965.2676  took : 52.94308042526245
====> Test loss: 3972.3508
iteration 0000: loss: 3961.536
iteration 0100: loss: 3980.236
iteration 0200: loss: 3978.522
iteration 0300: loss: 3966.318
iteration 0400: loss: 3966.475
iteration 0500: loss: 3969.850
iteration 0600: loss: 3958.552
iteration 0700: loss: 3954.310
iteration 0800: loss: 3956.814
iteration 0900: loss: 3962.515
iteration 1000: loss: 3955.998
iteration 1100: loss: 3971.099
iteration 1200: loss: 3969.290
iteration 1300: loss: 3967.780
iteration 1400: loss: 3961.098
iteration 1500: loss: 3967.957
iteration 1600: loss: 3960.951
iteration 1700: loss: 3967.290
iteration 1800: loss: 3959.222
====> Epoch: 014 Train loss: 3965.6522  took : 52.91345572471619
====> Test loss: 3972.2622
iteration 0000: loss: 3960.834
iteration 0100: loss: 3960.292
iteration 0200: loss: 3965.573
iteration 0300: loss: 3960.735
iteration 0400: loss: 3962.818
iteration 0500: loss: 3964.698
iteration 0600: loss: 3963.440
iteration 0700: loss: 3962.181
iteration 0800: loss: 3971.297
iteration 0900: loss: 3965.624
iteration 1000: loss: 3951.642
iteration 1100: loss: 3963.503
iteration 1200: loss: 3961.418
iteration 1300: loss: 3967.457
iteration 1400: loss: 3970.115
iteration 1500: loss: 3969.277
iteration 1600: loss: 3974.320
iteration 1700: loss: 3970.894
iteration 1800: loss: 3970.066
====> Epoch: 015 Train loss: 3965.0859  took : 52.96029186248779
====> Test loss: 3971.1413
iteration 0000: loss: 3965.038
iteration 0100: loss: 3959.088
iteration 0200: loss: 3960.741
iteration 0300: loss: 3965.949
iteration 0400: loss: 3964.138
iteration 0500: loss: 3959.647
iteration 0600: loss: 3960.458
iteration 0700: loss: 3967.142
iteration 0800: loss: 3962.199
iteration 0900: loss: 3963.837
iteration 1000: loss: 3976.374
iteration 1100: loss: 3953.462
iteration 1200: loss: 3977.802
iteration 1300: loss: 3971.937
iteration 1400: loss: 3958.521
iteration 1500: loss: 3971.924
iteration 1600: loss: 3960.484
iteration 1700: loss: 3964.425
iteration 1800: loss: 3962.596
====> Epoch: 016 Train loss: 3964.1559  took : 53.01770734786987
====> Test loss: 3970.3365
iteration 0000: loss: 3962.095
iteration 0100: loss: 3971.286
iteration 0200: loss: 3958.208
iteration 0300: loss: 3958.721
iteration 0400: loss: 3963.414
iteration 0500: loss: 3965.172
iteration 0600: loss: 3958.057
iteration 0700: loss: 3960.215
iteration 0800: loss: 3966.658
iteration 0900: loss: 3957.286
iteration 1000: loss: 3964.295
iteration 1100: loss: 3968.145
iteration 1200: loss: 3960.455
iteration 1300: loss: 3965.156
iteration 1400: loss: 3969.452
iteration 1500: loss: 3963.981
iteration 1600: loss: 3965.736
iteration 1700: loss: 3959.266
iteration 1800: loss: 3956.951
====> Epoch: 017 Train loss: 3963.8295  took : 52.70373463630676
====> Test loss: 3970.7287
iteration 0000: loss: 3956.432
iteration 0100: loss: 3958.900
iteration 0200: loss: 3968.664
iteration 0300: loss: 3958.647
iteration 0400: loss: 3963.533
iteration 0500: loss: 3961.707
iteration 0600: loss: 3961.124
iteration 0700: loss: 3960.919
iteration 0800: loss: 3968.137
iteration 0900: loss: 3963.229
iteration 1000: loss: 3969.609
iteration 1100: loss: 3962.250
iteration 1200: loss: 3956.248
iteration 1300: loss: 3957.942
iteration 1400: loss: 3960.994
iteration 1500: loss: 3961.662
iteration 1600: loss: 3963.355
iteration 1700: loss: 3970.302
iteration 1800: loss: 3956.072
====> Epoch: 018 Train loss: 3963.9831  took : 53.11947584152222
====> Test loss: 3971.4035
iteration 0000: loss: 3965.992
iteration 0100: loss: 3963.354
iteration 0200: loss: 3953.652
iteration 0300: loss: 3967.084
iteration 0400: loss: 3961.382
iteration 0500: loss: 3969.105
iteration 0600: loss: 3969.296
iteration 0700: loss: 3968.964
iteration 0800: loss: 3961.948
iteration 0900: loss: 3965.148
iteration 1000: loss: 3963.201
iteration 1100: loss: 3966.185
iteration 1200: loss: 3976.411
iteration 1300: loss: 3967.203
iteration 1400: loss: 3964.746
iteration 1500: loss: 3965.816
iteration 1600: loss: 3961.986
iteration 1700: loss: 3968.110
iteration 1800: loss: 3971.747
====> Epoch: 019 Train loss: 3963.5591  took : 53.21319532394409
====> Test loss: 3969.9111
iteration 0000: loss: 3966.782
iteration 0100: loss: 3962.424
iteration 0200: loss: 3967.988
iteration 0300: loss: 3961.840
iteration 0400: loss: 3969.658
iteration 0500: loss: 3960.580
iteration 0600: loss: 3963.142
iteration 0700: loss: 3961.952
iteration 0800: loss: 3960.876
iteration 0900: loss: 3970.793
iteration 1000: loss: 3961.451
iteration 1100: loss: 3965.835
iteration 1200: loss: 3967.510
iteration 1300: loss: 3963.068
iteration 1400: loss: 3961.985
iteration 1500: loss: 3961.003
iteration 1600: loss: 3965.566
iteration 1700: loss: 3956.939
iteration 1800: loss: 3959.966
====> Epoch: 020 Train loss: 3963.3554  took : 53.15795111656189
====> Test loss: 3970.2903
iteration 0000: loss: 3965.406
iteration 0100: loss: 3963.334
iteration 0200: loss: 3959.809
iteration 0300: loss: 3954.760
iteration 0400: loss: 3965.411
iteration 0500: loss: 3963.562
iteration 0600: loss: 3965.288
iteration 0700: loss: 3965.770
iteration 0800: loss: 3951.073
iteration 0900: loss: 3957.083
iteration 1000: loss: 3971.261
iteration 1100: loss: 3959.353
iteration 1200: loss: 3962.070
iteration 1300: loss: 3964.805
iteration 1400: loss: 3970.880
iteration 1500: loss: 3965.741
iteration 1600: loss: 3954.672
iteration 1700: loss: 3961.613
iteration 1800: loss: 3965.892
====> Epoch: 021 Train loss: 3963.1554  took : 52.98717761039734
====> Test loss: 3970.3469
iteration 0000: loss: 3969.366
iteration 0100: loss: 3954.479
iteration 0200: loss: 3959.476
iteration 0300: loss: 3957.324
iteration 0400: loss: 3952.812
iteration 0500: loss: 3967.484
iteration 0600: loss: 3968.503
iteration 0700: loss: 3970.439
iteration 0800: loss: 3955.394
iteration 0900: loss: 3968.293
iteration 1000: loss: 3965.561
iteration 1100: loss: 3979.606
iteration 1200: loss: 3952.516
iteration 1300: loss: 3969.339
iteration 1400: loss: 3976.747
iteration 1500: loss: 3970.200
iteration 1600: loss: 3961.136
iteration 1700: loss: 3978.546
iteration 1800: loss: 3966.689
====> Epoch: 022 Train loss: 3962.9109  took : 52.84869647026062
====> Test loss: 3970.1255
iteration 0000: loss: 3969.220
iteration 0100: loss: 3968.304
iteration 0200: loss: 3953.959
iteration 0300: loss: 3965.155
iteration 0400: loss: 3964.478
iteration 0500: loss: 3957.960
iteration 0600: loss: 3964.002
iteration 0700: loss: 3966.270
iteration 0800: loss: 3960.601
iteration 0900: loss: 3960.838
iteration 1000: loss: 3961.921
iteration 1100: loss: 3956.930
iteration 1200: loss: 3952.849
iteration 1300: loss: 3961.698
iteration 1400: loss: 3966.673
iteration 1500: loss: 3962.535
iteration 1600: loss: 3966.926
iteration 1700: loss: 3958.490
iteration 1800: loss: 3962.968
====> Epoch: 023 Train loss: 3963.0502  took : 52.813459396362305
====> Test loss: 3970.3779
iteration 0000: loss: 3973.968
iteration 0100: loss: 3956.272
iteration 0200: loss: 3967.087
iteration 0300: loss: 3962.258
iteration 0400: loss: 3970.126
iteration 0500: loss: 3965.280
iteration 0600: loss: 3962.031
iteration 0700: loss: 3963.852
iteration 0800: loss: 3958.676
iteration 0900: loss: 3964.803
iteration 1000: loss: 3966.393
iteration 1100: loss: 3954.893
iteration 1200: loss: 3959.207
iteration 1300: loss: 3966.193
iteration 1400: loss: 3963.492
iteration 1500: loss: 3967.313
iteration 1600: loss: 3960.446
iteration 1700: loss: 3964.863
iteration 1800: loss: 3957.886
====> Epoch: 024 Train loss: 3963.9873  took : 53.21188163757324
====> Test loss: 3970.1690
iteration 0000: loss: 3953.936
iteration 0100: loss: 3965.928
iteration 0200: loss: 3956.958
iteration 0300: loss: 3966.465
iteration 0400: loss: 3968.634
iteration 0500: loss: 3955.306
iteration 0600: loss: 3963.028
iteration 0700: loss: 3965.104
iteration 0800: loss: 3964.790
iteration 0900: loss: 3967.286
iteration 1000: loss: 3968.824
iteration 1100: loss: 3966.985
iteration 1200: loss: 3968.646
iteration 1300: loss: 3957.860
iteration 1400: loss: 3960.127
iteration 1500: loss: 3958.836
iteration 1600: loss: 3964.258
iteration 1700: loss: 3963.486
iteration 1800: loss: 3961.720
====> Epoch: 025 Train loss: 3962.9566  took : 52.94840121269226
====> Test loss: 3969.8371
iteration 0000: loss: 3962.548
iteration 0100: loss: 3974.223
iteration 0200: loss: 3965.212
iteration 0300: loss: 3960.796
iteration 0400: loss: 3965.176
iteration 0500: loss: 3954.162
iteration 0600: loss: 3955.413
iteration 0700: loss: 3962.833
iteration 0800: loss: 3967.311
iteration 0900: loss: 3959.956
iteration 1000: loss: 3959.021
iteration 1100: loss: 3970.544
iteration 1200: loss: 3965.760
iteration 1300: loss: 3962.105
iteration 1400: loss: 3964.283
iteration 1500: loss: 3956.791
iteration 1600: loss: 3972.063
iteration 1700: loss: 3972.957
iteration 1800: loss: 3958.695
====> Epoch: 026 Train loss: 3964.6442  took : 52.71086025238037
====> Test loss: 4002.5706
iteration 0000: loss: 4008.204
iteration 0100: loss: 3985.696
iteration 0200: loss: 3981.266
iteration 0300: loss: 3979.660
iteration 0400: loss: 3979.609
iteration 0500: loss: 3967.146
iteration 0600: loss: 3969.471
iteration 0700: loss: 3973.162
iteration 0800: loss: 3981.774
iteration 0900: loss: 3976.773
iteration 1000: loss: 3968.957
iteration 1100: loss: 3967.122
iteration 1200: loss: 3960.055
iteration 1300: loss: 3960.416
iteration 1400: loss: 3970.840
iteration 1500: loss: 3972.549
iteration 1600: loss: 3961.203
iteration 1700: loss: 3970.010
iteration 1800: loss: 3969.230
====> Epoch: 027 Train loss: 3970.7392  took : 52.92503571510315
====> Test loss: 3973.1997
iteration 0000: loss: 3962.392
iteration 0100: loss: 3970.093
iteration 0200: loss: 3967.827
iteration 0300: loss: 3964.097
iteration 0400: loss: 3964.898
iteration 0500: loss: 3969.414
iteration 0600: loss: 3965.682
iteration 0700: loss: 3967.192
iteration 0800: loss: 3963.077
iteration 0900: loss: 3969.845
iteration 1000: loss: 3970.769
iteration 1100: loss: 3962.374
iteration 1200: loss: 3967.643
iteration 1300: loss: 3962.178
iteration 1400: loss: 3969.270
iteration 1500: loss: 3973.628
iteration 1600: loss: 3963.704
iteration 1700: loss: 3972.916
iteration 1800: loss: 3973.578
====> Epoch: 028 Train loss: 3965.8611  took : 53.069852113723755
====> Test loss: 3972.0355
iteration 0000: loss: 3971.432
iteration 0100: loss: 3979.092
iteration 0200: loss: 3956.175
iteration 0300: loss: 3961.287
iteration 0400: loss: 3956.922
iteration 0500: loss: 3963.037
iteration 0600: loss: 3953.202
iteration 0700: loss: 3967.435
iteration 0800: loss: 3965.658
iteration 0900: loss: 3961.623
iteration 1000: loss: 3969.261
iteration 1100: loss: 3967.675
iteration 1200: loss: 3971.489
iteration 1300: loss: 3972.464
iteration 1400: loss: 3963.488
iteration 1500: loss: 3967.755
iteration 1600: loss: 3962.995
iteration 1700: loss: 3967.528
iteration 1800: loss: 3965.946
====> Epoch: 029 Train loss: 3965.0517  took : 53.12574505805969
====> Test loss: 3971.7402
iteration 0000: loss: 3963.333
iteration 0100: loss: 3965.449
iteration 0200: loss: 3973.986
iteration 0300: loss: 3975.251
iteration 0400: loss: 3965.516
iteration 0500: loss: 3964.392
iteration 0600: loss: 3969.717
iteration 0700: loss: 3962.989
iteration 0800: loss: 3956.455
iteration 0900: loss: 3972.824
iteration 1000: loss: 3976.364
iteration 1100: loss: 3962.599
iteration 1200: loss: 3958.168
iteration 1300: loss: 3963.857
iteration 1400: loss: 3964.296
iteration 1500: loss: 3961.264
iteration 1600: loss: 3958.208
iteration 1700: loss: 3967.351
iteration 1800: loss: 3962.854
====> Epoch: 030 Train loss: 3963.7640  took : 53.196513414382935
====> Test loss: 3970.1576
iteration 0000: loss: 3963.494
iteration 0100: loss: 3968.141
iteration 0200: loss: 3966.842
iteration 0300: loss: 3963.276
iteration 0400: loss: 3959.385
iteration 0500: loss: 3963.016
iteration 0600: loss: 3962.968
iteration 0700: loss: 3955.755
iteration 0800: loss: 3960.453
iteration 0900: loss: 3962.864
iteration 1000: loss: 3970.803
iteration 1100: loss: 3964.561
iteration 1200: loss: 3962.772
iteration 1300: loss: 3957.301
iteration 1400: loss: 3952.905
iteration 1500: loss: 3961.535
iteration 1600: loss: 3972.881
iteration 1700: loss: 3981.311
iteration 1800: loss: 3957.621
====> Epoch: 031 Train loss: 3963.1925  took : 52.9972038269043
====> Test loss: 3971.1668
iteration 0000: loss: 3958.150
iteration 0100: loss: 3965.877
iteration 0200: loss: 3968.317
iteration 0300: loss: 3959.946
iteration 0400: loss: 3964.022
iteration 0500: loss: 3961.155
iteration 0600: loss: 3959.404
iteration 0700: loss: 3959.233
iteration 0800: loss: 3967.165
iteration 0900: loss: 3962.069
iteration 1000: loss: 3957.975
iteration 1100: loss: 3957.718
iteration 1200: loss: 3967.529
iteration 1300: loss: 3968.455
iteration 1400: loss: 3963.721
iteration 1500: loss: 3959.594
iteration 1600: loss: 3960.988
iteration 1700: loss: 3962.079
iteration 1800: loss: 3956.552
====> Epoch: 032 Train loss: 3963.3654  took : 53.06346797943115
====> Test loss: 3969.9479
iteration 0000: loss: 3964.135
iteration 0100: loss: 3964.656
iteration 0200: loss: 3964.657
iteration 0300: loss: 3960.765
iteration 0400: loss: 3962.792
iteration 0500: loss: 3967.217
iteration 0600: loss: 3968.731
iteration 0700: loss: 3964.955
iteration 0800: loss: 3967.927
iteration 0900: loss: 3958.384
iteration 1000: loss: 3963.750
iteration 1100: loss: 3967.110
iteration 1200: loss: 3969.828
iteration 1300: loss: 3965.374
iteration 1400: loss: 3955.147
iteration 1500: loss: 3965.924
iteration 1600: loss: 3958.799
iteration 1700: loss: 3974.359
iteration 1800: loss: 3954.599
====> Epoch: 033 Train loss: 3962.9589  took : 52.73498725891113
====> Test loss: 3969.3942
iteration 0000: loss: 3969.300
iteration 0100: loss: 3948.527
iteration 0200: loss: 3967.098
iteration 0300: loss: 3960.834
iteration 0400: loss: 3960.175
iteration 0500: loss: 3953.604
iteration 0600: loss: 3967.869
iteration 0700: loss: 3956.237
iteration 0800: loss: 3955.793
iteration 0900: loss: 3959.741
iteration 1000: loss: 3962.744
iteration 1100: loss: 3967.058
iteration 1200: loss: 3965.384
iteration 1300: loss: 3968.946
iteration 1400: loss: 3963.116
iteration 1500: loss: 3961.904
iteration 1600: loss: 3972.171
iteration 1700: loss: 3969.084
iteration 1800: loss: 3973.024
====> Epoch: 034 Train loss: 3962.6117  took : 52.81248188018799
====> Test loss: 3969.6566
iteration 0000: loss: 3960.019
iteration 0100: loss: 3962.946
iteration 0200: loss: 3962.668
iteration 0300: loss: 3964.174
iteration 0400: loss: 3973.795
iteration 0500: loss: 3964.277
iteration 0600: loss: 3960.588
iteration 0700: loss: 3961.063
iteration 0800: loss: 3955.433
iteration 0900: loss: 3964.172
iteration 1000: loss: 3958.108
iteration 1100: loss: 3961.223
iteration 1200: loss: 3965.615
iteration 1300: loss: 3968.066
iteration 1400: loss: 3971.655
iteration 1500: loss: 3955.928
iteration 1600: loss: 3964.446
iteration 1700: loss: 3970.831
iteration 1800: loss: 3966.563
====> Epoch: 035 Train loss: 3962.5115  took : 53.092031717300415
====> Test loss: 3969.7820
iteration 0000: loss: 3971.793
iteration 0100: loss: 3962.571
iteration 0200: loss: 3969.817
iteration 0300: loss: 3960.311
iteration 0400: loss: 3970.760
iteration 0500: loss: 3966.294
iteration 0600: loss: 3961.530
iteration 0700: loss: 3966.671
iteration 0800: loss: 3961.155
iteration 0900: loss: 3975.384
iteration 1000: loss: 3954.851
iteration 1100: loss: 3960.820
iteration 1200: loss: 3972.584
iteration 1300: loss: 3953.990
iteration 1400: loss: 3962.953
iteration 1500: loss: 3963.134
iteration 1600: loss: 3957.239
iteration 1700: loss: 3959.221
iteration 1800: loss: 3962.791
====> Epoch: 036 Train loss: 3962.6946  took : 53.208927392959595
====> Test loss: 3969.8852
iteration 0000: loss: 3961.079
iteration 0100: loss: 3974.420
iteration 0200: loss: 3967.079
iteration 0300: loss: 3965.801
iteration 0400: loss: 3963.448
iteration 0500: loss: 3952.622
iteration 0600: loss: 3958.220
iteration 0700: loss: 3961.858
iteration 0800: loss: 3960.969
iteration 0900: loss: 3953.400
iteration 1000: loss: 3962.617
iteration 1100: loss: 3967.925
iteration 1200: loss: 3966.361
iteration 1300: loss: 3961.756
iteration 1400: loss: 3961.787
iteration 1500: loss: 3967.038
iteration 1600: loss: 3959.700
iteration 1700: loss: 3960.186
iteration 1800: loss: 3964.438
====> Epoch: 037 Train loss: 3962.2380  took : 53.11717891693115
====> Test loss: 3969.4641
iteration 0000: loss: 3958.115
iteration 0100: loss: 3952.168
iteration 0200: loss: 3954.742
iteration 0300: loss: 3966.796
iteration 0400: loss: 3969.518
iteration 0500: loss: 3962.379
iteration 0600: loss: 3964.383
iteration 0700: loss: 3964.475
iteration 0800: loss: 3970.400
iteration 0900: loss: 3958.892
iteration 1000: loss: 3961.433
iteration 1100: loss: 3966.802
iteration 1200: loss: 3956.267
iteration 1300: loss: 3962.128
iteration 1400: loss: 3965.647
iteration 1500: loss: 3958.797
iteration 1600: loss: 3961.925
iteration 1700: loss: 3959.691
iteration 1800: loss: 3955.668
====> Epoch: 038 Train loss: 3961.6063  took : 53.17870116233826
====> Test loss: 3969.5602
iteration 0000: loss: 3963.189
iteration 0100: loss: 3960.090
iteration 0200: loss: 3956.420
iteration 0300: loss: 3959.853
iteration 0400: loss: 3959.362
iteration 0500: loss: 3965.816
iteration 0600: loss: 3952.052
iteration 0700: loss: 3968.760
iteration 0800: loss: 3961.938
iteration 0900: loss: 3960.688
iteration 1000: loss: 3958.019
iteration 1100: loss: 3964.185
iteration 1200: loss: 3966.349
iteration 1300: loss: 3957.268
iteration 1400: loss: 3963.603
iteration 1500: loss: 3954.655
iteration 1600: loss: 3957.612
iteration 1700: loss: 3957.411
iteration 1800: loss: 3969.505
====> Epoch: 039 Train loss: 3961.6150  took : 53.020278215408325
====> Test loss: 3969.5496
iteration 0000: loss: 3961.831
iteration 0100: loss: 3959.371
iteration 0200: loss: 3953.106
iteration 0300: loss: 3962.085
iteration 0400: loss: 3961.551
iteration 0500: loss: 3967.924
iteration 0600: loss: 3956.171
iteration 0700: loss: 3958.564
iteration 0800: loss: 3959.573
iteration 0900: loss: 3950.125
iteration 1000: loss: 3954.845
iteration 1100: loss: 3970.727
iteration 1200: loss: 3956.518
iteration 1300: loss: 3975.747
iteration 1400: loss: 3962.828
iteration 1500: loss: 3961.615
iteration 1600: loss: 3958.181
iteration 1700: loss: 3956.920
iteration 1800: loss: 3961.756
====> Epoch: 040 Train loss: 3961.8981  took : 52.78147220611572
====> Test loss: 3969.9711
iteration 0000: loss: 3956.882
iteration 0100: loss: 3961.739
iteration 0200: loss: 3957.061
iteration 0300: loss: 3957.949
iteration 0400: loss: 3962.036
iteration 0500: loss: 3967.978
iteration 0600: loss: 3962.842
iteration 0700: loss: 3971.323
iteration 0800: loss: 3959.351
iteration 0900: loss: 3963.522
iteration 1000: loss: 3957.224
iteration 1100: loss: 3959.458
iteration 1200: loss: 3970.160
iteration 1300: loss: 3971.495
iteration 1400: loss: 3966.819
iteration 1500: loss: 3974.068
iteration 1600: loss: 3968.037
iteration 1700: loss: 3959.101
iteration 1800: loss: 3953.353
====> Epoch: 041 Train loss: 3961.6599  took : 52.80504608154297
====> Test loss: 3969.5306
iteration 0000: loss: 3966.163
iteration 0100: loss: 3961.811
iteration 0200: loss: 3958.641
iteration 0300: loss: 3957.176
iteration 0400: loss: 3962.848
iteration 0500: loss: 3967.183
iteration 0600: loss: 3960.952
iteration 0700: loss: 3961.778
iteration 0800: loss: 3949.807
iteration 0900: loss: 3964.537
iteration 1000: loss: 3962.634
iteration 1100: loss: 3960.081
iteration 1200: loss: 3957.369
iteration 1300: loss: 3960.978
iteration 1400: loss: 3964.977
iteration 1500: loss: 3956.569
iteration 1600: loss: 3966.295
iteration 1700: loss: 3972.301
iteration 1800: loss: 3963.961
====> Epoch: 042 Train loss: 3961.8198  took : 52.94013023376465
====> Test loss: 3969.3009
iteration 0000: loss: 3959.302
iteration 0100: loss: 3973.581
iteration 0200: loss: 3961.708
iteration 0300: loss: 3964.655
iteration 0400: loss: 3963.068
iteration 0500: loss: 3960.471
iteration 0600: loss: 3958.020
iteration 0700: loss: 3965.938
iteration 0800: loss: 3966.765
iteration 0900: loss: 3962.985
iteration 1000: loss: 3960.965
iteration 1100: loss: 3963.536
iteration 1200: loss: 3952.204
iteration 1300: loss: 3958.266
iteration 1400: loss: 3959.464
iteration 1500: loss: 3966.731
iteration 1600: loss: 3958.805
iteration 1700: loss: 3962.581
iteration 1800: loss: 3963.405
====> Epoch: 043 Train loss: 3961.4514  took : 53.33925700187683
====> Test loss: 3969.4626
iteration 0000: loss: 3953.541
iteration 0100: loss: 3962.986
iteration 0200: loss: 3956.131
iteration 0300: loss: 3958.231
iteration 0400: loss: 3954.073
iteration 0500: loss: 3961.292
iteration 0600: loss: 3966.316
iteration 0700: loss: 3961.011
iteration 0800: loss: 3957.812
iteration 0900: loss: 3951.275
iteration 1000: loss: 3965.212
iteration 1100: loss: 3955.429
iteration 1200: loss: 3966.434
iteration 1300: loss: 3957.898
iteration 1400: loss: 3960.985
iteration 1500: loss: 3958.994
iteration 1600: loss: 3966.465
iteration 1700: loss: 3956.840
iteration 1800: loss: 3957.563
====> Epoch: 044 Train loss: 3961.1259  took : 53.28993487358093
====> Test loss: 3969.3891
iteration 0000: loss: 3955.015
iteration 0100: loss: 3957.829
iteration 0200: loss: 3966.901
iteration 0300: loss: 3962.426
iteration 0400: loss: 3956.614
iteration 0500: loss: 3954.661
iteration 0600: loss: 3965.965
iteration 0700: loss: 3952.270
iteration 0800: loss: 3969.063
iteration 0900: loss: 3969.152
iteration 1000: loss: 3964.386
iteration 1100: loss: 3959.678
iteration 1200: loss: 3961.581
iteration 1300: loss: 3965.284
iteration 1400: loss: 3954.954
iteration 1500: loss: 3965.802
iteration 1600: loss: 3962.779
iteration 1700: loss: 3955.937
iteration 1800: loss: 3962.966
====> Epoch: 045 Train loss: 3960.8585  took : 52.77081084251404
====> Test loss: 3969.0115
iteration 0000: loss: 3960.079
iteration 0100: loss: 3972.338
iteration 0200: loss: 3963.710
iteration 0300: loss: 3961.334
iteration 0400: loss: 3962.107
iteration 0500: loss: 3959.404
iteration 0600: loss: 3961.735
iteration 0700: loss: 3963.445
iteration 0800: loss: 3970.762
iteration 0900: loss: 3953.931
iteration 1000: loss: 3954.309
iteration 1100: loss: 3964.233
iteration 1200: loss: 3961.382
iteration 1300: loss: 3962.427
iteration 1400: loss: 3963.006
iteration 1500: loss: 3969.000
iteration 1600: loss: 3959.913
iteration 1700: loss: 3962.476
iteration 1800: loss: 3956.698
====> Epoch: 046 Train loss: 3960.7928  took : 52.985743045806885
====> Test loss: 3969.4176
iteration 0000: loss: 3959.268
iteration 0100: loss: 3961.675
iteration 0200: loss: 3956.098
iteration 0300: loss: 3958.894
iteration 0400: loss: 3959.328
iteration 0500: loss: 3958.585
iteration 0600: loss: 3970.359
iteration 0700: loss: 3960.152
iteration 0800: loss: 3962.600
iteration 0900: loss: 3968.768
iteration 1000: loss: 3961.883
iteration 1100: loss: 3958.628
iteration 1200: loss: 3954.132
iteration 1300: loss: 3954.681
iteration 1400: loss: 3963.342
iteration 1500: loss: 3964.817
iteration 1600: loss: 3961.456
iteration 1700: loss: 3973.991
iteration 1800: loss: 3964.021
====> Epoch: 047 Train loss: 3960.6449  took : 53.02718424797058
====> Test loss: 3970.0978
iteration 0000: loss: 3955.976
iteration 0100: loss: 3961.433
iteration 0200: loss: 3958.198
iteration 0300: loss: 3971.084
iteration 0400: loss: 3964.200
iteration 0500: loss: 3960.789
iteration 0600: loss: 3961.094
iteration 0700: loss: 3959.950
iteration 0800: loss: 3970.658
iteration 0900: loss: 3963.552
iteration 1000: loss: 3961.877
iteration 1100: loss: 3963.911
iteration 1200: loss: 3951.418
iteration 1300: loss: 3963.931
iteration 1400: loss: 3954.362
iteration 1500: loss: 3964.783
iteration 1600: loss: 3968.978
iteration 1700: loss: 3961.568
iteration 1800: loss: 3977.866
====> Epoch: 048 Train loss: 3961.0833  took : 52.86830425262451
====> Test loss: 3969.4674
iteration 0000: loss: 3968.478
iteration 0100: loss: 3975.768
iteration 0200: loss: 3956.930
iteration 0300: loss: 3961.499
iteration 0400: loss: 3964.994
iteration 0500: loss: 3950.111
iteration 0600: loss: 3963.342
iteration 0700: loss: 3960.444
iteration 0800: loss: 3962.677
iteration 0900: loss: 3966.103
iteration 1000: loss: 3958.490
iteration 1100: loss: 3970.233
iteration 1200: loss: 3969.461
iteration 1300: loss: 3956.374
iteration 1400: loss: 3965.394
iteration 1500: loss: 3964.918
iteration 1600: loss: 3962.863
iteration 1700: loss: 3960.934
iteration 1800: loss: 3953.734
====> Epoch: 049 Train loss: 3960.5579  took : 52.796823024749756
====> Test loss: 3968.9168
iteration 0000: loss: 3966.907
iteration 0100: loss: 3957.430
iteration 0200: loss: 3959.206
iteration 0300: loss: 3966.444
iteration 0400: loss: 3949.464
iteration 0500: loss: 3961.372
iteration 0600: loss: 3963.086
iteration 0700: loss: 3958.554
iteration 0800: loss: 3956.769
iteration 0900: loss: 3957.693
iteration 1000: loss: 3961.162
iteration 1100: loss: 3965.767
iteration 1200: loss: 3964.823
iteration 1300: loss: 3957.611
iteration 1400: loss: 3964.832
iteration 1500: loss: 3970.490
iteration 1600: loss: 3950.286
iteration 1700: loss: 3965.376
iteration 1800: loss: 3958.045
====> Epoch: 050 Train loss: 3960.6215  took : 52.99257469177246
====> Test loss: 3968.6229
====> [MM-VAE] Time: 3137.348s or 00:52:17
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  3
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_3
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1987.080
iteration 0100: loss: 1568.583
iteration 0200: loss: 1564.826
iteration 0300: loss: 1560.798
iteration 0400: loss: 1549.401
iteration 0500: loss: 1550.541
iteration 0600: loss: 1538.434
iteration 0700: loss: 1543.741
iteration 0800: loss: 1540.146
iteration 0900: loss: 1542.020
====> Epoch: 001 Train loss: 1555.7767  took : 8.46049451828003
====> Test loss: 1538.8283
iteration 0000: loss: 1534.970
iteration 0100: loss: 1537.988
iteration 0200: loss: 1536.271
iteration 0300: loss: 1531.686
iteration 0400: loss: 1530.500
iteration 0500: loss: 1532.269
iteration 0600: loss: 1534.199
iteration 0700: loss: 1529.752
iteration 0800: loss: 1531.057
iteration 0900: loss: 1530.191
====> Epoch: 002 Train loss: 1532.8123  took : 8.444931983947754
====> Test loss: 1531.9168
iteration 0000: loss: 1531.631
iteration 0100: loss: 1527.447
iteration 0200: loss: 1530.980
iteration 0300: loss: 1525.677
iteration 0400: loss: 1527.721
iteration 0500: loss: 1526.224
iteration 0600: loss: 1525.620
iteration 0700: loss: 1523.034
iteration 0800: loss: 1524.535
iteration 0900: loss: 1527.792
====> Epoch: 003 Train loss: 1528.1753  took : 8.503000497817993
====> Test loss: 1528.4993
iteration 0000: loss: 1523.613
iteration 0100: loss: 1525.418
iteration 0200: loss: 1524.652
iteration 0300: loss: 1527.772
iteration 0400: loss: 1525.505
iteration 0500: loss: 1530.244
iteration 0600: loss: 1524.200
iteration 0700: loss: 1525.544
iteration 0800: loss: 1525.260
iteration 0900: loss: 1527.123
====> Epoch: 004 Train loss: 1525.5898  took : 8.418065309524536
====> Test loss: 1527.0328
iteration 0000: loss: 1521.106
iteration 0100: loss: 1531.245
iteration 0200: loss: 1526.079
iteration 0300: loss: 1522.481
iteration 0400: loss: 1523.271
iteration 0500: loss: 1526.671
iteration 0600: loss: 1524.635
iteration 0700: loss: 1527.828
iteration 0800: loss: 1526.119
iteration 0900: loss: 1524.987
====> Epoch: 005 Train loss: 1524.0327  took : 8.433109760284424
====> Test loss: 1525.7783
iteration 0000: loss: 1524.282
iteration 0100: loss: 1525.198
iteration 0200: loss: 1522.078
iteration 0300: loss: 1521.253
iteration 0400: loss: 1523.541
iteration 0500: loss: 1525.399
iteration 0600: loss: 1519.773
iteration 0700: loss: 1520.523
iteration 0800: loss: 1525.647
iteration 0900: loss: 1524.524
====> Epoch: 006 Train loss: 1522.8070  took : 8.451866626739502
====> Test loss: 1524.6790
iteration 0000: loss: 1523.816
iteration 0100: loss: 1521.541
iteration 0200: loss: 1524.303
iteration 0300: loss: 1524.542
iteration 0400: loss: 1524.374
iteration 0500: loss: 1521.570
iteration 0600: loss: 1523.427
iteration 0700: loss: 1521.584
iteration 0800: loss: 1525.629
iteration 0900: loss: 1524.178
====> Epoch: 007 Train loss: 1521.8984  took : 8.492352724075317
====> Test loss: 1524.0114
iteration 0000: loss: 1523.603
iteration 0100: loss: 1525.203
iteration 0200: loss: 1523.338
iteration 0300: loss: 1521.285
iteration 0400: loss: 1518.670
iteration 0500: loss: 1522.045
iteration 0600: loss: 1522.345
iteration 0700: loss: 1521.543
iteration 0800: loss: 1518.450
iteration 0900: loss: 1522.907
====> Epoch: 008 Train loss: 1521.2001  took : 8.523706912994385
====> Test loss: 1523.4414
iteration 0000: loss: 1524.101
iteration 0100: loss: 1517.865
iteration 0200: loss: 1523.673
iteration 0300: loss: 1516.128
iteration 0400: loss: 1518.493
iteration 0500: loss: 1517.923
iteration 0600: loss: 1522.086
iteration 0700: loss: 1521.662
iteration 0800: loss: 1518.747
iteration 0900: loss: 1520.422
====> Epoch: 009 Train loss: 1520.5626  took : 8.550010204315186
====> Test loss: 1523.0933
iteration 0000: loss: 1522.168
iteration 0100: loss: 1519.646
iteration 0200: loss: 1518.116
iteration 0300: loss: 1519.578
iteration 0400: loss: 1518.603
iteration 0500: loss: 1515.947
iteration 0600: loss: 1520.037
iteration 0700: loss: 1519.616
iteration 0800: loss: 1520.995
iteration 0900: loss: 1520.252
====> Epoch: 010 Train loss: 1520.0387  took : 8.575785636901855
====> Test loss: 1522.6097
iteration 0000: loss: 1523.629
iteration 0100: loss: 1522.044
iteration 0200: loss: 1524.462
iteration 0300: loss: 1517.526
iteration 0400: loss: 1515.980
iteration 0500: loss: 1515.909
iteration 0600: loss: 1521.756
iteration 0700: loss: 1518.993
iteration 0800: loss: 1519.844
iteration 0900: loss: 1519.397
====> Epoch: 011 Train loss: 1519.6022  took : 8.299620866775513
====> Test loss: 1522.1319
iteration 0000: loss: 1517.438
iteration 0100: loss: 1519.738
iteration 0200: loss: 1520.058
iteration 0300: loss: 1514.928
iteration 0400: loss: 1521.960
iteration 0500: loss: 1522.221
iteration 0600: loss: 1520.743
iteration 0700: loss: 1518.270
iteration 0800: loss: 1519.012
iteration 0900: loss: 1519.606
====> Epoch: 012 Train loss: 1519.1275  took : 8.38248586654663
====> Test loss: 1521.7782
iteration 0000: loss: 1517.946
iteration 0100: loss: 1520.327
iteration 0200: loss: 1520.158
iteration 0300: loss: 1519.924
iteration 0400: loss: 1519.994
iteration 0500: loss: 1516.547
iteration 0600: loss: 1518.738
iteration 0700: loss: 1521.533
iteration 0800: loss: 1516.668
iteration 0900: loss: 1517.814
====> Epoch: 013 Train loss: 1518.8124  took : 8.53507137298584
====> Test loss: 1521.2658
iteration 0000: loss: 1517.773
iteration 0100: loss: 1519.454
iteration 0200: loss: 1518.315
iteration 0300: loss: 1519.128
iteration 0400: loss: 1520.355
iteration 0500: loss: 1518.526
iteration 0600: loss: 1518.690
iteration 0700: loss: 1517.738
iteration 0800: loss: 1517.788
iteration 0900: loss: 1517.940
====> Epoch: 014 Train loss: 1518.4608  took : 8.418444395065308
====> Test loss: 1521.2773
iteration 0000: loss: 1514.661
iteration 0100: loss: 1520.205
iteration 0200: loss: 1517.896
iteration 0300: loss: 1515.362
iteration 0400: loss: 1515.818
iteration 0500: loss: 1519.091
iteration 0600: loss: 1516.644
iteration 0700: loss: 1519.236
iteration 0800: loss: 1518.982
iteration 0900: loss: 1518.211
====> Epoch: 015 Train loss: 1518.1894  took : 8.506171941757202
====> Test loss: 1521.0019
iteration 0000: loss: 1519.673
iteration 0100: loss: 1521.029
iteration 0200: loss: 1519.019
iteration 0300: loss: 1517.733
iteration 0400: loss: 1516.174
iteration 0500: loss: 1518.235
iteration 0600: loss: 1518.710
iteration 0700: loss: 1519.783
iteration 0800: loss: 1517.633
iteration 0900: loss: 1520.576
====> Epoch: 016 Train loss: 1517.9557  took : 8.466363906860352
====> Test loss: 1520.7000
iteration 0000: loss: 1515.656
iteration 0100: loss: 1517.758
iteration 0200: loss: 1517.794
iteration 0300: loss: 1517.045
iteration 0400: loss: 1518.798
iteration 0500: loss: 1516.234
iteration 0600: loss: 1516.147
iteration 0700: loss: 1519.054
iteration 0800: loss: 1517.622
iteration 0900: loss: 1517.268
====> Epoch: 017 Train loss: 1517.7234  took : 8.410357475280762
====> Test loss: 1520.7306
iteration 0000: loss: 1521.165
iteration 0100: loss: 1515.984
iteration 0200: loss: 1518.810
iteration 0300: loss: 1518.382
iteration 0400: loss: 1515.210
iteration 0500: loss: 1519.845
iteration 0600: loss: 1519.255
iteration 0700: loss: 1519.292
iteration 0800: loss: 1519.303
iteration 0900: loss: 1514.274
====> Epoch: 018 Train loss: 1517.5619  took : 8.421794652938843
====> Test loss: 1520.3479
iteration 0000: loss: 1515.276
iteration 0100: loss: 1517.028
iteration 0200: loss: 1517.047
iteration 0300: loss: 1517.384
iteration 0400: loss: 1517.286
iteration 0500: loss: 1517.530
iteration 0600: loss: 1522.503
iteration 0700: loss: 1518.193
iteration 0800: loss: 1517.170
iteration 0900: loss: 1515.346
====> Epoch: 019 Train loss: 1517.3358  took : 8.417875528335571
====> Test loss: 1520.1736
iteration 0000: loss: 1521.274
iteration 0100: loss: 1516.278
iteration 0200: loss: 1520.725
iteration 0300: loss: 1518.161
iteration 0400: loss: 1519.273
iteration 0500: loss: 1516.837
iteration 0600: loss: 1514.371
iteration 0700: loss: 1515.539
iteration 0800: loss: 1517.084
iteration 0900: loss: 1516.110
====> Epoch: 020 Train loss: 1517.2333  took : 8.45822811126709
====> Test loss: 1520.2518
iteration 0000: loss: 1517.216
iteration 0100: loss: 1516.303
iteration 0200: loss: 1517.437
iteration 0300: loss: 1513.836
iteration 0400: loss: 1521.193
iteration 0500: loss: 1516.152
iteration 0600: loss: 1517.361
iteration 0700: loss: 1516.365
iteration 0800: loss: 1516.958
iteration 0900: loss: 1516.156
====> Epoch: 021 Train loss: 1517.0469  took : 8.50770902633667
====> Test loss: 1520.1174
iteration 0000: loss: 1514.905
iteration 0100: loss: 1516.644
iteration 0200: loss: 1513.104
iteration 0300: loss: 1515.660
iteration 0400: loss: 1516.346
iteration 0500: loss: 1517.319
iteration 0600: loss: 1518.529
iteration 0700: loss: 1519.435
iteration 0800: loss: 1516.352
iteration 0900: loss: 1516.150
====> Epoch: 022 Train loss: 1516.9301  took : 8.488374710083008
====> Test loss: 1520.0448
iteration 0000: loss: 1516.042
iteration 0100: loss: 1515.205
iteration 0200: loss: 1513.401
iteration 0300: loss: 1518.177
iteration 0400: loss: 1518.610
iteration 0500: loss: 1518.430
iteration 0600: loss: 1518.570
iteration 0700: loss: 1515.736
iteration 0800: loss: 1514.609
iteration 0900: loss: 1514.685
====> Epoch: 023 Train loss: 1516.8373  took : 8.518215417861938
====> Test loss: 1519.9800
iteration 0000: loss: 1518.219
iteration 0100: loss: 1515.932
iteration 0200: loss: 1512.622
iteration 0300: loss: 1515.070
iteration 0400: loss: 1514.501
iteration 0500: loss: 1515.584
iteration 0600: loss: 1517.963
iteration 0700: loss: 1514.971
iteration 0800: loss: 1515.096
iteration 0900: loss: 1517.708
====> Epoch: 024 Train loss: 1516.7403  took : 8.475161790847778
====> Test loss: 1519.7265
iteration 0000: loss: 1518.965
iteration 0100: loss: 1516.888
iteration 0200: loss: 1518.311
iteration 0300: loss: 1516.851
iteration 0400: loss: 1516.376
iteration 0500: loss: 1517.702
iteration 0600: loss: 1517.845
iteration 0700: loss: 1515.091
iteration 0800: loss: 1514.252
iteration 0900: loss: 1518.631
====> Epoch: 025 Train loss: 1516.5717  took : 8.38205885887146
====> Test loss: 1519.7194
iteration 0000: loss: 1517.876
iteration 0100: loss: 1517.744
iteration 0200: loss: 1516.636
iteration 0300: loss: 1518.950
iteration 0400: loss: 1518.395
iteration 0500: loss: 1518.538
iteration 0600: loss: 1517.910
iteration 0700: loss: 1517.316
iteration 0800: loss: 1516.162
iteration 0900: loss: 1522.365
====> Epoch: 026 Train loss: 1516.5065  took : 8.46268892288208
====> Test loss: 1519.7931
iteration 0000: loss: 1515.866
iteration 0100: loss: 1517.517
iteration 0200: loss: 1515.231
iteration 0300: loss: 1514.381
iteration 0400: loss: 1517.448
iteration 0500: loss: 1517.553
iteration 0600: loss: 1517.542
iteration 0700: loss: 1516.117
iteration 0800: loss: 1513.958
iteration 0900: loss: 1518.299
====> Epoch: 027 Train loss: 1516.3881  took : 8.447523355484009
====> Test loss: 1519.5703
iteration 0000: loss: 1515.408
iteration 0100: loss: 1516.705
iteration 0200: loss: 1517.148
iteration 0300: loss: 1515.933
iteration 0400: loss: 1515.209
iteration 0500: loss: 1515.806
iteration 0600: loss: 1518.391
iteration 0700: loss: 1519.029
iteration 0800: loss: 1518.441
iteration 0900: loss: 1515.726
====> Epoch: 028 Train loss: 1516.3398  took : 8.510223388671875
====> Test loss: 1519.4742
iteration 0000: loss: 1518.354
iteration 0100: loss: 1517.523
iteration 0200: loss: 1517.420
iteration 0300: loss: 1514.291
iteration 0400: loss: 1516.184
iteration 0500: loss: 1516.986
iteration 0600: loss: 1517.282
iteration 0700: loss: 1516.609
iteration 0800: loss: 1514.917
iteration 0900: loss: 1516.147
====> Epoch: 029 Train loss: 1516.2194  took : 8.516366481781006
====> Test loss: 1519.2890
iteration 0000: loss: 1515.100
iteration 0100: loss: 1516.011
iteration 0200: loss: 1517.877
iteration 0300: loss: 1511.822
iteration 0400: loss: 1514.138
iteration 0500: loss: 1514.672
iteration 0600: loss: 1517.724
iteration 0700: loss: 1516.403
iteration 0800: loss: 1512.907
iteration 0900: loss: 1516.281
====> Epoch: 030 Train loss: 1516.1430  took : 8.511368751525879
====> Test loss: 1519.5507
iteration 0000: loss: 1515.428
iteration 0100: loss: 1514.745
iteration 0200: loss: 1518.034
iteration 0300: loss: 1516.465
iteration 0400: loss: 1513.318
iteration 0500: loss: 1516.805
iteration 0600: loss: 1516.300
iteration 0700: loss: 1515.820
iteration 0800: loss: 1516.071
iteration 0900: loss: 1514.374
====> Epoch: 031 Train loss: 1516.0497  took : 8.498219013214111
====> Test loss: 1519.3756
iteration 0000: loss: 1518.009
iteration 0100: loss: 1513.118
iteration 0200: loss: 1515.269
iteration 0300: loss: 1512.866
iteration 0400: loss: 1517.259
iteration 0500: loss: 1516.614
iteration 0600: loss: 1516.049
iteration 0700: loss: 1514.572
iteration 0800: loss: 1519.521
iteration 0900: loss: 1515.214
====> Epoch: 032 Train loss: 1515.9526  took : 8.503603458404541
====> Test loss: 1519.2423
iteration 0000: loss: 1515.828
iteration 0100: loss: 1516.064
iteration 0200: loss: 1513.315
iteration 0300: loss: 1515.078
iteration 0400: loss: 1513.007
iteration 0500: loss: 1518.120
iteration 0600: loss: 1516.542
iteration 0700: loss: 1513.707
iteration 0800: loss: 1515.812
iteration 0900: loss: 1517.912
====> Epoch: 033 Train loss: 1515.9171  took : 8.427592039108276
====> Test loss: 1519.3004
iteration 0000: loss: 1516.528
iteration 0100: loss: 1513.978
iteration 0200: loss: 1516.665
iteration 0300: loss: 1514.982
iteration 0400: loss: 1515.742
iteration 0500: loss: 1516.644
iteration 0600: loss: 1512.829
iteration 0700: loss: 1514.098
iteration 0800: loss: 1515.215
iteration 0900: loss: 1516.694
====> Epoch: 034 Train loss: 1515.8536  took : 8.490127563476562
====> Test loss: 1519.2429
iteration 0000: loss: 1513.770
iteration 0100: loss: 1515.448
iteration 0200: loss: 1518.275
iteration 0300: loss: 1517.052
iteration 0400: loss: 1518.763
iteration 0500: loss: 1516.764
iteration 0600: loss: 1514.954
iteration 0700: loss: 1518.100
iteration 0800: loss: 1516.705
iteration 0900: loss: 1516.909
====> Epoch: 035 Train loss: 1515.8109  took : 8.485101222991943
====> Test loss: 1519.2004
iteration 0000: loss: 1516.238
iteration 0100: loss: 1517.776
iteration 0200: loss: 1512.384
iteration 0300: loss: 1517.352
iteration 0400: loss: 1513.815
iteration 0500: loss: 1512.468
iteration 0600: loss: 1514.714
iteration 0700: loss: 1516.341
iteration 0800: loss: 1514.175
iteration 0900: loss: 1516.481
====> Epoch: 036 Train loss: 1515.8397  took : 8.435303211212158
====> Test loss: 1519.1794
iteration 0000: loss: 1517.726
iteration 0100: loss: 1517.220
iteration 0200: loss: 1516.714
iteration 0300: loss: 1516.929
iteration 0400: loss: 1513.173
iteration 0500: loss: 1514.796
iteration 0600: loss: 1515.489
iteration 0700: loss: 1516.958
iteration 0800: loss: 1516.311
iteration 0900: loss: 1517.017
====> Epoch: 037 Train loss: 1515.7032  took : 8.436691999435425
====> Test loss: 1519.3746
iteration 0000: loss: 1514.828
iteration 0100: loss: 1514.168
iteration 0200: loss: 1513.432
iteration 0300: loss: 1518.703
iteration 0400: loss: 1515.129
iteration 0500: loss: 1516.590
iteration 0600: loss: 1513.610
iteration 0700: loss: 1513.777
iteration 0800: loss: 1514.119
iteration 0900: loss: 1515.476
====> Epoch: 038 Train loss: 1515.6397  took : 8.396894931793213
====> Test loss: 1519.0618
iteration 0000: loss: 1514.763
iteration 0100: loss: 1514.621
iteration 0200: loss: 1516.364
iteration 0300: loss: 1514.983
iteration 0400: loss: 1515.735
iteration 0500: loss: 1515.571
iteration 0600: loss: 1514.153
iteration 0700: loss: 1515.898
iteration 0800: loss: 1515.752
iteration 0900: loss: 1516.373
====> Epoch: 039 Train loss: 1515.5932  took : 8.501457452774048
====> Test loss: 1519.0166
iteration 0000: loss: 1514.999
iteration 0100: loss: 1516.448
iteration 0200: loss: 1517.339
iteration 0300: loss: 1514.152
iteration 0400: loss: 1516.448
iteration 0500: loss: 1516.215
iteration 0600: loss: 1514.170
iteration 0700: loss: 1516.698
iteration 0800: loss: 1517.057
iteration 0900: loss: 1515.420
====> Epoch: 040 Train loss: 1515.5226  took : 8.541345357894897
====> Test loss: 1519.0148
iteration 0000: loss: 1515.063
iteration 0100: loss: 1515.003
iteration 0200: loss: 1516.397
iteration 0300: loss: 1515.057
iteration 0400: loss: 1516.063
iteration 0500: loss: 1515.521
iteration 0600: loss: 1515.241
iteration 0700: loss: 1516.676
iteration 0800: loss: 1518.062
iteration 0900: loss: 1514.943
====> Epoch: 041 Train loss: 1515.4694  took : 8.36769700050354
====> Test loss: 1518.9648
iteration 0000: loss: 1513.643
iteration 0100: loss: 1517.078
iteration 0200: loss: 1516.104
iteration 0300: loss: 1515.326
iteration 0400: loss: 1516.093
iteration 0500: loss: 1517.044
iteration 0600: loss: 1516.587
iteration 0700: loss: 1514.263
iteration 0800: loss: 1513.569
iteration 0900: loss: 1515.249
====> Epoch: 042 Train loss: 1515.4258  took : 8.41632080078125
====> Test loss: 1518.9881
iteration 0000: loss: 1514.531
iteration 0100: loss: 1516.159
iteration 0200: loss: 1516.610
iteration 0300: loss: 1516.614
iteration 0400: loss: 1516.418
iteration 0500: loss: 1515.429
iteration 0600: loss: 1516.149
iteration 0700: loss: 1514.349
iteration 0800: loss: 1515.708
iteration 0900: loss: 1515.627
====> Epoch: 043 Train loss: 1515.3762  took : 8.522230863571167
====> Test loss: 1519.0554
iteration 0000: loss: 1516.369
iteration 0100: loss: 1514.211
iteration 0200: loss: 1517.336
iteration 0300: loss: 1515.271
iteration 0400: loss: 1512.361
iteration 0500: loss: 1514.874
iteration 0600: loss: 1514.782
iteration 0700: loss: 1514.865
iteration 0800: loss: 1515.337
iteration 0900: loss: 1514.311
====> Epoch: 044 Train loss: 1515.3416  took : 8.502105236053467
====> Test loss: 1519.0661
iteration 0000: loss: 1513.674
iteration 0100: loss: 1513.056
iteration 0200: loss: 1512.230
iteration 0300: loss: 1513.477
iteration 0400: loss: 1512.884
iteration 0500: loss: 1515.668
iteration 0600: loss: 1514.890
iteration 0700: loss: 1515.453
iteration 0800: loss: 1517.033
iteration 0900: loss: 1514.671
====> Epoch: 045 Train loss: 1515.3266  took : 8.500853776931763
====> Test loss: 1518.8428
iteration 0000: loss: 1513.385
iteration 0100: loss: 1513.708
iteration 0200: loss: 1513.134
iteration 0300: loss: 1513.155
iteration 0400: loss: 1514.352
iteration 0500: loss: 1514.677
iteration 0600: loss: 1513.275
iteration 0700: loss: 1518.115
iteration 0800: loss: 1514.767
iteration 0900: loss: 1514.791
====> Epoch: 046 Train loss: 1515.2448  took : 8.49129581451416
====> Test loss: 1518.8532
iteration 0000: loss: 1515.583
iteration 0100: loss: 1518.012
iteration 0200: loss: 1512.211
iteration 0300: loss: 1516.076
iteration 0400: loss: 1514.275
iteration 0500: loss: 1515.500
iteration 0600: loss: 1513.409
iteration 0700: loss: 1512.604
iteration 0800: loss: 1517.674
iteration 0900: loss: 1512.728
====> Epoch: 047 Train loss: 1515.2110  took : 8.399947166442871
====> Test loss: 1518.7317
iteration 0000: loss: 1519.299
iteration 0100: loss: 1517.109
iteration 0200: loss: 1514.491
iteration 0300: loss: 1516.761
iteration 0400: loss: 1515.233
iteration 0500: loss: 1515.062
iteration 0600: loss: 1515.444
iteration 0700: loss: 1515.240
iteration 0800: loss: 1515.468
iteration 0900: loss: 1515.837
====> Epoch: 048 Train loss: 1515.1542  took : 8.490661859512329
====> Test loss: 1518.8595
iteration 0000: loss: 1514.139
iteration 0100: loss: 1515.362
iteration 0200: loss: 1514.934
iteration 0300: loss: 1516.248
iteration 0400: loss: 1513.118
iteration 0500: loss: 1513.170
iteration 0600: loss: 1514.354
iteration 0700: loss: 1513.627
iteration 0800: loss: 1513.401
iteration 0900: loss: 1514.747
====> Epoch: 049 Train loss: 1515.1138  took : 8.411081552505493
====> Test loss: 1518.7306
iteration 0000: loss: 1515.890
iteration 0100: loss: 1515.894
iteration 0200: loss: 1513.460
iteration 0300: loss: 1513.057
iteration 0400: loss: 1517.994
iteration 0500: loss: 1515.710
iteration 0600: loss: 1515.991
iteration 0700: loss: 1517.755
iteration 0800: loss: 1513.637
iteration 0900: loss: 1514.638
====> Epoch: 050 Train loss: 1515.0982  took : 8.431446313858032
====> Test loss: 1518.7516
====> [MM-VAE] Time: 507.122s or 00:08:27
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  3
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_3
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.050
iteration 0100: loss: 2055.225
iteration 0200: loss: 2036.061
iteration 0300: loss: 2006.851
iteration 0400: loss: 2003.393
iteration 0500: loss: 1999.044
iteration 0600: loss: 1998.648
iteration 0700: loss: 1990.349
iteration 0800: loss: 1998.275
iteration 0900: loss: 1995.292
====> Epoch: 001 Train loss: 2014.8352  took : 12.483612537384033
====> Test loss: 1993.1212
iteration 0000: loss: 1990.592
iteration 0100: loss: 1988.277
iteration 0200: loss: 1988.644
iteration 0300: loss: 1982.234
iteration 0400: loss: 1987.440
iteration 0500: loss: 1983.702
iteration 0600: loss: 1981.357
iteration 0700: loss: 1981.237
iteration 0800: loss: 1975.498
iteration 0900: loss: 1977.492
====> Epoch: 002 Train loss: 1982.7966  took : 12.83695650100708
====> Test loss: 1979.0062
iteration 0000: loss: 1977.192
iteration 0100: loss: 1978.031
iteration 0200: loss: 1974.665
iteration 0300: loss: 1976.642
iteration 0400: loss: 1970.827
iteration 0500: loss: 1968.883
iteration 0600: loss: 1967.816
iteration 0700: loss: 1968.657
iteration 0800: loss: 1968.877
iteration 0900: loss: 1966.079
====> Epoch: 003 Train loss: 1970.6627  took : 12.90010929107666
====> Test loss: 1969.3021
iteration 0000: loss: 1967.029
iteration 0100: loss: 1964.504
iteration 0200: loss: 1966.305
iteration 0300: loss: 1964.067
iteration 0400: loss: 1964.870
iteration 0500: loss: 1961.945
iteration 0600: loss: 1965.668
iteration 0700: loss: 1963.657
iteration 0800: loss: 1962.896
iteration 0900: loss: 1963.412
====> Epoch: 004 Train loss: 1964.3867  took : 12.590676546096802
====> Test loss: 1965.3172
iteration 0000: loss: 1960.758
iteration 0100: loss: 1964.402
iteration 0200: loss: 1963.138
iteration 0300: loss: 1961.977
iteration 0400: loss: 1960.983
iteration 0500: loss: 1960.153
iteration 0600: loss: 1961.037
iteration 0700: loss: 1962.556
iteration 0800: loss: 1959.944
iteration 0900: loss: 1959.375
====> Epoch: 005 Train loss: 1960.9735  took : 12.029341220855713
====> Test loss: 1964.2795
iteration 0000: loss: 1960.291
iteration 0100: loss: 1958.871
iteration 0200: loss: 1958.510
iteration 0300: loss: 1955.685
iteration 0400: loss: 1960.014
iteration 0500: loss: 1956.451
iteration 0600: loss: 1957.088
iteration 0700: loss: 1958.856
iteration 0800: loss: 1954.510
iteration 0900: loss: 1954.927
====> Epoch: 006 Train loss: 1958.3059  took : 12.833859205245972
====> Test loss: 1960.1906
iteration 0000: loss: 1957.603
iteration 0100: loss: 1956.208
iteration 0200: loss: 1960.005
iteration 0300: loss: 1957.427
iteration 0400: loss: 1955.492
iteration 0500: loss: 1955.184
iteration 0600: loss: 1953.819
iteration 0700: loss: 1954.854
iteration 0800: loss: 1956.800
iteration 0900: loss: 1956.834
====> Epoch: 007 Train loss: 1956.2154  took : 12.06180739402771
====> Test loss: 1958.5235
iteration 0000: loss: 1953.582
iteration 0100: loss: 1954.479
iteration 0200: loss: 1961.697
iteration 0300: loss: 1955.176
iteration 0400: loss: 1955.410
iteration 0500: loss: 1954.581
iteration 0600: loss: 1954.832
iteration 0700: loss: 1956.596
iteration 0800: loss: 1955.846
iteration 0900: loss: 1951.546
====> Epoch: 008 Train loss: 1954.7081  took : 12.31356692314148
====> Test loss: 1957.8249
iteration 0000: loss: 1956.504
iteration 0100: loss: 1955.620
iteration 0200: loss: 1953.708
iteration 0300: loss: 1953.194
iteration 0400: loss: 1952.798
iteration 0500: loss: 1955.293
iteration 0600: loss: 1956.889
iteration 0700: loss: 1951.929
iteration 0800: loss: 1953.550
iteration 0900: loss: 1954.206
====> Epoch: 009 Train loss: 1953.5358  took : 12.214365482330322
====> Test loss: 1955.7925
iteration 0000: loss: 1951.883
iteration 0100: loss: 1950.664
iteration 0200: loss: 1954.192
iteration 0300: loss: 1954.557
iteration 0400: loss: 1951.781
iteration 0500: loss: 1954.519
iteration 0600: loss: 1952.246
iteration 0700: loss: 1954.159
iteration 0800: loss: 1952.133
iteration 0900: loss: 1953.639
====> Epoch: 010 Train loss: 1952.5524  took : 12.940757274627686
====> Test loss: 1954.9152
iteration 0000: loss: 1953.825
iteration 0100: loss: 1952.137
iteration 0200: loss: 1953.953
iteration 0300: loss: 1952.733
iteration 0400: loss: 1950.790
iteration 0500: loss: 1950.453
iteration 0600: loss: 1951.399
iteration 0700: loss: 1950.383
iteration 0800: loss: 1951.523
iteration 0900: loss: 1951.470
====> Epoch: 011 Train loss: 1951.9455  took : 11.757307052612305
====> Test loss: 1954.3318
iteration 0000: loss: 1950.257
iteration 0100: loss: 1951.880
iteration 0200: loss: 1952.331
iteration 0300: loss: 1952.309
iteration 0400: loss: 1949.056
iteration 0500: loss: 1951.215
iteration 0600: loss: 1952.296
iteration 0700: loss: 1954.534
iteration 0800: loss: 1952.126
iteration 0900: loss: 1951.009
====> Epoch: 012 Train loss: 1951.3656  took : 13.031893014907837
====> Test loss: 1954.1428
iteration 0000: loss: 1950.828
iteration 0100: loss: 1951.571
iteration 0200: loss: 1952.476
iteration 0300: loss: 1953.676
iteration 0400: loss: 1951.072
iteration 0500: loss: 1950.590
iteration 0600: loss: 1951.813
iteration 0700: loss: 1950.578
iteration 0800: loss: 1949.957
iteration 0900: loss: 1950.945
====> Epoch: 013 Train loss: 1951.0452  took : 13.222123622894287
====> Test loss: 1953.4067
iteration 0000: loss: 1951.417
iteration 0100: loss: 1952.439
iteration 0200: loss: 1950.458
iteration 0300: loss: 1953.080
iteration 0400: loss: 1950.307
iteration 0500: loss: 1950.951
iteration 0600: loss: 1951.133
iteration 0700: loss: 1950.306
iteration 0800: loss: 1949.659
iteration 0900: loss: 1950.587
====> Epoch: 014 Train loss: 1950.8633  took : 13.059763431549072
====> Test loss: 1953.1525
iteration 0000: loss: 1949.036
iteration 0100: loss: 1950.468
iteration 0200: loss: 1950.406
iteration 0300: loss: 1950.249
iteration 0400: loss: 1950.066
iteration 0500: loss: 1950.251
iteration 0600: loss: 1950.529
iteration 0700: loss: 1951.743
iteration 0800: loss: 1949.856
iteration 0900: loss: 1949.944
====> Epoch: 015 Train loss: 1950.7107  took : 12.519849300384521
====> Test loss: 1952.7715
iteration 0000: loss: 1949.678
iteration 0100: loss: 1950.924
iteration 0200: loss: 1949.364
iteration 0300: loss: 1950.389
iteration 0400: loss: 1952.145
iteration 0500: loss: 1950.182
iteration 0600: loss: 1948.533
iteration 0700: loss: 1949.133
iteration 0800: loss: 1950.125
iteration 0900: loss: 1952.402
====> Epoch: 016 Train loss: 1950.3448  took : 12.981943368911743
====> Test loss: 1952.7526
iteration 0000: loss: 1949.381
iteration 0100: loss: 1952.031
iteration 0200: loss: 1951.917
iteration 0300: loss: 1953.829
iteration 0400: loss: 1949.732
iteration 0500: loss: 1952.045
iteration 0600: loss: 1949.893
iteration 0700: loss: 1949.100
iteration 0800: loss: 1949.279
iteration 0900: loss: 1950.847
====> Epoch: 017 Train loss: 1950.2699  took : 12.600676536560059
====> Test loss: 1952.7854
iteration 0000: loss: 1949.472
iteration 0100: loss: 1950.171
iteration 0200: loss: 1950.112
iteration 0300: loss: 1950.502
iteration 0400: loss: 1951.964
iteration 0500: loss: 1949.105
iteration 0600: loss: 1950.730
iteration 0700: loss: 1948.862
iteration 0800: loss: 1948.748
iteration 0900: loss: 1951.145
====> Epoch: 018 Train loss: 1949.9348  took : 12.555222034454346
====> Test loss: 1951.9763
iteration 0000: loss: 1949.090
iteration 0100: loss: 1949.294
iteration 0200: loss: 1949.789
iteration 0300: loss: 1949.718
iteration 0400: loss: 1950.508
iteration 0500: loss: 1951.280
iteration 0600: loss: 1949.144
iteration 0700: loss: 1950.615
iteration 0800: loss: 1950.729
iteration 0900: loss: 1949.223
====> Epoch: 019 Train loss: 1949.6905  took : 12.523059606552124
====> Test loss: 1951.5775
iteration 0000: loss: 1948.262
iteration 0100: loss: 1950.188
iteration 0200: loss: 1948.614
iteration 0300: loss: 1949.131
iteration 0400: loss: 1948.456
iteration 0500: loss: 1950.765
iteration 0600: loss: 1949.643
iteration 0700: loss: 1950.406
iteration 0800: loss: 1948.406
iteration 0900: loss: 1950.001
====> Epoch: 020 Train loss: 1949.4461  took : 12.44044804573059
====> Test loss: 1951.6898
iteration 0000: loss: 1948.845
iteration 0100: loss: 1950.402
iteration 0200: loss: 1950.426
iteration 0300: loss: 1949.508
iteration 0400: loss: 1949.391
iteration 0500: loss: 1949.517
iteration 0600: loss: 1948.792
iteration 0700: loss: 1948.861
iteration 0800: loss: 1949.123
iteration 0900: loss: 1947.827
====> Epoch: 021 Train loss: 1949.4198  took : 11.67773175239563
====> Test loss: 1951.1814
iteration 0000: loss: 1947.687
iteration 0100: loss: 1948.601
iteration 0200: loss: 1949.121
iteration 0300: loss: 1948.391
iteration 0400: loss: 1948.609
iteration 0500: loss: 1950.071
iteration 0600: loss: 1948.681
iteration 0700: loss: 1949.428
iteration 0800: loss: 1947.367
iteration 0900: loss: 1949.448
====> Epoch: 022 Train loss: 1949.0360  took : 12.046489953994751
====> Test loss: 1951.2546
iteration 0000: loss: 1947.104
iteration 0100: loss: 1948.378
iteration 0200: loss: 1949.439
iteration 0300: loss: 1947.885
iteration 0400: loss: 1948.434
iteration 0500: loss: 1949.249
iteration 0600: loss: 1947.718
iteration 0700: loss: 1948.932
iteration 0800: loss: 1948.351
iteration 0900: loss: 1948.847
====> Epoch: 023 Train loss: 1948.9096  took : 12.162382125854492
====> Test loss: 1951.3472
iteration 0000: loss: 1948.166
iteration 0100: loss: 1946.926
iteration 0200: loss: 1949.216
iteration 0300: loss: 1948.475
iteration 0400: loss: 1951.213
iteration 0500: loss: 1947.385
iteration 0600: loss: 1948.626
iteration 0700: loss: 1947.669
iteration 0800: loss: 1949.343
iteration 0900: loss: 1948.595
====> Epoch: 024 Train loss: 1948.7976  took : 13.522509813308716
====> Test loss: 1951.2867
iteration 0000: loss: 1948.760
iteration 0100: loss: 1949.243
iteration 0200: loss: 1950.852
iteration 0300: loss: 1947.561
iteration 0400: loss: 1948.178
iteration 0500: loss: 1948.505
iteration 0600: loss: 1947.826
iteration 0700: loss: 1947.590
iteration 0800: loss: 1948.359
iteration 0900: loss: 1949.149
====> Epoch: 025 Train loss: 1948.8051  took : 12.762485980987549
====> Test loss: 1951.0357
iteration 0000: loss: 1950.693
iteration 0100: loss: 1950.589
iteration 0200: loss: 1949.276
iteration 0300: loss: 1948.871
iteration 0400: loss: 1949.103
iteration 0500: loss: 1948.149
iteration 0600: loss: 1948.399
iteration 0700: loss: 1949.102
iteration 0800: loss: 1947.064
iteration 0900: loss: 1949.203
====> Epoch: 026 Train loss: 1948.6604  took : 13.116109848022461
====> Test loss: 1951.0681
iteration 0000: loss: 1948.603
iteration 0100: loss: 1950.314
iteration 0200: loss: 1947.297
iteration 0300: loss: 1948.175
iteration 0400: loss: 1948.312
iteration 0500: loss: 1948.657
iteration 0600: loss: 1949.358
iteration 0700: loss: 1948.386
iteration 0800: loss: 1950.260
iteration 0900: loss: 1948.310
====> Epoch: 027 Train loss: 1948.8015  took : 12.299951553344727
====> Test loss: 1950.7515
iteration 0000: loss: 1947.441
iteration 0100: loss: 1949.339
iteration 0200: loss: 1946.830
iteration 0300: loss: 1948.682
iteration 0400: loss: 1948.952
iteration 0500: loss: 1948.775
iteration 0600: loss: 1949.571
iteration 0700: loss: 1947.971
iteration 0800: loss: 1949.106
iteration 0900: loss: 1950.157
====> Epoch: 028 Train loss: 1948.7257  took : 12.573360681533813
====> Test loss: 1950.9071
iteration 0000: loss: 1947.952
iteration 0100: loss: 1948.265
iteration 0200: loss: 1948.042
iteration 0300: loss: 1947.973
iteration 0400: loss: 1948.355
iteration 0500: loss: 1949.455
iteration 0600: loss: 1948.484
iteration 0700: loss: 1948.695
iteration 0800: loss: 1949.683
iteration 0900: loss: 1946.891
====> Epoch: 029 Train loss: 1948.5477  took : 13.21328616142273
====> Test loss: 1950.2791
iteration 0000: loss: 1947.449
iteration 0100: loss: 1948.986
iteration 0200: loss: 1948.739
iteration 0300: loss: 1948.538
iteration 0400: loss: 1948.074
iteration 0500: loss: 1948.169
iteration 0600: loss: 1948.977
iteration 0700: loss: 1947.411
iteration 0800: loss: 1947.297
iteration 0900: loss: 1948.834
====> Epoch: 030 Train loss: 1948.3868  took : 13.358049631118774
====> Test loss: 1950.3411
iteration 0000: loss: 1948.910
iteration 0100: loss: 1948.109
iteration 0200: loss: 1949.271
iteration 0300: loss: 1947.222
iteration 0400: loss: 1949.054
iteration 0500: loss: 1949.066
iteration 0600: loss: 1947.984
iteration 0700: loss: 1948.448
iteration 0800: loss: 1948.621
iteration 0900: loss: 1947.822
====> Epoch: 031 Train loss: 1948.2540  took : 12.901395559310913
====> Test loss: 1950.4790
iteration 0000: loss: 1948.617
iteration 0100: loss: 1948.382
iteration 0200: loss: 1948.416
iteration 0300: loss: 1947.773
iteration 0400: loss: 1948.064
iteration 0500: loss: 1948.593
iteration 0600: loss: 1947.516
iteration 0700: loss: 1947.822
iteration 0800: loss: 1949.590
iteration 0900: loss: 1948.089
====> Epoch: 032 Train loss: 1948.3155  took : 12.734960556030273
====> Test loss: 1950.6784
iteration 0000: loss: 1949.083
iteration 0100: loss: 1949.672
iteration 0200: loss: 1947.382
iteration 0300: loss: 1947.328
iteration 0400: loss: 1947.656
iteration 0500: loss: 1947.757
iteration 0600: loss: 1948.406
iteration 0700: loss: 1948.712
iteration 0800: loss: 1947.617
iteration 0900: loss: 1948.313
====> Epoch: 033 Train loss: 1948.2181  took : 12.888812065124512
====> Test loss: 1950.1197
iteration 0000: loss: 1947.397
iteration 0100: loss: 1947.536
iteration 0200: loss: 1948.179
iteration 0300: loss: 1948.860
iteration 0400: loss: 1948.177
iteration 0500: loss: 1948.837
iteration 0600: loss: 1948.097
iteration 0700: loss: 1949.145
iteration 0800: loss: 1947.552
iteration 0900: loss: 1948.125
====> Epoch: 034 Train loss: 1948.1679  took : 11.818933725357056
====> Test loss: 1950.2026
iteration 0000: loss: 1948.938
iteration 0100: loss: 1947.195
iteration 0200: loss: 1947.850
iteration 0300: loss: 1948.506
iteration 0400: loss: 1947.694
iteration 0500: loss: 1949.011
iteration 0600: loss: 1948.674
iteration 0700: loss: 1948.441
iteration 0800: loss: 1949.612
iteration 0900: loss: 1947.654
====> Epoch: 035 Train loss: 1948.2093  took : 12.606337308883667
====> Test loss: 1950.5495
iteration 0000: loss: 1948.514
iteration 0100: loss: 1947.779
iteration 0200: loss: 1948.914
iteration 0300: loss: 1948.803
iteration 0400: loss: 1949.050
iteration 0500: loss: 1948.161
iteration 0600: loss: 1948.613
iteration 0700: loss: 1949.879
iteration 0800: loss: 1947.879
iteration 0900: loss: 1948.460
====> Epoch: 036 Train loss: 1948.1639  took : 12.300667762756348
====> Test loss: 1950.3709
iteration 0000: loss: 1948.988
iteration 0100: loss: 1947.933
iteration 0200: loss: 1948.666
iteration 0300: loss: 1949.578
iteration 0400: loss: 1947.994
iteration 0500: loss: 1948.088
iteration 0600: loss: 1947.942
iteration 0700: loss: 1948.298
iteration 0800: loss: 1947.560
iteration 0900: loss: 1947.153
====> Epoch: 037 Train loss: 1948.0464  took : 12.463917970657349
====> Test loss: 1950.2744
iteration 0000: loss: 1947.558
iteration 0100: loss: 1947.613
iteration 0200: loss: 1948.646
iteration 0300: loss: 1948.817
iteration 0400: loss: 1949.229
iteration 0500: loss: 1947.098
iteration 0600: loss: 1950.073
iteration 0700: loss: 1949.624
iteration 0800: loss: 1947.631
iteration 0900: loss: 1947.547
====> Epoch: 038 Train loss: 1948.1109  took : 13.299334287643433
====> Test loss: 1949.9522
iteration 0000: loss: 1947.628
iteration 0100: loss: 1947.613
iteration 0200: loss: 1948.114
iteration 0300: loss: 1947.019
iteration 0400: loss: 1947.264
iteration 0500: loss: 1948.545
iteration 0600: loss: 1947.811
iteration 0700: loss: 1948.624
iteration 0800: loss: 1948.605
iteration 0900: loss: 1948.847
====> Epoch: 039 Train loss: 1948.0162  took : 13.037379264831543
====> Test loss: 1950.1860
iteration 0000: loss: 1947.399
iteration 0100: loss: 1948.529
iteration 0200: loss: 1948.280
iteration 0300: loss: 1948.173
iteration 0400: loss: 1948.361
iteration 0500: loss: 1947.714
iteration 0600: loss: 1947.557
iteration 0700: loss: 1948.147
iteration 0800: loss: 1949.399
iteration 0900: loss: 1948.500
====> Epoch: 040 Train loss: 1948.1022  took : 12.089457511901855
====> Test loss: 1950.1429
iteration 0000: loss: 1947.440
iteration 0100: loss: 1947.403
iteration 0200: loss: 1947.528
iteration 0300: loss: 1947.310
iteration 0400: loss: 1947.530
iteration 0500: loss: 1947.016
iteration 0600: loss: 1947.231
iteration 0700: loss: 1949.120
iteration 0800: loss: 1947.632
iteration 0900: loss: 1948.183
====> Epoch: 041 Train loss: 1947.9304  took : 13.419375896453857
====> Test loss: 1950.2452
iteration 0000: loss: 1947.732
iteration 0100: loss: 1947.250
iteration 0200: loss: 1948.180
iteration 0300: loss: 1948.672
iteration 0400: loss: 1949.529
iteration 0500: loss: 1947.377
iteration 0600: loss: 1949.523
iteration 0700: loss: 1947.612
iteration 0800: loss: 1948.406
iteration 0900: loss: 1949.086
====> Epoch: 042 Train loss: 1948.1051  took : 13.170036315917969
====> Test loss: 1950.6852
iteration 0000: loss: 1947.297
iteration 0100: loss: 1948.527
iteration 0200: loss: 1949.595
iteration 0300: loss: 1949.160
iteration 0400: loss: 1947.988
iteration 0500: loss: 1946.910
iteration 0600: loss: 1947.992
iteration 0700: loss: 1947.905
iteration 0800: loss: 1947.775
iteration 0900: loss: 1947.458
====> Epoch: 043 Train loss: 1947.9317  took : 11.97904920578003
====> Test loss: 1949.7212
iteration 0000: loss: 1947.434
iteration 0100: loss: 1947.016
iteration 0200: loss: 1946.863
iteration 0300: loss: 1947.169
iteration 0400: loss: 1947.258
iteration 0500: loss: 1948.211
iteration 0600: loss: 1947.554
iteration 0700: loss: 1946.896
iteration 0800: loss: 1946.857
iteration 0900: loss: 1948.062
====> Epoch: 044 Train loss: 1947.8731  took : 12.397186994552612
====> Test loss: 1949.7258
iteration 0000: loss: 1947.690
iteration 0100: loss: 1947.385
iteration 0200: loss: 1946.959
iteration 0300: loss: 1947.367
iteration 0400: loss: 1948.997
iteration 0500: loss: 1947.074
iteration 0600: loss: 1948.459
iteration 0700: loss: 1947.852
iteration 0800: loss: 1947.119
iteration 0900: loss: 1947.499
====> Epoch: 045 Train loss: 1947.9869  took : 12.594151258468628
====> Test loss: 1950.1745
iteration 0000: loss: 1947.682
iteration 0100: loss: 1949.117
iteration 0200: loss: 1948.489
iteration 0300: loss: 1948.099
iteration 0400: loss: 1947.752
iteration 0500: loss: 1948.841
iteration 0600: loss: 1948.383
iteration 0700: loss: 1948.312
iteration 0800: loss: 1947.575
iteration 0900: loss: 1947.694
====> Epoch: 046 Train loss: 1948.0318  took : 12.094932794570923
====> Test loss: 1949.9450
iteration 0000: loss: 1947.545
iteration 0100: loss: 1947.517
iteration 0200: loss: 1947.620
iteration 0300: loss: 1947.862
iteration 0400: loss: 1947.694
iteration 0500: loss: 1947.640
iteration 0600: loss: 1946.879
iteration 0700: loss: 1947.367
iteration 0800: loss: 1948.315
iteration 0900: loss: 1948.091
====> Epoch: 047 Train loss: 1947.9367  took : 13.090028285980225
====> Test loss: 1950.0471
iteration 0000: loss: 1947.188
iteration 0100: loss: 1949.417
iteration 0200: loss: 1947.405
iteration 0300: loss: 1947.614
iteration 0400: loss: 1948.450
iteration 0500: loss: 1948.138
iteration 0600: loss: 1947.272
iteration 0700: loss: 1948.404
iteration 0800: loss: 1948.306
iteration 0900: loss: 1947.088
====> Epoch: 048 Train loss: 1947.9658  took : 12.316888093948364
====> Test loss: 1950.1524
iteration 0000: loss: 1947.579
iteration 0100: loss: 1948.176
iteration 0200: loss: 1947.145
iteration 0300: loss: 1948.249
iteration 0400: loss: 1948.957
iteration 0500: loss: 1950.749
iteration 0600: loss: 1949.093
iteration 0700: loss: 1947.155
iteration 0800: loss: 1947.144
iteration 0900: loss: 1948.635
====> Epoch: 049 Train loss: 1948.0408  took : 12.170578479766846
====> Test loss: 1949.6138
iteration 0000: loss: 1948.548
iteration 0100: loss: 1948.105
iteration 0200: loss: 1947.780
iteration 0300: loss: 1948.667
iteration 0400: loss: 1948.112
iteration 0500: loss: 1948.406
iteration 0600: loss: 1947.154
iteration 0700: loss: 1947.363
iteration 0800: loss: 1948.012
iteration 0900: loss: 1948.513
====> Epoch: 050 Train loss: 1947.9660  took : 12.177905797958374
====> Test loss: 1950.7234
====> [MM-VAE] Time: 701.586s or 00:11:41
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  3
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_3
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5221.418
iteration 0100: loss: 4183.938
iteration 0200: loss: 4096.311
iteration 0300: loss: 4055.198
iteration 0400: loss: 4023.088
iteration 0500: loss: 4029.969
iteration 0600: loss: 4018.084
iteration 0700: loss: 4003.249
iteration 0800: loss: 4009.544
iteration 0900: loss: 4005.857
iteration 1000: loss: 4002.858
iteration 1100: loss: 4007.229
iteration 1200: loss: 4014.546
iteration 1300: loss: 4001.113
iteration 1400: loss: 4000.274
iteration 1500: loss: 3994.940
iteration 1600: loss: 3994.259
iteration 1700: loss: 4005.824
iteration 1800: loss: 4002.242
====> Epoch: 001 Train loss: 4029.0665  took : 53.48523831367493
====> Test loss: 3993.8703
iteration 0000: loss: 3985.410
iteration 0100: loss: 3979.321
iteration 0200: loss: 3987.367
iteration 0300: loss: 3981.713
iteration 0400: loss: 3980.499
iteration 0500: loss: 3984.154
iteration 0600: loss: 3988.222
iteration 0700: loss: 3972.776
iteration 0800: loss: 3984.354
iteration 0900: loss: 3975.809
iteration 1000: loss: 3980.516
iteration 1100: loss: 3970.248
iteration 1200: loss: 3965.045
iteration 1300: loss: 3976.543
iteration 1400: loss: 3972.875
iteration 1500: loss: 3964.740
iteration 1600: loss: 3979.842
iteration 1700: loss: 3972.355
iteration 1800: loss: 3977.645
====> Epoch: 002 Train loss: 3979.4918  took : 53.39589738845825
====> Test loss: 3972.2926
iteration 0000: loss: 3967.301
iteration 0100: loss: 3979.780
iteration 0200: loss: 3972.022
iteration 0300: loss: 3973.956
iteration 0400: loss: 3968.653
iteration 0500: loss: 3968.640
iteration 0600: loss: 3968.845
iteration 0700: loss: 3961.163
iteration 0800: loss: 3967.481
iteration 0900: loss: 3955.529
iteration 1000: loss: 3964.975
iteration 1100: loss: 3962.240
iteration 1200: loss: 3963.056
iteration 1300: loss: 3957.053
iteration 1400: loss: 3960.385
iteration 1500: loss: 3968.792
iteration 1600: loss: 3954.612
iteration 1700: loss: 3959.296
iteration 1800: loss: 3955.708
====> Epoch: 003 Train loss: 3964.2458  took : 53.41507911682129
====> Test loss: 3962.0153
iteration 0000: loss: 3964.032
iteration 0100: loss: 3954.649
iteration 0200: loss: 3962.659
iteration 0300: loss: 3956.105
iteration 0400: loss: 3955.279
iteration 0500: loss: 3952.605
iteration 0600: loss: 3951.780
iteration 0700: loss: 3962.355
iteration 0800: loss: 3951.918
iteration 0900: loss: 3961.475
iteration 1000: loss: 3955.884
iteration 1100: loss: 3950.497
iteration 1200: loss: 3953.697
iteration 1300: loss: 3957.839
iteration 1400: loss: 3955.093
iteration 1500: loss: 3950.037
iteration 1600: loss: 3948.795
iteration 1700: loss: 3951.268
iteration 1800: loss: 3949.629
====> Epoch: 004 Train loss: 3954.6512  took : 53.304667711257935
====> Test loss: 3953.9922
iteration 0000: loss: 3950.793
iteration 0100: loss: 3944.575
iteration 0200: loss: 3949.150
iteration 0300: loss: 3949.138
iteration 0400: loss: 3952.767
iteration 0500: loss: 3953.777
iteration 0600: loss: 3948.267
iteration 0700: loss: 3940.973
iteration 0800: loss: 3941.702
iteration 0900: loss: 3945.930
iteration 1000: loss: 3950.183
iteration 1100: loss: 3951.900
iteration 1200: loss: 3948.199
iteration 1300: loss: 3946.108
iteration 1400: loss: 3945.762
iteration 1500: loss: 3948.694
iteration 1600: loss: 3944.888
iteration 1700: loss: 3947.524
iteration 1800: loss: 3943.960
====> Epoch: 005 Train loss: 3947.9028  took : 53.05950474739075
====> Test loss: 3949.3786
iteration 0000: loss: 3947.440
iteration 0100: loss: 3940.773
iteration 0200: loss: 3943.948
iteration 0300: loss: 3941.540
iteration 0400: loss: 3948.398
iteration 0500: loss: 3943.753
iteration 0600: loss: 3951.652
iteration 0700: loss: 3945.739
iteration 0800: loss: 3953.053
iteration 0900: loss: 3943.821
iteration 1000: loss: 3941.782
iteration 1100: loss: 3950.989
iteration 1200: loss: 3939.635
iteration 1300: loss: 3943.942
iteration 1400: loss: 3941.534
iteration 1500: loss: 3933.926
iteration 1600: loss: 3945.693
iteration 1700: loss: 3943.960
iteration 1800: loss: 3946.879
====> Epoch: 006 Train loss: 3944.3393  took : 53.107818841934204
====> Test loss: 3946.4382
iteration 0000: loss: 3945.099
iteration 0100: loss: 3941.118
iteration 0200: loss: 3946.216
iteration 0300: loss: 3943.723
iteration 0400: loss: 3943.344
iteration 0500: loss: 3945.663
iteration 0600: loss: 3940.161
iteration 0700: loss: 3941.274
iteration 0800: loss: 3941.315
iteration 0900: loss: 3941.831
iteration 1000: loss: 3949.056
iteration 1100: loss: 3943.695
iteration 1200: loss: 3940.716
iteration 1300: loss: 3938.331
iteration 1400: loss: 3945.573
iteration 1500: loss: 3940.426
iteration 1600: loss: 3940.558
iteration 1700: loss: 3941.192
iteration 1800: loss: 3947.568
====> Epoch: 007 Train loss: 3943.0005  took : 53.21246933937073
====> Test loss: 3946.6370
iteration 0000: loss: 3942.547
iteration 0100: loss: 3939.567
iteration 0200: loss: 3939.934
iteration 0300: loss: 3946.320
iteration 0400: loss: 3947.940
iteration 0500: loss: 3945.707
iteration 0600: loss: 3936.329
iteration 0700: loss: 3940.085
iteration 0800: loss: 3936.597
iteration 0900: loss: 3938.863
iteration 1000: loss: 3943.216
iteration 1100: loss: 3942.958
iteration 1200: loss: 3938.111
iteration 1300: loss: 3937.212
iteration 1400: loss: 3949.496
iteration 1500: loss: 3941.255
iteration 1600: loss: 3940.911
iteration 1700: loss: 3937.724
iteration 1800: loss: 3942.271
====> Epoch: 008 Train loss: 3941.9506  took : 53.052451610565186
====> Test loss: 3945.3622
iteration 0000: loss: 3937.102
iteration 0100: loss: 3933.190
iteration 0200: loss: 3944.021
iteration 0300: loss: 3940.901
iteration 0400: loss: 3942.500
iteration 0500: loss: 3942.427
iteration 0600: loss: 3939.325
iteration 0700: loss: 3937.072
iteration 0800: loss: 3944.100
iteration 0900: loss: 3938.291
iteration 1000: loss: 3947.838
iteration 1100: loss: 3939.827
iteration 1200: loss: 3943.858
iteration 1300: loss: 3939.873
iteration 1400: loss: 3939.604
iteration 1500: loss: 3942.251
iteration 1600: loss: 3939.844
iteration 1700: loss: 3941.481
iteration 1800: loss: 3942.774
====> Epoch: 009 Train loss: 3941.5482  took : 53.0528450012207
====> Test loss: 3945.3106
iteration 0000: loss: 3936.585
iteration 0100: loss: 3950.243
iteration 0200: loss: 3939.828
iteration 0300: loss: 3939.358
iteration 0400: loss: 3939.395
iteration 0500: loss: 3942.492
iteration 0600: loss: 3949.148
iteration 0700: loss: 3943.404
iteration 0800: loss: 3944.102
iteration 0900: loss: 3944.046
iteration 1000: loss: 3938.903
iteration 1100: loss: 3941.078
iteration 1200: loss: 3939.186
iteration 1300: loss: 3946.390
iteration 1400: loss: 3941.606
iteration 1500: loss: 3943.244
iteration 1600: loss: 3943.255
iteration 1700: loss: 3942.250
iteration 1800: loss: 3940.326
====> Epoch: 010 Train loss: 3941.4457  took : 52.95079207420349
====> Test loss: 3944.9531
iteration 0000: loss: 3944.948
iteration 0100: loss: 3940.656
iteration 0200: loss: 3940.940
iteration 0300: loss: 3940.053
iteration 0400: loss: 3948.710
iteration 0500: loss: 3938.882
iteration 0600: loss: 3949.732
iteration 0700: loss: 3938.489
iteration 0800: loss: 3938.913
iteration 0900: loss: 3941.465
iteration 1000: loss: 3939.854
iteration 1100: loss: 3942.955
iteration 1200: loss: 3935.506
iteration 1300: loss: 3943.061
iteration 1400: loss: 3939.489
iteration 1500: loss: 3939.450
iteration 1600: loss: 3943.050
iteration 1700: loss: 3939.993
iteration 1800: loss: 3936.515
====> Epoch: 011 Train loss: 3940.8764  took : 52.78965997695923
====> Test loss: 3944.9004
iteration 0000: loss: 3937.863
iteration 0100: loss: 3937.703
iteration 0200: loss: 3938.224
iteration 0300: loss: 3943.917
iteration 0400: loss: 3940.687
iteration 0500: loss: 3941.029
iteration 0600: loss: 3935.278
iteration 0700: loss: 3941.818
iteration 0800: loss: 3941.055
iteration 0900: loss: 3937.878
iteration 1000: loss: 3934.670
iteration 1100: loss: 3942.314
iteration 1200: loss: 3940.566
iteration 1300: loss: 3939.327
iteration 1400: loss: 3947.698
iteration 1500: loss: 3937.307
iteration 1600: loss: 3940.125
iteration 1700: loss: 3938.542
iteration 1800: loss: 3936.468
====> Epoch: 012 Train loss: 3940.7502  took : 52.98575806617737
====> Test loss: 3944.0183
iteration 0000: loss: 3940.287
iteration 0100: loss: 3938.857
iteration 0200: loss: 3939.728
iteration 0300: loss: 3941.406
iteration 0400: loss: 3936.764
iteration 0500: loss: 3937.169
iteration 0600: loss: 3937.139
iteration 0700: loss: 3939.417
iteration 0800: loss: 3938.676
iteration 0900: loss: 3944.861
iteration 1000: loss: 3934.866
iteration 1100: loss: 3940.979
iteration 1200: loss: 3940.887
iteration 1300: loss: 3942.245
iteration 1400: loss: 3943.612
iteration 1500: loss: 3942.472
iteration 1600: loss: 3944.775
iteration 1700: loss: 3941.992
iteration 1800: loss: 3941.283
====> Epoch: 013 Train loss: 3940.2608  took : 52.9319863319397
====> Test loss: 3944.3117
iteration 0000: loss: 3945.810
iteration 0100: loss: 3946.138
iteration 0200: loss: 3935.992
iteration 0300: loss: 3938.096
iteration 0400: loss: 3937.991
iteration 0500: loss: 3941.193
iteration 0600: loss: 3938.049
iteration 0700: loss: 3945.630
iteration 0800: loss: 3941.875
iteration 0900: loss: 3933.056
iteration 1000: loss: 3934.181
iteration 1100: loss: 3936.313
iteration 1200: loss: 3934.775
iteration 1300: loss: 3935.393
iteration 1400: loss: 3937.269
iteration 1500: loss: 3937.172
iteration 1600: loss: 3943.770
iteration 1700: loss: 3948.993
iteration 1800: loss: 3936.828
====> Epoch: 014 Train loss: 3940.2235  took : 53.05524230003357
====> Test loss: 3943.6891
iteration 0000: loss: 3934.055
iteration 0100: loss: 3940.699
iteration 0200: loss: 3935.262
iteration 0300: loss: 3938.144
iteration 0400: loss: 3943.320
iteration 0500: loss: 3939.478
iteration 0600: loss: 3939.826
iteration 0700: loss: 3942.852
iteration 0800: loss: 3938.868
iteration 0900: loss: 3943.213
iteration 1000: loss: 3936.315
iteration 1100: loss: 3945.549
iteration 1200: loss: 3940.513
iteration 1300: loss: 3939.827
iteration 1400: loss: 3943.562
iteration 1500: loss: 3939.091
iteration 1600: loss: 3944.311
iteration 1700: loss: 3938.961
iteration 1800: loss: 3935.056
====> Epoch: 015 Train loss: 3940.0950  took : 53.017678022384644
====> Test loss: 3943.4867
iteration 0000: loss: 3940.961
iteration 0100: loss: 3943.508
iteration 0200: loss: 3941.213
iteration 0300: loss: 3938.467
iteration 0400: loss: 3933.851
iteration 0500: loss: 3942.312
iteration 0600: loss: 3934.155
iteration 0700: loss: 3941.784
iteration 0800: loss: 3937.698
iteration 0900: loss: 3940.573
iteration 1000: loss: 3941.816
iteration 1100: loss: 3937.444
iteration 1200: loss: 3944.908
iteration 1300: loss: 3938.443
iteration 1400: loss: 3940.816
iteration 1500: loss: 3941.119
iteration 1600: loss: 3932.269
iteration 1700: loss: 3937.309
iteration 1800: loss: 3932.782
====> Epoch: 016 Train loss: 3939.8269  took : 53.132365465164185
====> Test loss: 3943.2531
iteration 0000: loss: 3937.102
iteration 0100: loss: 3944.474
iteration 0200: loss: 3936.566
iteration 0300: loss: 3934.891
iteration 0400: loss: 3936.743
iteration 0500: loss: 3940.220
iteration 0600: loss: 3934.755
iteration 0700: loss: 3932.748
iteration 0800: loss: 3936.271
iteration 0900: loss: 3937.705
iteration 1000: loss: 3939.399
iteration 1100: loss: 3937.200
iteration 1200: loss: 3949.130
iteration 1300: loss: 3939.493
iteration 1400: loss: 3932.870
iteration 1500: loss: 3945.385
iteration 1600: loss: 3942.335
iteration 1700: loss: 3939.863
iteration 1800: loss: 3941.505
====> Epoch: 017 Train loss: 3939.8564  took : 52.90735650062561
====> Test loss: 3944.6442
iteration 0000: loss: 3950.941
iteration 0100: loss: 3937.174
iteration 0200: loss: 3934.598
iteration 0300: loss: 3936.564
iteration 0400: loss: 3942.170
iteration 0500: loss: 3938.192
iteration 0600: loss: 3942.099
iteration 0700: loss: 3942.890
iteration 0800: loss: 3942.305
iteration 0900: loss: 3938.453
iteration 1000: loss: 3942.421
iteration 1100: loss: 3938.754
iteration 1200: loss: 3947.133
iteration 1300: loss: 3940.260
iteration 1400: loss: 3946.362
iteration 1500: loss: 3942.773
iteration 1600: loss: 3941.068
iteration 1700: loss: 3940.672
iteration 1800: loss: 3935.156
====> Epoch: 018 Train loss: 3939.9006  took : 52.822550535202026
====> Test loss: 3944.0473
iteration 0000: loss: 3940.223
iteration 0100: loss: 3944.837
iteration 0200: loss: 3935.671
iteration 0300: loss: 3939.966
iteration 0400: loss: 3938.076
iteration 0500: loss: 3936.495
iteration 0600: loss: 3941.379
iteration 0700: loss: 3940.879
iteration 0800: loss: 3939.891
iteration 0900: loss: 3937.640
iteration 1000: loss: 3936.039
iteration 1100: loss: 3936.410
iteration 1200: loss: 3935.857
iteration 1300: loss: 3938.031
iteration 1400: loss: 3933.990
iteration 1500: loss: 3942.265
iteration 1600: loss: 3937.980
iteration 1700: loss: 3940.544
iteration 1800: loss: 3943.289
====> Epoch: 019 Train loss: 3939.5491  took : 53.04608201980591
====> Test loss: 3943.0118
iteration 0000: loss: 3938.591
iteration 0100: loss: 3940.476
iteration 0200: loss: 3940.553
iteration 0300: loss: 3938.738
iteration 0400: loss: 3939.966
iteration 0500: loss: 3934.545
iteration 0600: loss: 3946.514
iteration 0700: loss: 3938.475
iteration 0800: loss: 3942.478
iteration 0900: loss: 3936.516
iteration 1000: loss: 3937.461
iteration 1100: loss: 3938.667
iteration 1200: loss: 3937.156
iteration 1300: loss: 3944.754
iteration 1400: loss: 3935.704
iteration 1500: loss: 3932.891
iteration 1600: loss: 3945.717
iteration 1700: loss: 3930.012
iteration 1800: loss: 3932.019
====> Epoch: 020 Train loss: 3939.5832  took : 52.993680477142334
====> Test loss: 3943.4284
iteration 0000: loss: 3950.180
iteration 0100: loss: 3939.169
iteration 0200: loss: 3945.286
iteration 0300: loss: 3937.625
iteration 0400: loss: 3938.186
iteration 0500: loss: 3938.881
iteration 0600: loss: 3939.892
iteration 0700: loss: 3938.192
iteration 0800: loss: 3933.069
iteration 0900: loss: 3934.085
iteration 1000: loss: 3936.275
iteration 1100: loss: 3936.445
iteration 1200: loss: 3939.875
iteration 1300: loss: 3932.998
iteration 1400: loss: 3935.759
iteration 1500: loss: 3941.385
iteration 1600: loss: 3944.904
iteration 1700: loss: 3940.085
iteration 1800: loss: 3938.434
====> Epoch: 021 Train loss: 3939.3730  took : 53.01331305503845
====> Test loss: 3943.0194
iteration 0000: loss: 3943.983
iteration 0100: loss: 3938.150
iteration 0200: loss: 3942.064
iteration 0300: loss: 3944.368
iteration 0400: loss: 3938.677
iteration 0500: loss: 3938.583
iteration 0600: loss: 3939.499
iteration 0700: loss: 3934.151
iteration 0800: loss: 3937.526
iteration 0900: loss: 3942.812
iteration 1000: loss: 3936.236
iteration 1100: loss: 3934.212
iteration 1200: loss: 3944.079
iteration 1300: loss: 3945.137
iteration 1400: loss: 3935.979
iteration 1500: loss: 3934.465
iteration 1600: loss: 3951.810
iteration 1700: loss: 3934.332
iteration 1800: loss: 3938.481
====> Epoch: 022 Train loss: 3939.3185  took : 52.826438903808594
====> Test loss: 3943.6090
iteration 0000: loss: 3945.701
iteration 0100: loss: 3937.661
iteration 0200: loss: 3933.453
iteration 0300: loss: 3941.006
iteration 0400: loss: 3934.195
iteration 0500: loss: 3936.271
iteration 0600: loss: 3941.059
iteration 0700: loss: 3943.327
iteration 0800: loss: 3941.318
iteration 0900: loss: 3943.670
iteration 1000: loss: 3940.285
iteration 1100: loss: 3937.212
iteration 1200: loss: 3941.593
iteration 1300: loss: 3934.470
iteration 1400: loss: 3934.964
iteration 1500: loss: 3937.462
iteration 1600: loss: 3941.750
iteration 1700: loss: 3941.710
iteration 1800: loss: 3934.968
====> Epoch: 023 Train loss: 3939.0842  took : 52.9794237613678
====> Test loss: 3943.4545
iteration 0000: loss: 3937.177
iteration 0100: loss: 3934.338
iteration 0200: loss: 3934.642
iteration 0300: loss: 3945.591
iteration 0400: loss: 3947.922
iteration 0500: loss: 3943.565
iteration 0600: loss: 3943.647
iteration 0700: loss: 3937.254
iteration 0800: loss: 3942.812
iteration 0900: loss: 3940.113
iteration 1000: loss: 3943.179
iteration 1100: loss: 3941.374
iteration 1200: loss: 3938.492
iteration 1300: loss: 3940.062
iteration 1400: loss: 3937.101
iteration 1500: loss: 3935.327
iteration 1600: loss: 3939.164
iteration 1700: loss: 3934.413
iteration 1800: loss: 3936.747
====> Epoch: 024 Train loss: 3939.4382  took : 52.911776065826416
====> Test loss: 3944.0553
iteration 0000: loss: 3946.832
iteration 0100: loss: 3936.237
iteration 0200: loss: 3942.388
iteration 0300: loss: 3937.849
iteration 0400: loss: 3943.382
iteration 0500: loss: 3933.574
iteration 0600: loss: 3941.767
iteration 0700: loss: 3938.406
iteration 0800: loss: 3939.124
iteration 0900: loss: 3937.536
iteration 1000: loss: 3939.737
iteration 1100: loss: 3940.376
iteration 1200: loss: 3940.333
iteration 1300: loss: 3936.419
iteration 1400: loss: 3941.745
iteration 1500: loss: 3937.818
iteration 1600: loss: 3935.105
iteration 1700: loss: 3939.650
iteration 1800: loss: 3934.585
====> Epoch: 025 Train loss: 3939.1712  took : 52.98697519302368
====> Test loss: 3943.3887
iteration 0000: loss: 3949.269
iteration 0100: loss: 3937.859
iteration 0200: loss: 3939.865
iteration 0300: loss: 3938.017
iteration 0400: loss: 3933.914
iteration 0500: loss: 3939.477
iteration 0600: loss: 3936.822
iteration 0700: loss: 3939.595
iteration 0800: loss: 3938.871
iteration 0900: loss: 3934.135
iteration 1000: loss: 3937.987
iteration 1100: loss: 3936.069
iteration 1200: loss: 3941.887
iteration 1300: loss: 3941.999
iteration 1400: loss: 3938.000
iteration 1500: loss: 3938.286
iteration 1600: loss: 3940.663
iteration 1700: loss: 3940.027
iteration 1800: loss: 3938.281
====> Epoch: 026 Train loss: 3938.9695  took : 53.06297731399536
====> Test loss: 3943.1682
iteration 0000: loss: 3934.963
iteration 0100: loss: 3934.736
iteration 0200: loss: 3937.003
iteration 0300: loss: 3940.350
iteration 0400: loss: 3934.059
iteration 0500: loss: 3942.018
iteration 0600: loss: 3934.968
iteration 0700: loss: 3939.871
iteration 0800: loss: 3943.643
iteration 0900: loss: 3935.845
iteration 1000: loss: 3943.133
iteration 1100: loss: 3941.896
iteration 1200: loss: 3934.614
iteration 1300: loss: 3942.709
iteration 1400: loss: 3938.434
iteration 1500: loss: 3935.124
iteration 1600: loss: 3937.348
iteration 1700: loss: 3945.139
iteration 1800: loss: 3937.053
====> Epoch: 027 Train loss: 3938.7822  took : 52.96478819847107
====> Test loss: 3943.4980
iteration 0000: loss: 3947.190
iteration 0100: loss: 3938.568
iteration 0200: loss: 3942.581
iteration 0300: loss: 3938.743
iteration 0400: loss: 3936.473
iteration 0500: loss: 3935.058
iteration 0600: loss: 3942.239
iteration 0700: loss: 3944.567
iteration 0800: loss: 3938.564
iteration 0900: loss: 3936.077
iteration 1000: loss: 3941.012
iteration 1100: loss: 3939.949
iteration 1200: loss: 3940.099
iteration 1300: loss: 3933.688
iteration 1400: loss: 3938.811
iteration 1500: loss: 3941.329
iteration 1600: loss: 3938.705
iteration 1700: loss: 3945.479
iteration 1800: loss: 3941.064
====> Epoch: 028 Train loss: 3938.9980  took : 52.77429533004761
====> Test loss: 3942.2347
iteration 0000: loss: 3939.706
iteration 0100: loss: 3942.866
iteration 0200: loss: 3935.397
iteration 0300: loss: 3940.916
iteration 0400: loss: 3937.469
iteration 0500: loss: 3943.222
iteration 0600: loss: 3932.859
iteration 0700: loss: 3943.345
iteration 0800: loss: 3939.727
iteration 0900: loss: 3940.508
iteration 1000: loss: 3936.927
iteration 1100: loss: 3936.319
iteration 1200: loss: 3933.789
iteration 1300: loss: 3943.947
iteration 1400: loss: 3942.010
iteration 1500: loss: 3937.086
iteration 1600: loss: 3944.366
iteration 1700: loss: 3945.297
iteration 1800: loss: 3934.841
====> Epoch: 029 Train loss: 3938.6070  took : 53.0966010093689
====> Test loss: 3942.3796
iteration 0000: loss: 3931.542
iteration 0100: loss: 3937.357
iteration 0200: loss: 3938.031
iteration 0300: loss: 3937.942
iteration 0400: loss: 3939.886
iteration 0500: loss: 3938.328
iteration 0600: loss: 3939.042
iteration 0700: loss: 3937.779
iteration 0800: loss: 3934.768
iteration 0900: loss: 3939.274
iteration 1000: loss: 3933.712
iteration 1100: loss: 3934.307
iteration 1200: loss: 3938.496
iteration 1300: loss: 3941.699
iteration 1400: loss: 3941.930
iteration 1500: loss: 3933.556
iteration 1600: loss: 3939.530
iteration 1700: loss: 3935.842
iteration 1800: loss: 3942.763
====> Epoch: 030 Train loss: 3938.5549  took : 52.85443162918091
====> Test loss: 3942.6588
iteration 0000: loss: 3938.500
iteration 0100: loss: 3940.134
iteration 0200: loss: 3938.690
iteration 0300: loss: 3936.231
iteration 0400: loss: 3939.280
iteration 0500: loss: 3941.037
iteration 0600: loss: 3937.966
iteration 0700: loss: 3941.540
iteration 0800: loss: 3933.871
iteration 0900: loss: 3935.363
iteration 1000: loss: 3933.100
iteration 1100: loss: 3944.401
iteration 1200: loss: 3942.035
iteration 1300: loss: 3942.408
iteration 1400: loss: 3944.227
iteration 1500: loss: 3931.693
iteration 1600: loss: 3938.438
iteration 1700: loss: 3937.022
iteration 1800: loss: 3939.018
====> Epoch: 031 Train loss: 3938.6614  took : 52.95381283760071
====> Test loss: 3942.5799
iteration 0000: loss: 3941.750
iteration 0100: loss: 3934.789
iteration 0200: loss: 3945.605
iteration 0300: loss: 3939.027
iteration 0400: loss: 3945.158
iteration 0500: loss: 3939.898
iteration 0600: loss: 3943.878
iteration 0700: loss: 3933.497
iteration 0800: loss: 3941.319
iteration 0900: loss: 3940.908
iteration 1000: loss: 3934.250
iteration 1100: loss: 3939.789
iteration 1200: loss: 3937.443
iteration 1300: loss: 3936.904
iteration 1400: loss: 3934.532
iteration 1500: loss: 3939.854
iteration 1600: loss: 3938.794
iteration 1700: loss: 3943.740
iteration 1800: loss: 3937.963
====> Epoch: 032 Train loss: 3938.7131  took : 52.99247145652771
====> Test loss: 3943.4953
iteration 0000: loss: 3941.983
iteration 0100: loss: 3935.470
iteration 0200: loss: 3935.329
iteration 0300: loss: 3933.709
iteration 0400: loss: 3936.774
iteration 0500: loss: 3936.081
iteration 0600: loss: 3938.854
iteration 0700: loss: 3933.965
iteration 0800: loss: 3941.000
iteration 0900: loss: 3936.691
iteration 1000: loss: 3942.882
iteration 1100: loss: 3936.417
iteration 1200: loss: 3938.877
iteration 1300: loss: 3936.828
iteration 1400: loss: 3939.415
iteration 1500: loss: 3934.503
iteration 1600: loss: 3939.504
iteration 1700: loss: 3933.698
iteration 1800: loss: 3942.294
====> Epoch: 033 Train loss: 3938.6707  took : 52.80147361755371
====> Test loss: 3942.9996
iteration 0000: loss: 3938.472
iteration 0100: loss: 3945.704
iteration 0200: loss: 3939.869
iteration 0300: loss: 3934.620
iteration 0400: loss: 3935.955
iteration 0500: loss: 3943.906
iteration 0600: loss: 3940.754
iteration 0700: loss: 3937.785
iteration 0800: loss: 3930.824
iteration 0900: loss: 3941.630
iteration 1000: loss: 3935.257
iteration 1100: loss: 3941.360
iteration 1200: loss: 3933.837
iteration 1300: loss: 3936.168
iteration 1400: loss: 3939.614
iteration 1500: loss: 3945.765
iteration 1600: loss: 3941.668
iteration 1700: loss: 3946.312
iteration 1800: loss: 3933.367
====> Epoch: 034 Train loss: 3938.5707  took : 52.856876611709595
====> Test loss: 3943.0046
iteration 0000: loss: 3936.917
iteration 0100: loss: 3943.096
iteration 0200: loss: 3935.259
iteration 0300: loss: 3934.325
iteration 0400: loss: 3947.773
iteration 0500: loss: 3941.791
iteration 0600: loss: 3940.695
iteration 0700: loss: 3933.176
iteration 0800: loss: 3935.097
iteration 0900: loss: 3940.235
iteration 1000: loss: 3939.969
iteration 1100: loss: 3942.094
iteration 1200: loss: 3946.270
iteration 1300: loss: 3941.786
iteration 1400: loss: 3941.162
iteration 1500: loss: 3944.839
iteration 1600: loss: 3936.637
iteration 1700: loss: 3938.952
iteration 1800: loss: 3945.032
====> Epoch: 035 Train loss: 3938.5287  took : 52.97626233100891
====> Test loss: 3942.8958
iteration 0000: loss: 3942.830
iteration 0100: loss: 3932.820
iteration 0200: loss: 3946.771
iteration 0300: loss: 3939.267
iteration 0400: loss: 3937.511
iteration 0500: loss: 3936.203
iteration 0600: loss: 3932.233
iteration 0700: loss: 3935.189
iteration 0800: loss: 3935.959
iteration 0900: loss: 3938.870
iteration 1000: loss: 3942.808
iteration 1100: loss: 3937.091
iteration 1200: loss: 3930.193
iteration 1300: loss: 3936.442
iteration 1400: loss: 3935.287
iteration 1500: loss: 3931.709
iteration 1600: loss: 3942.356
iteration 1700: loss: 3936.885
iteration 1800: loss: 3939.612
====> Epoch: 036 Train loss: 3938.2739  took : 52.93719983100891
====> Test loss: 3942.2070
iteration 0000: loss: 3941.331
iteration 0100: loss: 3939.825
iteration 0200: loss: 3935.615
iteration 0300: loss: 3940.208
iteration 0400: loss: 3937.622
iteration 0500: loss: 3933.519
iteration 0600: loss: 3935.656
iteration 0700: loss: 3934.742
iteration 0800: loss: 3936.138
iteration 0900: loss: 3944.639
iteration 1000: loss: 3937.069
iteration 1100: loss: 3937.871
iteration 1200: loss: 3935.349
iteration 1300: loss: 3936.391
iteration 1400: loss: 3941.037
iteration 1500: loss: 3934.241
iteration 1600: loss: 3936.532
iteration 1700: loss: 3932.729
iteration 1800: loss: 3942.465
====> Epoch: 037 Train loss: 3938.1831  took : 52.8685998916626
====> Test loss: 3942.2553
iteration 0000: loss: 3942.657
iteration 0100: loss: 3938.490
iteration 0200: loss: 3941.941
iteration 0300: loss: 3940.778
iteration 0400: loss: 3938.340
iteration 0500: loss: 3937.512
iteration 0600: loss: 3938.154
iteration 0700: loss: 3935.104
iteration 0800: loss: 3940.022
iteration 0900: loss: 3939.602
iteration 1000: loss: 3933.537
iteration 1100: loss: 3939.677
iteration 1200: loss: 3939.585
iteration 1300: loss: 3935.546
iteration 1400: loss: 3935.538
iteration 1500: loss: 3940.937
iteration 1600: loss: 3935.498
iteration 1700: loss: 3942.066
iteration 1800: loss: 3933.888
====> Epoch: 038 Train loss: 3938.2168  took : 52.94590902328491
====> Test loss: 3943.0240
iteration 0000: loss: 3934.671
iteration 0100: loss: 3937.288
iteration 0200: loss: 3933.135
iteration 0300: loss: 3944.811
iteration 0400: loss: 3936.287
iteration 0500: loss: 3938.826
iteration 0600: loss: 3944.756
iteration 0700: loss: 3937.383
iteration 0800: loss: 3936.261
iteration 0900: loss: 3938.052
iteration 1000: loss: 3938.756
iteration 1100: loss: 3943.762
iteration 1200: loss: 3940.347
iteration 1300: loss: 3936.178
iteration 1400: loss: 3936.918
iteration 1500: loss: 3944.835
iteration 1600: loss: 3944.334
iteration 1700: loss: 3935.891
iteration 1800: loss: 3938.804
====> Epoch: 039 Train loss: 3938.2430  took : 53.01607036590576
====> Test loss: 3943.2055
iteration 0000: loss: 3942.735
iteration 0100: loss: 3934.761
iteration 0200: loss: 3939.644
iteration 0300: loss: 3930.771
iteration 0400: loss: 3934.771
iteration 0500: loss: 3939.859
iteration 0600: loss: 3937.999
iteration 0700: loss: 3939.034
iteration 0800: loss: 3940.014
iteration 0900: loss: 3935.408
iteration 1000: loss: 3934.378
iteration 1100: loss: 3932.296
iteration 1200: loss: 3937.105
iteration 1300: loss: 3934.575
iteration 1400: loss: 3935.097
iteration 1500: loss: 3937.285
iteration 1600: loss: 3941.579
iteration 1700: loss: 3948.943
iteration 1800: loss: 3937.411
====> Epoch: 040 Train loss: 3938.3345  took : 52.88575887680054
====> Test loss: 3942.6541
iteration 0000: loss: 3935.372
iteration 0100: loss: 3934.649
iteration 0200: loss: 3935.433
iteration 0300: loss: 3931.112
iteration 0400: loss: 3934.860
iteration 0500: loss: 3939.167
iteration 0600: loss: 3933.322
iteration 0700: loss: 3941.431
iteration 0800: loss: 3936.755
iteration 0900: loss: 3933.428
iteration 1000: loss: 3936.973
iteration 1100: loss: 3935.832
iteration 1200: loss: 3940.334
iteration 1300: loss: 3935.704
iteration 1400: loss: 3936.696
iteration 1500: loss: 3941.736
iteration 1600: loss: 3940.812
iteration 1700: loss: 3933.956
iteration 1800: loss: 3935.370
====> Epoch: 041 Train loss: 3938.0867  took : 53.185075521469116
====> Test loss: 3941.7547
iteration 0000: loss: 3941.068
iteration 0100: loss: 3948.531
iteration 0200: loss: 3936.094
iteration 0300: loss: 3944.379
iteration 0400: loss: 3931.881
iteration 0500: loss: 3939.797
iteration 0600: loss: 3938.586
iteration 0700: loss: 3939.583
iteration 0800: loss: 3931.426
iteration 0900: loss: 3936.188
iteration 1000: loss: 3938.282
iteration 1100: loss: 3937.274
iteration 1200: loss: 3943.379
iteration 1300: loss: 3937.362
iteration 1400: loss: 3934.442
iteration 1500: loss: 3930.074
iteration 1600: loss: 3933.081
iteration 1700: loss: 3939.101
iteration 1800: loss: 3932.929
====> Epoch: 042 Train loss: 3938.1970  took : 52.921764612197876
====> Test loss: 3942.6417
iteration 0000: loss: 3939.586
iteration 0100: loss: 3937.522
iteration 0200: loss: 3941.274
iteration 0300: loss: 3944.591
iteration 0400: loss: 3940.313
iteration 0500: loss: 3940.417
iteration 0600: loss: 3941.511
iteration 0700: loss: 3937.163
iteration 0800: loss: 3942.023
iteration 0900: loss: 3942.108
iteration 1000: loss: 3933.542
iteration 1100: loss: 3937.950
iteration 1200: loss: 3940.017
iteration 1300: loss: 3941.978
iteration 1400: loss: 3936.700
iteration 1500: loss: 3937.416
iteration 1600: loss: 3939.546
iteration 1700: loss: 3933.788
iteration 1800: loss: 3941.356
====> Epoch: 043 Train loss: 3938.1758  took : 53.034607887268066
====> Test loss: 3941.9519
iteration 0000: loss: 3942.425
iteration 0100: loss: 3948.812
iteration 0200: loss: 3942.743
iteration 0300: loss: 3937.076
iteration 0400: loss: 3939.541
iteration 0500: loss: 3936.543
iteration 0600: loss: 3938.211
iteration 0700: loss: 3936.125
iteration 0800: loss: 3936.290
iteration 0900: loss: 3940.498
iteration 1000: loss: 3937.319
iteration 1100: loss: 3932.806
iteration 1200: loss: 3937.858
iteration 1300: loss: 3936.699
iteration 1400: loss: 3934.079
iteration 1500: loss: 3934.518
iteration 1600: loss: 3932.913
iteration 1700: loss: 3939.545
iteration 1800: loss: 3933.871
====> Epoch: 044 Train loss: 3937.8923  took : 53.04865598678589
====> Test loss: 3941.8824
iteration 0000: loss: 3935.651
iteration 0100: loss: 3942.244
iteration 0200: loss: 3935.344
iteration 0300: loss: 3941.187
iteration 0400: loss: 3934.973
iteration 0500: loss: 3938.228
iteration 0600: loss: 3936.216
iteration 0700: loss: 3932.979
iteration 0800: loss: 3946.625
iteration 0900: loss: 3945.681
iteration 1000: loss: 3931.465
iteration 1100: loss: 3933.229
iteration 1200: loss: 3929.631
iteration 1300: loss: 3939.100
iteration 1400: loss: 3939.618
iteration 1500: loss: 3934.217
iteration 1600: loss: 3933.888
iteration 1700: loss: 3937.333
iteration 1800: loss: 3942.229
====> Epoch: 045 Train loss: 3938.1252  took : 53.00577402114868
====> Test loss: 3942.3632
iteration 0000: loss: 3936.285
iteration 0100: loss: 3933.777
iteration 0200: loss: 3935.033
iteration 0300: loss: 3936.725
iteration 0400: loss: 3935.381
iteration 0500: loss: 3944.412
iteration 0600: loss: 3934.188
iteration 0700: loss: 3945.843
iteration 0800: loss: 3939.991
iteration 0900: loss: 3936.938
iteration 1000: loss: 3936.514
iteration 1100: loss: 3936.175
iteration 1200: loss: 3934.022
iteration 1300: loss: 3938.493
iteration 1400: loss: 3936.235
iteration 1500: loss: 3939.358
iteration 1600: loss: 3939.473
iteration 1700: loss: 3937.283
iteration 1800: loss: 3936.636
====> Epoch: 046 Train loss: 3938.0412  took : 52.937148094177246
====> Test loss: 3942.1421
iteration 0000: loss: 3940.031
iteration 0100: loss: 3943.152
iteration 0200: loss: 3938.396
iteration 0300: loss: 3933.217
iteration 0400: loss: 3938.252
iteration 0500: loss: 3932.890
iteration 0600: loss: 3945.296
iteration 0700: loss: 3936.744
iteration 0800: loss: 3936.438
iteration 0900: loss: 3935.829
iteration 1000: loss: 3941.106
iteration 1100: loss: 3934.435
iteration 1200: loss: 3939.641
iteration 1300: loss: 3938.397
iteration 1400: loss: 3950.368
iteration 1500: loss: 3940.166
iteration 1600: loss: 3937.840
iteration 1700: loss: 3936.513
iteration 1800: loss: 3944.650
====> Epoch: 047 Train loss: 3938.0618  took : 52.889010190963745
====> Test loss: 3942.2660
iteration 0000: loss: 3933.893
iteration 0100: loss: 3941.455
iteration 0200: loss: 3939.802
iteration 0300: loss: 3931.180
iteration 0400: loss: 3938.846
iteration 0500: loss: 3942.970
iteration 0600: loss: 3940.770
iteration 0700: loss: 3937.729
iteration 0800: loss: 3941.454
iteration 0900: loss: 3942.754
iteration 1000: loss: 3936.942
iteration 1100: loss: 3938.603
iteration 1200: loss: 3936.846
iteration 1300: loss: 3939.062
iteration 1400: loss: 3937.885
iteration 1500: loss: 3934.430
iteration 1600: loss: 3934.720
iteration 1700: loss: 3937.868
iteration 1800: loss: 3935.164
====> Epoch: 048 Train loss: 3937.7937  took : 52.77387452125549
====> Test loss: 3942.4589
iteration 0000: loss: 3940.733
iteration 0100: loss: 3937.110
iteration 0200: loss: 3940.479
iteration 0300: loss: 3937.321
iteration 0400: loss: 3937.868
iteration 0500: loss: 3935.030
iteration 0600: loss: 3937.984
iteration 0700: loss: 3939.611
iteration 0800: loss: 3937.421
iteration 0900: loss: 3938.152
iteration 1000: loss: 3937.602
iteration 1100: loss: 3936.637
iteration 1200: loss: 3934.522
iteration 1300: loss: 3937.163
iteration 1400: loss: 3933.924
iteration 1500: loss: 3940.401
iteration 1600: loss: 3937.863
iteration 1700: loss: 3943.248
iteration 1800: loss: 3942.344
====> Epoch: 049 Train loss: 3937.7704  took : 52.97958469390869
====> Test loss: 3941.9715
iteration 0000: loss: 3943.280
iteration 0100: loss: 3941.091
iteration 0200: loss: 3932.000
iteration 0300: loss: 3940.958
iteration 0400: loss: 3939.719
iteration 0500: loss: 3936.262
iteration 0600: loss: 3935.676
iteration 0700: loss: 3933.113
iteration 0800: loss: 3944.744
iteration 0900: loss: 3935.739
iteration 1000: loss: 3941.723
iteration 1100: loss: 3932.449
iteration 1200: loss: 3933.235
iteration 1300: loss: 3940.390
iteration 1400: loss: 3937.773
iteration 1500: loss: 3941.276
iteration 1600: loss: 3938.540
iteration 1700: loss: 3932.556
iteration 1800: loss: 3939.602
====> Epoch: 050 Train loss: 3937.9285  took : 52.851266622543335
====> Test loss: 3942.1710
====> [MM-VAE] Time: 3155.223s or 00:52:35
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  4
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_4
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1988.061
iteration 0100: loss: 1571.930
iteration 0200: loss: 1572.024
iteration 0300: loss: 1565.931
iteration 0400: loss: 1554.076
iteration 0500: loss: 1549.657
iteration 0600: loss: 1543.283
iteration 0700: loss: 1541.977
iteration 0800: loss: 1539.472
iteration 0900: loss: 1539.679
====> Epoch: 001 Train loss: 1555.0413  took : 8.490424871444702
====> Test loss: 1537.9019
iteration 0000: loss: 1537.036
iteration 0100: loss: 1533.366
iteration 0200: loss: 1536.965
iteration 0300: loss: 1535.248
iteration 0400: loss: 1529.737
iteration 0500: loss: 1527.968
iteration 0600: loss: 1532.230
iteration 0700: loss: 1530.488
iteration 0800: loss: 1531.466
iteration 0900: loss: 1528.907
====> Epoch: 002 Train loss: 1531.4893  took : 8.386680126190186
====> Test loss: 1529.3789
iteration 0000: loss: 1527.894
iteration 0100: loss: 1526.884
iteration 0200: loss: 1527.339
iteration 0300: loss: 1527.447
iteration 0400: loss: 1525.604
iteration 0500: loss: 1528.264
iteration 0600: loss: 1520.614
iteration 0700: loss: 1523.421
iteration 0800: loss: 1521.538
iteration 0900: loss: 1525.895
====> Epoch: 003 Train loss: 1525.8802  took : 8.406660556793213
====> Test loss: 1526.5925
iteration 0000: loss: 1522.899
iteration 0100: loss: 1525.440
iteration 0200: loss: 1525.733
iteration 0300: loss: 1524.135
iteration 0400: loss: 1524.598
iteration 0500: loss: 1527.786
iteration 0600: loss: 1521.249
iteration 0700: loss: 1523.095
iteration 0800: loss: 1518.759
iteration 0900: loss: 1522.048
====> Epoch: 004 Train loss: 1523.4680  took : 8.487503290176392
====> Test loss: 1524.9222
iteration 0000: loss: 1525.019
iteration 0100: loss: 1522.638
iteration 0200: loss: 1518.363
iteration 0300: loss: 1527.749
iteration 0400: loss: 1525.020
iteration 0500: loss: 1522.158
iteration 0600: loss: 1520.722
iteration 0700: loss: 1523.416
iteration 0800: loss: 1525.485
iteration 0900: loss: 1520.969
====> Epoch: 005 Train loss: 1522.1095  took : 8.454360485076904
====> Test loss: 1523.8136
iteration 0000: loss: 1522.382
iteration 0100: loss: 1521.087
iteration 0200: loss: 1519.781
iteration 0300: loss: 1519.872
iteration 0400: loss: 1522.374
iteration 0500: loss: 1520.231
iteration 0600: loss: 1522.082
iteration 0700: loss: 1519.039
iteration 0800: loss: 1523.755
iteration 0900: loss: 1519.539
====> Epoch: 006 Train loss: 1521.0381  took : 8.438964366912842
====> Test loss: 1523.0499
iteration 0000: loss: 1518.772
iteration 0100: loss: 1520.366
iteration 0200: loss: 1523.361
iteration 0300: loss: 1517.554
iteration 0400: loss: 1520.498
iteration 0500: loss: 1519.233
iteration 0600: loss: 1520.965
iteration 0700: loss: 1524.699
iteration 0800: loss: 1519.671
iteration 0900: loss: 1519.067
====> Epoch: 007 Train loss: 1520.2180  took : 8.47880244255066
====> Test loss: 1522.3168
iteration 0000: loss: 1516.832
iteration 0100: loss: 1523.931
iteration 0200: loss: 1516.994
iteration 0300: loss: 1521.157
iteration 0400: loss: 1519.632
iteration 0500: loss: 1517.734
iteration 0600: loss: 1520.609
iteration 0700: loss: 1519.218
iteration 0800: loss: 1519.390
iteration 0900: loss: 1519.090
====> Epoch: 008 Train loss: 1519.5695  took : 8.535489082336426
====> Test loss: 1521.8995
iteration 0000: loss: 1518.779
iteration 0100: loss: 1519.530
iteration 0200: loss: 1515.764
iteration 0300: loss: 1519.985
iteration 0400: loss: 1517.163
iteration 0500: loss: 1520.079
iteration 0600: loss: 1520.470
iteration 0700: loss: 1518.115
iteration 0800: loss: 1516.678
iteration 0900: loss: 1519.464
====> Epoch: 009 Train loss: 1519.0468  took : 8.527569055557251
====> Test loss: 1521.5074
iteration 0000: loss: 1519.029
iteration 0100: loss: 1519.461
iteration 0200: loss: 1516.495
iteration 0300: loss: 1518.268
iteration 0400: loss: 1517.245
iteration 0500: loss: 1516.679
iteration 0600: loss: 1519.041
iteration 0700: loss: 1521.684
iteration 0800: loss: 1518.462
iteration 0900: loss: 1519.879
====> Epoch: 010 Train loss: 1518.5961  took : 8.488601446151733
====> Test loss: 1521.0165
iteration 0000: loss: 1516.209
iteration 0100: loss: 1515.818
iteration 0200: loss: 1517.925
iteration 0300: loss: 1518.284
iteration 0400: loss: 1521.567
iteration 0500: loss: 1518.791
iteration 0600: loss: 1519.534
iteration 0700: loss: 1519.905
iteration 0800: loss: 1518.691
iteration 0900: loss: 1520.343
====> Epoch: 011 Train loss: 1518.2354  took : 8.396972179412842
====> Test loss: 1520.9083
iteration 0000: loss: 1517.480
iteration 0100: loss: 1516.545
iteration 0200: loss: 1519.338
iteration 0300: loss: 1519.479
iteration 0400: loss: 1516.099
iteration 0500: loss: 1519.567
iteration 0600: loss: 1515.929
iteration 0700: loss: 1520.191
iteration 0800: loss: 1519.522
iteration 0900: loss: 1517.778
====> Epoch: 012 Train loss: 1517.8962  took : 8.432813167572021
====> Test loss: 1520.5113
iteration 0000: loss: 1517.502
iteration 0100: loss: 1515.375
iteration 0200: loss: 1516.836
iteration 0300: loss: 1518.259
iteration 0400: loss: 1518.378
iteration 0500: loss: 1516.035
iteration 0600: loss: 1518.280
iteration 0700: loss: 1518.436
iteration 0800: loss: 1518.576
iteration 0900: loss: 1517.311
====> Epoch: 013 Train loss: 1517.6480  took : 8.457914590835571
====> Test loss: 1520.2556
iteration 0000: loss: 1515.719
iteration 0100: loss: 1515.214
iteration 0200: loss: 1519.871
iteration 0300: loss: 1517.356
iteration 0400: loss: 1516.459
iteration 0500: loss: 1518.570
iteration 0600: loss: 1518.030
iteration 0700: loss: 1515.586
iteration 0800: loss: 1519.403
iteration 0900: loss: 1517.153
====> Epoch: 014 Train loss: 1517.3730  took : 8.510736465454102
====> Test loss: 1520.1064
iteration 0000: loss: 1514.763
iteration 0100: loss: 1517.021
iteration 0200: loss: 1518.146
iteration 0300: loss: 1514.850
iteration 0400: loss: 1519.650
iteration 0500: loss: 1515.683
iteration 0600: loss: 1514.959
iteration 0700: loss: 1515.888
iteration 0800: loss: 1519.355
iteration 0900: loss: 1518.973
====> Epoch: 015 Train loss: 1517.1599  took : 8.455247402191162
====> Test loss: 1519.8903
iteration 0000: loss: 1520.700
iteration 0100: loss: 1514.971
iteration 0200: loss: 1517.292
iteration 0300: loss: 1517.850
iteration 0400: loss: 1515.676
iteration 0500: loss: 1515.320
iteration 0600: loss: 1517.610
iteration 0700: loss: 1519.540
iteration 0800: loss: 1518.795
iteration 0900: loss: 1518.837
====> Epoch: 016 Train loss: 1516.9658  took : 8.423643827438354
====> Test loss: 1519.6599
iteration 0000: loss: 1517.292
iteration 0100: loss: 1516.579
iteration 0200: loss: 1513.704
iteration 0300: loss: 1514.218
iteration 0400: loss: 1517.618
iteration 0500: loss: 1517.147
iteration 0600: loss: 1515.906
iteration 0700: loss: 1515.312
iteration 0800: loss: 1517.643
iteration 0900: loss: 1514.315
====> Epoch: 017 Train loss: 1516.8170  took : 8.47741436958313
====> Test loss: 1519.7099
iteration 0000: loss: 1515.983
iteration 0100: loss: 1514.901
iteration 0200: loss: 1516.742
iteration 0300: loss: 1516.066
iteration 0400: loss: 1514.109
iteration 0500: loss: 1514.128
iteration 0600: loss: 1515.395
iteration 0700: loss: 1517.619
iteration 0800: loss: 1515.898
iteration 0900: loss: 1519.231
====> Epoch: 018 Train loss: 1516.6832  took : 8.558739423751831
====> Test loss: 1519.5562
iteration 0000: loss: 1518.765
iteration 0100: loss: 1518.542
iteration 0200: loss: 1514.962
iteration 0300: loss: 1519.313
iteration 0400: loss: 1515.536
iteration 0500: loss: 1513.891
iteration 0600: loss: 1514.316
iteration 0700: loss: 1519.612
iteration 0800: loss: 1517.472
iteration 0900: loss: 1516.938
====> Epoch: 019 Train loss: 1516.5260  took : 8.409303665161133
====> Test loss: 1519.4552
iteration 0000: loss: 1518.123
iteration 0100: loss: 1519.289
iteration 0200: loss: 1514.916
iteration 0300: loss: 1516.321
iteration 0400: loss: 1518.840
iteration 0500: loss: 1516.815
iteration 0600: loss: 1517.908
iteration 0700: loss: 1519.654
iteration 0800: loss: 1516.966
iteration 0900: loss: 1516.022
====> Epoch: 020 Train loss: 1516.3752  took : 8.454580545425415
====> Test loss: 1519.2621
iteration 0000: loss: 1513.982
iteration 0100: loss: 1519.651
iteration 0200: loss: 1513.166
iteration 0300: loss: 1517.296
iteration 0400: loss: 1514.061
iteration 0500: loss: 1515.528
iteration 0600: loss: 1514.052
iteration 0700: loss: 1515.774
iteration 0800: loss: 1517.119
iteration 0900: loss: 1514.621
====> Epoch: 021 Train loss: 1516.2847  took : 8.413832902908325
====> Test loss: 1519.2440
iteration 0000: loss: 1517.887
iteration 0100: loss: 1516.354
iteration 0200: loss: 1516.311
iteration 0300: loss: 1518.011
iteration 0400: loss: 1514.347
iteration 0500: loss: 1513.324
iteration 0600: loss: 1515.312
iteration 0700: loss: 1515.343
iteration 0800: loss: 1516.247
iteration 0900: loss: 1513.313
====> Epoch: 022 Train loss: 1516.1650  took : 8.363263130187988
====> Test loss: 1519.0960
iteration 0000: loss: 1516.931
iteration 0100: loss: 1518.536
iteration 0200: loss: 1514.873
iteration 0300: loss: 1514.497
iteration 0400: loss: 1515.378
iteration 0500: loss: 1515.085
iteration 0600: loss: 1518.910
iteration 0700: loss: 1519.176
iteration 0800: loss: 1513.389
iteration 0900: loss: 1515.095
====> Epoch: 023 Train loss: 1516.0639  took : 8.505311727523804
====> Test loss: 1519.2573
iteration 0000: loss: 1514.898
iteration 0100: loss: 1513.817
iteration 0200: loss: 1515.091
iteration 0300: loss: 1515.396
iteration 0400: loss: 1516.956
iteration 0500: loss: 1516.191
iteration 0600: loss: 1517.260
iteration 0700: loss: 1516.152
iteration 0800: loss: 1517.048
iteration 0900: loss: 1514.027
====> Epoch: 024 Train loss: 1515.9523  took : 8.53508448600769
====> Test loss: 1519.1125
iteration 0000: loss: 1517.026
iteration 0100: loss: 1517.036
iteration 0200: loss: 1516.288
iteration 0300: loss: 1519.331
iteration 0400: loss: 1517.207
iteration 0500: loss: 1516.567
iteration 0600: loss: 1513.844
iteration 0700: loss: 1512.250
iteration 0800: loss: 1516.038
iteration 0900: loss: 1517.715
====> Epoch: 025 Train loss: 1515.8566  took : 8.4505136013031
====> Test loss: 1518.9651
iteration 0000: loss: 1514.771
iteration 0100: loss: 1514.410
iteration 0200: loss: 1517.365
iteration 0300: loss: 1514.622
iteration 0400: loss: 1517.002
iteration 0500: loss: 1516.528
iteration 0600: loss: 1516.480
iteration 0700: loss: 1518.440
iteration 0800: loss: 1515.191
iteration 0900: loss: 1517.039
====> Epoch: 026 Train loss: 1515.7492  took : 8.394915103912354
====> Test loss: 1518.9933
iteration 0000: loss: 1513.748
iteration 0100: loss: 1516.010
iteration 0200: loss: 1513.888
iteration 0300: loss: 1515.731
iteration 0400: loss: 1513.453
iteration 0500: loss: 1515.334
iteration 0600: loss: 1516.165
iteration 0700: loss: 1515.791
iteration 0800: loss: 1514.273
iteration 0900: loss: 1517.488
====> Epoch: 027 Train loss: 1515.6674  took : 8.46688175201416
====> Test loss: 1518.8931
iteration 0000: loss: 1515.581
iteration 0100: loss: 1516.060
iteration 0200: loss: 1514.178
iteration 0300: loss: 1512.716
iteration 0400: loss: 1513.077
iteration 0500: loss: 1513.018
iteration 0600: loss: 1513.294
iteration 0700: loss: 1517.287
iteration 0800: loss: 1514.863
iteration 0900: loss: 1515.804
====> Epoch: 028 Train loss: 1515.6238  took : 8.505982637405396
====> Test loss: 1518.9129
iteration 0000: loss: 1514.535
iteration 0100: loss: 1518.496
iteration 0200: loss: 1515.885
iteration 0300: loss: 1517.083
iteration 0400: loss: 1515.777
iteration 0500: loss: 1514.721
iteration 0600: loss: 1516.749
iteration 0700: loss: 1515.437
iteration 0800: loss: 1515.183
iteration 0900: loss: 1513.097
====> Epoch: 029 Train loss: 1515.5327  took : 8.418867349624634
====> Test loss: 1518.8383
iteration 0000: loss: 1513.651
iteration 0100: loss: 1518.829
iteration 0200: loss: 1513.302
iteration 0300: loss: 1514.532
iteration 0400: loss: 1514.704
iteration 0500: loss: 1513.752
iteration 0600: loss: 1515.687
iteration 0700: loss: 1516.589
iteration 0800: loss: 1514.570
iteration 0900: loss: 1512.612
====> Epoch: 030 Train loss: 1515.4914  took : 8.524345874786377
====> Test loss: 1518.8911
iteration 0000: loss: 1514.575
iteration 0100: loss: 1515.568
iteration 0200: loss: 1516.904
iteration 0300: loss: 1516.175
iteration 0400: loss: 1517.348
iteration 0500: loss: 1514.824
iteration 0600: loss: 1515.006
iteration 0700: loss: 1516.774
iteration 0800: loss: 1516.594
iteration 0900: loss: 1515.020
====> Epoch: 031 Train loss: 1515.3977  took : 8.4583420753479
====> Test loss: 1518.9450
iteration 0000: loss: 1515.816
iteration 0100: loss: 1514.584
iteration 0200: loss: 1513.953
iteration 0300: loss: 1516.076
iteration 0400: loss: 1513.897
iteration 0500: loss: 1518.000
iteration 0600: loss: 1514.310
iteration 0700: loss: 1515.500
iteration 0800: loss: 1516.555
iteration 0900: loss: 1516.005
====> Epoch: 032 Train loss: 1515.4145  took : 8.508688688278198
====> Test loss: 1518.8903
iteration 0000: loss: 1516.189
iteration 0100: loss: 1516.628
iteration 0200: loss: 1514.052
iteration 0300: loss: 1514.015
iteration 0400: loss: 1514.888
iteration 0500: loss: 1515.366
iteration 0600: loss: 1516.185
iteration 0700: loss: 1516.642
iteration 0800: loss: 1513.157
iteration 0900: loss: 1514.264
====> Epoch: 033 Train loss: 1515.2943  took : 8.52432131767273
====> Test loss: 1518.8302
iteration 0000: loss: 1514.903
iteration 0100: loss: 1514.859
iteration 0200: loss: 1512.079
iteration 0300: loss: 1517.142
iteration 0400: loss: 1515.108
iteration 0500: loss: 1517.270
iteration 0600: loss: 1516.182
iteration 0700: loss: 1513.177
iteration 0800: loss: 1515.493
iteration 0900: loss: 1514.240
====> Epoch: 034 Train loss: 1515.2090  took : 8.510835647583008
====> Test loss: 1518.7957
iteration 0000: loss: 1515.868
iteration 0100: loss: 1513.561
iteration 0200: loss: 1514.751
iteration 0300: loss: 1511.904
iteration 0400: loss: 1512.829
iteration 0500: loss: 1516.103
iteration 0600: loss: 1513.333
iteration 0700: loss: 1515.384
iteration 0800: loss: 1513.897
iteration 0900: loss: 1515.693
====> Epoch: 035 Train loss: 1515.1643  took : 8.49895715713501
====> Test loss: 1518.6409
iteration 0000: loss: 1512.677
iteration 0100: loss: 1515.566
iteration 0200: loss: 1514.132
iteration 0300: loss: 1515.563
iteration 0400: loss: 1514.789
iteration 0500: loss: 1517.593
iteration 0600: loss: 1514.593
iteration 0700: loss: 1517.112
iteration 0800: loss: 1515.198
iteration 0900: loss: 1516.814
====> Epoch: 036 Train loss: 1515.0875  took : 8.505669116973877
====> Test loss: 1518.4955
iteration 0000: loss: 1514.344
iteration 0100: loss: 1519.371
iteration 0200: loss: 1514.206
iteration 0300: loss: 1513.515
iteration 0400: loss: 1513.460
iteration 0500: loss: 1515.412
iteration 0600: loss: 1514.959
iteration 0700: loss: 1511.060
iteration 0800: loss: 1511.187
iteration 0900: loss: 1514.305
====> Epoch: 037 Train loss: 1515.0371  took : 8.544728755950928
====> Test loss: 1518.6714
iteration 0000: loss: 1513.652
iteration 0100: loss: 1513.218
iteration 0200: loss: 1512.501
iteration 0300: loss: 1515.139
iteration 0400: loss: 1515.194
iteration 0500: loss: 1514.549
iteration 0600: loss: 1516.672
iteration 0700: loss: 1516.574
iteration 0800: loss: 1513.467
iteration 0900: loss: 1516.054
====> Epoch: 038 Train loss: 1514.9876  took : 8.53101372718811
====> Test loss: 1518.5918
iteration 0000: loss: 1515.295
iteration 0100: loss: 1514.396
iteration 0200: loss: 1513.240
iteration 0300: loss: 1514.297
iteration 0400: loss: 1516.461
iteration 0500: loss: 1513.735
iteration 0600: loss: 1514.551
iteration 0700: loss: 1514.986
iteration 0800: loss: 1513.788
iteration 0900: loss: 1515.424
====> Epoch: 039 Train loss: 1514.9481  took : 8.507845640182495
====> Test loss: 1518.7435
iteration 0000: loss: 1513.988
iteration 0100: loss: 1515.996
iteration 0200: loss: 1517.496
iteration 0300: loss: 1513.252
iteration 0400: loss: 1515.384
iteration 0500: loss: 1514.413
iteration 0600: loss: 1514.363
iteration 0700: loss: 1514.000
iteration 0800: loss: 1512.472
iteration 0900: loss: 1514.588
====> Epoch: 040 Train loss: 1514.8827  took : 8.499990463256836
====> Test loss: 1518.6272
iteration 0000: loss: 1514.120
iteration 0100: loss: 1515.031
iteration 0200: loss: 1515.563
iteration 0300: loss: 1516.957
iteration 0400: loss: 1514.909
iteration 0500: loss: 1512.513
iteration 0600: loss: 1515.304
iteration 0700: loss: 1514.931
iteration 0800: loss: 1513.810
iteration 0900: loss: 1515.795
====> Epoch: 041 Train loss: 1514.8392  took : 8.477344036102295
====> Test loss: 1518.4370
iteration 0000: loss: 1516.671
iteration 0100: loss: 1514.882
iteration 0200: loss: 1513.295
iteration 0300: loss: 1514.485
iteration 0400: loss: 1514.531
iteration 0500: loss: 1514.503
iteration 0600: loss: 1513.141
iteration 0700: loss: 1513.792
iteration 0800: loss: 1514.096
iteration 0900: loss: 1514.240
====> Epoch: 042 Train loss: 1514.8191  took : 8.526070833206177
====> Test loss: 1518.6113
iteration 0000: loss: 1517.452
iteration 0100: loss: 1515.895
iteration 0200: loss: 1514.138
iteration 0300: loss: 1515.545
iteration 0400: loss: 1514.975
iteration 0500: loss: 1516.763
iteration 0600: loss: 1514.522
iteration 0700: loss: 1511.843
iteration 0800: loss: 1515.844
iteration 0900: loss: 1514.607
====> Epoch: 043 Train loss: 1514.7831  took : 8.404802083969116
====> Test loss: 1518.4559
iteration 0000: loss: 1513.635
iteration 0100: loss: 1517.146
iteration 0200: loss: 1514.059
iteration 0300: loss: 1513.776
iteration 0400: loss: 1516.205
iteration 0500: loss: 1514.988
iteration 0600: loss: 1516.273
iteration 0700: loss: 1516.002
iteration 0800: loss: 1515.063
iteration 0900: loss: 1515.549
====> Epoch: 044 Train loss: 1514.6989  took : 8.450505495071411
====> Test loss: 1518.5504
iteration 0000: loss: 1515.879
iteration 0100: loss: 1514.074
iteration 0200: loss: 1512.721
iteration 0300: loss: 1514.954
iteration 0400: loss: 1514.301
iteration 0500: loss: 1513.629
iteration 0600: loss: 1514.973
iteration 0700: loss: 1514.897
iteration 0800: loss: 1516.707
iteration 0900: loss: 1517.413
====> Epoch: 045 Train loss: 1514.6512  took : 8.426071405410767
====> Test loss: 1518.5852
iteration 0000: loss: 1514.171
iteration 0100: loss: 1514.032
iteration 0200: loss: 1512.464
iteration 0300: loss: 1516.018
iteration 0400: loss: 1514.735
iteration 0500: loss: 1515.837
iteration 0600: loss: 1517.234
iteration 0700: loss: 1516.422
iteration 0800: loss: 1516.100
iteration 0900: loss: 1514.194
====> Epoch: 046 Train loss: 1514.6173  took : 8.55583667755127
====> Test loss: 1518.3813
iteration 0000: loss: 1513.668
iteration 0100: loss: 1517.719
iteration 0200: loss: 1513.835
iteration 0300: loss: 1515.777
iteration 0400: loss: 1516.004
iteration 0500: loss: 1515.035
iteration 0600: loss: 1513.760
iteration 0700: loss: 1517.749
iteration 0800: loss: 1513.704
iteration 0900: loss: 1513.437
====> Epoch: 047 Train loss: 1514.5880  took : 8.556404113769531
====> Test loss: 1518.5081
iteration 0000: loss: 1514.720
iteration 0100: loss: 1515.224
iteration 0200: loss: 1517.195
iteration 0300: loss: 1514.845
iteration 0400: loss: 1513.709
iteration 0500: loss: 1514.056
iteration 0600: loss: 1514.068
iteration 0700: loss: 1514.870
iteration 0800: loss: 1514.754
iteration 0900: loss: 1514.779
====> Epoch: 048 Train loss: 1514.5275  took : 8.517571449279785
====> Test loss: 1518.4691
iteration 0000: loss: 1511.893
iteration 0100: loss: 1513.400
iteration 0200: loss: 1512.149
iteration 0300: loss: 1514.002
iteration 0400: loss: 1515.136
iteration 0500: loss: 1512.203
iteration 0600: loss: 1513.368
iteration 0700: loss: 1514.908
iteration 0800: loss: 1514.768
iteration 0900: loss: 1513.715
====> Epoch: 049 Train loss: 1514.5123  took : 8.419212579727173
====> Test loss: 1518.5223
iteration 0000: loss: 1512.043
iteration 0100: loss: 1516.258
iteration 0200: loss: 1512.649
iteration 0300: loss: 1516.179
iteration 0400: loss: 1516.918
iteration 0500: loss: 1515.813
iteration 0600: loss: 1514.399
iteration 0700: loss: 1515.055
iteration 0800: loss: 1514.982
iteration 0900: loss: 1514.371
====> Epoch: 050 Train loss: 1514.4844  took : 8.540027141571045
====> Test loss: 1518.3657
====> [MM-VAE] Time: 506.472s or 00:08:26
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  4
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_4
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.347
iteration 0100: loss: 2101.059
iteration 0200: loss: 2055.562
iteration 0300: loss: 2022.719
iteration 0400: loss: 2002.874
iteration 0500: loss: 2000.806
iteration 0600: loss: 1995.901
iteration 0700: loss: 1999.146
iteration 0800: loss: 1999.349
iteration 0900: loss: 1991.256
====> Epoch: 001 Train loss: 2027.1055  took : 12.705588340759277
====> Test loss: 1996.2596
iteration 0000: loss: 1990.977
iteration 0100: loss: 2001.121
iteration 0200: loss: 1987.025
iteration 0300: loss: 1985.233
iteration 0400: loss: 1985.492
iteration 0500: loss: 1985.125
iteration 0600: loss: 1984.911
iteration 0700: loss: 1981.593
iteration 0800: loss: 1980.968
iteration 0900: loss: 1980.193
====> Epoch: 002 Train loss: 1986.8158  took : 12.519768476486206
====> Test loss: 1984.2590
iteration 0000: loss: 1985.510
iteration 0100: loss: 1982.091
iteration 0200: loss: 1989.643
iteration 0300: loss: 1979.251
iteration 0400: loss: 1979.944
iteration 0500: loss: 1976.713
iteration 0600: loss: 1978.314
iteration 0700: loss: 1979.090
iteration 0800: loss: 1977.155
iteration 0900: loss: 1976.598
====> Epoch: 003 Train loss: 1978.5322  took : 13.211490392684937
====> Test loss: 1978.2321
iteration 0000: loss: 1976.788
iteration 0100: loss: 1970.104
iteration 0200: loss: 1974.652
iteration 0300: loss: 1972.645
iteration 0400: loss: 1972.625
iteration 0500: loss: 1970.402
iteration 0600: loss: 1969.252
iteration 0700: loss: 1968.524
iteration 0800: loss: 1965.700
iteration 0900: loss: 1969.506
====> Epoch: 004 Train loss: 1972.1586  took : 12.873950242996216
====> Test loss: 1971.0404
iteration 0000: loss: 1970.057
iteration 0100: loss: 1965.677
iteration 0200: loss: 1963.845
iteration 0300: loss: 1963.236
iteration 0400: loss: 1966.584
iteration 0500: loss: 1966.768
iteration 0600: loss: 1968.111
iteration 0700: loss: 1961.816
iteration 0800: loss: 1967.101
iteration 0900: loss: 1963.765
====> Epoch: 005 Train loss: 1966.6855  took : 13.08543062210083
====> Test loss: 1966.8181
iteration 0000: loss: 1964.528
iteration 0100: loss: 1962.872
iteration 0200: loss: 1968.739
iteration 0300: loss: 1961.040
iteration 0400: loss: 1962.388
iteration 0500: loss: 1967.934
iteration 0600: loss: 1960.235
iteration 0700: loss: 1964.600
iteration 0800: loss: 1963.216
iteration 0900: loss: 1963.544
====> Epoch: 006 Train loss: 1963.3171  took : 12.110617399215698
====> Test loss: 1964.5392
iteration 0000: loss: 1961.829
iteration 0100: loss: 1961.174
iteration 0200: loss: 1962.855
iteration 0300: loss: 1963.945
iteration 0400: loss: 1961.899
iteration 0500: loss: 1959.886
iteration 0600: loss: 1957.430
iteration 0700: loss: 1960.920
iteration 0800: loss: 1958.814
iteration 0900: loss: 1956.723
====> Epoch: 007 Train loss: 1961.1593  took : 12.337942123413086
====> Test loss: 1962.1613
iteration 0000: loss: 1957.473
iteration 0100: loss: 1961.069
iteration 0200: loss: 1960.810
iteration 0300: loss: 1958.646
iteration 0400: loss: 1961.276
iteration 0500: loss: 1957.562
iteration 0600: loss: 1958.433
iteration 0700: loss: 1959.346
iteration 0800: loss: 1960.102
iteration 0900: loss: 1959.576
====> Epoch: 008 Train loss: 1959.5412  took : 13.52457857131958
====> Test loss: 1961.8337
iteration 0000: loss: 1959.689
iteration 0100: loss: 1959.083
iteration 0200: loss: 1958.291
iteration 0300: loss: 1959.850
iteration 0400: loss: 1957.517
iteration 0500: loss: 1959.584
iteration 0600: loss: 1957.668
iteration 0700: loss: 1959.207
iteration 0800: loss: 1956.672
iteration 0900: loss: 1956.981
====> Epoch: 009 Train loss: 1958.7476  took : 12.470066785812378
====> Test loss: 1960.9288
iteration 0000: loss: 1960.711
iteration 0100: loss: 1958.651
iteration 0200: loss: 1958.600
iteration 0300: loss: 1956.688
iteration 0400: loss: 1957.037
iteration 0500: loss: 1957.020
iteration 0600: loss: 1957.074
iteration 0700: loss: 1955.908
iteration 0800: loss: 1955.628
iteration 0900: loss: 1956.039
====> Epoch: 010 Train loss: 1957.1539  took : 12.477945566177368
====> Test loss: 1958.8169
iteration 0000: loss: 1959.401
iteration 0100: loss: 1954.357
iteration 0200: loss: 1958.020
iteration 0300: loss: 1954.611
iteration 0400: loss: 1955.547
iteration 0500: loss: 1954.338
iteration 0600: loss: 1954.751
iteration 0700: loss: 1954.641
iteration 0800: loss: 1957.832
iteration 0900: loss: 1956.183
====> Epoch: 011 Train loss: 1956.0582  took : 13.179745197296143
====> Test loss: 1957.7083
iteration 0000: loss: 1958.040
iteration 0100: loss: 1951.859
iteration 0200: loss: 1957.644
iteration 0300: loss: 1958.117
iteration 0400: loss: 1958.461
iteration 0500: loss: 1957.815
iteration 0600: loss: 1954.069
iteration 0700: loss: 1956.223
iteration 0800: loss: 1956.943
iteration 0900: loss: 1955.385
====> Epoch: 012 Train loss: 1955.5799  took : 12.639139652252197
====> Test loss: 1957.9127
iteration 0000: loss: 1955.807
iteration 0100: loss: 1955.116
iteration 0200: loss: 1951.791
iteration 0300: loss: 1953.359
iteration 0400: loss: 1956.189
iteration 0500: loss: 1956.206
iteration 0600: loss: 1952.734
iteration 0700: loss: 1954.446
iteration 0800: loss: 1952.881
iteration 0900: loss: 1955.171
====> Epoch: 013 Train loss: 1955.0559  took : 12.68212342262268
====> Test loss: 1957.1749
iteration 0000: loss: 1953.013
iteration 0100: loss: 1952.459
iteration 0200: loss: 1957.272
iteration 0300: loss: 1956.940
iteration 0400: loss: 1953.390
iteration 0500: loss: 1952.741
iteration 0600: loss: 1955.703
iteration 0700: loss: 1953.980
iteration 0800: loss: 1955.238
iteration 0900: loss: 1959.824
====> Epoch: 014 Train loss: 1954.6766  took : 13.166910171508789
====> Test loss: 1956.8781
iteration 0000: loss: 1954.634
iteration 0100: loss: 1951.736
iteration 0200: loss: 1955.626
iteration 0300: loss: 1956.073
iteration 0400: loss: 1957.355
iteration 0500: loss: 1952.737
iteration 0600: loss: 1956.562
iteration 0700: loss: 1954.743
iteration 0800: loss: 1954.254
iteration 0900: loss: 1956.474
====> Epoch: 015 Train loss: 1954.3544  took : 13.187782287597656
====> Test loss: 1956.4447
iteration 0000: loss: 1955.072
iteration 0100: loss: 1954.873
iteration 0200: loss: 1954.379
iteration 0300: loss: 1952.877
iteration 0400: loss: 1954.297
iteration 0500: loss: 1953.796
iteration 0600: loss: 1955.004
iteration 0700: loss: 1952.804
iteration 0800: loss: 1954.296
iteration 0900: loss: 1952.074
====> Epoch: 016 Train loss: 1954.1068  took : 12.509953022003174
====> Test loss: 1956.2986
iteration 0000: loss: 1956.418
iteration 0100: loss: 1952.423
iteration 0200: loss: 1955.275
iteration 0300: loss: 1954.337
iteration 0400: loss: 1950.896
iteration 0500: loss: 1956.334
iteration 0600: loss: 1955.659
iteration 0700: loss: 1954.735
iteration 0800: loss: 1953.724
iteration 0900: loss: 1951.806
====> Epoch: 017 Train loss: 1954.0339  took : 12.743966579437256
====> Test loss: 1956.0613
iteration 0000: loss: 1952.560
iteration 0100: loss: 1956.798
iteration 0200: loss: 1952.883
iteration 0300: loss: 1955.374
iteration 0400: loss: 1954.487
iteration 0500: loss: 1955.314
iteration 0600: loss: 1953.122
iteration 0700: loss: 1951.600
iteration 0800: loss: 1954.089
iteration 0900: loss: 1954.568
====> Epoch: 018 Train loss: 1953.7927  took : 12.76643705368042
====> Test loss: 1955.8016
iteration 0000: loss: 1954.995
iteration 0100: loss: 1951.858
iteration 0200: loss: 1951.821
iteration 0300: loss: 1954.937
iteration 0400: loss: 1954.371
iteration 0500: loss: 1952.159
iteration 0600: loss: 1952.941
iteration 0700: loss: 1954.108
iteration 0800: loss: 1951.805
iteration 0900: loss: 1956.423
====> Epoch: 019 Train loss: 1953.6100  took : 13.326164960861206
====> Test loss: 1955.7305
iteration 0000: loss: 1953.625
iteration 0100: loss: 1950.423
iteration 0200: loss: 1953.659
iteration 0300: loss: 1955.062
iteration 0400: loss: 1952.407
iteration 0500: loss: 1953.023
iteration 0600: loss: 1952.836
iteration 0700: loss: 1953.200
iteration 0800: loss: 1953.926
iteration 0900: loss: 1952.955
====> Epoch: 020 Train loss: 1953.2683  took : 12.532708883285522
====> Test loss: 1955.4439
iteration 0000: loss: 1954.130
iteration 0100: loss: 1951.497
iteration 0200: loss: 1951.806
iteration 0300: loss: 1956.900
iteration 0400: loss: 1952.634
iteration 0500: loss: 1952.441
iteration 0600: loss: 1955.993
iteration 0700: loss: 1950.441
iteration 0800: loss: 1951.704
iteration 0900: loss: 1955.871
====> Epoch: 021 Train loss: 1953.2010  took : 12.37313175201416
====> Test loss: 1955.2496
iteration 0000: loss: 1956.016
iteration 0100: loss: 1951.839
iteration 0200: loss: 1951.772
iteration 0300: loss: 1953.808
iteration 0400: loss: 1955.023
iteration 0500: loss: 1953.756
iteration 0600: loss: 1953.749
iteration 0700: loss: 1951.335
iteration 0800: loss: 1955.652
iteration 0900: loss: 1953.322
====> Epoch: 022 Train loss: 1953.2055  took : 11.772790908813477
====> Test loss: 1955.3932
iteration 0000: loss: 1953.255
iteration 0100: loss: 1953.450
iteration 0200: loss: 1953.814
iteration 0300: loss: 1953.130
iteration 0400: loss: 1950.791
iteration 0500: loss: 1952.317
iteration 0600: loss: 1952.320
iteration 0700: loss: 1952.495
iteration 0800: loss: 1951.580
iteration 0900: loss: 1953.251
====> Epoch: 023 Train loss: 1953.3104  took : 13.260530233383179
====> Test loss: 1955.2185
iteration 0000: loss: 1953.916
iteration 0100: loss: 1960.457
iteration 0200: loss: 1952.853
iteration 0300: loss: 1954.053
iteration 0400: loss: 1956.857
iteration 0500: loss: 1955.321
iteration 0600: loss: 1952.787
iteration 0700: loss: 1952.615
iteration 0800: loss: 1952.536
iteration 0900: loss: 1954.439
====> Epoch: 024 Train loss: 1953.0738  took : 13.106348514556885
====> Test loss: 1955.4328
iteration 0000: loss: 1954.321
iteration 0100: loss: 1952.426
iteration 0200: loss: 1952.666
iteration 0300: loss: 1952.535
iteration 0400: loss: 1954.262
iteration 0500: loss: 1952.870
iteration 0600: loss: 1952.935
iteration 0700: loss: 1952.835
iteration 0800: loss: 1949.583
iteration 0900: loss: 1953.290
====> Epoch: 025 Train loss: 1952.9236  took : 12.839966297149658
====> Test loss: 1955.0261
iteration 0000: loss: 1949.926
iteration 0100: loss: 1952.864
iteration 0200: loss: 1955.216
iteration 0300: loss: 1953.126
iteration 0400: loss: 1951.941
iteration 0500: loss: 1950.754
iteration 0600: loss: 1953.746
iteration 0700: loss: 1951.640
iteration 0800: loss: 1951.464
iteration 0900: loss: 1953.477
====> Epoch: 026 Train loss: 1952.8110  took : 13.051287412643433
====> Test loss: 1954.7790
iteration 0000: loss: 1952.625
iteration 0100: loss: 1951.555
iteration 0200: loss: 1952.812
iteration 0300: loss: 1952.436
iteration 0400: loss: 1954.850
iteration 0500: loss: 1953.025
iteration 0600: loss: 1950.593
iteration 0700: loss: 1953.146
iteration 0800: loss: 1952.995
iteration 0900: loss: 1953.911
====> Epoch: 027 Train loss: 1952.6474  took : 11.762312889099121
====> Test loss: 1954.6734
iteration 0000: loss: 1953.198
iteration 0100: loss: 1952.742
iteration 0200: loss: 1952.705
iteration 0300: loss: 1951.163
iteration 0400: loss: 1953.006
iteration 0500: loss: 1952.018
iteration 0600: loss: 1952.084
iteration 0700: loss: 1952.679
iteration 0800: loss: 1951.554
iteration 0900: loss: 1950.850
====> Epoch: 028 Train loss: 1952.6860  took : 12.432149887084961
====> Test loss: 1954.5216
iteration 0000: loss: 1955.170
iteration 0100: loss: 1950.648
iteration 0200: loss: 1952.218
iteration 0300: loss: 1949.910
iteration 0400: loss: 1953.635
iteration 0500: loss: 1952.363
iteration 0600: loss: 1950.756
iteration 0700: loss: 1952.572
iteration 0800: loss: 1954.817
iteration 0900: loss: 1951.334
====> Epoch: 029 Train loss: 1952.3099  took : 12.174710273742676
====> Test loss: 1955.0151
iteration 0000: loss: 1952.049
iteration 0100: loss: 1952.750
iteration 0200: loss: 1952.488
iteration 0300: loss: 1952.980
iteration 0400: loss: 1951.569
iteration 0500: loss: 1952.797
iteration 0600: loss: 1952.307
iteration 0700: loss: 1954.036
iteration 0800: loss: 1953.346
iteration 0900: loss: 1953.233
====> Epoch: 030 Train loss: 1952.2037  took : 12.510648488998413
====> Test loss: 1954.0528
iteration 0000: loss: 1952.108
iteration 0100: loss: 1955.596
iteration 0200: loss: 1951.669
iteration 0300: loss: 1950.698
iteration 0400: loss: 1950.042
iteration 0500: loss: 1954.425
iteration 0600: loss: 1954.101
iteration 0700: loss: 1951.917
iteration 0800: loss: 1953.678
iteration 0900: loss: 1951.878
====> Epoch: 031 Train loss: 1951.9873  took : 13.041579961776733
====> Test loss: 1954.6416
iteration 0000: loss: 1953.352
iteration 0100: loss: 1952.605
iteration 0200: loss: 1952.919
iteration 0300: loss: 1952.813
iteration 0400: loss: 1952.121
iteration 0500: loss: 1952.899
iteration 0600: loss: 1951.181
iteration 0700: loss: 1949.266
iteration 0800: loss: 1956.188
iteration 0900: loss: 1955.312
====> Epoch: 032 Train loss: 1952.3113  took : 13.001423358917236
====> Test loss: 1954.1300
iteration 0000: loss: 1953.426
iteration 0100: loss: 1951.223
iteration 0200: loss: 1952.813
iteration 0300: loss: 1952.616
iteration 0400: loss: 1953.134
iteration 0500: loss: 1950.427
iteration 0600: loss: 1953.347
iteration 0700: loss: 1950.254
iteration 0800: loss: 1950.856
iteration 0900: loss: 1952.920
====> Epoch: 033 Train loss: 1952.0884  took : 12.824813842773438
====> Test loss: 1953.6735
iteration 0000: loss: 1951.912
iteration 0100: loss: 1950.995
iteration 0200: loss: 1954.472
iteration 0300: loss: 1953.270
iteration 0400: loss: 1952.171
iteration 0500: loss: 1951.824
iteration 0600: loss: 1951.062
iteration 0700: loss: 1952.179
iteration 0800: loss: 1955.307
iteration 0900: loss: 1952.380
====> Epoch: 034 Train loss: 1952.0693  took : 12.86746883392334
====> Test loss: 1954.5114
iteration 0000: loss: 1953.989
iteration 0100: loss: 1953.235
iteration 0200: loss: 1957.226
iteration 0300: loss: 1956.079
iteration 0400: loss: 1951.567
iteration 0500: loss: 1952.827
iteration 0600: loss: 1953.739
iteration 0700: loss: 1952.601
iteration 0800: loss: 1952.767
iteration 0900: loss: 1953.065
====> Epoch: 035 Train loss: 1952.1886  took : 12.375411748886108
====> Test loss: 1954.1237
iteration 0000: loss: 1951.047
iteration 0100: loss: 1952.282
iteration 0200: loss: 1951.728
iteration 0300: loss: 1948.692
iteration 0400: loss: 1952.332
iteration 0500: loss: 1951.960
iteration 0600: loss: 1952.237
iteration 0700: loss: 1952.814
iteration 0800: loss: 1951.142
iteration 0900: loss: 1951.150
====> Epoch: 036 Train loss: 1951.8926  took : 12.853318691253662
====> Test loss: 1953.6395
iteration 0000: loss: 1953.275
iteration 0100: loss: 1950.623
iteration 0200: loss: 1949.395
iteration 0300: loss: 1950.290
iteration 0400: loss: 1950.903
iteration 0500: loss: 1952.518
iteration 0600: loss: 1950.917
iteration 0700: loss: 1950.441
iteration 0800: loss: 1951.978
iteration 0900: loss: 1951.205
====> Epoch: 037 Train loss: 1951.9366  took : 11.635851860046387
====> Test loss: 1953.7008
iteration 0000: loss: 1950.678
iteration 0100: loss: 1952.113
iteration 0200: loss: 1952.298
iteration 0300: loss: 1952.163
iteration 0400: loss: 1952.043
iteration 0500: loss: 1950.943
iteration 0600: loss: 1952.176
iteration 0700: loss: 1950.959
iteration 0800: loss: 1952.185
iteration 0900: loss: 1950.008
====> Epoch: 038 Train loss: 1951.8113  took : 12.488640308380127
====> Test loss: 1953.9829
iteration 0000: loss: 1951.145
iteration 0100: loss: 1952.239
iteration 0200: loss: 1952.714
iteration 0300: loss: 1955.786
iteration 0400: loss: 1951.286
iteration 0500: loss: 1951.102
iteration 0600: loss: 1950.689
iteration 0700: loss: 1952.505
iteration 0800: loss: 1952.396
iteration 0900: loss: 1953.664
====> Epoch: 039 Train loss: 1951.6815  took : 12.692750215530396
====> Test loss: 1953.4801
iteration 0000: loss: 1950.357
iteration 0100: loss: 1951.956
iteration 0200: loss: 1949.012
iteration 0300: loss: 1950.012
iteration 0400: loss: 1950.591
iteration 0500: loss: 1949.696
iteration 0600: loss: 1949.826
iteration 0700: loss: 1952.273
iteration 0800: loss: 1957.296
iteration 0900: loss: 1953.311
====> Epoch: 040 Train loss: 1951.8317  took : 11.924967288970947
====> Test loss: 1953.3287
iteration 0000: loss: 1953.530
iteration 0100: loss: 1949.652
iteration 0200: loss: 1950.265
iteration 0300: loss: 1952.836
iteration 0400: loss: 1951.709
iteration 0500: loss: 1952.498
iteration 0600: loss: 1949.189
iteration 0700: loss: 1951.639
iteration 0800: loss: 1952.055
iteration 0900: loss: 1950.638
====> Epoch: 041 Train loss: 1951.5389  took : 12.018312931060791
====> Test loss: 1953.4373
iteration 0000: loss: 1953.644
iteration 0100: loss: 1950.648
iteration 0200: loss: 1953.231
iteration 0300: loss: 1953.551
iteration 0400: loss: 1949.879
iteration 0500: loss: 1950.154
iteration 0600: loss: 1950.035
iteration 0700: loss: 1951.311
iteration 0800: loss: 1951.836
iteration 0900: loss: 1949.984
====> Epoch: 042 Train loss: 1951.7053  took : 13.233733415603638
====> Test loss: 1953.4784
iteration 0000: loss: 1950.197
iteration 0100: loss: 1949.574
iteration 0200: loss: 1950.609
iteration 0300: loss: 1949.612
iteration 0400: loss: 1952.943
iteration 0500: loss: 1953.066
iteration 0600: loss: 1953.498
iteration 0700: loss: 1951.764
iteration 0800: loss: 1952.864
iteration 0900: loss: 1954.083
====> Epoch: 043 Train loss: 1951.6309  took : 12.426526069641113
====> Test loss: 1953.2702
iteration 0000: loss: 1953.963
iteration 0100: loss: 1950.692
iteration 0200: loss: 1950.674
iteration 0300: loss: 1951.954
iteration 0400: loss: 1949.993
iteration 0500: loss: 1950.850
iteration 0600: loss: 1952.195
iteration 0700: loss: 1950.941
iteration 0800: loss: 1951.660
iteration 0900: loss: 1949.312
====> Epoch: 044 Train loss: 1951.4302  took : 12.72992730140686
====> Test loss: 1953.0323
iteration 0000: loss: 1952.658
iteration 0100: loss: 1951.687
iteration 0200: loss: 1952.154
iteration 0300: loss: 1949.903
iteration 0400: loss: 1951.442
iteration 0500: loss: 1951.758
iteration 0600: loss: 1952.133
iteration 0700: loss: 1950.878
iteration 0800: loss: 1954.718
iteration 0900: loss: 1953.381
====> Epoch: 045 Train loss: 1951.5597  took : 12.862076044082642
====> Test loss: 1954.2659
iteration 0000: loss: 1953.142
iteration 0100: loss: 1951.199
iteration 0200: loss: 1953.638
iteration 0300: loss: 1951.927
iteration 0400: loss: 1949.765
iteration 0500: loss: 1950.541
iteration 0600: loss: 1951.355
iteration 0700: loss: 1953.439
iteration 0800: loss: 1951.697
iteration 0900: loss: 1949.029
====> Epoch: 046 Train loss: 1951.4471  took : 12.351425170898438
====> Test loss: 1952.9583
iteration 0000: loss: 1952.270
iteration 0100: loss: 1949.595
iteration 0200: loss: 1950.916
iteration 0300: loss: 1950.048
iteration 0400: loss: 1952.182
iteration 0500: loss: 1951.421
iteration 0600: loss: 1953.143
iteration 0700: loss: 1954.658
iteration 0800: loss: 1950.467
iteration 0900: loss: 1951.505
====> Epoch: 047 Train loss: 1951.3225  took : 12.162278175354004
====> Test loss: 1953.2545
iteration 0000: loss: 1952.948
iteration 0100: loss: 1949.536
iteration 0200: loss: 1951.301
iteration 0300: loss: 1950.380
iteration 0400: loss: 1951.827
iteration 0500: loss: 1951.601
iteration 0600: loss: 1951.913
iteration 0700: loss: 1951.313
iteration 0800: loss: 1950.913
iteration 0900: loss: 1949.951
====> Epoch: 048 Train loss: 1951.4458  took : 12.528526067733765
====> Test loss: 1953.2494
iteration 0000: loss: 1950.818
iteration 0100: loss: 1951.749
iteration 0200: loss: 1949.997
iteration 0300: loss: 1952.465
iteration 0400: loss: 1950.837
iteration 0500: loss: 1951.179
iteration 0600: loss: 1952.464
iteration 0700: loss: 1950.653
iteration 0800: loss: 1950.101
iteration 0900: loss: 1953.944
====> Epoch: 049 Train loss: 1951.2172  took : 12.761398792266846
====> Test loss: 1953.0264
iteration 0000: loss: 1952.711
iteration 0100: loss: 1950.986
iteration 0200: loss: 1949.862
iteration 0300: loss: 1951.202
iteration 0400: loss: 1950.755
iteration 0500: loss: 1949.720
iteration 0600: loss: 1951.230
iteration 0700: loss: 1953.406
iteration 0800: loss: 1949.177
iteration 0900: loss: 1950.611
====> Epoch: 050 Train loss: 1951.1382  took : 12.713849544525146
====> Test loss: 1952.8116
====> [MM-VAE] Time: 704.128s or 00:11:44
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  4
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_4
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5216.667
iteration 0100: loss: 4104.722
iteration 0200: loss: 4080.750
iteration 0300: loss: 4045.465
iteration 0400: loss: 4028.003
iteration 0500: loss: 4029.985
iteration 0600: loss: 4014.964
iteration 0700: loss: 4027.957
iteration 0800: loss: 4013.837
iteration 0900: loss: 4017.793
iteration 1000: loss: 4006.717
iteration 1100: loss: 4007.244
iteration 1200: loss: 3995.040
iteration 1300: loss: 3993.943
iteration 1400: loss: 3992.720
iteration 1500: loss: 3990.607
iteration 1600: loss: 3985.277
iteration 1700: loss: 3993.952
iteration 1800: loss: 3989.927
====> Epoch: 001 Train loss: 4026.6520  took : 53.011375427246094
====> Test loss: 3995.5536
iteration 0000: loss: 3997.380
iteration 0100: loss: 4001.738
iteration 0200: loss: 4003.945
iteration 0300: loss: 3998.706
iteration 0400: loss: 3984.602
iteration 0500: loss: 3992.778
iteration 0600: loss: 3988.405
iteration 0700: loss: 3984.430
iteration 0800: loss: 3977.863
iteration 0900: loss: 3986.979
iteration 1000: loss: 3988.280
iteration 1100: loss: 3977.799
iteration 1200: loss: 3991.505
iteration 1300: loss: 3971.651
iteration 1400: loss: 3980.896
iteration 1500: loss: 3973.037
iteration 1600: loss: 3979.094
iteration 1700: loss: 3977.547
iteration 1800: loss: 3975.006
====> Epoch: 002 Train loss: 3983.8096  took : 53.33316683769226
====> Test loss: 3982.1418
iteration 0000: loss: 3980.336
iteration 0100: loss: 3971.953
iteration 0200: loss: 3979.379
iteration 0300: loss: 3971.539
iteration 0400: loss: 3975.061
iteration 0500: loss: 3971.565
iteration 0600: loss: 3976.117
iteration 0700: loss: 3968.408
iteration 0800: loss: 3977.148
iteration 0900: loss: 3983.626
iteration 1000: loss: 3977.107
iteration 1100: loss: 3967.021
iteration 1200: loss: 3970.792
iteration 1300: loss: 3977.830
iteration 1400: loss: 3966.917
iteration 1500: loss: 3969.871
iteration 1600: loss: 3968.118
iteration 1700: loss: 3982.042
iteration 1800: loss: 3971.146
====> Epoch: 003 Train loss: 3974.8454  took : 52.94185972213745
====> Test loss: 3977.3051
iteration 0000: loss: 3970.953
iteration 0100: loss: 3969.261
iteration 0200: loss: 3971.758
iteration 0300: loss: 3971.607
iteration 0400: loss: 3972.823
iteration 0500: loss: 3970.780
iteration 0600: loss: 3976.928
iteration 0700: loss: 3969.471
iteration 0800: loss: 3966.349
iteration 0900: loss: 3977.253
iteration 1000: loss: 3969.862
iteration 1100: loss: 3974.458
iteration 1200: loss: 3966.175
iteration 1300: loss: 3964.622
iteration 1400: loss: 3972.641
iteration 1500: loss: 3972.026
iteration 1600: loss: 3984.833
iteration 1700: loss: 3973.446
iteration 1800: loss: 3971.407
====> Epoch: 004 Train loss: 3970.9303  took : 53.15173411369324
====> Test loss: 3974.9444
iteration 0000: loss: 3970.055
iteration 0100: loss: 3970.326
iteration 0200: loss: 3963.742
iteration 0300: loss: 3972.183
iteration 0400: loss: 3974.917
iteration 0500: loss: 3972.513
iteration 0600: loss: 3969.531
iteration 0700: loss: 3961.523
iteration 0800: loss: 3962.517
iteration 0900: loss: 3964.349
iteration 1000: loss: 3971.475
iteration 1100: loss: 3974.809
iteration 1200: loss: 3971.080
iteration 1300: loss: 3968.636
iteration 1400: loss: 3975.479
iteration 1500: loss: 3960.267
iteration 1600: loss: 3965.565
iteration 1700: loss: 3962.887
iteration 1800: loss: 3959.733
====> Epoch: 005 Train loss: 3969.6347  took : 53.121723890304565
====> Test loss: 3973.8797
iteration 0000: loss: 3967.471
iteration 0100: loss: 3971.402
iteration 0200: loss: 3974.336
iteration 0300: loss: 3976.870
iteration 0400: loss: 3968.736
iteration 0500: loss: 3963.794
iteration 0600: loss: 3967.958
iteration 0700: loss: 3982.355
iteration 0800: loss: 3973.626
iteration 0900: loss: 3963.977
iteration 1000: loss: 3971.090
iteration 1100: loss: 3973.247
iteration 1200: loss: 3969.421
iteration 1300: loss: 3966.478
iteration 1400: loss: 3968.469
iteration 1500: loss: 3970.337
iteration 1600: loss: 3967.663
iteration 1700: loss: 3965.865
iteration 1800: loss: 3972.249
====> Epoch: 006 Train loss: 3967.7515  took : 52.998183250427246
====> Test loss: 3972.4932
iteration 0000: loss: 3964.223
iteration 0100: loss: 3970.877
iteration 0200: loss: 3965.069
iteration 0300: loss: 3966.307
iteration 0400: loss: 3962.735
iteration 0500: loss: 3959.672
iteration 0600: loss: 3967.268
iteration 0700: loss: 3967.567
iteration 0800: loss: 3965.824
iteration 0900: loss: 3963.520
iteration 1000: loss: 3967.780
iteration 1100: loss: 3971.405
iteration 1200: loss: 3967.230
iteration 1300: loss: 3969.082
iteration 1400: loss: 3960.458
iteration 1500: loss: 3959.776
iteration 1600: loss: 3972.696
iteration 1700: loss: 3970.260
iteration 1800: loss: 3974.056
====> Epoch: 007 Train loss: 3966.3890  took : 53.100454330444336
====> Test loss: 3971.8075
iteration 0000: loss: 3970.030
iteration 0100: loss: 3967.219
iteration 0200: loss: 3974.646
iteration 0300: loss: 3963.040
iteration 0400: loss: 3964.416
iteration 0500: loss: 3976.846
iteration 0600: loss: 3966.901
iteration 0700: loss: 3965.566
iteration 0800: loss: 3968.153
iteration 0900: loss: 3964.665
iteration 1000: loss: 3969.685
iteration 1100: loss: 3961.589
iteration 1200: loss: 3956.927
iteration 1300: loss: 3961.622
iteration 1400: loss: 3968.218
iteration 1500: loss: 3959.723
iteration 1600: loss: 3973.557
iteration 1700: loss: 3959.854
iteration 1800: loss: 3974.468
====> Epoch: 008 Train loss: 3965.2719  took : 52.872936725616455
====> Test loss: 3970.4727
iteration 0000: loss: 3963.881
iteration 0100: loss: 3968.626
iteration 0200: loss: 3963.723
iteration 0300: loss: 3960.931
iteration 0400: loss: 3961.589
iteration 0500: loss: 3965.445
iteration 0600: loss: 3953.568
iteration 0700: loss: 3956.973
iteration 0800: loss: 3961.278
iteration 0900: loss: 3966.653
iteration 1000: loss: 3957.677
iteration 1100: loss: 3961.903
iteration 1200: loss: 3968.149
iteration 1300: loss: 3973.332
iteration 1400: loss: 3964.670
iteration 1500: loss: 3965.387
iteration 1600: loss: 3969.469
iteration 1700: loss: 3977.158
iteration 1800: loss: 3963.040
====> Epoch: 009 Train loss: 3964.8119  took : 52.998225927352905
====> Test loss: 3971.5616
iteration 0000: loss: 3976.920
iteration 0100: loss: 3962.893
iteration 0200: loss: 3956.466
iteration 0300: loss: 3964.006
iteration 0400: loss: 3969.750
iteration 0500: loss: 3963.038
iteration 0600: loss: 3973.622
iteration 0700: loss: 3962.211
iteration 0800: loss: 3962.197
iteration 0900: loss: 3965.737
iteration 1000: loss: 3955.327
iteration 1100: loss: 3955.159
iteration 1200: loss: 3958.414
iteration 1300: loss: 3956.586
iteration 1400: loss: 3970.726
iteration 1500: loss: 3971.158
iteration 1600: loss: 3963.297
iteration 1700: loss: 3961.486
iteration 1800: loss: 3960.747
====> Epoch: 010 Train loss: 3964.8940  took : 52.879854679107666
====> Test loss: 3970.5916
iteration 0000: loss: 3951.024
iteration 0100: loss: 3972.675
iteration 0200: loss: 3973.310
iteration 0300: loss: 3973.586
iteration 0400: loss: 3966.492
iteration 0500: loss: 3959.381
iteration 0600: loss: 3961.460
iteration 0700: loss: 3964.653
iteration 0800: loss: 3963.806
iteration 0900: loss: 3976.148
iteration 1000: loss: 3958.616
iteration 1100: loss: 3959.326
iteration 1200: loss: 3960.168
iteration 1300: loss: 3962.812
iteration 1400: loss: 3959.953
iteration 1500: loss: 3958.996
iteration 1600: loss: 3965.286
iteration 1700: loss: 3961.435
iteration 1800: loss: 3963.739
====> Epoch: 011 Train loss: 3963.6733  took : 52.79145860671997
====> Test loss: 3969.5972
iteration 0000: loss: 3958.100
iteration 0100: loss: 3960.685
iteration 0200: loss: 3950.909
iteration 0300: loss: 3959.076
iteration 0400: loss: 3965.309
iteration 0500: loss: 3963.767
iteration 0600: loss: 3958.564
iteration 0700: loss: 3961.645
iteration 0800: loss: 3964.091
iteration 0900: loss: 3967.344
iteration 1000: loss: 3964.284
iteration 1100: loss: 3969.144
iteration 1200: loss: 3966.232
iteration 1300: loss: 3961.529
iteration 1400: loss: 3965.556
iteration 1500: loss: 3956.922
iteration 1600: loss: 3967.314
iteration 1700: loss: 3960.881
iteration 1800: loss: 3960.600
====> Epoch: 012 Train loss: 3963.3926  took : 53.0136821269989
====> Test loss: 3969.4678
iteration 0000: loss: 3965.355
iteration 0100: loss: 3970.440
iteration 0200: loss: 3949.395
iteration 0300: loss: 3951.119
iteration 0400: loss: 3959.913
iteration 0500: loss: 3970.329
iteration 0600: loss: 3961.617
iteration 0700: loss: 3967.642
iteration 0800: loss: 3954.169
iteration 0900: loss: 3964.205
iteration 1000: loss: 3957.302
iteration 1100: loss: 3960.399
iteration 1200: loss: 3958.630
iteration 1300: loss: 3963.962
iteration 1400: loss: 3962.287
iteration 1500: loss: 3960.442
iteration 1600: loss: 3955.732
iteration 1700: loss: 3957.563
iteration 1800: loss: 3960.967
====> Epoch: 013 Train loss: 3962.8545  took : 52.85065698623657
====> Test loss: 3968.9983
iteration 0000: loss: 3962.051
iteration 0100: loss: 3965.609
iteration 0200: loss: 3958.376
iteration 0300: loss: 3956.070
iteration 0400: loss: 3955.473
iteration 0500: loss: 3965.101
iteration 0600: loss: 3964.676
iteration 0700: loss: 3961.464
iteration 0800: loss: 3965.927
iteration 0900: loss: 3960.439
iteration 1000: loss: 3968.298
iteration 1100: loss: 3963.541
iteration 1200: loss: 3959.670
iteration 1300: loss: 3962.896
iteration 1400: loss: 3963.075
iteration 1500: loss: 3964.669
iteration 1600: loss: 3957.375
iteration 1700: loss: 3958.857
iteration 1800: loss: 3958.912
====> Epoch: 014 Train loss: 3962.4199  took : 52.99909067153931
====> Test loss: 3969.0434
iteration 0000: loss: 3971.250
iteration 0100: loss: 3960.497
iteration 0200: loss: 3964.354
iteration 0300: loss: 3971.090
iteration 0400: loss: 3958.822
iteration 0500: loss: 3964.066
iteration 0600: loss: 3959.798
iteration 0700: loss: 3961.311
iteration 0800: loss: 3960.202
iteration 0900: loss: 3967.305
iteration 1000: loss: 3962.748
iteration 1100: loss: 3966.308
iteration 1200: loss: 3963.114
iteration 1300: loss: 3966.271
iteration 1400: loss: 3961.958
iteration 1500: loss: 3961.286
iteration 1600: loss: 3957.491
iteration 1700: loss: 3970.803
iteration 1800: loss: 3962.180
====> Epoch: 015 Train loss: 3962.3465  took : 53.15173530578613
====> Test loss: 3968.7214
iteration 0000: loss: 3964.834
iteration 0100: loss: 3969.414
iteration 0200: loss: 3957.052
iteration 0300: loss: 3963.825
iteration 0400: loss: 3969.271
iteration 0500: loss: 3960.204
iteration 0600: loss: 3967.590
iteration 0700: loss: 3951.424
iteration 0800: loss: 3959.126
iteration 0900: loss: 3973.160
iteration 1000: loss: 3962.456
iteration 1100: loss: 3965.127
iteration 1200: loss: 3959.106
iteration 1300: loss: 3961.205
iteration 1400: loss: 3963.436
iteration 1500: loss: 3958.522
iteration 1600: loss: 3958.253
iteration 1700: loss: 3968.674
iteration 1800: loss: 3962.443
====> Epoch: 016 Train loss: 3961.8371  took : 52.81053876876831
====> Test loss: 3968.6177
iteration 0000: loss: 3967.634
iteration 0100: loss: 3957.880
iteration 0200: loss: 3965.858
iteration 0300: loss: 3965.322
iteration 0400: loss: 3960.822
iteration 0500: loss: 3958.539
iteration 0600: loss: 3964.681
iteration 0700: loss: 3969.397
iteration 0800: loss: 3964.915
iteration 0900: loss: 3955.579
iteration 1000: loss: 3960.029
iteration 1100: loss: 3962.529
iteration 1200: loss: 3970.089
iteration 1300: loss: 3967.436
iteration 1400: loss: 3961.383
iteration 1500: loss: 3956.284
iteration 1600: loss: 3949.645
iteration 1700: loss: 3960.132
iteration 1800: loss: 3952.654
====> Epoch: 017 Train loss: 3961.1295  took : 53.04871869087219
====> Test loss: 3967.8703
iteration 0000: loss: 3965.180
iteration 0100: loss: 3958.611
iteration 0200: loss: 3956.446
iteration 0300: loss: 3963.994
iteration 0400: loss: 3952.903
iteration 0500: loss: 3961.449
iteration 0600: loss: 3958.630
iteration 0700: loss: 3960.162
iteration 0800: loss: 3953.414
iteration 0900: loss: 3957.293
iteration 1000: loss: 3963.459
iteration 1100: loss: 3952.414
iteration 1200: loss: 3964.559
iteration 1300: loss: 3950.294
iteration 1400: loss: 3957.585
iteration 1500: loss: 3948.260
iteration 1600: loss: 3961.348
iteration 1700: loss: 3967.080
iteration 1800: loss: 3964.502
====> Epoch: 018 Train loss: 3960.8021  took : 52.89000487327576
====> Test loss: 3966.8558
iteration 0000: loss: 3963.847
iteration 0100: loss: 3971.875
iteration 0200: loss: 3956.573
iteration 0300: loss: 3956.985
iteration 0400: loss: 3959.479
iteration 0500: loss: 3961.385
iteration 0600: loss: 3964.668
iteration 0700: loss: 3953.604
iteration 0800: loss: 3955.667
iteration 0900: loss: 3951.899
iteration 1000: loss: 3955.927
iteration 1100: loss: 3955.683
iteration 1200: loss: 3955.683
iteration 1300: loss: 3967.125
iteration 1400: loss: 3950.782
iteration 1500: loss: 3964.047
iteration 1600: loss: 3958.017
iteration 1700: loss: 3964.403
iteration 1800: loss: 3961.625
====> Epoch: 019 Train loss: 3960.6409  took : 52.83723187446594
====> Test loss: 3967.3219
iteration 0000: loss: 3964.818
iteration 0100: loss: 3958.244
iteration 0200: loss: 3966.534
iteration 0300: loss: 3961.742
iteration 0400: loss: 3955.027
iteration 0500: loss: 3959.755
iteration 0600: loss: 3967.155
iteration 0700: loss: 3957.660
iteration 0800: loss: 3963.237
iteration 0900: loss: 3956.499
iteration 1000: loss: 3950.274
iteration 1100: loss: 3968.635
iteration 1200: loss: 3954.508
iteration 1300: loss: 3962.270
iteration 1400: loss: 3958.109
iteration 1500: loss: 3961.157
iteration 1600: loss: 3953.761
iteration 1700: loss: 3961.622
iteration 1800: loss: 3969.491
====> Epoch: 020 Train loss: 3960.4288  took : 53.03459334373474
====> Test loss: 3967.5744
iteration 0000: loss: 3958.631
iteration 0100: loss: 3968.401
iteration 0200: loss: 3964.156
iteration 0300: loss: 3965.775
iteration 0400: loss: 3962.842
iteration 0500: loss: 3970.117
iteration 0600: loss: 3964.216
iteration 0700: loss: 3955.690
iteration 0800: loss: 3953.230
iteration 0900: loss: 3958.423
iteration 1000: loss: 3960.387
iteration 1100: loss: 3962.244
iteration 1200: loss: 3968.231
iteration 1300: loss: 3956.399
iteration 1400: loss: 3953.334
iteration 1500: loss: 3955.097
iteration 1600: loss: 3955.902
iteration 1700: loss: 3963.261
iteration 1800: loss: 3962.898
====> Epoch: 021 Train loss: 3960.3567  took : 52.89552330970764
====> Test loss: 3968.5017
iteration 0000: loss: 3960.101
iteration 0100: loss: 3954.428
iteration 0200: loss: 3967.608
iteration 0300: loss: 3963.011
iteration 0400: loss: 3966.834
iteration 0500: loss: 3962.671
iteration 0600: loss: 3966.489
iteration 0700: loss: 3958.375
iteration 0800: loss: 3964.065
iteration 0900: loss: 3958.062
iteration 1000: loss: 3954.678
iteration 1100: loss: 3963.066
iteration 1200: loss: 3952.867
iteration 1300: loss: 3964.061
iteration 1400: loss: 3956.957
iteration 1500: loss: 3955.729
iteration 1600: loss: 3960.991
iteration 1700: loss: 3960.806
iteration 1800: loss: 3956.152
====> Epoch: 022 Train loss: 3960.9460  took : 52.94342279434204
====> Test loss: 3968.0175
iteration 0000: loss: 3968.098
iteration 0100: loss: 3970.466
iteration 0200: loss: 3960.784
iteration 0300: loss: 3964.508
iteration 0400: loss: 3954.550
iteration 0500: loss: 3958.064
iteration 0600: loss: 3959.155
iteration 0700: loss: 3959.005
iteration 0800: loss: 3962.293
iteration 0900: loss: 3959.845
iteration 1000: loss: 3950.784
iteration 1100: loss: 3950.604
iteration 1200: loss: 3966.616
iteration 1300: loss: 3969.434
iteration 1400: loss: 3958.219
iteration 1500: loss: 3959.395
iteration 1600: loss: 3962.961
iteration 1700: loss: 3961.304
iteration 1800: loss: 3959.150
====> Epoch: 023 Train loss: 3960.5435  took : 53.128891706466675
====> Test loss: 3968.0703
iteration 0000: loss: 3961.587
iteration 0100: loss: 3964.800
iteration 0200: loss: 3956.128
iteration 0300: loss: 3962.569
iteration 0400: loss: 3953.923
iteration 0500: loss: 3959.301
iteration 0600: loss: 3960.072
iteration 0700: loss: 3964.425
iteration 0800: loss: 3960.194
iteration 0900: loss: 3964.704
iteration 1000: loss: 3956.436
iteration 1100: loss: 3957.956
iteration 1200: loss: 3957.797
iteration 1300: loss: 3962.233
iteration 1400: loss: 3955.325
iteration 1500: loss: 3960.582
iteration 1600: loss: 3960.338
iteration 1700: loss: 3950.093
iteration 1800: loss: 3956.167
====> Epoch: 024 Train loss: 3960.1583  took : 52.819544315338135
====> Test loss: 3966.8030
iteration 0000: loss: 3963.048
iteration 0100: loss: 3957.921
iteration 0200: loss: 3963.623
iteration 0300: loss: 3963.309
iteration 0400: loss: 3962.460
iteration 0500: loss: 3960.114
iteration 0600: loss: 3961.626
iteration 0700: loss: 3964.482
iteration 0800: loss: 3956.803
iteration 0900: loss: 3971.996
iteration 1000: loss: 3961.949
iteration 1100: loss: 3952.816
iteration 1200: loss: 3959.463
iteration 1300: loss: 3959.859
iteration 1400: loss: 3957.693
iteration 1500: loss: 3954.983
iteration 1600: loss: 3960.785
iteration 1700: loss: 3961.453
iteration 1800: loss: 3954.647
====> Epoch: 025 Train loss: 3959.9930  took : 52.928605794906616
====> Test loss: 3966.3659
iteration 0000: loss: 3965.281
iteration 0100: loss: 3963.685
iteration 0200: loss: 3955.496
iteration 0300: loss: 3961.526
iteration 0400: loss: 3959.631
iteration 0500: loss: 3955.187
iteration 0600: loss: 3966.516
iteration 0700: loss: 3954.569
iteration 0800: loss: 3972.071
iteration 0900: loss: 3967.367
iteration 1000: loss: 3954.511
iteration 1100: loss: 3966.297
iteration 1200: loss: 3949.836
iteration 1300: loss: 3961.328
iteration 1400: loss: 3957.446
iteration 1500: loss: 3964.237
iteration 1600: loss: 3959.898
iteration 1700: loss: 3953.583
iteration 1800: loss: 3960.331
====> Epoch: 026 Train loss: 3960.3256  took : 53.02125573158264
====> Test loss: 3966.7219
iteration 0000: loss: 3957.136
iteration 0100: loss: 3955.700
iteration 0200: loss: 3963.563
iteration 0300: loss: 3960.754
iteration 0400: loss: 3955.631
iteration 0500: loss: 3957.367
iteration 0600: loss: 3961.215
iteration 0700: loss: 3970.631
iteration 0800: loss: 3958.378
iteration 0900: loss: 3955.845
iteration 1000: loss: 3955.999
iteration 1100: loss: 3963.727
iteration 1200: loss: 3966.703
iteration 1300: loss: 3954.417
iteration 1400: loss: 3959.924
iteration 1500: loss: 3958.032
iteration 1600: loss: 3953.460
iteration 1700: loss: 3959.093
iteration 1800: loss: 3964.528
====> Epoch: 027 Train loss: 3959.6040  took : 52.66275691986084
====> Test loss: 3967.6390
iteration 0000: loss: 3962.789
iteration 0100: loss: 3954.308
iteration 0200: loss: 3960.912
iteration 0300: loss: 3957.699
iteration 0400: loss: 3956.004
iteration 0500: loss: 3960.998
iteration 0600: loss: 3955.935
iteration 0700: loss: 3961.545
iteration 0800: loss: 3959.936
iteration 0900: loss: 3959.346
iteration 1000: loss: 3958.562
iteration 1100: loss: 3957.292
iteration 1200: loss: 3955.674
iteration 1300: loss: 3962.996
iteration 1400: loss: 3968.673
iteration 1500: loss: 3961.222
iteration 1600: loss: 3973.322
iteration 1700: loss: 3959.891
iteration 1800: loss: 3958.536
====> Epoch: 028 Train loss: 3959.8494  took : 52.95133852958679
====> Test loss: 3966.9310
iteration 0000: loss: 3956.392
iteration 0100: loss: 3963.642
iteration 0200: loss: 3953.813
iteration 0300: loss: 3960.985
iteration 0400: loss: 3956.351
iteration 0500: loss: 3958.387
iteration 0600: loss: 3968.816
iteration 0700: loss: 3963.433
iteration 0800: loss: 3956.792
iteration 0900: loss: 3958.689
iteration 1000: loss: 3959.850
iteration 1100: loss: 3957.633
iteration 1200: loss: 3961.404
iteration 1300: loss: 3965.792
iteration 1400: loss: 3957.795
iteration 1500: loss: 3961.082
iteration 1600: loss: 3959.057
iteration 1700: loss: 3955.775
iteration 1800: loss: 3950.553
====> Epoch: 029 Train loss: 3959.3444  took : 53.047990560531616
====> Test loss: 3966.6688
iteration 0000: loss: 3954.351
iteration 0100: loss: 3961.830
iteration 0200: loss: 3949.855
iteration 0300: loss: 3954.368
iteration 0400: loss: 3964.004
iteration 0500: loss: 3958.564
iteration 0600: loss: 3959.659
iteration 0700: loss: 3954.491
iteration 0800: loss: 3961.822
iteration 0900: loss: 3959.582
iteration 1000: loss: 3961.271
iteration 1100: loss: 3955.259
iteration 1200: loss: 3954.567
iteration 1300: loss: 3977.400
iteration 1400: loss: 3963.773
iteration 1500: loss: 3964.312
iteration 1600: loss: 3957.593
iteration 1700: loss: 3964.184
iteration 1800: loss: 3957.885
====> Epoch: 030 Train loss: 3959.0819  took : 52.956294298172
====> Test loss: 3966.9081
iteration 0000: loss: 3954.654
iteration 0100: loss: 3958.766
iteration 0200: loss: 3960.361
iteration 0300: loss: 3956.815
iteration 0400: loss: 3959.731
iteration 0500: loss: 3967.531
iteration 0600: loss: 3958.320
iteration 0700: loss: 3960.835
iteration 0800: loss: 3959.142
iteration 0900: loss: 3958.357
iteration 1000: loss: 3955.241
iteration 1100: loss: 3953.331
iteration 1200: loss: 3970.010
iteration 1300: loss: 3966.871
iteration 1400: loss: 3961.733
iteration 1500: loss: 3963.531
iteration 1600: loss: 3958.457
iteration 1700: loss: 3958.011
iteration 1800: loss: 3955.491
====> Epoch: 031 Train loss: 3958.9152  took : 52.989989042282104
====> Test loss: 3966.4695
iteration 0000: loss: 3958.638
iteration 0100: loss: 3971.491
iteration 0200: loss: 3954.307
iteration 0300: loss: 3966.735
iteration 0400: loss: 3964.161
iteration 0500: loss: 3965.767
iteration 0600: loss: 3945.708
iteration 0700: loss: 3957.681
iteration 0800: loss: 3961.134
iteration 0900: loss: 3959.365
iteration 1000: loss: 3955.599
iteration 1100: loss: 3965.205
iteration 1200: loss: 3958.225
iteration 1300: loss: 3954.038
iteration 1400: loss: 3952.582
iteration 1500: loss: 3960.570
iteration 1600: loss: 3955.946
iteration 1700: loss: 3960.808
iteration 1800: loss: 3960.242
====> Epoch: 032 Train loss: 3958.7499  took : 52.84066867828369
====> Test loss: 3967.1102
iteration 0000: loss: 3963.600
iteration 0100: loss: 3960.140
iteration 0200: loss: 3960.004
iteration 0300: loss: 3954.601
iteration 0400: loss: 3954.700
iteration 0500: loss: 3963.079
iteration 0600: loss: 3965.040
iteration 0700: loss: 3954.390
iteration 0800: loss: 3959.724
iteration 0900: loss: 3952.577
iteration 1000: loss: 3956.486
iteration 1100: loss: 3954.228
iteration 1200: loss: 3961.331
iteration 1300: loss: 3951.896
iteration 1400: loss: 3950.687
iteration 1500: loss: 3957.208
iteration 1600: loss: 3959.059
iteration 1700: loss: 3958.632
iteration 1800: loss: 3964.911
====> Epoch: 033 Train loss: 3958.9620  took : 53.068406105041504
====> Test loss: 3966.8168
iteration 0000: loss: 3967.507
iteration 0100: loss: 3956.629
iteration 0200: loss: 3958.437
iteration 0300: loss: 3959.566
iteration 0400: loss: 3971.735
iteration 0500: loss: 3954.817
iteration 0600: loss: 3950.819
iteration 0700: loss: 3965.323
iteration 0800: loss: 3957.578
iteration 0900: loss: 3956.169
iteration 1000: loss: 3953.010
iteration 1100: loss: 3959.260
iteration 1200: loss: 3964.626
iteration 1300: loss: 3967.302
iteration 1400: loss: 3956.678
iteration 1500: loss: 3958.980
iteration 1600: loss: 3962.446
iteration 1700: loss: 3960.006
iteration 1800: loss: 3955.217
====> Epoch: 034 Train loss: 3958.6890  took : 52.850574254989624
====> Test loss: 3966.5418
iteration 0000: loss: 3958.922
iteration 0100: loss: 3962.671
iteration 0200: loss: 3954.609
iteration 0300: loss: 3967.812
iteration 0400: loss: 3956.312
iteration 0500: loss: 3963.313
iteration 0600: loss: 3961.361
iteration 0700: loss: 3968.441
iteration 0800: loss: 3954.107
iteration 0900: loss: 3961.011
iteration 1000: loss: 3956.717
iteration 1100: loss: 3956.079
iteration 1200: loss: 3950.944
iteration 1300: loss: 3975.057
iteration 1400: loss: 3953.589
iteration 1500: loss: 3957.394
iteration 1600: loss: 3961.236
iteration 1700: loss: 3958.166
iteration 1800: loss: 3960.712
====> Epoch: 035 Train loss: 3958.4931  took : 52.95841717720032
====> Test loss: 3965.5382
iteration 0000: loss: 3961.500
iteration 0100: loss: 3957.222
iteration 0200: loss: 3957.991
iteration 0300: loss: 3960.305
iteration 0400: loss: 3965.350
iteration 0500: loss: 3953.406
iteration 0600: loss: 3958.792
iteration 0700: loss: 3957.398
iteration 0800: loss: 3955.161
iteration 0900: loss: 3953.583
iteration 1000: loss: 3968.011
iteration 1100: loss: 3963.923
iteration 1200: loss: 3964.321
iteration 1300: loss: 3960.826
iteration 1400: loss: 3942.080
iteration 1500: loss: 3961.700
iteration 1600: loss: 3966.037
iteration 1700: loss: 3955.171
iteration 1800: loss: 3960.327
====> Epoch: 036 Train loss: 3958.5226  took : 53.03201198577881
====> Test loss: 3965.4911
iteration 0000: loss: 3954.536
iteration 0100: loss: 3960.810
iteration 0200: loss: 3951.889
iteration 0300: loss: 3961.448
iteration 0400: loss: 3950.850
iteration 0500: loss: 3962.861
iteration 0600: loss: 3959.086
iteration 0700: loss: 3964.039
iteration 0800: loss: 3964.354
iteration 0900: loss: 3955.765
iteration 1000: loss: 3959.384
iteration 1100: loss: 3961.362
iteration 1200: loss: 3960.968
iteration 1300: loss: 3959.723
iteration 1400: loss: 3961.812
iteration 1500: loss: 3951.912
iteration 1600: loss: 3966.620
iteration 1700: loss: 3963.601
iteration 1800: loss: 3958.200
====> Epoch: 037 Train loss: 3958.2510  took : 52.887173891067505
====> Test loss: 3967.5471
iteration 0000: loss: 3963.776
iteration 0100: loss: 3956.013
iteration 0200: loss: 3952.206
iteration 0300: loss: 3972.640
iteration 0400: loss: 3956.413
iteration 0500: loss: 3962.715
iteration 0600: loss: 3959.151
iteration 0700: loss: 3956.195
iteration 0800: loss: 3966.434
iteration 0900: loss: 3957.230
iteration 1000: loss: 3963.475
iteration 1100: loss: 3957.753
iteration 1200: loss: 3954.549
iteration 1300: loss: 3955.530
iteration 1400: loss: 3953.863
iteration 1500: loss: 3959.896
iteration 1600: loss: 3959.563
iteration 1700: loss: 3957.911
iteration 1800: loss: 3966.382
====> Epoch: 038 Train loss: 3958.1463  took : 52.72722578048706
====> Test loss: 3965.6193
iteration 0000: loss: 3965.769
iteration 0100: loss: 3951.378
iteration 0200: loss: 3953.735
iteration 0300: loss: 3961.288
iteration 0400: loss: 3950.633
iteration 0500: loss: 3955.979
iteration 0600: loss: 3953.786
iteration 0700: loss: 3953.912
iteration 0800: loss: 3951.311
iteration 0900: loss: 3960.272
iteration 1000: loss: 3951.753
iteration 1100: loss: 3952.119
iteration 1200: loss: 3967.237
iteration 1300: loss: 3960.008
iteration 1400: loss: 3962.218
iteration 1500: loss: 3950.417
iteration 1600: loss: 3962.533
iteration 1700: loss: 3951.892
iteration 1800: loss: 3956.966
====> Epoch: 039 Train loss: 3957.9082  took : 52.94919967651367
====> Test loss: 3967.3803
iteration 0000: loss: 3953.561
iteration 0100: loss: 3960.895
iteration 0200: loss: 3953.784
iteration 0300: loss: 3956.231
iteration 0400: loss: 3961.538
iteration 0500: loss: 3959.813
iteration 0600: loss: 3954.439
iteration 0700: loss: 3952.215
iteration 0800: loss: 3957.364
iteration 0900: loss: 3956.532
iteration 1000: loss: 3962.000
iteration 1100: loss: 3954.688
iteration 1200: loss: 3954.543
iteration 1300: loss: 3957.179
iteration 1400: loss: 3957.133
iteration 1500: loss: 3957.359
iteration 1600: loss: 3956.912
iteration 1700: loss: 3953.477
iteration 1800: loss: 3955.116
====> Epoch: 040 Train loss: 3957.9988  took : 52.9358971118927
====> Test loss: 3965.8605
iteration 0000: loss: 3955.697
iteration 0100: loss: 3956.932
iteration 0200: loss: 3957.409
iteration 0300: loss: 3951.574
iteration 0400: loss: 3958.678
iteration 0500: loss: 3961.227
iteration 0600: loss: 3958.891
iteration 0700: loss: 3957.361
iteration 0800: loss: 3962.757
iteration 0900: loss: 3965.440
iteration 1000: loss: 3954.031
iteration 1100: loss: 3956.598
iteration 1200: loss: 3959.404
iteration 1300: loss: 3951.359
iteration 1400: loss: 3957.602
iteration 1500: loss: 3954.062
iteration 1600: loss: 3959.609
iteration 1700: loss: 3956.882
iteration 1800: loss: 3953.093
====> Epoch: 041 Train loss: 3958.0384  took : 52.45728278160095
====> Test loss: 3965.8090
iteration 0000: loss: 3967.209
iteration 0100: loss: 3951.116
iteration 0200: loss: 3961.701
iteration 0300: loss: 3966.016
iteration 0400: loss: 3956.574
iteration 0500: loss: 3961.848
iteration 0600: loss: 3966.670
iteration 0700: loss: 3957.755
iteration 0800: loss: 3949.812
iteration 0900: loss: 3961.959
iteration 1000: loss: 3954.511
iteration 1100: loss: 3947.987
iteration 1200: loss: 3960.141
iteration 1300: loss: 3950.968
iteration 1400: loss: 3955.389
iteration 1500: loss: 3960.122
iteration 1600: loss: 3954.233
iteration 1700: loss: 3955.213
iteration 1800: loss: 3962.972
====> Epoch: 042 Train loss: 3958.1297  took : 52.67129564285278
====> Test loss: 3967.0167
iteration 0000: loss: 3956.211
iteration 0100: loss: 3960.761
iteration 0200: loss: 3967.171
iteration 0300: loss: 3952.748
iteration 0400: loss: 3963.059
iteration 0500: loss: 3962.793
iteration 0600: loss: 3955.213
iteration 0700: loss: 3956.238
iteration 0800: loss: 3956.979
iteration 0900: loss: 3957.088
iteration 1000: loss: 3957.341
iteration 1100: loss: 3953.540
iteration 1200: loss: 3955.865
iteration 1300: loss: 3959.688
iteration 1400: loss: 3965.654
iteration 1500: loss: 3956.863
iteration 1600: loss: 3958.515
iteration 1700: loss: 3959.490
iteration 1800: loss: 3951.700
====> Epoch: 043 Train loss: 3957.9715  took : 52.87091779708862
====> Test loss: 3966.4301
iteration 0000: loss: 3962.027
iteration 0100: loss: 3962.100
iteration 0200: loss: 3950.107
iteration 0300: loss: 3955.763
iteration 0400: loss: 3958.647
iteration 0500: loss: 3949.569
iteration 0600: loss: 3961.302
iteration 0700: loss: 3960.528
iteration 0800: loss: 3957.942
iteration 0900: loss: 3951.748
iteration 1000: loss: 3968.584
iteration 1100: loss: 3964.867
iteration 1200: loss: 3953.624
iteration 1300: loss: 3959.462
iteration 1400: loss: 3955.987
iteration 1500: loss: 3959.467
iteration 1600: loss: 3953.499
iteration 1700: loss: 3960.499
iteration 1800: loss: 3953.993
====> Epoch: 044 Train loss: 3957.7725  took : 52.75127673149109
====> Test loss: 3965.7478
iteration 0000: loss: 3955.674
iteration 0100: loss: 3960.869
iteration 0200: loss: 3954.287
iteration 0300: loss: 3959.907
iteration 0400: loss: 3956.985
iteration 0500: loss: 3968.863
iteration 0600: loss: 3955.098
iteration 0700: loss: 3960.720
iteration 0800: loss: 3956.730
iteration 0900: loss: 3968.877
iteration 1000: loss: 3957.171
iteration 1100: loss: 3965.758
iteration 1200: loss: 3962.702
iteration 1300: loss: 3953.498
iteration 1400: loss: 3955.954
iteration 1500: loss: 3967.947
iteration 1600: loss: 3959.659
iteration 1700: loss: 3949.196
iteration 1800: loss: 3946.486
====> Epoch: 045 Train loss: 3958.3362  took : 52.9489324092865
====> Test loss: 3966.6406
iteration 0000: loss: 3956.520
iteration 0100: loss: 3952.511
iteration 0200: loss: 3953.669
iteration 0300: loss: 3956.294
iteration 0400: loss: 3957.659
iteration 0500: loss: 3955.730
iteration 0600: loss: 3961.662
iteration 0700: loss: 3968.134
iteration 0800: loss: 3953.417
iteration 0900: loss: 3964.429
iteration 1000: loss: 3958.240
iteration 1100: loss: 3958.042
iteration 1200: loss: 3954.668
iteration 1300: loss: 3955.925
iteration 1400: loss: 3960.783
iteration 1500: loss: 3960.257
iteration 1600: loss: 3957.001
iteration 1700: loss: 3958.809
iteration 1800: loss: 3953.555
====> Epoch: 046 Train loss: 3957.8620  took : 52.74060559272766
====> Test loss: 3965.4869
iteration 0000: loss: 3956.435
iteration 0100: loss: 3956.934
iteration 0200: loss: 3960.720
iteration 0300: loss: 3960.368
iteration 0400: loss: 3959.845
iteration 0500: loss: 3959.191
iteration 0600: loss: 3954.414
iteration 0700: loss: 3963.331
iteration 0800: loss: 3960.352
iteration 0900: loss: 3956.896
iteration 1000: loss: 3959.623
iteration 1100: loss: 3964.806
iteration 1200: loss: 3949.624
iteration 1300: loss: 3961.412
iteration 1400: loss: 3957.449
iteration 1500: loss: 3959.116
iteration 1600: loss: 3954.231
iteration 1700: loss: 3964.463
iteration 1800: loss: 3956.200
====> Epoch: 047 Train loss: 3957.9327  took : 52.93539309501648
====> Test loss: 3965.6571
iteration 0000: loss: 3960.285
iteration 0100: loss: 3962.346
iteration 0200: loss: 3962.930
iteration 0300: loss: 3961.463
iteration 0400: loss: 3955.587
iteration 0500: loss: 3962.347
iteration 0600: loss: 3961.280
iteration 0700: loss: 3957.130
iteration 0800: loss: 3962.802
iteration 0900: loss: 3952.414
iteration 1000: loss: 3957.984
iteration 1100: loss: 3957.953
iteration 1200: loss: 3949.248
iteration 1300: loss: 3950.143
iteration 1400: loss: 3954.819
iteration 1500: loss: 3959.273
iteration 1600: loss: 3951.594
iteration 1700: loss: 3959.535
iteration 1800: loss: 3952.844
====> Epoch: 048 Train loss: 3958.1052  took : 52.93102955818176
====> Test loss: 3966.3198
iteration 0000: loss: 3953.572
iteration 0100: loss: 3961.254
iteration 0200: loss: 3962.998
iteration 0300: loss: 3950.358
iteration 0400: loss: 3958.758
iteration 0500: loss: 3965.939
iteration 0600: loss: 3958.542
iteration 0700: loss: 3957.378
iteration 0800: loss: 3959.470
iteration 0900: loss: 3962.303
iteration 1000: loss: 3952.420
iteration 1100: loss: 3959.842
iteration 1200: loss: 3956.945
iteration 1300: loss: 3955.685
iteration 1400: loss: 3955.255
iteration 1500: loss: 3961.971
iteration 1600: loss: 3960.764
iteration 1700: loss: 3956.256
iteration 1800: loss: 3951.437
====> Epoch: 049 Train loss: 3957.7529  took : 53.06707286834717
====> Test loss: 3966.0391
iteration 0000: loss: 3963.035
iteration 0100: loss: 3952.050
iteration 0200: loss: 3950.688
iteration 0300: loss: 3956.427
iteration 0400: loss: 3956.427
iteration 0500: loss: 3960.623
iteration 0600: loss: 3963.681
iteration 0700: loss: 3965.072
iteration 0800: loss: 3965.494
iteration 0900: loss: 3961.269
iteration 1000: loss: 3958.725
iteration 1100: loss: 3953.070
iteration 1200: loss: 3950.312
iteration 1300: loss: 3967.087
iteration 1400: loss: 3958.084
iteration 1500: loss: 3952.624
iteration 1600: loss: 3960.090
iteration 1700: loss: 3958.356
iteration 1800: loss: 3963.582
====> Epoch: 050 Train loss: 3957.6569  took : 53.09164476394653
====> Test loss: 3965.8007
====> [MM-VAE] Time: 3132.980s or 00:52:12
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  5
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_5
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_5
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1986.942
iteration 0100: loss: 1566.170
iteration 0200: loss: 1568.793
iteration 0300: loss: 1565.633
iteration 0400: loss: 1552.913
iteration 0500: loss: 1542.345
iteration 0600: loss: 1541.560
iteration 0700: loss: 1548.360
iteration 0800: loss: 1541.556
iteration 0900: loss: 1534.206
====> Epoch: 001 Train loss: 1554.8939  took : 8.44294023513794
====> Test loss: 1537.9869
iteration 0000: loss: 1537.031
iteration 0100: loss: 1537.272
iteration 0200: loss: 1535.781
iteration 0300: loss: 1533.655
iteration 0400: loss: 1533.442
iteration 0500: loss: 1527.955
iteration 0600: loss: 1530.814
iteration 0700: loss: 1530.289
iteration 0800: loss: 1528.095
iteration 0900: loss: 1531.205
====> Epoch: 002 Train loss: 1531.6248  took : 8.46858286857605
====> Test loss: 1529.7802
iteration 0000: loss: 1531.118
iteration 0100: loss: 1525.967
iteration 0200: loss: 1529.932
iteration 0300: loss: 1527.175
iteration 0400: loss: 1523.160
iteration 0500: loss: 1523.891
iteration 0600: loss: 1527.219
iteration 0700: loss: 1522.655
iteration 0800: loss: 1528.865
iteration 0900: loss: 1521.004
====> Epoch: 003 Train loss: 1525.9710  took : 8.412256479263306
====> Test loss: 1526.5924
iteration 0000: loss: 1525.537
iteration 0100: loss: 1524.289
iteration 0200: loss: 1523.509
iteration 0300: loss: 1521.750
iteration 0400: loss: 1525.372
iteration 0500: loss: 1524.215
iteration 0600: loss: 1525.481
iteration 0700: loss: 1519.921
iteration 0800: loss: 1518.965
iteration 0900: loss: 1525.832
====> Epoch: 004 Train loss: 1523.5980  took : 8.536869049072266
====> Test loss: 1525.1023
iteration 0000: loss: 1526.224
iteration 0100: loss: 1520.766
iteration 0200: loss: 1519.488
iteration 0300: loss: 1522.251
iteration 0400: loss: 1520.632
iteration 0500: loss: 1520.967
iteration 0600: loss: 1520.962
iteration 0700: loss: 1521.760
iteration 0800: loss: 1518.464
iteration 0900: loss: 1523.567
====> Epoch: 005 Train loss: 1522.1810  took : 8.510091304779053
====> Test loss: 1524.0924
iteration 0000: loss: 1521.004
iteration 0100: loss: 1523.001
iteration 0200: loss: 1519.321
iteration 0300: loss: 1519.458
iteration 0400: loss: 1522.485
iteration 0500: loss: 1522.853
iteration 0600: loss: 1517.657
iteration 0700: loss: 1517.228
iteration 0800: loss: 1516.002
iteration 0900: loss: 1524.563
====> Epoch: 006 Train loss: 1521.1892  took : 8.415558576583862
====> Test loss: 1523.3505
iteration 0000: loss: 1524.208
iteration 0100: loss: 1520.966
iteration 0200: loss: 1518.986
iteration 0300: loss: 1520.088
iteration 0400: loss: 1515.993
iteration 0500: loss: 1520.535
iteration 0600: loss: 1517.608
iteration 0700: loss: 1519.946
iteration 0800: loss: 1518.612
iteration 0900: loss: 1518.503
====> Epoch: 007 Train loss: 1520.3954  took : 8.499914646148682
====> Test loss: 1522.6476
iteration 0000: loss: 1519.102
iteration 0100: loss: 1518.844
iteration 0200: loss: 1519.636
iteration 0300: loss: 1521.444
iteration 0400: loss: 1519.841
iteration 0500: loss: 1518.748
iteration 0600: loss: 1520.311
iteration 0700: loss: 1519.116
iteration 0800: loss: 1515.339
iteration 0900: loss: 1525.781
====> Epoch: 008 Train loss: 1519.7605  took : 8.456019878387451
====> Test loss: 1522.2190
iteration 0000: loss: 1520.724
iteration 0100: loss: 1517.147
iteration 0200: loss: 1521.718
iteration 0300: loss: 1517.991
iteration 0400: loss: 1517.929
iteration 0500: loss: 1518.070
iteration 0600: loss: 1519.696
iteration 0700: loss: 1519.412
iteration 0800: loss: 1520.198
iteration 0900: loss: 1519.018
====> Epoch: 009 Train loss: 1519.2548  took : 8.450052976608276
====> Test loss: 1521.6309
iteration 0000: loss: 1520.314
iteration 0100: loss: 1519.327
iteration 0200: loss: 1518.173
iteration 0300: loss: 1521.969
iteration 0400: loss: 1522.396
iteration 0500: loss: 1517.397
iteration 0600: loss: 1519.461
iteration 0700: loss: 1517.754
iteration 0800: loss: 1518.408
iteration 0900: loss: 1519.944
====> Epoch: 010 Train loss: 1518.8063  took : 8.432908296585083
====> Test loss: 1521.2993
iteration 0000: loss: 1518.738
iteration 0100: loss: 1518.895
iteration 0200: loss: 1516.469
iteration 0300: loss: 1516.507
iteration 0400: loss: 1520.240
iteration 0500: loss: 1517.249
iteration 0600: loss: 1518.990
iteration 0700: loss: 1521.345
iteration 0800: loss: 1519.207
iteration 0900: loss: 1517.345
====> Epoch: 011 Train loss: 1518.4195  took : 8.484037637710571
====> Test loss: 1520.9861
iteration 0000: loss: 1518.205
iteration 0100: loss: 1518.818
iteration 0200: loss: 1517.925
iteration 0300: loss: 1515.896
iteration 0400: loss: 1516.574
iteration 0500: loss: 1517.827
iteration 0600: loss: 1519.104
iteration 0700: loss: 1518.054
iteration 0800: loss: 1517.417
iteration 0900: loss: 1521.619
====> Epoch: 012 Train loss: 1518.0894  took : 8.44677209854126
====> Test loss: 1520.7936
iteration 0000: loss: 1517.748
iteration 0100: loss: 1517.650
iteration 0200: loss: 1518.152
iteration 0300: loss: 1519.702
iteration 0400: loss: 1517.144
iteration 0500: loss: 1519.362
iteration 0600: loss: 1516.498
iteration 0700: loss: 1516.269
iteration 0800: loss: 1520.962
iteration 0900: loss: 1518.215
====> Epoch: 013 Train loss: 1517.8284  took : 8.400078535079956
====> Test loss: 1520.4641
iteration 0000: loss: 1516.829
iteration 0100: loss: 1518.605
iteration 0200: loss: 1517.258
iteration 0300: loss: 1516.209
iteration 0400: loss: 1516.825
iteration 0500: loss: 1516.556
iteration 0600: loss: 1517.057
iteration 0700: loss: 1519.618
iteration 0800: loss: 1518.093
iteration 0900: loss: 1517.321
====> Epoch: 014 Train loss: 1517.5776  took : 8.389127016067505
====> Test loss: 1520.3964
iteration 0000: loss: 1517.713
iteration 0100: loss: 1518.545
iteration 0200: loss: 1517.123
iteration 0300: loss: 1515.819
iteration 0400: loss: 1517.272
iteration 0500: loss: 1515.718
iteration 0600: loss: 1516.113
iteration 0700: loss: 1516.755
iteration 0800: loss: 1517.184
iteration 0900: loss: 1516.524
====> Epoch: 015 Train loss: 1517.3548  took : 8.507639646530151
====> Test loss: 1520.1706
iteration 0000: loss: 1518.369
iteration 0100: loss: 1515.295
iteration 0200: loss: 1516.527
iteration 0300: loss: 1516.789
iteration 0400: loss: 1514.257
iteration 0500: loss: 1517.141
iteration 0600: loss: 1516.200
iteration 0700: loss: 1518.882
iteration 0800: loss: 1518.246
iteration 0900: loss: 1513.675
====> Epoch: 016 Train loss: 1517.1889  took : 8.461408615112305
====> Test loss: 1520.0567
iteration 0000: loss: 1518.210
iteration 0100: loss: 1516.156
iteration 0200: loss: 1517.355
iteration 0300: loss: 1518.134
iteration 0400: loss: 1516.084
iteration 0500: loss: 1520.511
iteration 0600: loss: 1517.220
iteration 0700: loss: 1517.306
iteration 0800: loss: 1516.307
iteration 0900: loss: 1517.530
====> Epoch: 017 Train loss: 1517.0170  took : 8.52393126487732
====> Test loss: 1519.8671
iteration 0000: loss: 1517.864
iteration 0100: loss: 1518.022
iteration 0200: loss: 1516.222
iteration 0300: loss: 1514.630
iteration 0400: loss: 1516.423
iteration 0500: loss: 1516.586
iteration 0600: loss: 1514.584
iteration 0700: loss: 1517.569
iteration 0800: loss: 1517.119
iteration 0900: loss: 1518.300
====> Epoch: 018 Train loss: 1516.8608  took : 8.47209644317627
====> Test loss: 1519.7589
iteration 0000: loss: 1516.675
iteration 0100: loss: 1516.517
iteration 0200: loss: 1516.788
iteration 0300: loss: 1514.928
iteration 0400: loss: 1520.093
iteration 0500: loss: 1517.036
iteration 0600: loss: 1514.434
iteration 0700: loss: 1516.804
iteration 0800: loss: 1515.837
iteration 0900: loss: 1514.462
====> Epoch: 019 Train loss: 1516.6931  took : 8.418377161026001
====> Test loss: 1519.6747
iteration 0000: loss: 1518.910
iteration 0100: loss: 1515.493
iteration 0200: loss: 1517.823
iteration 0300: loss: 1515.598
iteration 0400: loss: 1514.662
iteration 0500: loss: 1514.523
iteration 0600: loss: 1516.529
iteration 0700: loss: 1512.647
iteration 0800: loss: 1514.303
iteration 0900: loss: 1515.828
====> Epoch: 020 Train loss: 1516.6121  took : 8.443262815475464
====> Test loss: 1519.5560
iteration 0000: loss: 1515.226
iteration 0100: loss: 1518.829
iteration 0200: loss: 1515.516
iteration 0300: loss: 1514.282
iteration 0400: loss: 1515.614
iteration 0500: loss: 1513.987
iteration 0600: loss: 1517.810
iteration 0700: loss: 1517.919
iteration 0800: loss: 1517.999
iteration 0900: loss: 1516.371
====> Epoch: 021 Train loss: 1516.4524  took : 8.40892505645752
====> Test loss: 1519.4175
iteration 0000: loss: 1516.430
iteration 0100: loss: 1512.259
iteration 0200: loss: 1512.649
iteration 0300: loss: 1517.250
iteration 0400: loss: 1517.016
iteration 0500: loss: 1517.236
iteration 0600: loss: 1513.367
iteration 0700: loss: 1517.428
iteration 0800: loss: 1517.413
iteration 0900: loss: 1517.137
====> Epoch: 022 Train loss: 1516.3735  took : 8.515368700027466
====> Test loss: 1519.3902
iteration 0000: loss: 1516.447
iteration 0100: loss: 1513.370
iteration 0200: loss: 1515.148
iteration 0300: loss: 1515.327
iteration 0400: loss: 1518.897
iteration 0500: loss: 1516.168
iteration 0600: loss: 1515.490
iteration 0700: loss: 1517.394
iteration 0800: loss: 1516.909
iteration 0900: loss: 1517.444
====> Epoch: 023 Train loss: 1516.2547  took : 8.427546739578247
====> Test loss: 1519.4977
iteration 0000: loss: 1518.018
iteration 0100: loss: 1517.090
iteration 0200: loss: 1516.925
iteration 0300: loss: 1514.092
iteration 0400: loss: 1514.346
iteration 0500: loss: 1516.997
iteration 0600: loss: 1515.027
iteration 0700: loss: 1517.824
iteration 0800: loss: 1516.397
iteration 0900: loss: 1513.881
====> Epoch: 024 Train loss: 1516.1438  took : 8.407415390014648
====> Test loss: 1519.2378
iteration 0000: loss: 1518.704
iteration 0100: loss: 1517.335
iteration 0200: loss: 1515.327
iteration 0300: loss: 1518.520
iteration 0400: loss: 1518.924
iteration 0500: loss: 1515.334
iteration 0600: loss: 1516.642
iteration 0700: loss: 1516.177
iteration 0800: loss: 1515.836
iteration 0900: loss: 1516.809
====> Epoch: 025 Train loss: 1516.0902  took : 8.504819869995117
====> Test loss: 1519.1312
iteration 0000: loss: 1513.679
iteration 0100: loss: 1512.651
iteration 0200: loss: 1516.826
iteration 0300: loss: 1516.949
iteration 0400: loss: 1513.861
iteration 0500: loss: 1513.176
iteration 0600: loss: 1515.838
iteration 0700: loss: 1515.279
iteration 0800: loss: 1514.701
iteration 0900: loss: 1515.520
====> Epoch: 026 Train loss: 1515.9529  took : 8.43623971939087
====> Test loss: 1519.2571
iteration 0000: loss: 1517.458
iteration 0100: loss: 1514.760
iteration 0200: loss: 1515.663
iteration 0300: loss: 1516.148
iteration 0400: loss: 1516.453
iteration 0500: loss: 1516.284
iteration 0600: loss: 1513.865
iteration 0700: loss: 1516.535
iteration 0800: loss: 1517.637
iteration 0900: loss: 1515.996
====> Epoch: 027 Train loss: 1515.8571  took : 8.506316184997559
====> Test loss: 1519.1071
iteration 0000: loss: 1519.587
iteration 0100: loss: 1517.276
iteration 0200: loss: 1516.921
iteration 0300: loss: 1515.062
iteration 0400: loss: 1515.362
iteration 0500: loss: 1515.740
iteration 0600: loss: 1517.038
iteration 0700: loss: 1517.810
iteration 0800: loss: 1516.433
iteration 0900: loss: 1516.105
====> Epoch: 028 Train loss: 1515.8180  took : 8.524372577667236
====> Test loss: 1519.0687
iteration 0000: loss: 1514.430
iteration 0100: loss: 1517.376
iteration 0200: loss: 1515.208
iteration 0300: loss: 1515.118
iteration 0400: loss: 1516.205
iteration 0500: loss: 1515.780
iteration 0600: loss: 1516.831
iteration 0700: loss: 1514.795
iteration 0800: loss: 1513.595
iteration 0900: loss: 1515.451
====> Epoch: 029 Train loss: 1515.7501  took : 8.46976375579834
====> Test loss: 1519.0549
iteration 0000: loss: 1519.378
iteration 0100: loss: 1515.838
iteration 0200: loss: 1517.593
iteration 0300: loss: 1517.464
iteration 0400: loss: 1513.810
iteration 0500: loss: 1513.412
iteration 0600: loss: 1513.524
iteration 0700: loss: 1517.428
iteration 0800: loss: 1514.833
iteration 0900: loss: 1519.153
====> Epoch: 030 Train loss: 1515.6444  took : 8.458897590637207
====> Test loss: 1518.8595
iteration 0000: loss: 1513.966
iteration 0100: loss: 1519.595
iteration 0200: loss: 1518.910
iteration 0300: loss: 1515.878
iteration 0400: loss: 1514.116
iteration 0500: loss: 1515.046
iteration 0600: loss: 1518.279
iteration 0700: loss: 1515.639
iteration 0800: loss: 1518.979
iteration 0900: loss: 1512.766
====> Epoch: 031 Train loss: 1515.5835  took : 8.449256658554077
====> Test loss: 1518.8721
iteration 0000: loss: 1514.256
iteration 0100: loss: 1515.089
iteration 0200: loss: 1516.177
iteration 0300: loss: 1514.203
iteration 0400: loss: 1513.714
iteration 0500: loss: 1514.526
iteration 0600: loss: 1517.544
iteration 0700: loss: 1516.832
iteration 0800: loss: 1518.285
iteration 0900: loss: 1513.850
====> Epoch: 032 Train loss: 1515.5184  took : 8.451012134552002
====> Test loss: 1518.7574
iteration 0000: loss: 1513.371
iteration 0100: loss: 1516.462
iteration 0200: loss: 1513.567
iteration 0300: loss: 1516.127
iteration 0400: loss: 1514.875
iteration 0500: loss: 1514.646
iteration 0600: loss: 1512.841
iteration 0700: loss: 1517.057
iteration 0800: loss: 1514.966
iteration 0900: loss: 1515.854
====> Epoch: 033 Train loss: 1515.4392  took : 8.476911783218384
====> Test loss: 1518.7889
iteration 0000: loss: 1516.035
iteration 0100: loss: 1516.148
iteration 0200: loss: 1516.519
iteration 0300: loss: 1515.365
iteration 0400: loss: 1517.414
iteration 0500: loss: 1517.186
iteration 0600: loss: 1517.677
iteration 0700: loss: 1514.181
iteration 0800: loss: 1515.808
iteration 0900: loss: 1513.823
====> Epoch: 034 Train loss: 1515.4350  took : 8.54530644416809
====> Test loss: 1518.7601
iteration 0000: loss: 1515.242
iteration 0100: loss: 1514.829
iteration 0200: loss: 1516.651
iteration 0300: loss: 1515.103
iteration 0400: loss: 1516.629
iteration 0500: loss: 1515.383
iteration 0600: loss: 1516.969
iteration 0700: loss: 1514.581
iteration 0800: loss: 1515.989
iteration 0900: loss: 1515.916
====> Epoch: 035 Train loss: 1515.3572  took : 8.459338426589966
====> Test loss: 1519.0131
iteration 0000: loss: 1512.769
iteration 0100: loss: 1513.876
iteration 0200: loss: 1515.147
iteration 0300: loss: 1515.344
iteration 0400: loss: 1516.340
iteration 0500: loss: 1513.425
iteration 0600: loss: 1512.652
iteration 0700: loss: 1515.480
iteration 0800: loss: 1512.889
iteration 0900: loss: 1517.903
====> Epoch: 036 Train loss: 1515.3585  took : 8.508084774017334
====> Test loss: 1518.8186
iteration 0000: loss: 1516.832
iteration 0100: loss: 1514.702
iteration 0200: loss: 1514.954
iteration 0300: loss: 1514.408
iteration 0400: loss: 1515.205
iteration 0500: loss: 1513.635
iteration 0600: loss: 1516.144
iteration 0700: loss: 1514.963
iteration 0800: loss: 1514.916
iteration 0900: loss: 1514.272
====> Epoch: 037 Train loss: 1515.2414  took : 8.507688045501709
====> Test loss: 1518.6498
iteration 0000: loss: 1515.342
iteration 0100: loss: 1517.206
iteration 0200: loss: 1515.601
iteration 0300: loss: 1514.845
iteration 0400: loss: 1513.830
iteration 0500: loss: 1514.995
iteration 0600: loss: 1517.550
iteration 0700: loss: 1516.573
iteration 0800: loss: 1514.742
iteration 0900: loss: 1513.047
====> Epoch: 038 Train loss: 1515.1714  took : 8.557801485061646
====> Test loss: 1518.6620
iteration 0000: loss: 1514.738
iteration 0100: loss: 1515.073
iteration 0200: loss: 1517.353
iteration 0300: loss: 1513.582
iteration 0400: loss: 1514.521
iteration 0500: loss: 1517.207
iteration 0600: loss: 1513.910
iteration 0700: loss: 1517.000
iteration 0800: loss: 1515.188
iteration 0900: loss: 1515.430
====> Epoch: 039 Train loss: 1515.0884  took : 8.443326711654663
====> Test loss: 1518.6837
iteration 0000: loss: 1516.794
iteration 0100: loss: 1514.078
iteration 0200: loss: 1514.502
iteration 0300: loss: 1515.161
iteration 0400: loss: 1518.356
iteration 0500: loss: 1515.215
iteration 0600: loss: 1517.351
iteration 0700: loss: 1516.826
iteration 0800: loss: 1515.544
iteration 0900: loss: 1512.303
====> Epoch: 040 Train loss: 1515.1014  took : 8.51192021369934
====> Test loss: 1518.5989
iteration 0000: loss: 1515.911
iteration 0100: loss: 1516.940
iteration 0200: loss: 1515.556
iteration 0300: loss: 1515.410
iteration 0400: loss: 1514.728
iteration 0500: loss: 1515.532
iteration 0600: loss: 1514.311
iteration 0700: loss: 1513.302
iteration 0800: loss: 1514.919
iteration 0900: loss: 1516.942
====> Epoch: 041 Train loss: 1515.0101  took : 8.484400272369385
====> Test loss: 1518.4591
iteration 0000: loss: 1516.769
iteration 0100: loss: 1512.014
iteration 0200: loss: 1515.268
iteration 0300: loss: 1515.843
iteration 0400: loss: 1515.360
iteration 0500: loss: 1518.245
iteration 0600: loss: 1515.829
iteration 0700: loss: 1512.989
iteration 0800: loss: 1515.274
iteration 0900: loss: 1510.754
====> Epoch: 042 Train loss: 1514.9859  took : 8.466723442077637
====> Test loss: 1518.5034
iteration 0000: loss: 1514.836
iteration 0100: loss: 1514.848
iteration 0200: loss: 1514.578
iteration 0300: loss: 1514.613
iteration 0400: loss: 1511.556
iteration 0500: loss: 1515.271
iteration 0600: loss: 1515.339
iteration 0700: loss: 1517.616
iteration 0800: loss: 1515.681
iteration 0900: loss: 1515.621
====> Epoch: 043 Train loss: 1514.9347  took : 8.449769496917725
====> Test loss: 1518.4252
iteration 0000: loss: 1513.232
iteration 0100: loss: 1515.148
iteration 0200: loss: 1517.696
iteration 0300: loss: 1514.395
iteration 0400: loss: 1514.688
iteration 0500: loss: 1513.677
iteration 0600: loss: 1514.252
iteration 0700: loss: 1516.880
iteration 0800: loss: 1515.205
iteration 0900: loss: 1513.352
====> Epoch: 044 Train loss: 1514.8916  took : 8.494005680084229
====> Test loss: 1518.4837
iteration 0000: loss: 1515.587
iteration 0100: loss: 1513.825
iteration 0200: loss: 1513.587
iteration 0300: loss: 1516.944
iteration 0400: loss: 1515.618
iteration 0500: loss: 1515.684
iteration 0600: loss: 1514.584
iteration 0700: loss: 1514.098
iteration 0800: loss: 1514.439
iteration 0900: loss: 1515.058
====> Epoch: 045 Train loss: 1514.8645  took : 8.508925914764404
====> Test loss: 1518.5284
iteration 0000: loss: 1513.368
iteration 0100: loss: 1513.241
iteration 0200: loss: 1511.935
iteration 0300: loss: 1515.446
iteration 0400: loss: 1514.077
iteration 0500: loss: 1516.252
iteration 0600: loss: 1516.454
iteration 0700: loss: 1516.231
iteration 0800: loss: 1514.519
iteration 0900: loss: 1514.783
====> Epoch: 046 Train loss: 1514.7945  took : 8.402248620986938
====> Test loss: 1518.3753
iteration 0000: loss: 1514.024
iteration 0100: loss: 1511.533
iteration 0200: loss: 1514.863
iteration 0300: loss: 1515.833
iteration 0400: loss: 1516.840
iteration 0500: loss: 1515.824
iteration 0600: loss: 1513.697
iteration 0700: loss: 1512.592
iteration 0800: loss: 1514.633
iteration 0900: loss: 1514.336
====> Epoch: 047 Train loss: 1514.8593  took : 8.538813591003418
====> Test loss: 1518.4991
iteration 0000: loss: 1515.593
iteration 0100: loss: 1513.292
iteration 0200: loss: 1516.236
iteration 0300: loss: 1515.375
iteration 0400: loss: 1513.199
iteration 0500: loss: 1513.001
iteration 0600: loss: 1515.494
iteration 0700: loss: 1517.610
iteration 0800: loss: 1514.337
iteration 0900: loss: 1516.102
====> Epoch: 048 Train loss: 1514.7725  took : 8.411149024963379
====> Test loss: 1518.4412
iteration 0000: loss: 1514.376
iteration 0100: loss: 1513.103
iteration 0200: loss: 1511.475
iteration 0300: loss: 1516.057
iteration 0400: loss: 1515.398
iteration 0500: loss: 1513.705
iteration 0600: loss: 1514.570
iteration 0700: loss: 1516.318
iteration 0800: loss: 1515.095
iteration 0900: loss: 1515.917
====> Epoch: 049 Train loss: 1514.7841  took : 8.397563934326172
====> Test loss: 1518.4891
iteration 0000: loss: 1516.417
iteration 0100: loss: 1513.292
iteration 0200: loss: 1515.722
iteration 0300: loss: 1513.857
iteration 0400: loss: 1513.583
iteration 0500: loss: 1516.389
iteration 0600: loss: 1514.484
iteration 0700: loss: 1513.543
iteration 0800: loss: 1511.539
iteration 0900: loss: 1514.387
====> Epoch: 050 Train loss: 1514.6926  took : 8.431556701660156
====> Test loss: 1518.4756
====> [MM-VAE] Time: 507.003s or 00:08:27
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  5
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_5
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_5
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.179
iteration 0100: loss: 2102.003
iteration 0200: loss: 2041.968
iteration 0300: loss: 2019.629
iteration 0400: loss: 2005.114
iteration 0500: loss: 2004.587
iteration 0600: loss: 2000.216
iteration 0700: loss: 2001.464
iteration 0800: loss: 1993.802
iteration 0900: loss: 1997.808
====> Epoch: 001 Train loss: 2026.6417  took : 12.116910934448242
====> Test loss: 1998.5347
iteration 0000: loss: 1998.201
iteration 0100: loss: 2004.125
iteration 0200: loss: 1998.327
iteration 0300: loss: 1992.929
iteration 0400: loss: 1988.255
iteration 0500: loss: 1990.475
iteration 0600: loss: 1986.828
iteration 0700: loss: 1987.284
iteration 0800: loss: 1982.810
iteration 0900: loss: 1989.739
====> Epoch: 002 Train loss: 1989.8239  took : 12.839915037155151
====> Test loss: 1984.8053
iteration 0000: loss: 1987.680
iteration 0100: loss: 1983.565
iteration 0200: loss: 1977.696
iteration 0300: loss: 1975.524
iteration 0400: loss: 1974.492
iteration 0500: loss: 1971.028
iteration 0600: loss: 1971.722
iteration 0700: loss: 1972.234
iteration 0800: loss: 1971.916
iteration 0900: loss: 1968.769
====> Epoch: 003 Train loss: 1975.4139  took : 13.191919326782227
====> Test loss: 1973.5650
iteration 0000: loss: 1972.883
iteration 0100: loss: 1972.878
iteration 0200: loss: 1968.956
iteration 0300: loss: 1969.180
iteration 0400: loss: 1966.402
iteration 0500: loss: 1963.536
iteration 0600: loss: 1966.809
iteration 0700: loss: 1960.088
iteration 0800: loss: 1961.398
iteration 0900: loss: 1965.363
====> Epoch: 004 Train loss: 1966.1926  took : 13.42940354347229
====> Test loss: 1965.2034
iteration 0000: loss: 1963.051
iteration 0100: loss: 1961.842
iteration 0200: loss: 1963.197
iteration 0300: loss: 1960.466
iteration 0400: loss: 1965.682
iteration 0500: loss: 1961.380
iteration 0600: loss: 1960.570
iteration 0700: loss: 1960.255
iteration 0800: loss: 1959.321
iteration 0900: loss: 1962.522
====> Epoch: 005 Train loss: 1961.8181  took : 11.982448101043701
====> Test loss: 1962.2634
iteration 0000: loss: 1959.096
iteration 0100: loss: 1959.759
iteration 0200: loss: 1963.007
iteration 0300: loss: 1960.994
iteration 0400: loss: 1960.222
iteration 0500: loss: 1965.186
iteration 0600: loss: 1960.776
iteration 0700: loss: 1961.231
iteration 0800: loss: 1957.530
iteration 0900: loss: 1954.959
====> Epoch: 006 Train loss: 1959.4071  took : 11.91424036026001
====> Test loss: 1960.6623
iteration 0000: loss: 1956.142
iteration 0100: loss: 1959.237
iteration 0200: loss: 1957.856
iteration 0300: loss: 1955.333
iteration 0400: loss: 1957.293
iteration 0500: loss: 1955.603
iteration 0600: loss: 1958.000
iteration 0700: loss: 1953.947
iteration 0800: loss: 1958.053
iteration 0900: loss: 1954.914
====> Epoch: 007 Train loss: 1956.9610  took : 12.451798915863037
====> Test loss: 1957.8657
iteration 0000: loss: 1955.907
iteration 0100: loss: 1954.776
iteration 0200: loss: 1954.991
iteration 0300: loss: 1955.029
iteration 0400: loss: 1954.549
iteration 0500: loss: 1956.656
iteration 0600: loss: 1953.811
iteration 0700: loss: 1953.705
iteration 0800: loss: 1956.308
iteration 0900: loss: 1952.906
====> Epoch: 008 Train loss: 1955.3263  took : 13.53459906578064
====> Test loss: 1957.2525
iteration 0000: loss: 1954.767
iteration 0100: loss: 1953.487
iteration 0200: loss: 1955.554
iteration 0300: loss: 1955.874
iteration 0400: loss: 1954.615
iteration 0500: loss: 1955.583
iteration 0600: loss: 1952.478
iteration 0700: loss: 1954.431
iteration 0800: loss: 1953.532
iteration 0900: loss: 1954.501
====> Epoch: 009 Train loss: 1954.6345  took : 11.912846326828003
====> Test loss: 1956.3531
iteration 0000: loss: 1956.347
iteration 0100: loss: 1952.247
iteration 0200: loss: 1955.240
iteration 0300: loss: 1953.102
iteration 0400: loss: 1952.930
iteration 0500: loss: 1952.521
iteration 0600: loss: 1954.646
iteration 0700: loss: 1953.611
iteration 0800: loss: 1953.198
iteration 0900: loss: 1953.593
====> Epoch: 010 Train loss: 1953.8645  took : 12.097487688064575
====> Test loss: 1955.6842
iteration 0000: loss: 1954.501
iteration 0100: loss: 1952.755
iteration 0200: loss: 1954.700
iteration 0300: loss: 1953.589
iteration 0400: loss: 1952.476
iteration 0500: loss: 1951.763
iteration 0600: loss: 1951.281
iteration 0700: loss: 1952.837
iteration 0800: loss: 1953.116
iteration 0900: loss: 1952.893
====> Epoch: 011 Train loss: 1953.5594  took : 12.998303413391113
====> Test loss: 1955.1860
iteration 0000: loss: 1951.755
iteration 0100: loss: 1955.374
iteration 0200: loss: 1952.130
iteration 0300: loss: 1951.923
iteration 0400: loss: 1953.451
iteration 0500: loss: 1952.074
iteration 0600: loss: 1952.740
iteration 0700: loss: 1954.074
iteration 0800: loss: 1953.107
iteration 0900: loss: 1952.968
====> Epoch: 012 Train loss: 1953.1231  took : 12.292739868164062
====> Test loss: 1954.6468
iteration 0000: loss: 1951.917
iteration 0100: loss: 1950.589
iteration 0200: loss: 1952.550
iteration 0300: loss: 1954.341
iteration 0400: loss: 1952.905
iteration 0500: loss: 1953.954
iteration 0600: loss: 1951.996
iteration 0700: loss: 1952.384
iteration 0800: loss: 1952.520
iteration 0900: loss: 1951.158
====> Epoch: 013 Train loss: 1952.7764  took : 12.525216341018677
====> Test loss: 1954.5878
iteration 0000: loss: 1951.740
iteration 0100: loss: 1953.832
iteration 0200: loss: 1954.027
iteration 0300: loss: 1952.301
iteration 0400: loss: 1951.552
iteration 0500: loss: 1951.981
iteration 0600: loss: 1952.407
iteration 0700: loss: 1954.597
iteration 0800: loss: 1951.300
iteration 0900: loss: 1953.030
====> Epoch: 014 Train loss: 1952.5594  took : 12.057127952575684
====> Test loss: 1954.4335
iteration 0000: loss: 1952.795
iteration 0100: loss: 1952.167
iteration 0200: loss: 1954.794
iteration 0300: loss: 1952.085
iteration 0400: loss: 1952.375
iteration 0500: loss: 1951.772
iteration 0600: loss: 1952.149
iteration 0700: loss: 1950.233
iteration 0800: loss: 1952.734
iteration 0900: loss: 1952.620
====> Epoch: 015 Train loss: 1952.4771  took : 12.359766960144043
====> Test loss: 1954.1989
iteration 0000: loss: 1951.376
iteration 0100: loss: 1952.724
iteration 0200: loss: 1951.484
iteration 0300: loss: 1956.156
iteration 0400: loss: 1952.334
iteration 0500: loss: 1952.906
iteration 0600: loss: 1951.572
iteration 0700: loss: 1952.005
iteration 0800: loss: 1953.400
iteration 0900: loss: 1951.791
====> Epoch: 016 Train loss: 1952.3142  took : 13.335865497589111
====> Test loss: 1954.0800
iteration 0000: loss: 1952.416
iteration 0100: loss: 1951.599
iteration 0200: loss: 1950.409
iteration 0300: loss: 1951.674
iteration 0400: loss: 1952.788
iteration 0500: loss: 1951.959
iteration 0600: loss: 1952.783
iteration 0700: loss: 1952.713
iteration 0800: loss: 1952.220
iteration 0900: loss: 1953.505
====> Epoch: 017 Train loss: 1952.1647  took : 12.651066303253174
====> Test loss: 1954.4038
iteration 0000: loss: 1952.323
iteration 0100: loss: 1952.681
iteration 0200: loss: 1953.019
iteration 0300: loss: 1953.690
iteration 0400: loss: 1952.346
iteration 0500: loss: 1953.625
iteration 0600: loss: 1951.259
iteration 0700: loss: 1953.836
iteration 0800: loss: 1951.721
iteration 0900: loss: 1951.600
====> Epoch: 018 Train loss: 1952.1489  took : 11.29683804512024
====> Test loss: 1953.8595
iteration 0000: loss: 1951.597
iteration 0100: loss: 1953.049
iteration 0200: loss: 1951.145
iteration 0300: loss: 1951.894
iteration 0400: loss: 1951.686
iteration 0500: loss: 1951.239
iteration 0600: loss: 1950.398
iteration 0700: loss: 1951.551
iteration 0800: loss: 1951.856
iteration 0900: loss: 1954.565
====> Epoch: 019 Train loss: 1951.9784  took : 12.277071475982666
====> Test loss: 1953.7350
iteration 0000: loss: 1951.937
iteration 0100: loss: 1950.773
iteration 0200: loss: 1951.603
iteration 0300: loss: 1952.523
iteration 0400: loss: 1954.226
iteration 0500: loss: 1951.850
iteration 0600: loss: 1951.767
iteration 0700: loss: 1951.928
iteration 0800: loss: 1951.218
iteration 0900: loss: 1951.793
====> Epoch: 020 Train loss: 1951.9005  took : 11.585777044296265
====> Test loss: 1953.7851
iteration 0000: loss: 1951.128
iteration 0100: loss: 1951.485
iteration 0200: loss: 1953.364
iteration 0300: loss: 1952.822
iteration 0400: loss: 1952.106
iteration 0500: loss: 1951.439
iteration 0600: loss: 1949.391
iteration 0700: loss: 1952.863
iteration 0800: loss: 1951.193
iteration 0900: loss: 1952.055
====> Epoch: 021 Train loss: 1951.7794  took : 13.024799108505249
====> Test loss: 1953.6099
iteration 0000: loss: 1951.979
iteration 0100: loss: 1951.093
iteration 0200: loss: 1952.597
iteration 0300: loss: 1951.571
iteration 0400: loss: 1952.164
iteration 0500: loss: 1952.068
iteration 0600: loss: 1950.436
iteration 0700: loss: 1952.185
iteration 0800: loss: 1951.682
iteration 0900: loss: 1951.266
====> Epoch: 022 Train loss: 1951.6588  took : 12.448592185974121
====> Test loss: 1953.6706
iteration 0000: loss: 1952.711
iteration 0100: loss: 1952.331
iteration 0200: loss: 1951.162
iteration 0300: loss: 1949.923
iteration 0400: loss: 1952.383
iteration 0500: loss: 1949.527
iteration 0600: loss: 1950.629
iteration 0700: loss: 1952.093
iteration 0800: loss: 1949.936
iteration 0900: loss: 1949.738
====> Epoch: 023 Train loss: 1951.7326  took : 12.635434865951538
====> Test loss: 1953.5103
iteration 0000: loss: 1950.633
iteration 0100: loss: 1953.532
iteration 0200: loss: 1951.664
iteration 0300: loss: 1952.161
iteration 0400: loss: 1950.427
iteration 0500: loss: 1950.297
iteration 0600: loss: 1953.419
iteration 0700: loss: 1951.567
iteration 0800: loss: 1951.750
iteration 0900: loss: 1950.708
====> Epoch: 024 Train loss: 1951.6040  took : 12.723792314529419
====> Test loss: 1953.4249
iteration 0000: loss: 1951.308
iteration 0100: loss: 1952.345
iteration 0200: loss: 1952.076
iteration 0300: loss: 1950.301
iteration 0400: loss: 1950.746
iteration 0500: loss: 1951.255
iteration 0600: loss: 1952.499
iteration 0700: loss: 1953.295
iteration 0800: loss: 1952.970
iteration 0900: loss: 1949.764
====> Epoch: 025 Train loss: 1951.5934  took : 12.556468725204468
====> Test loss: 1953.1403
iteration 0000: loss: 1950.800
iteration 0100: loss: 1953.360
iteration 0200: loss: 1951.260
iteration 0300: loss: 1950.914
iteration 0400: loss: 1950.614
iteration 0500: loss: 1951.662
iteration 0600: loss: 1951.879
iteration 0700: loss: 1951.223
iteration 0800: loss: 1950.897
iteration 0900: loss: 1953.085
====> Epoch: 026 Train loss: 1951.5227  took : 12.150895833969116
====> Test loss: 1953.5513
iteration 0000: loss: 1950.480
iteration 0100: loss: 1951.675
iteration 0200: loss: 1954.448
iteration 0300: loss: 1950.839
iteration 0400: loss: 1950.803
iteration 0500: loss: 1952.829
iteration 0600: loss: 1950.623
iteration 0700: loss: 1950.910
iteration 0800: loss: 1950.018
iteration 0900: loss: 1952.094
====> Epoch: 027 Train loss: 1951.4493  took : 11.65284252166748
====> Test loss: 1953.4235
iteration 0000: loss: 1951.171
iteration 0100: loss: 1952.162
iteration 0200: loss: 1951.427
iteration 0300: loss: 1949.783
iteration 0400: loss: 1952.093
iteration 0500: loss: 1952.238
iteration 0600: loss: 1951.146
iteration 0700: loss: 1952.270
iteration 0800: loss: 1951.697
iteration 0900: loss: 1952.418
====> Epoch: 028 Train loss: 1951.4967  took : 12.633375644683838
====> Test loss: 1953.2789
iteration 0000: loss: 1951.609
iteration 0100: loss: 1951.289
iteration 0200: loss: 1950.804
iteration 0300: loss: 1950.775
iteration 0400: loss: 1952.500
iteration 0500: loss: 1951.941
iteration 0600: loss: 1951.353
iteration 0700: loss: 1950.453
iteration 0800: loss: 1951.033
iteration 0900: loss: 1950.546
====> Epoch: 029 Train loss: 1951.5007  took : 12.914013624191284
====> Test loss: 1953.4335
iteration 0000: loss: 1952.593
iteration 0100: loss: 1950.753
iteration 0200: loss: 1952.014
iteration 0300: loss: 1951.660
iteration 0400: loss: 1950.510
iteration 0500: loss: 1950.867
iteration 0600: loss: 1949.996
iteration 0700: loss: 1950.814
iteration 0800: loss: 1951.175
iteration 0900: loss: 1950.431
====> Epoch: 030 Train loss: 1951.3861  took : 12.80952763557434
====> Test loss: 1953.2117
iteration 0000: loss: 1951.678
iteration 0100: loss: 1949.388
iteration 0200: loss: 1950.131
iteration 0300: loss: 1953.036
iteration 0400: loss: 1950.324
iteration 0500: loss: 1951.256
iteration 0600: loss: 1952.027
iteration 0700: loss: 1953.091
iteration 0800: loss: 1951.971
iteration 0900: loss: 1950.764
====> Epoch: 031 Train loss: 1951.3718  took : 12.09950566291809
====> Test loss: 1953.1698
iteration 0000: loss: 1950.245
iteration 0100: loss: 1952.432
iteration 0200: loss: 1953.281
iteration 0300: loss: 1950.747
iteration 0400: loss: 1950.322
iteration 0500: loss: 1951.203
iteration 0600: loss: 1951.334
iteration 0700: loss: 1952.389
iteration 0800: loss: 1950.406
iteration 0900: loss: 1951.775
====> Epoch: 032 Train loss: 1951.3124  took : 12.337086915969849
====> Test loss: 1953.7027
iteration 0000: loss: 1951.247
iteration 0100: loss: 1952.470
iteration 0200: loss: 1951.341
iteration 0300: loss: 1953.887
iteration 0400: loss: 1950.956
iteration 0500: loss: 1951.360
iteration 0600: loss: 1952.090
iteration 0700: loss: 1952.374
iteration 0800: loss: 1950.137
iteration 0900: loss: 1950.970
====> Epoch: 033 Train loss: 1951.2031  took : 12.248063325881958
====> Test loss: 1952.7538
iteration 0000: loss: 1951.234
iteration 0100: loss: 1952.574
iteration 0200: loss: 1954.158
iteration 0300: loss: 1950.526
iteration 0400: loss: 1950.635
iteration 0500: loss: 1952.032
iteration 0600: loss: 1951.902
iteration 0700: loss: 1953.027
iteration 0800: loss: 1950.602
iteration 0900: loss: 1950.701
====> Epoch: 034 Train loss: 1951.3668  took : 13.41305923461914
====> Test loss: 1953.0374
iteration 0000: loss: 1950.200
iteration 0100: loss: 1951.135
iteration 0200: loss: 1951.359
iteration 0300: loss: 1952.403
iteration 0400: loss: 1950.850
iteration 0500: loss: 1951.171
iteration 0600: loss: 1954.052
iteration 0700: loss: 1953.347
iteration 0800: loss: 1951.484
iteration 0900: loss: 1951.513
====> Epoch: 035 Train loss: 1951.3027  took : 13.183100938796997
====> Test loss: 1952.9851
iteration 0000: loss: 1949.503
iteration 0100: loss: 1951.226
iteration 0200: loss: 1951.585
iteration 0300: loss: 1951.187
iteration 0400: loss: 1950.481
iteration 0500: loss: 1950.960
iteration 0600: loss: 1952.181
iteration 0700: loss: 1952.143
iteration 0800: loss: 1951.934
iteration 0900: loss: 1949.990
====> Epoch: 036 Train loss: 1951.0879  took : 12.946390151977539
====> Test loss: 1952.8079
iteration 0000: loss: 1951.448
iteration 0100: loss: 1949.859
iteration 0200: loss: 1951.595
iteration 0300: loss: 1952.158
iteration 0400: loss: 1950.866
iteration 0500: loss: 1952.141
iteration 0600: loss: 1951.463
iteration 0700: loss: 1951.241
iteration 0800: loss: 1951.027
iteration 0900: loss: 1952.273
====> Epoch: 037 Train loss: 1951.2738  took : 12.293459415435791
====> Test loss: 1953.1584
iteration 0000: loss: 1950.839
iteration 0100: loss: 1949.995
iteration 0200: loss: 1951.156
iteration 0300: loss: 1949.444
iteration 0400: loss: 1951.137
iteration 0500: loss: 1950.873
iteration 0600: loss: 1950.357
iteration 0700: loss: 1950.894
iteration 0800: loss: 1950.989
iteration 0900: loss: 1951.472
====> Epoch: 038 Train loss: 1951.1855  took : 13.178790807723999
====> Test loss: 1953.1646
iteration 0000: loss: 1950.739
iteration 0100: loss: 1951.347
iteration 0200: loss: 1950.368
iteration 0300: loss: 1950.528
iteration 0400: loss: 1950.744
iteration 0500: loss: 1951.250
iteration 0600: loss: 1951.242
iteration 0700: loss: 1952.090
iteration 0800: loss: 1950.787
iteration 0900: loss: 1951.511
====> Epoch: 039 Train loss: 1951.1850  took : 12.917537212371826
====> Test loss: 1953.1471
iteration 0000: loss: 1950.423
iteration 0100: loss: 1952.128
iteration 0200: loss: 1950.476
iteration 0300: loss: 1949.680
iteration 0400: loss: 1951.593
iteration 0500: loss: 1951.202
iteration 0600: loss: 1950.539
iteration 0700: loss: 1950.034
iteration 0800: loss: 1950.863
iteration 0900: loss: 1950.060
====> Epoch: 040 Train loss: 1951.0880  took : 12.135839462280273
====> Test loss: 1952.6953
iteration 0000: loss: 1952.488
iteration 0100: loss: 1949.395
iteration 0200: loss: 1950.039
iteration 0300: loss: 1951.026
iteration 0400: loss: 1951.930
iteration 0500: loss: 1950.025
iteration 0600: loss: 1950.895
iteration 0700: loss: 1951.121
iteration 0800: loss: 1950.581
iteration 0900: loss: 1952.087
====> Epoch: 041 Train loss: 1950.9300  took : 12.972427368164062
====> Test loss: 1952.7715
iteration 0000: loss: 1952.541
iteration 0100: loss: 1949.486
iteration 0200: loss: 1950.138
iteration 0300: loss: 1949.437
iteration 0400: loss: 1953.879
iteration 0500: loss: 1951.470
iteration 0600: loss: 1951.472
iteration 0700: loss: 1953.435
iteration 0800: loss: 1950.220
iteration 0900: loss: 1950.582
====> Epoch: 042 Train loss: 1951.1713  took : 12.165634870529175
====> Test loss: 1952.8162
iteration 0000: loss: 1952.086
iteration 0100: loss: 1950.889
iteration 0200: loss: 1951.701
iteration 0300: loss: 1948.906
iteration 0400: loss: 1951.426
iteration 0500: loss: 1951.885
iteration 0600: loss: 1951.956
iteration 0700: loss: 1950.327
iteration 0800: loss: 1949.005
iteration 0900: loss: 1951.634
====> Epoch: 043 Train loss: 1950.9988  took : 12.678319215774536
====> Test loss: 1952.8713
iteration 0000: loss: 1950.646
iteration 0100: loss: 1950.481
iteration 0200: loss: 1950.436
iteration 0300: loss: 1952.862
iteration 0400: loss: 1950.076
iteration 0500: loss: 1950.913
iteration 0600: loss: 1950.684
iteration 0700: loss: 1949.284
iteration 0800: loss: 1950.728
iteration 0900: loss: 1951.421
====> Epoch: 044 Train loss: 1950.9030  took : 12.501086950302124
====> Test loss: 1952.9708
iteration 0000: loss: 1950.494
iteration 0100: loss: 1952.270
iteration 0200: loss: 1951.934
iteration 0300: loss: 1950.426
iteration 0400: loss: 1951.457
iteration 0500: loss: 1950.740
iteration 0600: loss: 1951.737
iteration 0700: loss: 1953.413
iteration 0800: loss: 1950.439
iteration 0900: loss: 1951.801
====> Epoch: 045 Train loss: 1951.1047  took : 12.087576150894165
====> Test loss: 1953.0449
iteration 0000: loss: 1950.431
iteration 0100: loss: 1950.478
iteration 0200: loss: 1951.615
iteration 0300: loss: 1950.453
iteration 0400: loss: 1950.138
iteration 0500: loss: 1950.906
iteration 0600: loss: 1951.849
iteration 0700: loss: 1951.647
iteration 0800: loss: 1950.622
iteration 0900: loss: 1951.275
====> Epoch: 046 Train loss: 1950.9621  took : 13.538802146911621
====> Test loss: 1952.6194
iteration 0000: loss: 1950.321
iteration 0100: loss: 1951.894
iteration 0200: loss: 1951.411
iteration 0300: loss: 1951.584
iteration 0400: loss: 1952.439
iteration 0500: loss: 1951.545
iteration 0600: loss: 1950.278
iteration 0700: loss: 1952.888
iteration 0800: loss: 1950.301
iteration 0900: loss: 1950.174
====> Epoch: 047 Train loss: 1950.9595  took : 12.060811281204224
====> Test loss: 1953.1853
iteration 0000: loss: 1951.537
iteration 0100: loss: 1952.315
iteration 0200: loss: 1951.452
iteration 0300: loss: 1950.288
iteration 0400: loss: 1951.161
iteration 0500: loss: 1952.065
iteration 0600: loss: 1950.265
iteration 0700: loss: 1950.870
iteration 0800: loss: 1952.752
iteration 0900: loss: 1952.014
====> Epoch: 048 Train loss: 1951.0127  took : 12.624117374420166
====> Test loss: 1952.9805
iteration 0000: loss: 1950.993
iteration 0100: loss: 1950.920
iteration 0200: loss: 1950.742
iteration 0300: loss: 1951.803
iteration 0400: loss: 1950.625
iteration 0500: loss: 1950.332
iteration 0600: loss: 1952.364
iteration 0700: loss: 1951.612
iteration 0800: loss: 1950.761
iteration 0900: loss: 1951.175
====> Epoch: 049 Train loss: 1950.8796  took : 12.87365460395813
====> Test loss: 1952.4093
iteration 0000: loss: 1949.526
iteration 0100: loss: 1951.093
iteration 0200: loss: 1951.204
iteration 0300: loss: 1950.150
iteration 0400: loss: 1951.089
iteration 0500: loss: 1951.370
iteration 0600: loss: 1951.089
iteration 0700: loss: 1950.800
iteration 0800: loss: 1951.093
iteration 0900: loss: 1952.674
====> Epoch: 050 Train loss: 1950.8511  took : 12.313308954238892
====> Test loss: 1952.7162
====> [MM-VAE] Time: 698.290s or 00:11:38
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  5
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_5
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_5
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5222.812
iteration 0100: loss: 4116.376
iteration 0200: loss: 4082.857
iteration 0300: loss: 4045.271
iteration 0400: loss: 4022.672
iteration 0500: loss: 4028.235
iteration 0600: loss: 4008.415
iteration 0700: loss: 4019.311
iteration 0800: loss: 4014.392
iteration 0900: loss: 4008.952
iteration 1000: loss: 4022.585
iteration 1100: loss: 4006.933
iteration 1200: loss: 4002.486
iteration 1300: loss: 4012.902
iteration 1400: loss: 4002.290
iteration 1500: loss: 4001.158
iteration 1600: loss: 3982.076
iteration 1700: loss: 4000.915
iteration 1800: loss: 4000.433
====> Epoch: 001 Train loss: 4026.6462  took : 53.02820038795471
====> Test loss: 3994.5100
iteration 0000: loss: 3990.511
iteration 0100: loss: 3997.109
iteration 0200: loss: 3992.350
iteration 0300: loss: 3994.665
iteration 0400: loss: 3990.293
iteration 0500: loss: 3988.819
iteration 0600: loss: 3982.607
iteration 0700: loss: 3986.953
iteration 0800: loss: 3986.742
iteration 0900: loss: 3971.939
iteration 1000: loss: 3974.059
iteration 1100: loss: 3982.649
iteration 1200: loss: 3971.605
iteration 1300: loss: 3965.424
iteration 1400: loss: 3996.732
iteration 1500: loss: 3973.606
iteration 1600: loss: 3984.737
iteration 1700: loss: 3990.004
iteration 1800: loss: 3981.915
====> Epoch: 002 Train loss: 3983.2115  took : 53.26769161224365
====> Test loss: 3982.6295
iteration 0000: loss: 3971.203
iteration 0100: loss: 3965.772
iteration 0200: loss: 3971.414
iteration 0300: loss: 3977.433
iteration 0400: loss: 3969.730
iteration 0500: loss: 3991.140
iteration 0600: loss: 3975.245
iteration 0700: loss: 3976.445
iteration 0800: loss: 3965.733
iteration 0900: loss: 3982.674
iteration 1000: loss: 3973.816
iteration 1100: loss: 3981.286
iteration 1200: loss: 3966.594
iteration 1300: loss: 3958.930
iteration 1400: loss: 3976.991
iteration 1500: loss: 3979.597
iteration 1600: loss: 3972.453
iteration 1700: loss: 3984.371
iteration 1800: loss: 3972.850
====> Epoch: 003 Train loss: 3975.7701  took : 53.12944173812866
====> Test loss: 3978.6925
iteration 0000: loss: 3973.486
iteration 0100: loss: 3962.765
iteration 0200: loss: 3974.293
iteration 0300: loss: 3975.495
iteration 0400: loss: 3975.920
iteration 0500: loss: 3971.983
iteration 0600: loss: 3983.645
iteration 0700: loss: 3989.562
iteration 0800: loss: 3968.270
iteration 0900: loss: 3971.599
iteration 1000: loss: 3973.055
iteration 1100: loss: 3962.228
iteration 1200: loss: 3975.685
iteration 1300: loss: 3974.031
iteration 1400: loss: 3961.170
iteration 1500: loss: 3969.906
iteration 1600: loss: 3967.358
iteration 1700: loss: 3967.286
iteration 1800: loss: 3966.999
====> Epoch: 004 Train loss: 3972.3463  took : 52.87161564826965
====> Test loss: 3977.3092
iteration 0000: loss: 3957.409
iteration 0100: loss: 3963.736
iteration 0200: loss: 3964.362
iteration 0300: loss: 3959.949
iteration 0400: loss: 3967.014
iteration 0500: loss: 3974.858
iteration 0600: loss: 3965.180
iteration 0700: loss: 3967.663
iteration 0800: loss: 3970.956
iteration 0900: loss: 3969.277
iteration 1000: loss: 3973.780
iteration 1100: loss: 3958.232
iteration 1200: loss: 3961.595
iteration 1300: loss: 3970.269
iteration 1400: loss: 3969.230
iteration 1500: loss: 3961.933
iteration 1600: loss: 3965.248
iteration 1700: loss: 3965.978
iteration 1800: loss: 3974.533
====> Epoch: 005 Train loss: 3970.3419  took : 53.12590265274048
====> Test loss: 3974.6645
iteration 0000: loss: 3960.615
iteration 0100: loss: 3964.787
iteration 0200: loss: 3971.741
iteration 0300: loss: 3960.714
iteration 0400: loss: 3980.829
iteration 0500: loss: 3969.863
iteration 0600: loss: 3962.347
iteration 0700: loss: 3961.598
iteration 0800: loss: 3964.961
iteration 0900: loss: 3983.189
iteration 1000: loss: 3962.336
iteration 1100: loss: 3978.957
iteration 1200: loss: 3968.729
iteration 1300: loss: 3974.050
iteration 1400: loss: 3964.594
iteration 1500: loss: 3968.289
iteration 1600: loss: 3970.644
iteration 1700: loss: 3978.646
iteration 1800: loss: 3967.892
====> Epoch: 006 Train loss: 3968.2935  took : 53.2209415435791
====> Test loss: 3973.5769
iteration 0000: loss: 3961.558
iteration 0100: loss: 3969.960
iteration 0200: loss: 3963.582
iteration 0300: loss: 3966.345
iteration 0400: loss: 3970.120
iteration 0500: loss: 3961.073
iteration 0600: loss: 3976.982
iteration 0700: loss: 3969.680
iteration 0800: loss: 3963.115
iteration 0900: loss: 3965.669
iteration 1000: loss: 3966.574
iteration 1100: loss: 3957.926
iteration 1200: loss: 3969.265
iteration 1300: loss: 3967.771
iteration 1400: loss: 3976.273
iteration 1500: loss: 3962.857
iteration 1600: loss: 3965.307
iteration 1700: loss: 3967.515
iteration 1800: loss: 3957.646
====> Epoch: 007 Train loss: 3967.2272  took : 52.960033655166626
====> Test loss: 3972.2068
iteration 0000: loss: 3963.817
iteration 0100: loss: 3971.217
iteration 0200: loss: 3966.625
iteration 0300: loss: 3956.506
iteration 0400: loss: 3961.924
iteration 0500: loss: 3964.944
iteration 0600: loss: 3964.987
iteration 0700: loss: 3964.010
iteration 0800: loss: 3967.335
iteration 0900: loss: 3970.732
iteration 1000: loss: 3962.796
iteration 1100: loss: 3964.171
iteration 1200: loss: 3971.165
iteration 1300: loss: 3961.718
iteration 1400: loss: 3963.460
iteration 1500: loss: 3965.755
iteration 1600: loss: 3968.638
iteration 1700: loss: 3968.049
iteration 1800: loss: 3970.346
====> Epoch: 008 Train loss: 3966.0810  took : 53.08514213562012
====> Test loss: 3970.9403
iteration 0000: loss: 3969.974
iteration 0100: loss: 3969.542
iteration 0200: loss: 3962.871
iteration 0300: loss: 3968.310
iteration 0400: loss: 3962.160
iteration 0500: loss: 3973.990
iteration 0600: loss: 3968.656
iteration 0700: loss: 3969.600
iteration 0800: loss: 3966.463
iteration 0900: loss: 3979.939
iteration 1000: loss: 3964.448
iteration 1100: loss: 3967.585
iteration 1200: loss: 3966.679
iteration 1300: loss: 3966.515
iteration 1400: loss: 3962.828
iteration 1500: loss: 3967.137
iteration 1600: loss: 3972.295
iteration 1700: loss: 3954.583
iteration 1800: loss: 3965.938
====> Epoch: 009 Train loss: 3965.5134  took : 52.835623025894165
====> Test loss: 3970.0584
iteration 0000: loss: 3965.902
iteration 0100: loss: 3969.704
iteration 0200: loss: 3963.527
iteration 0300: loss: 3955.972
iteration 0400: loss: 3958.275
iteration 0500: loss: 3959.908
iteration 0600: loss: 3958.433
iteration 0700: loss: 3963.136
iteration 0800: loss: 3962.699
iteration 0900: loss: 3958.465
iteration 1000: loss: 3967.718
iteration 1100: loss: 3962.102
iteration 1200: loss: 3956.841
iteration 1300: loss: 3972.946
iteration 1400: loss: 3966.532
iteration 1500: loss: 3960.244
iteration 1600: loss: 3963.586
iteration 1700: loss: 3959.974
iteration 1800: loss: 3970.990
====> Epoch: 010 Train loss: 3964.2521  took : 53.170448303222656
====> Test loss: 3969.5848
iteration 0000: loss: 3964.801
iteration 0100: loss: 3964.257
iteration 0200: loss: 3971.278
iteration 0300: loss: 3954.168
iteration 0400: loss: 3964.309
iteration 0500: loss: 3965.257
iteration 0600: loss: 3963.233
iteration 0700: loss: 3971.959
iteration 0800: loss: 3956.524
iteration 0900: loss: 3958.946
iteration 1000: loss: 3962.450
iteration 1100: loss: 3969.054
iteration 1200: loss: 3960.352
iteration 1300: loss: 3968.979
iteration 1400: loss: 3956.908
iteration 1500: loss: 3960.141
iteration 1600: loss: 3971.356
iteration 1700: loss: 3957.518
iteration 1800: loss: 3962.295
====> Epoch: 011 Train loss: 3963.7673  took : 53.126206398010254
====> Test loss: 3969.8081
iteration 0000: loss: 3961.074
iteration 0100: loss: 3958.299
iteration 0200: loss: 3964.164
iteration 0300: loss: 3965.298
iteration 0400: loss: 3970.568
iteration 0500: loss: 3951.256
iteration 0600: loss: 3958.533
iteration 0700: loss: 3958.571
iteration 0800: loss: 3964.480
iteration 0900: loss: 3957.479
iteration 1000: loss: 3952.541
iteration 1100: loss: 3962.364
iteration 1200: loss: 3969.551
iteration 1300: loss: 3961.102
iteration 1400: loss: 3965.339
iteration 1500: loss: 3965.050
iteration 1600: loss: 3973.021
iteration 1700: loss: 3952.225
iteration 1800: loss: 3964.193
====> Epoch: 012 Train loss: 3963.1596  took : 53.036426067352295
====> Test loss: 3969.0113
iteration 0000: loss: 3966.707
iteration 0100: loss: 3959.781
iteration 0200: loss: 3958.901
iteration 0300: loss: 3970.532
iteration 0400: loss: 3958.616
iteration 0500: loss: 3971.762
iteration 0600: loss: 3961.311
iteration 0700: loss: 3959.939
iteration 0800: loss: 3956.729
iteration 0900: loss: 3967.260
iteration 1000: loss: 3967.447
iteration 1100: loss: 3964.175
iteration 1200: loss: 3964.763
iteration 1300: loss: 3965.902
iteration 1400: loss: 3966.170
iteration 1500: loss: 3960.903
iteration 1600: loss: 3966.333
iteration 1700: loss: 3954.517
iteration 1800: loss: 3963.658
====> Epoch: 013 Train loss: 3962.8801  took : 52.83534288406372
====> Test loss: 3969.0643
iteration 0000: loss: 3964.891
iteration 0100: loss: 3968.692
iteration 0200: loss: 3954.912
iteration 0300: loss: 3958.722
iteration 0400: loss: 3963.835
iteration 0500: loss: 3958.011
iteration 0600: loss: 3966.122
iteration 0700: loss: 3966.402
iteration 0800: loss: 3960.851
iteration 0900: loss: 3967.266
iteration 1000: loss: 3962.276
iteration 1100: loss: 3956.112
iteration 1200: loss: 3960.946
iteration 1300: loss: 3955.871
iteration 1400: loss: 3957.219
iteration 1500: loss: 3954.316
iteration 1600: loss: 3965.874
iteration 1700: loss: 3959.946
iteration 1800: loss: 3963.896
====> Epoch: 014 Train loss: 3962.2944  took : 53.041553020477295
====> Test loss: 3968.5460
iteration 0000: loss: 3959.741
iteration 0100: loss: 3960.344
iteration 0200: loss: 3960.878
iteration 0300: loss: 3960.396
iteration 0400: loss: 3966.167
iteration 0500: loss: 3965.333
iteration 0600: loss: 3958.584
iteration 0700: loss: 3956.220
iteration 0800: loss: 3966.433
iteration 0900: loss: 3954.328
iteration 1000: loss: 3956.262
iteration 1100: loss: 3958.425
iteration 1200: loss: 3959.977
iteration 1300: loss: 3954.426
iteration 1400: loss: 3960.975
iteration 1500: loss: 3956.371
iteration 1600: loss: 3958.378
iteration 1700: loss: 3959.778
iteration 1800: loss: 3970.313
====> Epoch: 015 Train loss: 3961.8273  took : 52.91516041755676
====> Test loss: 3968.4629
iteration 0000: loss: 3966.989
iteration 0100: loss: 3965.877
iteration 0200: loss: 3961.073
iteration 0300: loss: 3970.590
iteration 0400: loss: 3956.021
iteration 0500: loss: 3966.544
iteration 0600: loss: 3969.304
iteration 0700: loss: 3968.491
iteration 0800: loss: 3960.833
iteration 0900: loss: 3975.527
iteration 1000: loss: 3952.701
iteration 1100: loss: 3962.483
iteration 1200: loss: 3962.322
iteration 1300: loss: 3967.826
iteration 1400: loss: 3956.044
iteration 1500: loss: 3958.422
iteration 1600: loss: 3961.017
iteration 1700: loss: 3967.288
iteration 1800: loss: 3960.125
====> Epoch: 016 Train loss: 3961.6306  took : 53.224926233291626
====> Test loss: 3968.1437
iteration 0000: loss: 3961.072
iteration 0100: loss: 3955.441
iteration 0200: loss: 3962.294
iteration 0300: loss: 3956.397
iteration 0400: loss: 3963.789
iteration 0500: loss: 3967.243
iteration 0600: loss: 3964.298
iteration 0700: loss: 3971.734
iteration 0800: loss: 3960.781
iteration 0900: loss: 3952.870
iteration 1000: loss: 3962.284
iteration 1100: loss: 3958.386
iteration 1200: loss: 3964.956
iteration 1300: loss: 3959.123
iteration 1400: loss: 3968.202
iteration 1500: loss: 3953.582
iteration 1600: loss: 3959.677
iteration 1700: loss: 3948.833
iteration 1800: loss: 3953.105
====> Epoch: 017 Train loss: 3961.6982  took : 52.92689800262451
====> Test loss: 3967.5185
iteration 0000: loss: 3963.476
iteration 0100: loss: 3962.866
iteration 0200: loss: 3953.459
iteration 0300: loss: 3968.055
iteration 0400: loss: 3958.060
iteration 0500: loss: 3957.077
iteration 0600: loss: 3961.345
iteration 0700: loss: 3963.198
iteration 0800: loss: 3957.144
iteration 0900: loss: 3961.606
iteration 1000: loss: 3953.446
iteration 1100: loss: 3951.777
iteration 1200: loss: 3961.610
iteration 1300: loss: 3964.664
iteration 1400: loss: 3964.362
iteration 1500: loss: 3962.314
iteration 1600: loss: 3965.182
iteration 1700: loss: 3953.266
iteration 1800: loss: 3959.614
====> Epoch: 018 Train loss: 3961.0740  took : 52.92676091194153
====> Test loss: 3968.1863
iteration 0000: loss: 3962.617
iteration 0100: loss: 3962.751
iteration 0200: loss: 3951.758
iteration 0300: loss: 3954.216
iteration 0400: loss: 3970.163
iteration 0500: loss: 3967.055
iteration 0600: loss: 3956.396
iteration 0700: loss: 3954.195
iteration 0800: loss: 3956.916
iteration 0900: loss: 3963.116
iteration 1000: loss: 3954.413
iteration 1100: loss: 3957.346
iteration 1200: loss: 3964.812
iteration 1300: loss: 3956.062
iteration 1400: loss: 3960.970
iteration 1500: loss: 3965.497
iteration 1600: loss: 3969.620
iteration 1700: loss: 3959.166
iteration 1800: loss: 3970.879
====> Epoch: 019 Train loss: 3961.0880  took : 53.05648756027222
====> Test loss: 3968.0779
iteration 0000: loss: 3965.455
iteration 0100: loss: 3965.595
iteration 0200: loss: 3962.049
iteration 0300: loss: 3966.639
iteration 0400: loss: 3952.027
iteration 0500: loss: 3962.394
iteration 0600: loss: 3963.905
iteration 0700: loss: 3965.087
iteration 0800: loss: 3968.149
iteration 0900: loss: 3955.854
iteration 1000: loss: 3963.803
iteration 1100: loss: 3964.088
iteration 1200: loss: 3958.893
iteration 1300: loss: 3968.302
iteration 1400: loss: 3957.017
iteration 1500: loss: 3963.090
iteration 1600: loss: 3959.667
iteration 1700: loss: 3959.714
iteration 1800: loss: 3954.300
====> Epoch: 020 Train loss: 3960.7997  took : 52.782161235809326
====> Test loss: 3967.8409
iteration 0000: loss: 3959.865
iteration 0100: loss: 3957.704
iteration 0200: loss: 3971.588
iteration 0300: loss: 3952.165
iteration 0400: loss: 3962.682
iteration 0500: loss: 3965.045
iteration 0600: loss: 3954.335
iteration 0700: loss: 3957.525
iteration 0800: loss: 3960.972
iteration 0900: loss: 3964.273
iteration 1000: loss: 3965.908
iteration 1100: loss: 3963.543
iteration 1200: loss: 3961.064
iteration 1300: loss: 3953.845
iteration 1400: loss: 3952.165
iteration 1500: loss: 3960.215
iteration 1600: loss: 3961.089
iteration 1700: loss: 3956.399
iteration 1800: loss: 3959.117
====> Epoch: 021 Train loss: 3960.6531  took : 53.07245063781738
====> Test loss: 3967.0756
iteration 0000: loss: 3962.734
iteration 0100: loss: 3955.881
iteration 0200: loss: 3960.015
iteration 0300: loss: 3957.179
iteration 0400: loss: 3948.062
iteration 0500: loss: 3958.733
iteration 0600: loss: 3964.589
iteration 0700: loss: 3967.167
iteration 0800: loss: 3965.229
iteration 0900: loss: 3953.069
iteration 1000: loss: 3965.460
iteration 1100: loss: 3964.533
iteration 1200: loss: 3963.904
iteration 1300: loss: 3958.164
iteration 1400: loss: 3958.847
iteration 1500: loss: 3963.023
iteration 1600: loss: 3958.909
iteration 1700: loss: 3960.056
iteration 1800: loss: 3957.650
====> Epoch: 022 Train loss: 3960.1936  took : 53.07149696350098
====> Test loss: 3966.7539
iteration 0000: loss: 3955.385
iteration 0100: loss: 3967.299
iteration 0200: loss: 3957.158
iteration 0300: loss: 3959.945
iteration 0400: loss: 3959.869
iteration 0500: loss: 3967.287
iteration 0600: loss: 3964.523
iteration 0700: loss: 3961.902
iteration 0800: loss: 3949.603
iteration 0900: loss: 3962.000
iteration 1000: loss: 3959.733
iteration 1100: loss: 3955.853
iteration 1200: loss: 3959.566
iteration 1300: loss: 3963.294
iteration 1400: loss: 3955.784
iteration 1500: loss: 3963.512
iteration 1600: loss: 3959.380
iteration 1700: loss: 3963.456
iteration 1800: loss: 3952.386
====> Epoch: 023 Train loss: 3959.8802  took : 53.160205602645874
====> Test loss: 3966.1119
iteration 0000: loss: 3959.368
iteration 0100: loss: 3960.286
iteration 0200: loss: 3966.820
iteration 0300: loss: 3962.193
iteration 0400: loss: 3969.440
iteration 0500: loss: 3956.036
iteration 0600: loss: 3951.803
iteration 0700: loss: 3958.490
iteration 0800: loss: 3963.465
iteration 0900: loss: 3973.921
iteration 1000: loss: 3967.099
iteration 1100: loss: 3956.049
iteration 1200: loss: 3969.909
iteration 1300: loss: 3963.947
iteration 1400: loss: 3964.634
iteration 1500: loss: 3964.578
iteration 1600: loss: 3963.034
iteration 1700: loss: 3967.795
iteration 1800: loss: 3961.308
====> Epoch: 024 Train loss: 3960.3594  took : 53.1886773109436
====> Test loss: 3966.9194
iteration 0000: loss: 3962.297
iteration 0100: loss: 3958.402
iteration 0200: loss: 3967.611
iteration 0300: loss: 3957.116
iteration 0400: loss: 3954.936
iteration 0500: loss: 3959.411
iteration 0600: loss: 3958.869
iteration 0700: loss: 3957.391
iteration 0800: loss: 3958.871
iteration 0900: loss: 3951.192
iteration 1000: loss: 3969.395
iteration 1100: loss: 3949.183
iteration 1200: loss: 3965.774
iteration 1300: loss: 3948.015
iteration 1400: loss: 3956.776
iteration 1500: loss: 3960.066
iteration 1600: loss: 3967.924
iteration 1700: loss: 3953.341
iteration 1800: loss: 3950.371
====> Epoch: 025 Train loss: 3959.6430  took : 52.95161986351013
====> Test loss: 3967.0935
iteration 0000: loss: 3954.717
iteration 0100: loss: 3958.580
iteration 0200: loss: 3962.160
iteration 0300: loss: 3956.657
iteration 0400: loss: 3960.587
iteration 0500: loss: 3961.833
iteration 0600: loss: 3962.462
iteration 0700: loss: 3965.692
iteration 0800: loss: 3958.456
iteration 0900: loss: 3957.074
iteration 1000: loss: 3957.966
iteration 1100: loss: 3968.884
iteration 1200: loss: 3966.562
iteration 1300: loss: 3963.422
iteration 1400: loss: 3953.799
iteration 1500: loss: 3951.638
iteration 1600: loss: 3959.135
iteration 1700: loss: 3963.646
iteration 1800: loss: 3955.604
====> Epoch: 026 Train loss: 3959.5278  took : 52.949925661087036
====> Test loss: 3966.7997
iteration 0000: loss: 3964.767
iteration 0100: loss: 3965.086
iteration 0200: loss: 3967.280
iteration 0300: loss: 3960.092
iteration 0400: loss: 3964.175
iteration 0500: loss: 3960.114
iteration 0600: loss: 3955.638
iteration 0700: loss: 3956.752
iteration 0800: loss: 3962.040
iteration 0900: loss: 3954.043
iteration 1000: loss: 3958.388
iteration 1100: loss: 3957.996
iteration 1200: loss: 3962.986
iteration 1300: loss: 3964.947
iteration 1400: loss: 3969.042
iteration 1500: loss: 3956.568
iteration 1600: loss: 3949.911
iteration 1700: loss: 3960.202
iteration 1800: loss: 3957.385
====> Epoch: 027 Train loss: 3960.5356  took : 52.93849563598633
====> Test loss: 3967.6582
iteration 0000: loss: 3971.188
iteration 0100: loss: 3955.638
iteration 0200: loss: 3958.021
iteration 0300: loss: 3968.488
iteration 0400: loss: 3956.049
iteration 0500: loss: 3968.192
iteration 0600: loss: 3963.393
iteration 0700: loss: 3964.048
iteration 0800: loss: 3966.427
iteration 0900: loss: 3960.800
iteration 1000: loss: 3957.873
iteration 1100: loss: 3966.629
iteration 1200: loss: 3961.976
iteration 1300: loss: 3959.760
iteration 1400: loss: 3960.307
iteration 1500: loss: 3959.554
iteration 1600: loss: 3956.939
iteration 1700: loss: 3963.578
iteration 1800: loss: 3963.763
====> Epoch: 028 Train loss: 3960.2643  took : 53.067676305770874
====> Test loss: 3967.0638
iteration 0000: loss: 3950.831
iteration 0100: loss: 3956.031
iteration 0200: loss: 3958.168
iteration 0300: loss: 3963.991
iteration 0400: loss: 3954.079
iteration 0500: loss: 3949.094
iteration 0600: loss: 3957.512
iteration 0700: loss: 3956.379
iteration 0800: loss: 3958.494
iteration 0900: loss: 3968.496
iteration 1000: loss: 3965.593
iteration 1100: loss: 3965.360
iteration 1200: loss: 3957.001
iteration 1300: loss: 3952.476
iteration 1400: loss: 3955.751
iteration 1500: loss: 3961.492
iteration 1600: loss: 3951.960
iteration 1700: loss: 3955.101
iteration 1800: loss: 3962.338
====> Epoch: 029 Train loss: 3960.2067  took : 53.13014554977417
====> Test loss: 3966.9305
iteration 0000: loss: 3955.833
iteration 0100: loss: 3957.862
iteration 0200: loss: 3962.211
iteration 0300: loss: 3952.003
iteration 0400: loss: 3956.014
iteration 0500: loss: 3960.921
iteration 0600: loss: 3967.542
iteration 0700: loss: 3966.398
iteration 0800: loss: 3954.915
iteration 0900: loss: 3970.875
iteration 1000: loss: 3955.416
iteration 1100: loss: 3962.604
iteration 1200: loss: 3960.080
iteration 1300: loss: 3961.170
iteration 1400: loss: 3960.984
iteration 1500: loss: 3954.539
iteration 1600: loss: 3966.663
iteration 1700: loss: 3955.172
iteration 1800: loss: 3948.364
====> Epoch: 030 Train loss: 3959.1381  took : 52.93978023529053
====> Test loss: 3966.2665
iteration 0000: loss: 3958.460
iteration 0100: loss: 3953.136
iteration 0200: loss: 3956.753
iteration 0300: loss: 3953.068
iteration 0400: loss: 3960.370
iteration 0500: loss: 3952.872
iteration 0600: loss: 3970.386
iteration 0700: loss: 3960.771
iteration 0800: loss: 3959.336
iteration 0900: loss: 3961.330
iteration 1000: loss: 3963.657
iteration 1100: loss: 3964.741
iteration 1200: loss: 3958.576
iteration 1300: loss: 3954.796
iteration 1400: loss: 3960.624
iteration 1500: loss: 3963.160
iteration 1600: loss: 3954.605
iteration 1700: loss: 3965.802
iteration 1800: loss: 3967.720
====> Epoch: 031 Train loss: 3959.2750  took : 53.071850299835205
====> Test loss: 3966.8846
iteration 0000: loss: 3953.768
iteration 0100: loss: 3966.828
iteration 0200: loss: 3952.119
iteration 0300: loss: 3954.885
iteration 0400: loss: 3964.033
iteration 0500: loss: 3962.189
iteration 0600: loss: 3960.902
iteration 0700: loss: 3951.673
iteration 0800: loss: 3950.516
iteration 0900: loss: 3961.447
iteration 1000: loss: 3963.834
iteration 1100: loss: 3947.197
iteration 1200: loss: 3953.799
iteration 1300: loss: 3959.461
iteration 1400: loss: 3959.318
iteration 1500: loss: 3955.607
iteration 1600: loss: 3951.752
iteration 1700: loss: 3956.520
iteration 1800: loss: 3954.415
====> Epoch: 032 Train loss: 3958.7760  took : 52.918267011642456
====> Test loss: 3965.9446
iteration 0000: loss: 3954.214
iteration 0100: loss: 3959.396
iteration 0200: loss: 3961.727
iteration 0300: loss: 3958.202
iteration 0400: loss: 3962.571
iteration 0500: loss: 3960.327
iteration 0600: loss: 3963.709
iteration 0700: loss: 3959.098
iteration 0800: loss: 3959.866
iteration 0900: loss: 3968.815
iteration 1000: loss: 3949.971
iteration 1100: loss: 3963.235
iteration 1200: loss: 3954.176
iteration 1300: loss: 3956.252
iteration 1400: loss: 3955.044
iteration 1500: loss: 3949.949
iteration 1600: loss: 3960.282
iteration 1700: loss: 3961.583
iteration 1800: loss: 3962.576
====> Epoch: 033 Train loss: 3959.3195  took : 52.96393609046936
====> Test loss: 3967.0463
iteration 0000: loss: 3953.473
iteration 0100: loss: 3961.160
iteration 0200: loss: 3960.168
iteration 0300: loss: 3958.025
iteration 0400: loss: 3961.568
iteration 0500: loss: 3967.551
iteration 0600: loss: 3950.235
iteration 0700: loss: 3961.234
iteration 0800: loss: 3966.534
iteration 0900: loss: 3951.656
iteration 1000: loss: 3954.877
iteration 1100: loss: 3951.802
iteration 1200: loss: 3956.150
iteration 1300: loss: 3954.418
iteration 1400: loss: 3959.207
iteration 1500: loss: 3957.584
iteration 1600: loss: 3949.324
iteration 1700: loss: 3960.063
iteration 1800: loss: 3955.257
====> Epoch: 034 Train loss: 3958.7938  took : 52.9721200466156
====> Test loss: 3966.1376
iteration 0000: loss: 3961.301
iteration 0100: loss: 3958.045
iteration 0200: loss: 3956.931
iteration 0300: loss: 3957.886
iteration 0400: loss: 3952.401
iteration 0500: loss: 3952.635
iteration 0600: loss: 3956.907
iteration 0700: loss: 3958.965
iteration 0800: loss: 3963.194
iteration 0900: loss: 3957.922
iteration 1000: loss: 3954.503
iteration 1100: loss: 3963.594
iteration 1200: loss: 3957.520
iteration 1300: loss: 3951.066
iteration 1400: loss: 3961.968
iteration 1500: loss: 3959.595
iteration 1600: loss: 3950.506
iteration 1700: loss: 3955.533
iteration 1800: loss: 3960.109
====> Epoch: 035 Train loss: 3958.2996  took : 52.83365750312805
====> Test loss: 3966.0043
iteration 0000: loss: 3956.435
iteration 0100: loss: 3956.903
iteration 0200: loss: 3957.528
iteration 0300: loss: 3959.086
iteration 0400: loss: 3955.295
iteration 0500: loss: 3958.439
iteration 0600: loss: 3963.632
iteration 0700: loss: 3965.501
iteration 0800: loss: 3956.121
iteration 0900: loss: 3956.729
iteration 1000: loss: 3952.385
iteration 1100: loss: 3951.768
iteration 1200: loss: 3960.793
iteration 1300: loss: 3956.408
iteration 1400: loss: 3956.093
iteration 1500: loss: 3947.508
iteration 1600: loss: 3952.917
iteration 1700: loss: 3957.002
iteration 1800: loss: 3953.184
====> Epoch: 036 Train loss: 3958.5558  took : 53.140225410461426
====> Test loss: 3966.2613
iteration 0000: loss: 3964.126
iteration 0100: loss: 3960.194
iteration 0200: loss: 3952.324
iteration 0300: loss: 3956.885
iteration 0400: loss: 3963.724
iteration 0500: loss: 3956.680
iteration 0600: loss: 3961.159
iteration 0700: loss: 3955.755
iteration 0800: loss: 3959.620
iteration 0900: loss: 3962.847
iteration 1000: loss: 3960.263
iteration 1100: loss: 3960.677
iteration 1200: loss: 3959.910
iteration 1300: loss: 3950.993
iteration 1400: loss: 3960.172
iteration 1500: loss: 3957.683
iteration 1600: loss: 3956.063
iteration 1700: loss: 3950.541
iteration 1800: loss: 3961.859
====> Epoch: 037 Train loss: 3958.2826  took : 52.67245411872864
====> Test loss: 3966.0058
iteration 0000: loss: 3948.984
iteration 0100: loss: 3952.838
iteration 0200: loss: 3955.821
iteration 0300: loss: 3957.765
iteration 0400: loss: 3961.824
iteration 0500: loss: 3957.772
iteration 0600: loss: 3947.436
iteration 0700: loss: 3959.747
iteration 0800: loss: 3954.219
iteration 0900: loss: 3956.918
iteration 1000: loss: 3958.419
iteration 1100: loss: 3960.359
iteration 1200: loss: 3951.205
iteration 1300: loss: 3957.985
iteration 1400: loss: 3961.181
iteration 1500: loss: 3961.788
iteration 1600: loss: 3955.505
iteration 1700: loss: 3957.034
iteration 1800: loss: 3960.701
====> Epoch: 038 Train loss: 3957.7500  took : 53.110360622406006
====> Test loss: 3966.3194
iteration 0000: loss: 3955.466
iteration 0100: loss: 3957.884
iteration 0200: loss: 3957.972
iteration 0300: loss: 3961.266
iteration 0400: loss: 3955.853
iteration 0500: loss: 3964.039
iteration 0600: loss: 3959.955
iteration 0700: loss: 3967.085
iteration 0800: loss: 3958.124
iteration 0900: loss: 3960.809
iteration 1000: loss: 3956.487
iteration 1100: loss: 3948.643
iteration 1200: loss: 3954.911
iteration 1300: loss: 3964.688
iteration 1400: loss: 3960.923
iteration 1500: loss: 3961.555
iteration 1600: loss: 3957.755
iteration 1700: loss: 3963.301
iteration 1800: loss: 3959.730
====> Epoch: 039 Train loss: 3958.7411  took : 53.07760953903198
====> Test loss: 3966.3709
iteration 0000: loss: 3963.701
iteration 0100: loss: 3954.826
iteration 0200: loss: 3955.782
iteration 0300: loss: 3950.791
iteration 0400: loss: 3956.252
iteration 0500: loss: 3956.773
iteration 0600: loss: 3962.068
iteration 0700: loss: 3959.330
iteration 0800: loss: 3954.694
iteration 0900: loss: 3956.316
iteration 1000: loss: 3960.144
iteration 1100: loss: 3959.870
iteration 1200: loss: 3955.670
iteration 1300: loss: 3961.910
iteration 1400: loss: 3956.747
iteration 1500: loss: 3946.801
iteration 1600: loss: 3958.192
iteration 1700: loss: 3963.973
iteration 1800: loss: 3958.634
====> Epoch: 040 Train loss: 3958.3012  took : 52.931098222732544
====> Test loss: 3966.0209
iteration 0000: loss: 3953.893
iteration 0100: loss: 3947.872
iteration 0200: loss: 3953.461
iteration 0300: loss: 3952.097
iteration 0400: loss: 3956.151
iteration 0500: loss: 3966.323
iteration 0600: loss: 3946.144
iteration 0700: loss: 3968.629
iteration 0800: loss: 3957.507
iteration 0900: loss: 3955.665
iteration 1000: loss: 3959.753
iteration 1100: loss: 3941.384
iteration 1200: loss: 3954.541
iteration 1300: loss: 3959.989
iteration 1400: loss: 3952.707
iteration 1500: loss: 3955.083
iteration 1600: loss: 3957.981
iteration 1700: loss: 3954.270
iteration 1800: loss: 3957.277
====> Epoch: 041 Train loss: 3957.9346  took : 53.09528040885925
====> Test loss: 3966.1575
iteration 0000: loss: 3957.185
iteration 0100: loss: 3952.434
iteration 0200: loss: 3949.984
iteration 0300: loss: 3962.834
iteration 0400: loss: 3973.889
iteration 0500: loss: 3960.091
iteration 0600: loss: 3949.055
iteration 0700: loss: 3971.632
iteration 0800: loss: 3960.930
iteration 0900: loss: 3949.799
iteration 1000: loss: 3954.466
iteration 1100: loss: 3952.966
iteration 1200: loss: 3963.668
iteration 1300: loss: 3959.174
iteration 1400: loss: 3964.752
iteration 1500: loss: 3956.260
iteration 1600: loss: 3961.215
iteration 1700: loss: 3953.678
iteration 1800: loss: 3949.885
====> Epoch: 042 Train loss: 3957.9311  took : 53.07858610153198
====> Test loss: 3966.1389
iteration 0000: loss: 3965.691
iteration 0100: loss: 3944.862
iteration 0200: loss: 3957.598
iteration 0300: loss: 3950.549
iteration 0400: loss: 3955.941
iteration 0500: loss: 3971.362
iteration 0600: loss: 3955.968
iteration 0700: loss: 3958.613
iteration 0800: loss: 3956.112
iteration 0900: loss: 3959.964
iteration 1000: loss: 3958.110
iteration 1100: loss: 3954.435
iteration 1200: loss: 3958.213
iteration 1300: loss: 3961.585
iteration 1400: loss: 3953.809
iteration 1500: loss: 3961.793
iteration 1600: loss: 3949.720
iteration 1700: loss: 3960.213
iteration 1800: loss: 3957.362
====> Epoch: 043 Train loss: 3958.0416  took : 53.188769578933716
====> Test loss: 3966.3489
iteration 0000: loss: 3957.572
iteration 0100: loss: 3960.303
iteration 0200: loss: 3955.850
iteration 0300: loss: 3958.293
iteration 0400: loss: 3953.741
iteration 0500: loss: 3956.244
iteration 0600: loss: 3952.800
iteration 0700: loss: 3945.893
iteration 0800: loss: 3953.922
iteration 0900: loss: 3957.771
iteration 1000: loss: 3955.978
iteration 1100: loss: 3956.572
iteration 1200: loss: 3957.428
iteration 1300: loss: 3960.503
iteration 1400: loss: 3957.498
iteration 1500: loss: 3962.772
iteration 1600: loss: 3972.626
iteration 1700: loss: 3958.442
iteration 1800: loss: 3955.962
====> Epoch: 044 Train loss: 3957.6367  took : 53.081681966781616
====> Test loss: 3965.3728
iteration 0000: loss: 3964.126
iteration 0100: loss: 3963.203
iteration 0200: loss: 3969.916
iteration 0300: loss: 3946.908
iteration 0400: loss: 3956.490
iteration 0500: loss: 3959.091
iteration 0600: loss: 3955.282
iteration 0700: loss: 3962.772
iteration 0800: loss: 3950.850
iteration 0900: loss: 3958.641
iteration 1000: loss: 3962.663
iteration 1100: loss: 3960.874
iteration 1200: loss: 3967.212
iteration 1300: loss: 3958.628
iteration 1400: loss: 3959.438
iteration 1500: loss: 3958.873
iteration 1600: loss: 3958.612
iteration 1700: loss: 3954.322
iteration 1800: loss: 3969.975
====> Epoch: 045 Train loss: 3957.4131  took : 53.13446068763733
====> Test loss: 3966.0281
iteration 0000: loss: 3955.115
iteration 0100: loss: 3956.807
iteration 0200: loss: 3953.700
iteration 0300: loss: 3954.178
iteration 0400: loss: 3950.964
iteration 0500: loss: 3962.237
iteration 0600: loss: 3962.795
iteration 0700: loss: 3956.424
iteration 0800: loss: 3966.121
iteration 0900: loss: 3952.718
iteration 1000: loss: 3958.990
iteration 1100: loss: 3961.668
iteration 1200: loss: 3960.966
iteration 1300: loss: 3957.406
iteration 1400: loss: 3948.553
iteration 1500: loss: 3955.266
iteration 1600: loss: 3956.646
iteration 1700: loss: 3959.396
iteration 1800: loss: 3956.423
====> Epoch: 046 Train loss: 3957.8562  took : 53.04237198829651
====> Test loss: 3966.4856
iteration 0000: loss: 3964.871
iteration 0100: loss: 3967.631
iteration 0200: loss: 3959.570
iteration 0300: loss: 3954.181
iteration 0400: loss: 3959.333
iteration 0500: loss: 3955.400
iteration 0600: loss: 3954.513
iteration 0700: loss: 3958.957
iteration 0800: loss: 3954.504
iteration 0900: loss: 3961.779
iteration 1000: loss: 3962.396
iteration 1100: loss: 3959.033
iteration 1200: loss: 3961.135
iteration 1300: loss: 3957.682
iteration 1400: loss: 3954.798
iteration 1500: loss: 3955.278
iteration 1600: loss: 3954.029
iteration 1700: loss: 3954.701
iteration 1800: loss: 3953.575
====> Epoch: 047 Train loss: 3957.0806  took : 52.88095474243164
====> Test loss: 3965.3719
iteration 0000: loss: 3961.319
iteration 0100: loss: 3959.819
iteration 0200: loss: 3961.655
iteration 0300: loss: 3957.568
iteration 0400: loss: 3960.611
iteration 0500: loss: 3958.361
iteration 0600: loss: 3957.251
iteration 0700: loss: 3963.217
iteration 0800: loss: 3959.127
iteration 0900: loss: 3957.438
iteration 1000: loss: 3965.064
iteration 1100: loss: 3949.982
iteration 1200: loss: 3958.325
iteration 1300: loss: 3949.024
iteration 1400: loss: 3958.584
iteration 1500: loss: 3951.785
iteration 1600: loss: 3957.042
iteration 1700: loss: 3959.177
iteration 1800: loss: 3957.810
====> Epoch: 048 Train loss: 3956.9242  took : 52.944451093673706
====> Test loss: 3965.4832
iteration 0000: loss: 3949.358
iteration 0100: loss: 3946.861
iteration 0200: loss: 3958.595
iteration 0300: loss: 3955.634
iteration 0400: loss: 3957.358
iteration 0500: loss: 3967.334
iteration 0600: loss: 3959.724
iteration 0700: loss: 3960.045
iteration 0800: loss: 3961.471
iteration 0900: loss: 3953.013
iteration 1000: loss: 3958.211
iteration 1100: loss: 3963.271
iteration 1200: loss: 3954.646
iteration 1300: loss: 3952.573
iteration 1400: loss: 3960.062
iteration 1500: loss: 3954.033
iteration 1600: loss: 3958.894
iteration 1700: loss: 3958.547
iteration 1800: loss: 3956.643
====> Epoch: 049 Train loss: 3956.9392  took : 52.85932636260986
====> Test loss: 3966.2242
iteration 0000: loss: 3961.519
iteration 0100: loss: 3964.049
iteration 0200: loss: 3951.873
iteration 0300: loss: 3961.082
iteration 0400: loss: 3955.611
iteration 0500: loss: 3964.178
iteration 0600: loss: 3956.207
iteration 0700: loss: 3957.037
iteration 0800: loss: 3953.655
iteration 0900: loss: 3951.102
iteration 1000: loss: 3961.128
iteration 1100: loss: 3954.583
iteration 1200: loss: 3958.184
iteration 1300: loss: 3960.144
iteration 1400: loss: 3959.350
iteration 1500: loss: 3955.524
iteration 1600: loss: 3965.051
iteration 1700: loss: 3956.975
iteration 1800: loss: 3952.137
====> Epoch: 050 Train loss: 3957.4498  took : 52.88097524642944
====> Test loss: 3965.6086
====> [MM-VAE] Time: 3143.546s or 00:52:23
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  6
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_6
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_6
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1990.080
iteration 0100: loss: 1572.088
iteration 0200: loss: 1564.825
iteration 0300: loss: 1560.778
iteration 0400: loss: 1548.651
iteration 0500: loss: 1547.336
iteration 0600: loss: 1539.317
iteration 0700: loss: 1538.724
iteration 0800: loss: 1534.357
iteration 0900: loss: 1535.455
====> Epoch: 001 Train loss: 1553.6231  took : 8.48625659942627
====> Test loss: 1536.5591
iteration 0000: loss: 1536.660
iteration 0100: loss: 1534.899
iteration 0200: loss: 1534.953
iteration 0300: loss: 1535.642
iteration 0400: loss: 1528.817
iteration 0500: loss: 1537.714
iteration 0600: loss: 1531.417
iteration 0700: loss: 1528.332
iteration 0800: loss: 1530.492
iteration 0900: loss: 1527.222
====> Epoch: 002 Train loss: 1531.1123  took : 8.550328731536865
====> Test loss: 1529.6258
iteration 0000: loss: 1528.777
iteration 0100: loss: 1526.023
iteration 0200: loss: 1524.145
iteration 0300: loss: 1529.731
iteration 0400: loss: 1525.640
iteration 0500: loss: 1528.464
iteration 0600: loss: 1528.692
iteration 0700: loss: 1522.886
iteration 0800: loss: 1522.575
iteration 0900: loss: 1523.145
====> Epoch: 003 Train loss: 1526.1483  took : 8.407358169555664
====> Test loss: 1527.0000
iteration 0000: loss: 1521.607
iteration 0100: loss: 1524.789
iteration 0200: loss: 1523.703
iteration 0300: loss: 1522.503
iteration 0400: loss: 1526.137
iteration 0500: loss: 1522.655
iteration 0600: loss: 1523.982
iteration 0700: loss: 1524.715
iteration 0800: loss: 1523.972
iteration 0900: loss: 1521.791
====> Epoch: 004 Train loss: 1523.7980  took : 8.625685214996338
====> Test loss: 1525.1254
iteration 0000: loss: 1521.873
iteration 0100: loss: 1520.634
iteration 0200: loss: 1519.785
iteration 0300: loss: 1518.365
iteration 0400: loss: 1524.458
iteration 0500: loss: 1523.004
iteration 0600: loss: 1522.679
iteration 0700: loss: 1522.410
iteration 0800: loss: 1523.117
iteration 0900: loss: 1521.015
====> Epoch: 005 Train loss: 1522.2705  took : 8.443947315216064
====> Test loss: 1524.0611
iteration 0000: loss: 1523.050
iteration 0100: loss: 1524.566
iteration 0200: loss: 1524.416
iteration 0300: loss: 1516.950
iteration 0400: loss: 1519.843
iteration 0500: loss: 1519.584
iteration 0600: loss: 1521.596
iteration 0700: loss: 1523.103
iteration 0800: loss: 1522.811
iteration 0900: loss: 1520.723
====> Epoch: 006 Train loss: 1521.2190  took : 8.5762197971344
====> Test loss: 1523.2118
iteration 0000: loss: 1519.487
iteration 0100: loss: 1524.236
iteration 0200: loss: 1518.993
iteration 0300: loss: 1524.222
iteration 0400: loss: 1520.979
iteration 0500: loss: 1519.185
iteration 0600: loss: 1520.592
iteration 0700: loss: 1519.183
iteration 0800: loss: 1517.654
iteration 0900: loss: 1520.499
====> Epoch: 007 Train loss: 1520.2885  took : 8.62842583656311
====> Test loss: 1522.5181
iteration 0000: loss: 1518.902
iteration 0100: loss: 1518.565
iteration 0200: loss: 1519.146
iteration 0300: loss: 1520.831
iteration 0400: loss: 1517.692
iteration 0500: loss: 1518.922
iteration 0600: loss: 1521.718
iteration 0700: loss: 1519.362
iteration 0800: loss: 1520.842
iteration 0900: loss: 1520.173
====> Epoch: 008 Train loss: 1519.5706  took : 8.546185970306396
====> Test loss: 1522.0980
iteration 0000: loss: 1521.213
iteration 0100: loss: 1517.586
iteration 0200: loss: 1513.702
iteration 0300: loss: 1518.784
iteration 0400: loss: 1520.053
iteration 0500: loss: 1519.631
iteration 0600: loss: 1517.641
iteration 0700: loss: 1520.879
iteration 0800: loss: 1518.480
iteration 0900: loss: 1519.541
====> Epoch: 009 Train loss: 1519.0067  took : 8.541398525238037
====> Test loss: 1521.4525
iteration 0000: loss: 1519.737
iteration 0100: loss: 1520.781
iteration 0200: loss: 1520.710
iteration 0300: loss: 1516.530
iteration 0400: loss: 1520.363
iteration 0500: loss: 1519.632
iteration 0600: loss: 1518.473
iteration 0700: loss: 1522.241
iteration 0800: loss: 1519.978
iteration 0900: loss: 1516.570
====> Epoch: 010 Train loss: 1518.5438  took : 8.516320943832397
====> Test loss: 1521.2486
iteration 0000: loss: 1517.568
iteration 0100: loss: 1516.763
iteration 0200: loss: 1515.322
iteration 0300: loss: 1519.005
iteration 0400: loss: 1518.684
iteration 0500: loss: 1519.555
iteration 0600: loss: 1518.761
iteration 0700: loss: 1519.275
iteration 0800: loss: 1517.614
iteration 0900: loss: 1515.682
====> Epoch: 011 Train loss: 1518.1188  took : 8.48472547531128
====> Test loss: 1520.7521
iteration 0000: loss: 1516.467
iteration 0100: loss: 1518.415
iteration 0200: loss: 1514.585
iteration 0300: loss: 1517.398
iteration 0400: loss: 1520.845
iteration 0500: loss: 1518.141
iteration 0600: loss: 1520.331
iteration 0700: loss: 1518.483
iteration 0800: loss: 1518.282
iteration 0900: loss: 1518.453
====> Epoch: 012 Train loss: 1517.8203  took : 8.529267311096191
====> Test loss: 1520.4428
iteration 0000: loss: 1520.999
iteration 0100: loss: 1520.467
iteration 0200: loss: 1518.739
iteration 0300: loss: 1514.701
iteration 0400: loss: 1517.088
iteration 0500: loss: 1516.072
iteration 0600: loss: 1520.188
iteration 0700: loss: 1518.638
iteration 0800: loss: 1517.853
iteration 0900: loss: 1516.556
====> Epoch: 013 Train loss: 1517.5305  took : 8.490241527557373
====> Test loss: 1520.2212
iteration 0000: loss: 1515.777
iteration 0100: loss: 1518.093
iteration 0200: loss: 1515.101
iteration 0300: loss: 1517.303
iteration 0400: loss: 1518.631
iteration 0500: loss: 1516.404
iteration 0600: loss: 1516.236
iteration 0700: loss: 1519.823
iteration 0800: loss: 1518.499
iteration 0900: loss: 1519.432
====> Epoch: 014 Train loss: 1517.3335  took : 8.487859010696411
====> Test loss: 1520.1439
iteration 0000: loss: 1516.049
iteration 0100: loss: 1515.667
iteration 0200: loss: 1518.255
iteration 0300: loss: 1518.522
iteration 0400: loss: 1517.262
iteration 0500: loss: 1513.969
iteration 0600: loss: 1517.970
iteration 0700: loss: 1517.334
iteration 0800: loss: 1518.492
iteration 0900: loss: 1514.575
====> Epoch: 015 Train loss: 1517.0921  took : 8.525037288665771
====> Test loss: 1520.1523
iteration 0000: loss: 1518.198
iteration 0100: loss: 1519.386
iteration 0200: loss: 1517.132
iteration 0300: loss: 1515.768
iteration 0400: loss: 1520.013
iteration 0500: loss: 1517.470
iteration 0600: loss: 1518.035
iteration 0700: loss: 1517.819
iteration 0800: loss: 1516.994
iteration 0900: loss: 1517.534
====> Epoch: 016 Train loss: 1516.8991  took : 8.60662055015564
====> Test loss: 1519.9057
iteration 0000: loss: 1516.860
iteration 0100: loss: 1514.679
iteration 0200: loss: 1517.298
iteration 0300: loss: 1519.036
iteration 0400: loss: 1521.180
iteration 0500: loss: 1517.499
iteration 0600: loss: 1514.600
iteration 0700: loss: 1516.169
iteration 0800: loss: 1512.985
iteration 0900: loss: 1515.474
====> Epoch: 017 Train loss: 1516.7206  took : 8.590009450912476
====> Test loss: 1519.5523
iteration 0000: loss: 1515.564
iteration 0100: loss: 1516.494
iteration 0200: loss: 1518.128
iteration 0300: loss: 1515.495
iteration 0400: loss: 1516.936
iteration 0500: loss: 1516.626
iteration 0600: loss: 1517.525
iteration 0700: loss: 1514.437
iteration 0800: loss: 1519.199
iteration 0900: loss: 1517.713
====> Epoch: 018 Train loss: 1516.5330  took : 8.516627073287964
====> Test loss: 1519.5890
iteration 0000: loss: 1514.703
iteration 0100: loss: 1516.005
iteration 0200: loss: 1515.373
iteration 0300: loss: 1517.807
iteration 0400: loss: 1519.112
iteration 0500: loss: 1515.027
iteration 0600: loss: 1517.495
iteration 0700: loss: 1518.032
iteration 0800: loss: 1516.725
iteration 0900: loss: 1515.444
====> Epoch: 019 Train loss: 1516.3548  took : 8.408182144165039
====> Test loss: 1519.2973
iteration 0000: loss: 1514.815
iteration 0100: loss: 1514.154
iteration 0200: loss: 1516.609
iteration 0300: loss: 1515.148
iteration 0400: loss: 1516.451
iteration 0500: loss: 1517.392
iteration 0600: loss: 1515.214
iteration 0700: loss: 1515.682
iteration 0800: loss: 1515.648
iteration 0900: loss: 1515.417
====> Epoch: 020 Train loss: 1516.2355  took : 8.460410594940186
====> Test loss: 1519.3057
iteration 0000: loss: 1516.304
iteration 0100: loss: 1515.207
iteration 0200: loss: 1516.140
iteration 0300: loss: 1518.288
iteration 0400: loss: 1514.530
iteration 0500: loss: 1519.033
iteration 0600: loss: 1515.482
iteration 0700: loss: 1516.276
iteration 0800: loss: 1515.938
iteration 0900: loss: 1517.299
====> Epoch: 021 Train loss: 1516.1133  took : 8.497087240219116
====> Test loss: 1519.1107
iteration 0000: loss: 1513.728
iteration 0100: loss: 1515.111
iteration 0200: loss: 1519.434
iteration 0300: loss: 1516.365
iteration 0400: loss: 1516.738
iteration 0500: loss: 1515.666
iteration 0600: loss: 1515.807
iteration 0700: loss: 1514.607
iteration 0800: loss: 1518.270
iteration 0900: loss: 1517.418
====> Epoch: 022 Train loss: 1516.0080  took : 8.437567472457886
====> Test loss: 1519.0725
iteration 0000: loss: 1518.279
iteration 0100: loss: 1517.727
iteration 0200: loss: 1516.086
iteration 0300: loss: 1514.811
iteration 0400: loss: 1517.221
iteration 0500: loss: 1514.521
iteration 0600: loss: 1515.185
iteration 0700: loss: 1514.931
iteration 0800: loss: 1518.632
iteration 0900: loss: 1514.571
====> Epoch: 023 Train loss: 1515.8922  took : 8.559258460998535
====> Test loss: 1519.0645
iteration 0000: loss: 1515.189
iteration 0100: loss: 1513.464
iteration 0200: loss: 1515.434
iteration 0300: loss: 1516.533
iteration 0400: loss: 1516.535
iteration 0500: loss: 1516.595
iteration 0600: loss: 1517.153
iteration 0700: loss: 1514.120
iteration 0800: loss: 1513.060
iteration 0900: loss: 1515.425
====> Epoch: 024 Train loss: 1515.7731  took : 8.516633033752441
====> Test loss: 1518.9083
iteration 0000: loss: 1513.247
iteration 0100: loss: 1518.046
iteration 0200: loss: 1516.238
iteration 0300: loss: 1516.468
iteration 0400: loss: 1515.029
iteration 0500: loss: 1518.264
iteration 0600: loss: 1513.956
iteration 0700: loss: 1515.153
iteration 0800: loss: 1516.997
iteration 0900: loss: 1515.579
====> Epoch: 025 Train loss: 1515.6841  took : 8.440566778182983
====> Test loss: 1518.8718
iteration 0000: loss: 1517.093
iteration 0100: loss: 1515.956
iteration 0200: loss: 1516.634
iteration 0300: loss: 1516.293
iteration 0400: loss: 1516.219
iteration 0500: loss: 1513.592
iteration 0600: loss: 1519.290
iteration 0700: loss: 1517.106
iteration 0800: loss: 1516.571
iteration 0900: loss: 1516.606
====> Epoch: 026 Train loss: 1515.5804  took : 8.419137001037598
====> Test loss: 1519.7752
iteration 0000: loss: 1514.122
iteration 0100: loss: 1514.126
iteration 0200: loss: 1515.383
iteration 0300: loss: 1516.200
iteration 0400: loss: 1516.138
iteration 0500: loss: 1514.529
iteration 0600: loss: 1514.188
iteration 0700: loss: 1516.038
iteration 0800: loss: 1515.883
iteration 0900: loss: 1515.495
====> Epoch: 027 Train loss: 1515.5478  took : 8.497908592224121
====> Test loss: 1518.6994
iteration 0000: loss: 1515.509
iteration 0100: loss: 1514.446
iteration 0200: loss: 1518.224
iteration 0300: loss: 1514.939
iteration 0400: loss: 1518.694
iteration 0500: loss: 1513.786
iteration 0600: loss: 1513.846
iteration 0700: loss: 1513.237
iteration 0800: loss: 1515.307
iteration 0900: loss: 1514.980
====> Epoch: 028 Train loss: 1515.4005  took : 8.460596084594727
====> Test loss: 1518.7547
iteration 0000: loss: 1516.455
iteration 0100: loss: 1515.710
iteration 0200: loss: 1521.046
iteration 0300: loss: 1515.774
iteration 0400: loss: 1514.323
iteration 0500: loss: 1514.286
iteration 0600: loss: 1514.155
iteration 0700: loss: 1515.108
iteration 0800: loss: 1517.847
iteration 0900: loss: 1518.257
====> Epoch: 029 Train loss: 1515.3268  took : 8.560428619384766
====> Test loss: 1518.6720
iteration 0000: loss: 1517.324
iteration 0100: loss: 1515.034
iteration 0200: loss: 1517.104
iteration 0300: loss: 1513.570
iteration 0400: loss: 1515.900
iteration 0500: loss: 1514.907
iteration 0600: loss: 1514.960
iteration 0700: loss: 1519.209
iteration 0800: loss: 1513.905
iteration 0900: loss: 1516.874
====> Epoch: 030 Train loss: 1515.3088  took : 8.465483665466309
====> Test loss: 1518.6337
iteration 0000: loss: 1515.217
iteration 0100: loss: 1514.816
iteration 0200: loss: 1518.087
iteration 0300: loss: 1516.976
iteration 0400: loss: 1519.897
iteration 0500: loss: 1515.286
iteration 0600: loss: 1513.569
iteration 0700: loss: 1515.015
iteration 0800: loss: 1513.542
iteration 0900: loss: 1514.788
====> Epoch: 031 Train loss: 1515.2122  took : 8.479334354400635
====> Test loss: 1518.7086
iteration 0000: loss: 1515.445
iteration 0100: loss: 1512.991
iteration 0200: loss: 1514.426
iteration 0300: loss: 1514.483
iteration 0400: loss: 1514.943
iteration 0500: loss: 1513.985
iteration 0600: loss: 1514.572
iteration 0700: loss: 1515.285
iteration 0800: loss: 1514.881
iteration 0900: loss: 1515.675
====> Epoch: 032 Train loss: 1515.1840  took : 8.572807550430298
====> Test loss: 1518.4925
iteration 0000: loss: 1517.595
iteration 0100: loss: 1514.476
iteration 0200: loss: 1513.874
iteration 0300: loss: 1514.762
iteration 0400: loss: 1515.640
iteration 0500: loss: 1514.330
iteration 0600: loss: 1514.884
iteration 0700: loss: 1515.280
iteration 0800: loss: 1517.473
iteration 0900: loss: 1514.404
====> Epoch: 033 Train loss: 1515.0488  took : 8.61499571800232
====> Test loss: 1518.4744
iteration 0000: loss: 1516.676
iteration 0100: loss: 1513.828
iteration 0200: loss: 1514.010
iteration 0300: loss: 1515.274
iteration 0400: loss: 1515.313
iteration 0500: loss: 1515.868
iteration 0600: loss: 1516.218
iteration 0700: loss: 1516.178
iteration 0800: loss: 1515.661
iteration 0900: loss: 1516.700
====> Epoch: 034 Train loss: 1515.0204  took : 8.453587055206299
====> Test loss: 1518.3053
iteration 0000: loss: 1516.486
iteration 0100: loss: 1516.346
iteration 0200: loss: 1514.881
iteration 0300: loss: 1514.082
iteration 0400: loss: 1515.529
iteration 0500: loss: 1516.040
iteration 0600: loss: 1514.474
iteration 0700: loss: 1516.284
iteration 0800: loss: 1515.734
iteration 0900: loss: 1518.711
====> Epoch: 035 Train loss: 1514.9148  took : 8.542221546173096
====> Test loss: 1518.4910
iteration 0000: loss: 1512.804
iteration 0100: loss: 1514.728
iteration 0200: loss: 1517.086
iteration 0300: loss: 1513.013
iteration 0400: loss: 1516.219
iteration 0500: loss: 1515.761
iteration 0600: loss: 1517.106
iteration 0700: loss: 1514.776
iteration 0800: loss: 1516.350
iteration 0900: loss: 1512.734
====> Epoch: 036 Train loss: 1514.8852  took : 8.45833969116211
====> Test loss: 1518.5738
iteration 0000: loss: 1515.033
iteration 0100: loss: 1515.904
iteration 0200: loss: 1515.203
iteration 0300: loss: 1514.660
iteration 0400: loss: 1511.151
iteration 0500: loss: 1518.270
iteration 0600: loss: 1514.303
iteration 0700: loss: 1513.711
iteration 0800: loss: 1517.428
iteration 0900: loss: 1512.753
====> Epoch: 037 Train loss: 1514.8044  took : 8.52048134803772
====> Test loss: 1518.3432
iteration 0000: loss: 1513.018
iteration 0100: loss: 1513.830
iteration 0200: loss: 1515.014
iteration 0300: loss: 1514.469
iteration 0400: loss: 1512.331
iteration 0500: loss: 1513.652
iteration 0600: loss: 1513.959
iteration 0700: loss: 1513.698
iteration 0800: loss: 1514.959
iteration 0900: loss: 1515.374
====> Epoch: 038 Train loss: 1514.7417  took : 8.404302597045898
====> Test loss: 1518.4833
iteration 0000: loss: 1514.681
iteration 0100: loss: 1514.273
iteration 0200: loss: 1513.301
iteration 0300: loss: 1516.424
iteration 0400: loss: 1516.648
iteration 0500: loss: 1513.908
iteration 0600: loss: 1514.735
iteration 0700: loss: 1515.123
iteration 0800: loss: 1516.247
iteration 0900: loss: 1516.022
====> Epoch: 039 Train loss: 1514.7176  took : 8.539961576461792
====> Test loss: 1518.2533
iteration 0000: loss: 1515.219
iteration 0100: loss: 1514.181
iteration 0200: loss: 1512.480
iteration 0300: loss: 1517.941
iteration 0400: loss: 1512.444
iteration 0500: loss: 1512.009
iteration 0600: loss: 1515.872
iteration 0700: loss: 1514.695
iteration 0800: loss: 1513.568
iteration 0900: loss: 1513.885
====> Epoch: 040 Train loss: 1514.6531  took : 8.501360416412354
====> Test loss: 1518.1751
iteration 0000: loss: 1514.894
iteration 0100: loss: 1516.151
iteration 0200: loss: 1514.936
iteration 0300: loss: 1514.695
iteration 0400: loss: 1514.620
iteration 0500: loss: 1515.532
iteration 0600: loss: 1516.737
iteration 0700: loss: 1514.019
iteration 0800: loss: 1513.506
iteration 0900: loss: 1517.242
====> Epoch: 041 Train loss: 1514.5848  took : 8.42465615272522
====> Test loss: 1518.2052
iteration 0000: loss: 1511.898
iteration 0100: loss: 1514.204
iteration 0200: loss: 1513.241
iteration 0300: loss: 1514.184
iteration 0400: loss: 1515.176
iteration 0500: loss: 1513.387
iteration 0600: loss: 1512.477
iteration 0700: loss: 1514.674
iteration 0800: loss: 1515.817
iteration 0900: loss: 1514.375
====> Epoch: 042 Train loss: 1514.5690  took : 8.567260503768921
====> Test loss: 1518.2390
iteration 0000: loss: 1516.165
iteration 0100: loss: 1512.914
iteration 0200: loss: 1515.857
iteration 0300: loss: 1516.396
iteration 0400: loss: 1516.621
iteration 0500: loss: 1516.325
iteration 0600: loss: 1514.485
iteration 0700: loss: 1513.704
iteration 0800: loss: 1516.123
iteration 0900: loss: 1516.814
====> Epoch: 043 Train loss: 1514.4861  took : 8.586283683776855
====> Test loss: 1518.1820
iteration 0000: loss: 1514.461
iteration 0100: loss: 1515.359
iteration 0200: loss: 1513.351
iteration 0300: loss: 1514.340
iteration 0400: loss: 1515.749
iteration 0500: loss: 1516.181
iteration 0600: loss: 1514.505
iteration 0700: loss: 1512.294
iteration 0800: loss: 1514.748
iteration 0900: loss: 1515.883
====> Epoch: 044 Train loss: 1514.4208  took : 8.596628665924072
====> Test loss: 1518.2041
iteration 0000: loss: 1513.190
iteration 0100: loss: 1513.972
iteration 0200: loss: 1514.056
iteration 0300: loss: 1514.360
iteration 0400: loss: 1515.050
iteration 0500: loss: 1515.927
iteration 0600: loss: 1513.593
iteration 0700: loss: 1514.563
iteration 0800: loss: 1514.392
iteration 0900: loss: 1513.558
====> Epoch: 045 Train loss: 1514.4406  took : 8.58465027809143
====> Test loss: 1518.2107
iteration 0000: loss: 1513.068
iteration 0100: loss: 1513.571
iteration 0200: loss: 1515.281
iteration 0300: loss: 1515.786
iteration 0400: loss: 1514.600
iteration 0500: loss: 1515.507
iteration 0600: loss: 1514.333
iteration 0700: loss: 1517.556
iteration 0800: loss: 1514.265
iteration 0900: loss: 1515.616
====> Epoch: 046 Train loss: 1514.3632  took : 8.604075193405151
====> Test loss: 1517.9715
iteration 0000: loss: 1515.919
iteration 0100: loss: 1514.691
iteration 0200: loss: 1513.766
iteration 0300: loss: 1515.295
iteration 0400: loss: 1513.489
iteration 0500: loss: 1514.930
iteration 0600: loss: 1513.841
iteration 0700: loss: 1516.424
iteration 0800: loss: 1512.697
iteration 0900: loss: 1513.052
====> Epoch: 047 Train loss: 1514.2827  took : 8.430630207061768
====> Test loss: 1518.0597
iteration 0000: loss: 1514.105
iteration 0100: loss: 1514.697
iteration 0200: loss: 1516.289
iteration 0300: loss: 1516.299
iteration 0400: loss: 1512.124
iteration 0500: loss: 1515.336
iteration 0600: loss: 1513.901
iteration 0700: loss: 1514.063
iteration 0800: loss: 1515.850
iteration 0900: loss: 1516.646
====> Epoch: 048 Train loss: 1514.2553  took : 8.487582206726074
====> Test loss: 1518.0910
iteration 0000: loss: 1511.399
iteration 0100: loss: 1512.589
iteration 0200: loss: 1515.213
iteration 0300: loss: 1515.547
iteration 0400: loss: 1512.391
iteration 0500: loss: 1513.989
iteration 0600: loss: 1517.292
iteration 0700: loss: 1518.382
iteration 0800: loss: 1513.755
iteration 0900: loss: 1517.072
====> Epoch: 049 Train loss: 1514.2718  took : 8.494471073150635
====> Test loss: 1518.0195
iteration 0000: loss: 1515.138
iteration 0100: loss: 1511.530
iteration 0200: loss: 1513.992
iteration 0300: loss: 1514.312
iteration 0400: loss: 1512.780
iteration 0500: loss: 1513.887
iteration 0600: loss: 1515.916
iteration 0700: loss: 1514.678
iteration 0800: loss: 1517.378
iteration 0900: loss: 1511.896
====> Epoch: 050 Train loss: 1514.1814  took : 8.480762481689453
====> Test loss: 1518.0697
====> [MM-VAE] Time: 507.914s or 00:08:27
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  6
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_6
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_6
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.197
iteration 0100: loss: 2089.374
iteration 0200: loss: 2039.218
iteration 0300: loss: 2014.194
iteration 0400: loss: 1996.819
iteration 0500: loss: 1994.548
iteration 0600: loss: 1995.313
iteration 0700: loss: 1986.777
iteration 0800: loss: 1993.530
iteration 0900: loss: 1990.089
====> Epoch: 001 Train loss: 2020.0018  took : 11.77878475189209
====> Test loss: 1993.2046
iteration 0000: loss: 1987.153
iteration 0100: loss: 1990.750
iteration 0200: loss: 1990.641
iteration 0300: loss: 1988.573
iteration 0400: loss: 1984.259
iteration 0500: loss: 1980.981
iteration 0600: loss: 1976.649
iteration 0700: loss: 1978.203
iteration 0800: loss: 1977.943
iteration 0900: loss: 1976.536
====> Epoch: 002 Train loss: 1983.2666  took : 12.12470030784607
====> Test loss: 1980.5844
iteration 0000: loss: 1975.332
iteration 0100: loss: 1973.337
iteration 0200: loss: 1974.271
iteration 0300: loss: 1971.149
iteration 0400: loss: 1973.955
iteration 0500: loss: 1973.323
iteration 0600: loss: 1967.403
iteration 0700: loss: 1969.121
iteration 0800: loss: 1967.956
iteration 0900: loss: 1967.750
====> Epoch: 003 Train loss: 1971.9269  took : 12.36455774307251
====> Test loss: 1970.2066
iteration 0000: loss: 1968.571
iteration 0100: loss: 1964.554
iteration 0200: loss: 1964.418
iteration 0300: loss: 1962.805
iteration 0400: loss: 1964.791
iteration 0500: loss: 1964.217
iteration 0600: loss: 1964.974
iteration 0700: loss: 1959.978
iteration 0800: loss: 1961.285
iteration 0900: loss: 1962.533
====> Epoch: 004 Train loss: 1964.2145  took : 12.19891881942749
====> Test loss: 1963.4024
iteration 0000: loss: 1959.725
iteration 0100: loss: 1959.927
iteration 0200: loss: 1961.544
iteration 0300: loss: 1958.779
iteration 0400: loss: 1957.978
iteration 0500: loss: 1960.150
iteration 0600: loss: 1957.068
iteration 0700: loss: 1956.900
iteration 0800: loss: 1955.019
iteration 0900: loss: 1955.597
====> Epoch: 005 Train loss: 1958.3531  took : 12.516381025314331
====> Test loss: 1959.7983
iteration 0000: loss: 1958.176
iteration 0100: loss: 1955.859
iteration 0200: loss: 1953.594
iteration 0300: loss: 1956.700
iteration 0400: loss: 1956.965
iteration 0500: loss: 1954.426
iteration 0600: loss: 1953.953
iteration 0700: loss: 1956.532
iteration 0800: loss: 1954.005
iteration 0900: loss: 1953.735
====> Epoch: 006 Train loss: 1954.6137  took : 12.52224326133728
====> Test loss: 1956.3458
iteration 0000: loss: 1953.386
iteration 0100: loss: 1954.951
iteration 0200: loss: 1952.591
iteration 0300: loss: 1950.470
iteration 0400: loss: 1953.547
iteration 0500: loss: 1953.226
iteration 0600: loss: 1952.087
iteration 0700: loss: 1950.728
iteration 0800: loss: 1952.417
iteration 0900: loss: 1950.840
====> Epoch: 007 Train loss: 1952.3018  took : 12.466169595718384
====> Test loss: 1954.0158
iteration 0000: loss: 1951.116
iteration 0100: loss: 1950.871
iteration 0200: loss: 1951.924
iteration 0300: loss: 1950.677
iteration 0400: loss: 1950.703
iteration 0500: loss: 1949.813
iteration 0600: loss: 1949.756
iteration 0700: loss: 1948.006
iteration 0800: loss: 1949.822
iteration 0900: loss: 1949.579
====> Epoch: 008 Train loss: 1950.7149  took : 12.727840185165405
====> Test loss: 1952.6449
iteration 0000: loss: 1950.439
iteration 0100: loss: 1949.077
iteration 0200: loss: 1949.363
iteration 0300: loss: 1950.624
iteration 0400: loss: 1949.769
iteration 0500: loss: 1949.781
iteration 0600: loss: 1950.654
iteration 0700: loss: 1949.260
iteration 0800: loss: 1951.229
iteration 0900: loss: 1948.556
====> Epoch: 009 Train loss: 1949.6582  took : 13.557688236236572
====> Test loss: 1951.5979
iteration 0000: loss: 1950.396
iteration 0100: loss: 1949.728
iteration 0200: loss: 1949.451
iteration 0300: loss: 1949.086
iteration 0400: loss: 1949.574
iteration 0500: loss: 1948.125
iteration 0600: loss: 1948.971
iteration 0700: loss: 1948.188
iteration 0800: loss: 1948.909
iteration 0900: loss: 1947.730
====> Epoch: 010 Train loss: 1948.9215  took : 13.420859575271606
====> Test loss: 1951.1360
iteration 0000: loss: 1949.141
iteration 0100: loss: 1947.884
iteration 0200: loss: 1949.759
iteration 0300: loss: 1951.316
iteration 0400: loss: 1948.644
iteration 0500: loss: 1948.538
iteration 0600: loss: 1949.522
iteration 0700: loss: 1947.789
iteration 0800: loss: 1948.727
iteration 0900: loss: 1948.044
====> Epoch: 011 Train loss: 1948.6841  took : 13.288283348083496
====> Test loss: 1950.4630
iteration 0000: loss: 1947.798
iteration 0100: loss: 1948.370
iteration 0200: loss: 1948.448
iteration 0300: loss: 1949.506
iteration 0400: loss: 1948.319
iteration 0500: loss: 1948.515
iteration 0600: loss: 1947.367
iteration 0700: loss: 1951.062
iteration 0800: loss: 1948.208
iteration 0900: loss: 1947.850
====> Epoch: 012 Train loss: 1948.3573  took : 12.064640760421753
====> Test loss: 1950.6037
iteration 0000: loss: 1948.680
iteration 0100: loss: 1947.709
iteration 0200: loss: 1948.828
iteration 0300: loss: 1948.379
iteration 0400: loss: 1947.782
iteration 0500: loss: 1947.479
iteration 0600: loss: 1947.690
iteration 0700: loss: 1948.116
iteration 0800: loss: 1948.287
iteration 0900: loss: 1948.177
====> Epoch: 013 Train loss: 1948.1913  took : 12.506441831588745
====> Test loss: 1949.9273
iteration 0000: loss: 1947.854
iteration 0100: loss: 1947.930
iteration 0200: loss: 1947.886
iteration 0300: loss: 1948.961
iteration 0400: loss: 1948.357
iteration 0500: loss: 1948.002
iteration 0600: loss: 1949.646
iteration 0700: loss: 1949.232
iteration 0800: loss: 1948.829
iteration 0900: loss: 1949.372
====> Epoch: 014 Train loss: 1948.3465  took : 13.047130346298218
====> Test loss: 1950.3745
iteration 0000: loss: 1948.716
iteration 0100: loss: 1948.636
iteration 0200: loss: 1948.850
iteration 0300: loss: 1949.514
iteration 0400: loss: 1947.696
iteration 0500: loss: 1950.011
iteration 0600: loss: 1949.977
iteration 0700: loss: 1948.326
iteration 0800: loss: 1948.375
iteration 0900: loss: 1947.507
====> Epoch: 015 Train loss: 1948.2934  took : 12.95372200012207
====> Test loss: 1950.2928
iteration 0000: loss: 1947.895
iteration 0100: loss: 1947.931
iteration 0200: loss: 1947.248
iteration 0300: loss: 1948.106
iteration 0400: loss: 1948.138
iteration 0500: loss: 1948.826
iteration 0600: loss: 1947.418
iteration 0700: loss: 1948.159
iteration 0800: loss: 1947.673
iteration 0900: loss: 1949.203
====> Epoch: 016 Train loss: 1947.9319  took : 13.168501853942871
====> Test loss: 1950.1466
iteration 0000: loss: 1947.312
iteration 0100: loss: 1948.180
iteration 0200: loss: 1947.325
iteration 0300: loss: 1947.449
iteration 0400: loss: 1948.434
iteration 0500: loss: 1948.079
iteration 0600: loss: 1947.731
iteration 0700: loss: 1947.479
iteration 0800: loss: 1947.613
iteration 0900: loss: 1948.162
====> Epoch: 017 Train loss: 1947.8171  took : 12.853140830993652
====> Test loss: 1949.7138
iteration 0000: loss: 1946.883
iteration 0100: loss: 1948.423
iteration 0200: loss: 1948.339
iteration 0300: loss: 1946.962
iteration 0400: loss: 1948.438
iteration 0500: loss: 1947.683
iteration 0600: loss: 1948.043
iteration 0700: loss: 1948.627
iteration 0800: loss: 1947.321
iteration 0900: loss: 1947.433
====> Epoch: 018 Train loss: 1947.9321  took : 12.542816638946533
====> Test loss: 1949.6510
iteration 0000: loss: 1947.853
iteration 0100: loss: 1948.535
iteration 0200: loss: 1947.958
iteration 0300: loss: 1947.934
iteration 0400: loss: 1947.356
iteration 0500: loss: 1947.799
iteration 0600: loss: 1947.422
iteration 0700: loss: 1947.576
iteration 0800: loss: 1948.581
iteration 0900: loss: 1947.663
====> Epoch: 019 Train loss: 1947.7236  took : 12.919901609420776
====> Test loss: 1949.9656
iteration 0000: loss: 1947.615
iteration 0100: loss: 1946.989
iteration 0200: loss: 1947.958
iteration 0300: loss: 1947.570
iteration 0400: loss: 1948.033
iteration 0500: loss: 1948.459
iteration 0600: loss: 1949.351
iteration 0700: loss: 1946.828
iteration 0800: loss: 1948.630
iteration 0900: loss: 1947.059
====> Epoch: 020 Train loss: 1947.6974  took : 13.02603530883789
====> Test loss: 1949.5591
iteration 0000: loss: 1947.841
iteration 0100: loss: 1948.448
iteration 0200: loss: 1947.155
iteration 0300: loss: 1946.913
iteration 0400: loss: 1947.948
iteration 0500: loss: 1947.602
iteration 0600: loss: 1947.561
iteration 0700: loss: 1947.945
iteration 0800: loss: 1948.652
iteration 0900: loss: 1947.451
====> Epoch: 021 Train loss: 1947.6763  took : 13.025460243225098
====> Test loss: 1949.6457
iteration 0000: loss: 1947.354
iteration 0100: loss: 1947.390
iteration 0200: loss: 1948.537
iteration 0300: loss: 1948.471
iteration 0400: loss: 1946.903
iteration 0500: loss: 1947.876
iteration 0600: loss: 1947.563
iteration 0700: loss: 1947.549
iteration 0800: loss: 1949.590
iteration 0900: loss: 1947.919
====> Epoch: 022 Train loss: 1947.6349  took : 12.53806185722351
====> Test loss: 1949.7303
iteration 0000: loss: 1947.401
iteration 0100: loss: 1948.028
iteration 0200: loss: 1947.162
iteration 0300: loss: 1946.899
iteration 0400: loss: 1948.004
iteration 0500: loss: 1948.462
iteration 0600: loss: 1947.389
iteration 0700: loss: 1947.195
iteration 0800: loss: 1947.463
iteration 0900: loss: 1948.651
====> Epoch: 023 Train loss: 1947.6094  took : 12.669062614440918
====> Test loss: 1949.4742
iteration 0000: loss: 1947.171
iteration 0100: loss: 1947.987
iteration 0200: loss: 1947.006
iteration 0300: loss: 1950.463
iteration 0400: loss: 1946.703
iteration 0500: loss: 1947.271
iteration 0600: loss: 1948.208
iteration 0700: loss: 1947.910
iteration 0800: loss: 1948.142
iteration 0900: loss: 1947.843
====> Epoch: 024 Train loss: 1947.7940  took : 13.076447248458862
====> Test loss: 1950.0747
iteration 0000: loss: 1948.750
iteration 0100: loss: 1948.430
iteration 0200: loss: 1948.511
iteration 0300: loss: 1947.922
iteration 0400: loss: 1949.265
iteration 0500: loss: 1948.555
iteration 0600: loss: 1947.724
iteration 0700: loss: 1946.786
iteration 0800: loss: 1947.481
iteration 0900: loss: 1947.060
====> Epoch: 025 Train loss: 1947.6723  took : 12.779331922531128
====> Test loss: 1949.4174
iteration 0000: loss: 1946.847
iteration 0100: loss: 1949.125
iteration 0200: loss: 1947.059
iteration 0300: loss: 1947.176
iteration 0400: loss: 1946.656
iteration 0500: loss: 1947.621
iteration 0600: loss: 1948.565
iteration 0700: loss: 1947.149
iteration 0800: loss: 1947.678
iteration 0900: loss: 1947.542
====> Epoch: 026 Train loss: 1947.5824  took : 12.618791341781616
====> Test loss: 1949.6846
iteration 0000: loss: 1947.682
iteration 0100: loss: 1947.307
iteration 0200: loss: 1946.889
iteration 0300: loss: 1948.114
iteration 0400: loss: 1947.587
iteration 0500: loss: 1947.759
iteration 0600: loss: 1947.112
iteration 0700: loss: 1948.829
iteration 0800: loss: 1947.703
iteration 0900: loss: 1947.813
====> Epoch: 027 Train loss: 1947.6162  took : 12.175813436508179
====> Test loss: 1950.2883
iteration 0000: loss: 1948.675
iteration 0100: loss: 1948.772
iteration 0200: loss: 1948.196
iteration 0300: loss: 1947.070
iteration 0400: loss: 1947.073
iteration 0500: loss: 1947.060
iteration 0600: loss: 1947.349
iteration 0700: loss: 1948.501
iteration 0800: loss: 1947.169
iteration 0900: loss: 1946.919
====> Epoch: 028 Train loss: 1947.5450  took : 12.333803653717041
====> Test loss: 1949.4247
iteration 0000: loss: 1946.964
iteration 0100: loss: 1946.787
iteration 0200: loss: 1947.631
iteration 0300: loss: 1947.654
iteration 0400: loss: 1947.180
iteration 0500: loss: 1947.140
iteration 0600: loss: 1947.197
iteration 0700: loss: 1947.990
iteration 0800: loss: 1947.395
iteration 0900: loss: 1947.678
====> Epoch: 029 Train loss: 1947.4145  took : 12.774368524551392
====> Test loss: 1949.8090
iteration 0000: loss: 1947.383
iteration 0100: loss: 1947.038
iteration 0200: loss: 1947.307
iteration 0300: loss: 1947.413
iteration 0400: loss: 1946.769
iteration 0500: loss: 1946.846
iteration 0600: loss: 1947.236
iteration 0700: loss: 1947.469
iteration 0800: loss: 1947.371
iteration 0900: loss: 1946.921
====> Epoch: 030 Train loss: 1947.4915  took : 13.016546726226807
====> Test loss: 1949.3312
iteration 0000: loss: 1947.280
iteration 0100: loss: 1946.818
iteration 0200: loss: 1948.239
iteration 0300: loss: 1947.700
iteration 0400: loss: 1947.112
iteration 0500: loss: 1946.939
iteration 0600: loss: 1947.612
iteration 0700: loss: 1946.948
iteration 0800: loss: 1947.921
iteration 0900: loss: 1947.446
====> Epoch: 031 Train loss: 1947.3290  took : 12.045881032943726
====> Test loss: 1949.0311
iteration 0000: loss: 1947.544
iteration 0100: loss: 1947.170
iteration 0200: loss: 1947.584
iteration 0300: loss: 1946.726
iteration 0400: loss: 1946.982
iteration 0500: loss: 1947.096
iteration 0600: loss: 1947.107
iteration 0700: loss: 1947.025
iteration 0800: loss: 1946.955
iteration 0900: loss: 1947.521
====> Epoch: 032 Train loss: 1947.3099  took : 12.404819965362549
====> Test loss: 1949.3656
iteration 0000: loss: 1947.373
iteration 0100: loss: 1946.983
iteration 0200: loss: 1947.021
iteration 0300: loss: 1947.497
iteration 0400: loss: 1947.186
iteration 0500: loss: 1946.955
iteration 0600: loss: 1947.097
iteration 0700: loss: 1946.622
iteration 0800: loss: 1947.222
iteration 0900: loss: 1946.889
====> Epoch: 033 Train loss: 1947.1997  took : 12.843347311019897
====> Test loss: 1949.6179
iteration 0000: loss: 1947.554
iteration 0100: loss: 1947.002
iteration 0200: loss: 1947.055
iteration 0300: loss: 1947.722
iteration 0400: loss: 1947.266
iteration 0500: loss: 1948.013
iteration 0600: loss: 1947.222
iteration 0700: loss: 1946.811
iteration 0800: loss: 1947.207
iteration 0900: loss: 1947.889
====> Epoch: 034 Train loss: 1947.4094  took : 13.458480596542358
====> Test loss: 1949.5576
iteration 0000: loss: 1947.037
iteration 0100: loss: 1947.820
iteration 0200: loss: 1947.262
iteration 0300: loss: 1947.706
iteration 0400: loss: 1947.316
iteration 0500: loss: 1947.867
iteration 0600: loss: 1948.315
iteration 0700: loss: 1947.268
iteration 0800: loss: 1947.107
iteration 0900: loss: 1947.020
====> Epoch: 035 Train loss: 1947.3293  took : 13.554108619689941
====> Test loss: 1949.7710
iteration 0000: loss: 1947.691
iteration 0100: loss: 1948.134
iteration 0200: loss: 1949.047
iteration 0300: loss: 1947.590
iteration 0400: loss: 1947.116
iteration 0500: loss: 1947.601
iteration 0600: loss: 1946.981
iteration 0700: loss: 1947.044
iteration 0800: loss: 1946.997
iteration 0900: loss: 1946.808
====> Epoch: 036 Train loss: 1947.4124  took : 12.643356323242188
====> Test loss: 1949.1199
iteration 0000: loss: 1946.972
iteration 0100: loss: 1947.375
iteration 0200: loss: 1948.376
iteration 0300: loss: 1947.387
iteration 0400: loss: 1947.018
iteration 0500: loss: 1947.119
iteration 0600: loss: 1947.078
iteration 0700: loss: 1947.342
iteration 0800: loss: 1947.934
iteration 0900: loss: 1947.898
====> Epoch: 037 Train loss: 1947.4762  took : 13.361661434173584
====> Test loss: 1949.1777
iteration 0000: loss: 1946.776
iteration 0100: loss: 1946.744
iteration 0200: loss: 1947.273
iteration 0300: loss: 1946.639
iteration 0400: loss: 1947.061
iteration 0500: loss: 1947.119
iteration 0600: loss: 1948.132
iteration 0700: loss: 1947.555
iteration 0800: loss: 1947.467
iteration 0900: loss: 1947.019
====> Epoch: 038 Train loss: 1947.2870  took : 13.442835092544556
====> Test loss: 1949.0484
iteration 0000: loss: 1946.976
iteration 0100: loss: 1947.174
iteration 0200: loss: 1947.224
iteration 0300: loss: 1946.888
iteration 0400: loss: 1947.752
iteration 0500: loss: 1946.775
iteration 0600: loss: 1947.409
iteration 0700: loss: 1947.066
iteration 0800: loss: 1948.080
iteration 0900: loss: 1946.838
====> Epoch: 039 Train loss: 1947.1724  took : 12.271550416946411
====> Test loss: 1949.0899
iteration 0000: loss: 1946.782
iteration 0100: loss: 1946.878
iteration 0200: loss: 1947.534
iteration 0300: loss: 1948.142
iteration 0400: loss: 1947.018
iteration 0500: loss: 1947.247
iteration 0600: loss: 1947.423
iteration 0700: loss: 1946.999
iteration 0800: loss: 1946.893
iteration 0900: loss: 1947.230
====> Epoch: 040 Train loss: 1947.2628  took : 11.999979972839355
====> Test loss: 1948.9606
iteration 0000: loss: 1947.002
iteration 0100: loss: 1947.723
iteration 0200: loss: 1946.913
iteration 0300: loss: 1946.854
iteration 0400: loss: 1948.224
iteration 0500: loss: 1947.690
iteration 0600: loss: 1946.764
iteration 0700: loss: 1946.883
iteration 0800: loss: 1947.036
iteration 0900: loss: 1946.979
====> Epoch: 041 Train loss: 1947.3293  took : 12.437950611114502
====> Test loss: 1949.4692
iteration 0000: loss: 1948.188
iteration 0100: loss: 1946.760
iteration 0200: loss: 1947.719
iteration 0300: loss: 1947.166
iteration 0400: loss: 1947.068
iteration 0500: loss: 1946.966
iteration 0600: loss: 1947.514
iteration 0700: loss: 1947.090
iteration 0800: loss: 1946.888
iteration 0900: loss: 1947.347
====> Epoch: 042 Train loss: 1947.1959  took : 12.30863881111145
====> Test loss: 1949.3853
iteration 0000: loss: 1947.707
iteration 0100: loss: 1947.391
iteration 0200: loss: 1946.888
iteration 0300: loss: 1947.481
iteration 0400: loss: 1947.007
iteration 0500: loss: 1947.123
iteration 0600: loss: 1947.064
iteration 0700: loss: 1947.012
iteration 0800: loss: 1948.631
iteration 0900: loss: 1946.948
====> Epoch: 043 Train loss: 1947.1003  took : 13.063105821609497
====> Test loss: 1949.0961
iteration 0000: loss: 1947.251
iteration 0100: loss: 1947.273
iteration 0200: loss: 1947.027
iteration 0300: loss: 1947.327
iteration 0400: loss: 1946.532
iteration 0500: loss: 1946.888
iteration 0600: loss: 1946.992
iteration 0700: loss: 1947.005
iteration 0800: loss: 1947.156
iteration 0900: loss: 1946.743
====> Epoch: 044 Train loss: 1947.1793  took : 12.890369176864624
====> Test loss: 1949.1895
iteration 0000: loss: 1946.825
iteration 0100: loss: 1946.827
iteration 0200: loss: 1948.224
iteration 0300: loss: 1947.113
iteration 0400: loss: 1946.841
iteration 0500: loss: 1947.377
iteration 0600: loss: 1947.917
iteration 0700: loss: 1947.614
iteration 0800: loss: 1947.575
iteration 0900: loss: 1946.902
====> Epoch: 045 Train loss: 1947.3555  took : 12.999851942062378
====> Test loss: 1948.9611
iteration 0000: loss: 1946.784
iteration 0100: loss: 1947.663
iteration 0200: loss: 1946.823
iteration 0300: loss: 1947.228
iteration 0400: loss: 1946.618
iteration 0500: loss: 1946.939
iteration 0600: loss: 1946.737
iteration 0700: loss: 1948.335
iteration 0800: loss: 1946.910
iteration 0900: loss: 1946.717
====> Epoch: 046 Train loss: 1947.1990  took : 13.166776180267334
====> Test loss: 1949.2337
iteration 0000: loss: 1947.610
iteration 0100: loss: 1946.601
iteration 0200: loss: 1947.627
iteration 0300: loss: 1946.898
iteration 0400: loss: 1946.812
iteration 0500: loss: 1947.712
iteration 0600: loss: 1947.121
iteration 0700: loss: 1946.618
iteration 0800: loss: 1946.642
iteration 0900: loss: 1946.523
====> Epoch: 047 Train loss: 1947.1058  took : 12.817199230194092
====> Test loss: 1949.7753
iteration 0000: loss: 1947.459
iteration 0100: loss: 1946.581
iteration 0200: loss: 1947.155
iteration 0300: loss: 1946.722
iteration 0400: loss: 1946.988
iteration 0500: loss: 1946.737
iteration 0600: loss: 1947.606
iteration 0700: loss: 1946.819
iteration 0800: loss: 1946.543
iteration 0900: loss: 1947.096
====> Epoch: 048 Train loss: 1947.3429  took : 12.902169704437256
====> Test loss: 1949.1151
iteration 0000: loss: 1946.976
iteration 0100: loss: 1947.063
iteration 0200: loss: 1947.127
iteration 0300: loss: 1946.819
iteration 0400: loss: 1947.266
iteration 0500: loss: 1946.721
iteration 0600: loss: 1948.767
iteration 0700: loss: 1947.275
iteration 0800: loss: 1946.661
iteration 0900: loss: 1947.306
====> Epoch: 049 Train loss: 1947.1920  took : 12.357781648635864
====> Test loss: 1949.1103
iteration 0000: loss: 1947.056
iteration 0100: loss: 1947.086
iteration 0200: loss: 1946.634
iteration 0300: loss: 1947.619
iteration 0400: loss: 1947.158
iteration 0500: loss: 1946.732
iteration 0600: loss: 1947.191
iteration 0700: loss: 1947.710
iteration 0800: loss: 1947.139
iteration 0900: loss: 1946.832
====> Epoch: 050 Train loss: 1947.1381  took : 12.254183292388916
====> Test loss: 1948.9821
====> [MM-VAE] Time: 706.437s or 00:11:46
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  6
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_6
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_6
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5218.802
iteration 0100: loss: 4167.208
iteration 0200: loss: 4083.940
iteration 0300: loss: 4059.706
iteration 0400: loss: 4028.025
iteration 0500: loss: 4020.219
iteration 0600: loss: 4022.345
iteration 0700: loss: 4023.051
iteration 0800: loss: 4010.370
iteration 0900: loss: 4019.816
iteration 1000: loss: 3995.399
iteration 1100: loss: 4002.430
iteration 1200: loss: 4000.988
iteration 1300: loss: 4000.919
iteration 1400: loss: 4002.971
iteration 1500: loss: 3988.051
iteration 1600: loss: 3996.025
iteration 1700: loss: 3994.889
iteration 1800: loss: 3999.451
====> Epoch: 001 Train loss: 4028.4273  took : 53.57225513458252
====> Test loss: 3995.2397
iteration 0000: loss: 3997.606
iteration 0100: loss: 3990.419
iteration 0200: loss: 3983.473
iteration 0300: loss: 3996.807
iteration 0400: loss: 3985.255
iteration 0500: loss: 3983.769
iteration 0600: loss: 3989.588
iteration 0700: loss: 3984.163
iteration 0800: loss: 3983.092
iteration 0900: loss: 3990.553
iteration 1000: loss: 3983.811
iteration 1100: loss: 3976.995
iteration 1200: loss: 3979.938
iteration 1300: loss: 3965.672
iteration 1400: loss: 3980.000
iteration 1500: loss: 3974.741
iteration 1600: loss: 3969.057
iteration 1700: loss: 3964.844
iteration 1800: loss: 3969.233
====> Epoch: 002 Train loss: 3981.0854  took : 53.34768772125244
====> Test loss: 3971.8748
iteration 0000: loss: 3961.942
iteration 0100: loss: 3975.188
iteration 0200: loss: 3972.961
iteration 0300: loss: 3969.129
iteration 0400: loss: 3961.062
iteration 0500: loss: 3957.758
iteration 0600: loss: 3963.643
iteration 0700: loss: 3961.123
iteration 0800: loss: 3970.217
iteration 0900: loss: 3956.731
iteration 1000: loss: 3960.695
iteration 1100: loss: 3967.109
iteration 1200: loss: 3964.467
iteration 1300: loss: 3960.739
iteration 1400: loss: 3969.042
iteration 1500: loss: 3956.992
iteration 1600: loss: 3966.681
iteration 1700: loss: 3961.302
iteration 1800: loss: 3960.841
====> Epoch: 003 Train loss: 3962.2056  took : 53.41058659553528
====> Test loss: 3959.7084
iteration 0000: loss: 3958.958
iteration 0100: loss: 3963.855
iteration 0200: loss: 3947.867
iteration 0300: loss: 3956.114
iteration 0400: loss: 3953.782
iteration 0500: loss: 3955.800
iteration 0600: loss: 3952.267
iteration 0700: loss: 3955.468
iteration 0800: loss: 3959.101
iteration 0900: loss: 3958.815
iteration 1000: loss: 3960.434
iteration 1100: loss: 3943.593
iteration 1200: loss: 3960.860
iteration 1300: loss: 3949.707
iteration 1400: loss: 3953.844
iteration 1500: loss: 3955.450
iteration 1600: loss: 3949.272
iteration 1700: loss: 3954.126
iteration 1800: loss: 3945.271
====> Epoch: 004 Train loss: 3953.1880  took : 53.276235818862915
====> Test loss: 3954.2454
iteration 0000: loss: 3941.291
iteration 0100: loss: 3953.884
iteration 0200: loss: 3945.710
iteration 0300: loss: 3942.495
iteration 0400: loss: 3945.936
iteration 0500: loss: 3952.594
iteration 0600: loss: 3948.234
iteration 0700: loss: 3950.532
iteration 0800: loss: 3951.878
iteration 0900: loss: 3945.130
iteration 1000: loss: 3958.118
iteration 1100: loss: 3949.166
iteration 1200: loss: 3945.279
iteration 1300: loss: 3955.400
iteration 1400: loss: 3943.829
iteration 1500: loss: 3948.902
iteration 1600: loss: 3946.548
iteration 1700: loss: 3943.307
iteration 1800: loss: 3948.883
====> Epoch: 005 Train loss: 3948.3290  took : 53.121604919433594
====> Test loss: 3949.8987
iteration 0000: loss: 3945.102
iteration 0100: loss: 3945.427
iteration 0200: loss: 3942.596
iteration 0300: loss: 3940.311
iteration 0400: loss: 3942.773
iteration 0500: loss: 3945.891
iteration 0600: loss: 3952.520
iteration 0700: loss: 3949.486
iteration 0800: loss: 3949.784
iteration 0900: loss: 3946.208
iteration 1000: loss: 3941.435
iteration 1100: loss: 3946.808
iteration 1200: loss: 3943.791
iteration 1300: loss: 3942.048
iteration 1400: loss: 3943.822
iteration 1500: loss: 3941.949
iteration 1600: loss: 3943.064
iteration 1700: loss: 3946.517
iteration 1800: loss: 3943.823
====> Epoch: 006 Train loss: 3945.8339  took : 53.22090768814087
====> Test loss: 3949.3748
iteration 0000: loss: 3946.101
iteration 0100: loss: 3939.299
iteration 0200: loss: 3948.244
iteration 0300: loss: 3950.925
iteration 0400: loss: 3950.730
iteration 0500: loss: 3946.813
iteration 0600: loss: 3940.417
iteration 0700: loss: 3941.614
iteration 0800: loss: 3940.403
iteration 0900: loss: 3942.509
iteration 1000: loss: 3946.023
iteration 1100: loss: 3936.967
iteration 1200: loss: 3944.441
iteration 1300: loss: 3937.024
iteration 1400: loss: 3944.452
iteration 1500: loss: 3941.325
iteration 1600: loss: 3942.346
iteration 1700: loss: 3940.246
iteration 1800: loss: 3942.197
====> Epoch: 007 Train loss: 3944.4950  took : 53.16652464866638
====> Test loss: 3947.4315
iteration 0000: loss: 3942.562
iteration 0100: loss: 3943.004
iteration 0200: loss: 3944.919
iteration 0300: loss: 3942.280
iteration 0400: loss: 3940.455
iteration 0500: loss: 3948.463
iteration 0600: loss: 3938.193
iteration 0700: loss: 3944.825
iteration 0800: loss: 3941.196
iteration 0900: loss: 3941.693
iteration 1000: loss: 3944.203
iteration 1100: loss: 3944.346
iteration 1200: loss: 3946.046
iteration 1300: loss: 3943.107
iteration 1400: loss: 3939.789
iteration 1500: loss: 3944.252
iteration 1600: loss: 3944.121
iteration 1700: loss: 3945.619
iteration 1800: loss: 3944.551
====> Epoch: 008 Train loss: 3943.7095  took : 53.255216121673584
====> Test loss: 3947.2996
iteration 0000: loss: 3945.770
iteration 0100: loss: 3938.375
iteration 0200: loss: 3944.901
iteration 0300: loss: 3940.929
iteration 0400: loss: 3944.388
iteration 0500: loss: 3940.916
iteration 0600: loss: 3938.351
iteration 0700: loss: 3946.640
iteration 0800: loss: 3946.475
iteration 0900: loss: 3938.335
iteration 1000: loss: 3948.505
iteration 1100: loss: 3937.820
iteration 1200: loss: 3947.562
iteration 1300: loss: 3944.758
iteration 1400: loss: 3944.337
iteration 1500: loss: 3946.250
iteration 1600: loss: 3947.096
iteration 1700: loss: 3946.966
iteration 1800: loss: 3940.743
====> Epoch: 009 Train loss: 3943.1049  took : 52.95014047622681
====> Test loss: 3946.0889
iteration 0000: loss: 3944.286
iteration 0100: loss: 3950.738
iteration 0200: loss: 3945.677
iteration 0300: loss: 3937.057
iteration 0400: loss: 3939.819
iteration 0500: loss: 3942.337
iteration 0600: loss: 3943.981
iteration 0700: loss: 3943.068
iteration 0800: loss: 3940.867
iteration 0900: loss: 3949.728
iteration 1000: loss: 3941.403
iteration 1100: loss: 3942.168
iteration 1200: loss: 3943.687
iteration 1300: loss: 3943.710
iteration 1400: loss: 3937.748
iteration 1500: loss: 3940.139
iteration 1600: loss: 3939.354
iteration 1700: loss: 3945.266
iteration 1800: loss: 3943.784
====> Epoch: 010 Train loss: 3942.5669  took : 53.10368871688843
====> Test loss: 3945.9199
iteration 0000: loss: 3936.571
iteration 0100: loss: 3946.628
iteration 0200: loss: 3941.287
iteration 0300: loss: 3936.131
iteration 0400: loss: 3942.526
iteration 0500: loss: 3943.039
iteration 0600: loss: 3944.903
iteration 0700: loss: 3938.640
iteration 0800: loss: 3937.980
iteration 0900: loss: 3944.702
iteration 1000: loss: 3941.683
iteration 1100: loss: 3943.881
iteration 1200: loss: 3939.780
iteration 1300: loss: 3940.513
iteration 1400: loss: 3950.772
iteration 1500: loss: 3936.985
iteration 1600: loss: 3939.919
iteration 1700: loss: 3949.711
iteration 1800: loss: 3942.860
====> Epoch: 011 Train loss: 3942.3374  took : 53.25036954879761
====> Test loss: 3946.6342
iteration 0000: loss: 3948.752
iteration 0100: loss: 3939.760
iteration 0200: loss: 3941.978
iteration 0300: loss: 3942.814
iteration 0400: loss: 3935.144
iteration 0500: loss: 3937.107
iteration 0600: loss: 3941.951
iteration 0700: loss: 3943.883
iteration 0800: loss: 3941.612
iteration 0900: loss: 3941.668
iteration 1000: loss: 3941.979
iteration 1100: loss: 3943.985
iteration 1200: loss: 3945.952
iteration 1300: loss: 3940.020
iteration 1400: loss: 3945.435
iteration 1500: loss: 3944.146
iteration 1600: loss: 3947.325
iteration 1700: loss: 3938.420
iteration 1800: loss: 3941.511
====> Epoch: 012 Train loss: 3942.2349  took : 53.04967713356018
====> Test loss: 3945.7807
iteration 0000: loss: 3936.472
iteration 0100: loss: 3943.784
iteration 0200: loss: 3938.281
iteration 0300: loss: 3941.738
iteration 0400: loss: 3937.140
iteration 0500: loss: 3939.477
iteration 0600: loss: 3941.700
iteration 0700: loss: 3936.086
iteration 0800: loss: 3945.273
iteration 0900: loss: 3941.948
iteration 1000: loss: 3949.131
iteration 1100: loss: 3946.380
iteration 1200: loss: 3943.373
iteration 1300: loss: 3940.099
iteration 1400: loss: 3938.222
iteration 1500: loss: 3945.220
iteration 1600: loss: 3944.543
iteration 1700: loss: 3945.771
iteration 1800: loss: 3940.877
====> Epoch: 013 Train loss: 3941.4833  took : 53.15470337867737
====> Test loss: 3945.5628
iteration 0000: loss: 3939.551
iteration 0100: loss: 3946.312
iteration 0200: loss: 3935.222
iteration 0300: loss: 3939.851
iteration 0400: loss: 3942.085
iteration 0500: loss: 3939.216
iteration 0600: loss: 3938.457
iteration 0700: loss: 3940.924
iteration 0800: loss: 3943.096
iteration 0900: loss: 3939.586
iteration 1000: loss: 3938.177
iteration 1100: loss: 3938.176
iteration 1200: loss: 3935.275
iteration 1300: loss: 3940.053
iteration 1400: loss: 3943.143
iteration 1500: loss: 3942.146
iteration 1600: loss: 3937.267
iteration 1700: loss: 3948.225
iteration 1800: loss: 3939.291
====> Epoch: 014 Train loss: 3941.3944  took : 53.18499255180359
====> Test loss: 3947.2387
iteration 0000: loss: 3939.244
iteration 0100: loss: 3941.209
iteration 0200: loss: 3943.244
iteration 0300: loss: 3945.776
iteration 0400: loss: 3933.656
iteration 0500: loss: 3947.194
iteration 0600: loss: 3941.718
iteration 0700: loss: 3939.520
iteration 0800: loss: 3941.906
iteration 0900: loss: 3942.259
iteration 1000: loss: 3936.091
iteration 1100: loss: 3939.819
iteration 1200: loss: 3941.397
iteration 1300: loss: 3941.317
iteration 1400: loss: 3939.806
iteration 1500: loss: 3941.037
iteration 1600: loss: 3940.177
iteration 1700: loss: 3938.812
iteration 1800: loss: 3937.828
====> Epoch: 015 Train loss: 3941.3137  took : 53.242286682128906
====> Test loss: 3944.9438
iteration 0000: loss: 3940.308
iteration 0100: loss: 3941.346
iteration 0200: loss: 3940.677
iteration 0300: loss: 3932.434
iteration 0400: loss: 3940.973
iteration 0500: loss: 3934.052
iteration 0600: loss: 3941.708
iteration 0700: loss: 3943.889
iteration 0800: loss: 3936.145
iteration 0900: loss: 3947.734
iteration 1000: loss: 3941.861
iteration 1100: loss: 3942.853
iteration 1200: loss: 3939.563
iteration 1300: loss: 3943.181
iteration 1400: loss: 3941.563
iteration 1500: loss: 3938.397
iteration 1600: loss: 3935.939
iteration 1700: loss: 3939.500
iteration 1800: loss: 3944.552
====> Epoch: 016 Train loss: 3940.8976  took : 53.06602430343628
====> Test loss: 3944.4449
iteration 0000: loss: 3944.664
iteration 0100: loss: 3937.230
iteration 0200: loss: 3943.303
iteration 0300: loss: 3936.980
iteration 0400: loss: 3937.515
iteration 0500: loss: 3937.415
iteration 0600: loss: 3941.886
iteration 0700: loss: 3945.721
iteration 0800: loss: 3941.755
iteration 0900: loss: 3948.181
iteration 1000: loss: 3934.205
iteration 1100: loss: 3939.257
iteration 1200: loss: 3938.870
iteration 1300: loss: 3946.668
iteration 1400: loss: 3943.917
iteration 1500: loss: 3939.905
iteration 1600: loss: 3934.576
iteration 1700: loss: 3949.041
iteration 1800: loss: 3935.788
====> Epoch: 017 Train loss: 3940.4980  took : 53.270124673843384
====> Test loss: 3944.1860
iteration 0000: loss: 3935.734
iteration 0100: loss: 3939.255
iteration 0200: loss: 3936.819
iteration 0300: loss: 3940.336
iteration 0400: loss: 3941.792
iteration 0500: loss: 3936.832
iteration 0600: loss: 3933.298
iteration 0700: loss: 3938.300
iteration 0800: loss: 3941.553
iteration 0900: loss: 3939.350
iteration 1000: loss: 3946.953
iteration 1100: loss: 3938.483
iteration 1200: loss: 3942.491
iteration 1300: loss: 3941.378
iteration 1400: loss: 3940.150
iteration 1500: loss: 3945.413
iteration 1600: loss: 3942.968
iteration 1700: loss: 3945.996
iteration 1800: loss: 3942.181
====> Epoch: 018 Train loss: 3940.2603  took : 52.99642992019653
====> Test loss: 3944.9309
iteration 0000: loss: 3945.721
iteration 0100: loss: 3942.167
iteration 0200: loss: 3940.656
iteration 0300: loss: 3944.467
iteration 0400: loss: 3947.067
iteration 0500: loss: 3944.242
iteration 0600: loss: 3937.969
iteration 0700: loss: 3940.883
iteration 0800: loss: 3939.771
iteration 0900: loss: 3943.067
iteration 1000: loss: 3936.730
iteration 1100: loss: 3939.583
iteration 1200: loss: 3929.696
iteration 1300: loss: 3940.795
iteration 1400: loss: 3938.183
iteration 1500: loss: 3940.167
iteration 1600: loss: 3936.395
iteration 1700: loss: 3939.031
iteration 1800: loss: 3940.926
====> Epoch: 019 Train loss: 3940.0936  took : 53.15405607223511
====> Test loss: 3944.1464
iteration 0000: loss: 3939.372
iteration 0100: loss: 3944.025
iteration 0200: loss: 3940.406
iteration 0300: loss: 3935.945
iteration 0400: loss: 3936.795
iteration 0500: loss: 3935.523
iteration 0600: loss: 3937.816
iteration 0700: loss: 3949.674
iteration 0800: loss: 3944.530
iteration 0900: loss: 3937.928
iteration 1000: loss: 3942.460
iteration 1100: loss: 3929.432
iteration 1200: loss: 3942.517
iteration 1300: loss: 3939.536
iteration 1400: loss: 3931.536
iteration 1500: loss: 3936.598
iteration 1600: loss: 3936.817
iteration 1700: loss: 3939.982
iteration 1800: loss: 3939.632
====> Epoch: 020 Train loss: 3940.1286  took : 52.81253695487976
====> Test loss: 3944.2442
iteration 0000: loss: 3938.865
iteration 0100: loss: 3940.793
iteration 0200: loss: 3941.712
iteration 0300: loss: 3936.993
iteration 0400: loss: 3936.661
iteration 0500: loss: 3942.567
iteration 0600: loss: 3937.895
iteration 0700: loss: 3943.266
iteration 0800: loss: 3943.408
iteration 0900: loss: 3940.406
iteration 1000: loss: 3937.841
iteration 1100: loss: 3941.956
iteration 1200: loss: 3942.815
iteration 1300: loss: 3940.764
iteration 1400: loss: 3941.776
iteration 1500: loss: 3946.301
iteration 1600: loss: 3940.167
iteration 1700: loss: 3935.769
iteration 1800: loss: 3940.090
====> Epoch: 021 Train loss: 3939.9483  took : 53.01568651199341
====> Test loss: 3944.3825
iteration 0000: loss: 3942.239
iteration 0100: loss: 3938.867
iteration 0200: loss: 3937.967
iteration 0300: loss: 3934.062
iteration 0400: loss: 3935.279
iteration 0500: loss: 3938.105
iteration 0600: loss: 3933.354
iteration 0700: loss: 3936.130
iteration 0800: loss: 3946.378
iteration 0900: loss: 3940.161
iteration 1000: loss: 3932.557
iteration 1100: loss: 3938.021
iteration 1200: loss: 3942.212
iteration 1300: loss: 3935.596
iteration 1400: loss: 3937.481
iteration 1500: loss: 3949.392
iteration 1600: loss: 3947.779
iteration 1700: loss: 3934.996
iteration 1800: loss: 3935.594
====> Epoch: 022 Train loss: 3939.8408  took : 53.040024280548096
====> Test loss: 3944.1574
iteration 0000: loss: 3947.240
iteration 0100: loss: 3947.142
iteration 0200: loss: 3933.767
iteration 0300: loss: 3943.587
iteration 0400: loss: 3939.260
iteration 0500: loss: 3939.011
iteration 0600: loss: 3945.073
iteration 0700: loss: 3939.687
iteration 0800: loss: 3939.525
iteration 0900: loss: 3936.442
iteration 1000: loss: 3942.309
iteration 1100: loss: 3943.521
iteration 1200: loss: 3950.623
iteration 1300: loss: 3946.145
iteration 1400: loss: 3942.563
iteration 1500: loss: 3943.344
iteration 1600: loss: 3935.843
iteration 1700: loss: 3937.847
iteration 1800: loss: 3940.493
====> Epoch: 023 Train loss: 3939.8732  took : 53.327977657318115
====> Test loss: 3943.7874
iteration 0000: loss: 3936.176
iteration 0100: loss: 3934.406
iteration 0200: loss: 3935.460
iteration 0300: loss: 3938.868
iteration 0400: loss: 3935.212
iteration 0500: loss: 3941.132
iteration 0600: loss: 3939.465
iteration 0700: loss: 3938.419
iteration 0800: loss: 3938.224
iteration 0900: loss: 3934.874
iteration 1000: loss: 3937.133
iteration 1100: loss: 3936.226
iteration 1200: loss: 3943.593
iteration 1300: loss: 3939.217
iteration 1400: loss: 3941.929
iteration 1500: loss: 3941.551
iteration 1600: loss: 3943.867
iteration 1700: loss: 3939.899
iteration 1800: loss: 3939.937
====> Epoch: 024 Train loss: 3939.8713  took : 53.230857849121094
====> Test loss: 3943.7188
iteration 0000: loss: 3932.786
iteration 0100: loss: 3945.926
iteration 0200: loss: 3932.990
iteration 0300: loss: 3938.518
iteration 0400: loss: 3933.392
iteration 0500: loss: 3949.888
iteration 0600: loss: 3937.061
iteration 0700: loss: 3941.962
iteration 0800: loss: 3936.757
iteration 0900: loss: 3936.129
iteration 1000: loss: 3944.213
iteration 1100: loss: 3940.147
iteration 1200: loss: 3935.759
iteration 1300: loss: 3937.675
iteration 1400: loss: 3938.356
iteration 1500: loss: 3944.220
iteration 1600: loss: 3940.544
iteration 1700: loss: 3941.327
iteration 1800: loss: 3943.025
====> Epoch: 025 Train loss: 3939.5859  took : 53.029046297073364
====> Test loss: 3943.4576
iteration 0000: loss: 3940.339
iteration 0100: loss: 3945.590
iteration 0200: loss: 3938.580
iteration 0300: loss: 3942.133
iteration 0400: loss: 3943.182
iteration 0500: loss: 3937.326
iteration 0600: loss: 3941.334
iteration 0700: loss: 3939.942
iteration 0800: loss: 3937.933
iteration 0900: loss: 3938.352
iteration 1000: loss: 3936.704
iteration 1100: loss: 3934.115
iteration 1200: loss: 3941.157
iteration 1300: loss: 3943.448
iteration 1400: loss: 3934.618
iteration 1500: loss: 3940.656
iteration 1600: loss: 3935.465
iteration 1700: loss: 3941.287
iteration 1800: loss: 3935.324
====> Epoch: 026 Train loss: 3939.4467  took : 53.125494718551636
====> Test loss: 3943.5710
iteration 0000: loss: 3944.441
iteration 0100: loss: 3941.661
iteration 0200: loss: 3936.281
iteration 0300: loss: 3941.884
iteration 0400: loss: 3943.377
iteration 0500: loss: 3933.883
iteration 0600: loss: 3940.421
iteration 0700: loss: 3941.774
iteration 0800: loss: 3941.861
iteration 0900: loss: 3939.681
iteration 1000: loss: 3943.675
iteration 1100: loss: 3940.706
iteration 1200: loss: 3937.120
iteration 1300: loss: 3942.336
iteration 1400: loss: 3940.969
iteration 1500: loss: 3941.539
iteration 1600: loss: 3935.557
iteration 1700: loss: 3936.615
iteration 1800: loss: 3946.694
====> Epoch: 027 Train loss: 3939.5125  took : 52.858986139297485
====> Test loss: 3943.8826
iteration 0000: loss: 3938.069
iteration 0100: loss: 3945.153
iteration 0200: loss: 3943.316
iteration 0300: loss: 3937.220
iteration 0400: loss: 3944.382
iteration 0500: loss: 3945.963
iteration 0600: loss: 3944.507
iteration 0700: loss: 3940.363
iteration 0800: loss: 3934.653
iteration 0900: loss: 3941.385
iteration 1000: loss: 3941.513
iteration 1100: loss: 3940.091
iteration 1200: loss: 3933.197
iteration 1300: loss: 3941.983
iteration 1400: loss: 3938.853
iteration 1500: loss: 3936.489
iteration 1600: loss: 3944.125
iteration 1700: loss: 3946.169
iteration 1800: loss: 3947.685
====> Epoch: 028 Train loss: 3939.2391  took : 53.089075326919556
====> Test loss: 3943.2368
iteration 0000: loss: 3945.509
iteration 0100: loss: 3940.979
iteration 0200: loss: 3937.059
iteration 0300: loss: 3934.185
iteration 0400: loss: 3940.260
iteration 0500: loss: 3938.275
iteration 0600: loss: 3938.150
iteration 0700: loss: 3937.872
iteration 0800: loss: 3939.254
iteration 0900: loss: 3939.880
iteration 1000: loss: 3932.973
iteration 1100: loss: 3942.102
iteration 1200: loss: 3934.514
iteration 1300: loss: 3934.221
iteration 1400: loss: 3942.026
iteration 1500: loss: 3940.027
iteration 1600: loss: 3930.752
iteration 1700: loss: 3939.436
iteration 1800: loss: 3938.610
====> Epoch: 029 Train loss: 3939.1775  took : 53.174132347106934
====> Test loss: 3943.3177
iteration 0000: loss: 3935.765
iteration 0100: loss: 3948.387
iteration 0200: loss: 3939.490
iteration 0300: loss: 3944.094
iteration 0400: loss: 3943.055
iteration 0500: loss: 3938.998
iteration 0600: loss: 3932.635
iteration 0700: loss: 3939.296
iteration 0800: loss: 3939.259
iteration 0900: loss: 3938.882
iteration 1000: loss: 3942.550
iteration 1100: loss: 3934.629
iteration 1200: loss: 3938.579
iteration 1300: loss: 3945.807
iteration 1400: loss: 3946.635
iteration 1500: loss: 3941.164
iteration 1600: loss: 3938.753
iteration 1700: loss: 3936.976
iteration 1800: loss: 3938.993
====> Epoch: 030 Train loss: 3939.1201  took : 53.035494327545166
====> Test loss: 3943.7031
iteration 0000: loss: 3935.474
iteration 0100: loss: 3929.922
iteration 0200: loss: 3945.034
iteration 0300: loss: 3940.341
iteration 0400: loss: 3932.951
iteration 0500: loss: 3945.500
iteration 0600: loss: 3935.274
iteration 0700: loss: 3939.101
iteration 0800: loss: 3946.226
iteration 0900: loss: 3937.538
iteration 1000: loss: 3940.027
iteration 1100: loss: 3938.595
iteration 1200: loss: 3942.189
iteration 1300: loss: 3940.610
iteration 1400: loss: 3936.045
iteration 1500: loss: 3937.099
iteration 1600: loss: 3942.330
iteration 1700: loss: 3939.271
iteration 1800: loss: 3934.833
====> Epoch: 031 Train loss: 3939.0629  took : 53.03597116470337
====> Test loss: 3943.1207
iteration 0000: loss: 3934.438
iteration 0100: loss: 3936.153
iteration 0200: loss: 3941.774
iteration 0300: loss: 3938.183
iteration 0400: loss: 3943.043
iteration 0500: loss: 3936.526
iteration 0600: loss: 3936.071
iteration 0700: loss: 3940.968
iteration 0800: loss: 3933.141
iteration 0900: loss: 3933.952
iteration 1000: loss: 3941.998
iteration 1100: loss: 3935.960
iteration 1200: loss: 3945.082
iteration 1300: loss: 3943.818
iteration 1400: loss: 3939.527
iteration 1500: loss: 3937.289
iteration 1600: loss: 3942.247
iteration 1700: loss: 3938.251
iteration 1800: loss: 3933.234
====> Epoch: 032 Train loss: 3939.1020  took : 53.136863708496094
====> Test loss: 3943.8025
iteration 0000: loss: 3949.957
iteration 0100: loss: 3936.735
iteration 0200: loss: 3937.514
iteration 0300: loss: 3946.859
iteration 0400: loss: 3946.161
iteration 0500: loss: 3940.072
iteration 0600: loss: 3933.155
iteration 0700: loss: 3935.258
iteration 0800: loss: 3938.270
iteration 0900: loss: 3940.363
iteration 1000: loss: 3943.452
iteration 1100: loss: 3942.857
iteration 1200: loss: 3937.117
iteration 1300: loss: 3939.562
iteration 1400: loss: 3941.284
iteration 1500: loss: 3944.701
iteration 1600: loss: 3942.352
iteration 1700: loss: 3938.214
iteration 1800: loss: 3935.611
====> Epoch: 033 Train loss: 3938.8803  took : 52.958439111709595
====> Test loss: 3944.0205
iteration 0000: loss: 3936.218
iteration 0100: loss: 3941.441
iteration 0200: loss: 3940.805
iteration 0300: loss: 3938.045
iteration 0400: loss: 3931.642
iteration 0500: loss: 3932.143
iteration 0600: loss: 3938.976
iteration 0700: loss: 3938.132
iteration 0800: loss: 3933.075
iteration 0900: loss: 3935.053
iteration 1000: loss: 3939.294
iteration 1100: loss: 3931.401
iteration 1200: loss: 3933.500
iteration 1300: loss: 3939.359
iteration 1400: loss: 3940.722
iteration 1500: loss: 3941.115
iteration 1600: loss: 3941.191
iteration 1700: loss: 3936.615
iteration 1800: loss: 3936.094
====> Epoch: 034 Train loss: 3939.0240  took : 53.12507724761963
====> Test loss: 3943.4906
iteration 0000: loss: 3938.716
iteration 0100: loss: 3941.665
iteration 0200: loss: 3934.161
iteration 0300: loss: 3938.044
iteration 0400: loss: 3950.936
iteration 0500: loss: 3934.839
iteration 0600: loss: 3937.382
iteration 0700: loss: 3945.517
iteration 0800: loss: 3940.835
iteration 0900: loss: 3940.620
iteration 1000: loss: 3945.608
iteration 1100: loss: 3942.002
iteration 1200: loss: 3939.294
iteration 1300: loss: 3941.177
iteration 1400: loss: 3939.880
iteration 1500: loss: 3940.997
iteration 1600: loss: 3941.212
iteration 1700: loss: 3938.964
iteration 1800: loss: 3936.110
====> Epoch: 035 Train loss: 3938.7794  took : 52.90047240257263
====> Test loss: 3943.3731
iteration 0000: loss: 3944.196
iteration 0100: loss: 3932.855
iteration 0200: loss: 3933.178
iteration 0300: loss: 3937.236
iteration 0400: loss: 3937.167
iteration 0500: loss: 3940.391
iteration 0600: loss: 3938.078
iteration 0700: loss: 3944.893
iteration 0800: loss: 3933.214
iteration 0900: loss: 3938.001
iteration 1000: loss: 3940.732
iteration 1100: loss: 3939.719
iteration 1200: loss: 3942.770
iteration 1300: loss: 3938.791
iteration 1400: loss: 3936.395
iteration 1500: loss: 3937.441
iteration 1600: loss: 3947.120
iteration 1700: loss: 3937.137
iteration 1800: loss: 3939.332
====> Epoch: 036 Train loss: 3938.9442  took : 52.922192096710205
====> Test loss: 3943.1273
iteration 0000: loss: 3938.808
iteration 0100: loss: 3934.194
iteration 0200: loss: 3939.486
iteration 0300: loss: 3937.086
iteration 0400: loss: 3944.225
iteration 0500: loss: 3937.533
iteration 0600: loss: 3937.246
iteration 0700: loss: 3939.204
iteration 0800: loss: 3942.234
iteration 0900: loss: 3936.849
iteration 1000: loss: 3932.153
iteration 1100: loss: 3940.760
iteration 1200: loss: 3940.132
iteration 1300: loss: 3934.637
iteration 1400: loss: 3942.546
iteration 1500: loss: 3943.801
iteration 1600: loss: 3946.484
iteration 1700: loss: 3942.598
iteration 1800: loss: 3935.489
====> Epoch: 037 Train loss: 3938.9262  took : 52.93255114555359
====> Test loss: 3943.1995
iteration 0000: loss: 3942.638
iteration 0100: loss: 3942.047
iteration 0200: loss: 3934.842
iteration 0300: loss: 3934.987
iteration 0400: loss: 3941.314
iteration 0500: loss: 3937.560
iteration 0600: loss: 3935.333
iteration 0700: loss: 3938.895
iteration 0800: loss: 3938.845
iteration 0900: loss: 3940.610
iteration 1000: loss: 3940.878
iteration 1100: loss: 3940.100
iteration 1200: loss: 3935.098
iteration 1300: loss: 3941.093
iteration 1400: loss: 3937.101
iteration 1500: loss: 3943.219
iteration 1600: loss: 3938.717
iteration 1700: loss: 3943.402
iteration 1800: loss: 3940.149
====> Epoch: 038 Train loss: 3938.6707  took : 52.96118450164795
====> Test loss: 3942.8647
iteration 0000: loss: 3940.807
iteration 0100: loss: 3936.294
iteration 0200: loss: 3933.685
iteration 0300: loss: 3935.665
iteration 0400: loss: 3939.479
iteration 0500: loss: 3940.884
iteration 0600: loss: 3943.553
iteration 0700: loss: 3937.478
iteration 0800: loss: 3941.809
iteration 0900: loss: 3937.630
iteration 1000: loss: 3938.709
iteration 1100: loss: 3935.962
iteration 1200: loss: 3938.697
iteration 1300: loss: 3938.327
iteration 1400: loss: 3943.899
iteration 1500: loss: 3938.248
iteration 1600: loss: 3933.645
iteration 1700: loss: 3937.891
iteration 1800: loss: 3932.427
====> Epoch: 039 Train loss: 3938.7323  took : 52.82674050331116
====> Test loss: 3943.0725
iteration 0000: loss: 3942.995
iteration 0100: loss: 3933.170
iteration 0200: loss: 3937.569
iteration 0300: loss: 3941.062
iteration 0400: loss: 3940.875
iteration 0500: loss: 3935.002
iteration 0600: loss: 3935.721
iteration 0700: loss: 3935.598
iteration 0800: loss: 3940.560
iteration 0900: loss: 3946.419
iteration 1000: loss: 3934.764
iteration 1100: loss: 3940.005
iteration 1200: loss: 3940.669
iteration 1300: loss: 3936.643
iteration 1400: loss: 3941.699
iteration 1500: loss: 3937.790
iteration 1600: loss: 3939.309
iteration 1700: loss: 3936.114
iteration 1800: loss: 3938.413
====> Epoch: 040 Train loss: 3938.5414  took : 52.760791063308716
====> Test loss: 3942.6677
iteration 0000: loss: 3943.944
iteration 0100: loss: 3935.991
iteration 0200: loss: 3936.037
iteration 0300: loss: 3935.472
iteration 0400: loss: 3933.583
iteration 0500: loss: 3939.605
iteration 0600: loss: 3933.548
iteration 0700: loss: 3939.271
iteration 0800: loss: 3938.416
iteration 0900: loss: 3939.844
iteration 1000: loss: 3942.045
iteration 1100: loss: 3943.839
iteration 1200: loss: 3939.126
iteration 1300: loss: 3943.166
iteration 1400: loss: 3936.639
iteration 1500: loss: 3939.274
iteration 1600: loss: 3936.531
iteration 1700: loss: 3935.062
iteration 1800: loss: 3938.318
====> Epoch: 041 Train loss: 3938.7174  took : 52.82336735725403
====> Test loss: 3942.8364
iteration 0000: loss: 3935.996
iteration 0100: loss: 3946.700
iteration 0200: loss: 3936.436
iteration 0300: loss: 3937.078
iteration 0400: loss: 3938.133
iteration 0500: loss: 3935.045
iteration 0600: loss: 3934.426
iteration 0700: loss: 3938.216
iteration 0800: loss: 3935.221
iteration 0900: loss: 3942.561
iteration 1000: loss: 3934.616
iteration 1100: loss: 3933.137
iteration 1200: loss: 3933.039
iteration 1300: loss: 3934.050
iteration 1400: loss: 3933.710
iteration 1500: loss: 3938.490
iteration 1600: loss: 3935.997
iteration 1700: loss: 3941.515
iteration 1800: loss: 3935.050
====> Epoch: 042 Train loss: 3938.5481  took : 52.86184906959534
====> Test loss: 3942.8644
iteration 0000: loss: 3945.091
iteration 0100: loss: 3939.829
iteration 0200: loss: 3933.331
iteration 0300: loss: 3936.542
iteration 0400: loss: 3941.018
iteration 0500: loss: 3940.603
iteration 0600: loss: 3932.650
iteration 0700: loss: 3942.299
iteration 0800: loss: 3939.473
iteration 0900: loss: 3938.045
iteration 1000: loss: 3940.657
iteration 1100: loss: 3938.712
iteration 1200: loss: 3933.293
iteration 1300: loss: 3939.313
iteration 1400: loss: 3934.765
iteration 1500: loss: 3935.094
iteration 1600: loss: 3935.300
iteration 1700: loss: 3939.990
iteration 1800: loss: 3940.555
====> Epoch: 043 Train loss: 3938.3154  took : 53.07056021690369
====> Test loss: 3942.6896
iteration 0000: loss: 3935.180
iteration 0100: loss: 3939.607
iteration 0200: loss: 3935.463
iteration 0300: loss: 3936.706
iteration 0400: loss: 3944.041
iteration 0500: loss: 3932.569
iteration 0600: loss: 3932.905
iteration 0700: loss: 3940.301
iteration 0800: loss: 3941.508
iteration 0900: loss: 3933.489
iteration 1000: loss: 3942.446
iteration 1100: loss: 3947.507
iteration 1200: loss: 3943.486
iteration 1300: loss: 3938.811
iteration 1400: loss: 3942.585
iteration 1500: loss: 3935.919
iteration 1600: loss: 3935.682
iteration 1700: loss: 3942.138
iteration 1800: loss: 3935.048
====> Epoch: 044 Train loss: 3938.4632  took : 52.73746061325073
====> Test loss: 3942.7393
iteration 0000: loss: 3940.737
iteration 0100: loss: 3938.106
iteration 0200: loss: 3936.521
iteration 0300: loss: 3938.947
iteration 0400: loss: 3938.419
iteration 0500: loss: 3937.661
iteration 0600: loss: 3935.595
iteration 0700: loss: 3938.230
iteration 0800: loss: 3933.496
iteration 0900: loss: 3935.105
iteration 1000: loss: 3939.059
iteration 1100: loss: 3944.900
iteration 1200: loss: 3940.214
iteration 1300: loss: 3938.458
iteration 1400: loss: 3942.051
iteration 1500: loss: 3942.074
iteration 1600: loss: 3939.440
iteration 1700: loss: 3937.534
iteration 1800: loss: 3938.227
====> Epoch: 045 Train loss: 3938.5633  took : 53.1134078502655
====> Test loss: 3943.4139
iteration 0000: loss: 3941.160
iteration 0100: loss: 3946.079
iteration 0200: loss: 3936.296
iteration 0300: loss: 3941.640
iteration 0400: loss: 3935.942
iteration 0500: loss: 3940.336
iteration 0600: loss: 3942.128
iteration 0700: loss: 3937.176
iteration 0800: loss: 3943.581
iteration 0900: loss: 3939.129
iteration 1000: loss: 3940.943
iteration 1100: loss: 3943.367
iteration 1200: loss: 3938.302
iteration 1300: loss: 3933.692
iteration 1400: loss: 3938.988
iteration 1500: loss: 3934.932
iteration 1600: loss: 3941.267
iteration 1700: loss: 3940.972
iteration 1800: loss: 3936.671
====> Epoch: 046 Train loss: 3938.3856  took : 53.05668497085571
====> Test loss: 3942.5120
iteration 0000: loss: 3937.545
iteration 0100: loss: 3937.244
iteration 0200: loss: 3938.120
iteration 0300: loss: 3940.113
iteration 0400: loss: 3942.337
iteration 0500: loss: 3940.544
iteration 0600: loss: 3940.379
iteration 0700: loss: 3937.708
iteration 0800: loss: 3938.052
iteration 0900: loss: 3937.993
iteration 1000: loss: 3939.522
iteration 1100: loss: 3940.704
iteration 1200: loss: 3933.908
iteration 1300: loss: 3939.614
iteration 1400: loss: 3942.193
iteration 1500: loss: 3937.696
iteration 1600: loss: 3939.771
iteration 1700: loss: 3938.171
iteration 1800: loss: 3936.080
====> Epoch: 047 Train loss: 3938.2428  took : 53.09695911407471
====> Test loss: 3942.5314
iteration 0000: loss: 3942.482
iteration 0100: loss: 3935.843
iteration 0200: loss: 3942.240
iteration 0300: loss: 3935.732
iteration 0400: loss: 3935.353
iteration 0500: loss: 3935.473
iteration 0600: loss: 3939.618
iteration 0700: loss: 3934.739
iteration 0800: loss: 3935.502
iteration 0900: loss: 3940.565
iteration 1000: loss: 3935.804
iteration 1100: loss: 3938.753
iteration 1200: loss: 3932.884
iteration 1300: loss: 3940.378
iteration 1400: loss: 3943.959
iteration 1500: loss: 3940.015
iteration 1600: loss: 3934.481
iteration 1700: loss: 3940.388
iteration 1800: loss: 3940.777
====> Epoch: 048 Train loss: 3938.0366  took : 52.9766948223114
====> Test loss: 3942.6471
iteration 0000: loss: 3938.561
iteration 0100: loss: 3938.344
iteration 0200: loss: 3938.465
iteration 0300: loss: 3939.173
iteration 0400: loss: 3932.613
iteration 0500: loss: 3934.325
iteration 0600: loss: 3932.107
iteration 0700: loss: 3942.071
iteration 0800: loss: 3933.339
iteration 0900: loss: 3940.258
iteration 1000: loss: 3939.581
iteration 1100: loss: 3940.073
iteration 1200: loss: 3938.264
iteration 1300: loss: 3941.016
iteration 1400: loss: 3939.693
iteration 1500: loss: 3936.944
iteration 1600: loss: 3936.287
iteration 1700: loss: 3943.850
iteration 1800: loss: 3939.575
====> Epoch: 049 Train loss: 3938.2746  took : 53.2054443359375
====> Test loss: 3944.0068
iteration 0000: loss: 3943.290
iteration 0100: loss: 3939.104
iteration 0200: loss: 3934.567
iteration 0300: loss: 3948.002
iteration 0400: loss: 3932.540
iteration 0500: loss: 3933.856
iteration 0600: loss: 3945.370
iteration 0700: loss: 3939.747
iteration 0800: loss: 3938.143
iteration 0900: loss: 3941.674
iteration 1000: loss: 3937.710
iteration 1100: loss: 3937.936
iteration 1200: loss: 3942.917
iteration 1300: loss: 3939.077
iteration 1400: loss: 3937.732
iteration 1500: loss: 3933.564
iteration 1600: loss: 3938.129
iteration 1700: loss: 3932.399
iteration 1800: loss: 3939.470
====> Epoch: 050 Train loss: 3938.2660  took : 53.1417760848999
====> Test loss: 3943.1876
====> [MM-VAE] Time: 3163.713s or 00:52:43
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  7
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_7
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_7
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1988.749
iteration 0100: loss: 1565.059
iteration 0200: loss: 1568.041
iteration 0300: loss: 1564.471
iteration 0400: loss: 1548.135
iteration 0500: loss: 1550.813
iteration 0600: loss: 1538.097
iteration 0700: loss: 1542.095
iteration 0800: loss: 1537.031
iteration 0900: loss: 1531.394
====> Epoch: 001 Train loss: 1554.1339  took : 8.516163349151611
====> Test loss: 1536.5234
iteration 0000: loss: 1537.937
iteration 0100: loss: 1529.732
iteration 0200: loss: 1540.981
iteration 0300: loss: 1529.025
iteration 0400: loss: 1529.353
iteration 0500: loss: 1531.654
iteration 0600: loss: 1530.837
iteration 0700: loss: 1532.356
iteration 0800: loss: 1531.708
iteration 0900: loss: 1531.120
====> Epoch: 002 Train loss: 1531.5048  took : 8.620962142944336
====> Test loss: 1530.3177
iteration 0000: loss: 1528.497
iteration 0100: loss: 1526.970
iteration 0200: loss: 1528.859
iteration 0300: loss: 1525.025
iteration 0400: loss: 1523.995
iteration 0500: loss: 1527.130
iteration 0600: loss: 1526.923
iteration 0700: loss: 1524.022
iteration 0800: loss: 1530.915
iteration 0900: loss: 1528.383
====> Epoch: 003 Train loss: 1526.9792  took : 8.60300612449646
====> Test loss: 1527.5615
iteration 0000: loss: 1529.659
iteration 0100: loss: 1528.280
iteration 0200: loss: 1523.986
iteration 0300: loss: 1532.159
iteration 0400: loss: 1526.129
iteration 0500: loss: 1525.437
iteration 0600: loss: 1524.850
iteration 0700: loss: 1526.972
iteration 0800: loss: 1525.542
iteration 0900: loss: 1526.175
====> Epoch: 004 Train loss: 1524.4773  took : 8.621031284332275
====> Test loss: 1525.8711
iteration 0000: loss: 1525.375
iteration 0100: loss: 1524.239
iteration 0200: loss: 1526.035
iteration 0300: loss: 1521.502
iteration 0400: loss: 1523.139
iteration 0500: loss: 1521.846
iteration 0600: loss: 1520.125
iteration 0700: loss: 1521.396
iteration 0800: loss: 1522.523
iteration 0900: loss: 1524.585
====> Epoch: 005 Train loss: 1522.8832  took : 8.454537391662598
====> Test loss: 1524.4706
iteration 0000: loss: 1521.406
iteration 0100: loss: 1518.735
iteration 0200: loss: 1521.095
iteration 0300: loss: 1524.199
iteration 0400: loss: 1521.630
iteration 0500: loss: 1521.597
iteration 0600: loss: 1518.735
iteration 0700: loss: 1518.926
iteration 0800: loss: 1519.055
iteration 0900: loss: 1520.039
====> Epoch: 006 Train loss: 1521.7400  took : 8.526581764221191
====> Test loss: 1523.4962
iteration 0000: loss: 1521.469
iteration 0100: loss: 1521.300
iteration 0200: loss: 1520.178
iteration 0300: loss: 1523.351
iteration 0400: loss: 1524.925
iteration 0500: loss: 1522.926
iteration 0600: loss: 1522.246
iteration 0700: loss: 1520.375
iteration 0800: loss: 1524.004
iteration 0900: loss: 1521.226
====> Epoch: 007 Train loss: 1520.8441  took : 8.58521032333374
====> Test loss: 1522.7280
iteration 0000: loss: 1520.913
iteration 0100: loss: 1517.599
iteration 0200: loss: 1520.591
iteration 0300: loss: 1519.722
iteration 0400: loss: 1519.660
iteration 0500: loss: 1521.694
iteration 0600: loss: 1517.439
iteration 0700: loss: 1518.977
iteration 0800: loss: 1517.738
iteration 0900: loss: 1519.554
====> Epoch: 008 Train loss: 1520.1379  took : 8.583922386169434
====> Test loss: 1522.1989
iteration 0000: loss: 1519.663
iteration 0100: loss: 1520.576
iteration 0200: loss: 1515.989
iteration 0300: loss: 1518.781
iteration 0400: loss: 1519.667
iteration 0500: loss: 1522.676
iteration 0600: loss: 1519.818
iteration 0700: loss: 1517.769
iteration 0800: loss: 1517.765
iteration 0900: loss: 1520.600
====> Epoch: 009 Train loss: 1519.5885  took : 8.498405456542969
====> Test loss: 1521.7403
iteration 0000: loss: 1517.477
iteration 0100: loss: 1520.971
iteration 0200: loss: 1516.784
iteration 0300: loss: 1519.215
iteration 0400: loss: 1516.634
iteration 0500: loss: 1518.240
iteration 0600: loss: 1518.800
iteration 0700: loss: 1519.290
iteration 0800: loss: 1520.463
iteration 0900: loss: 1519.640
====> Epoch: 010 Train loss: 1519.0910  took : 8.421019792556763
====> Test loss: 1521.4007
iteration 0000: loss: 1519.630
iteration 0100: loss: 1516.571
iteration 0200: loss: 1516.597
iteration 0300: loss: 1518.101
iteration 0400: loss: 1518.790
iteration 0500: loss: 1519.287
iteration 0600: loss: 1514.811
iteration 0700: loss: 1523.434
iteration 0800: loss: 1518.677
iteration 0900: loss: 1514.988
====> Epoch: 011 Train loss: 1518.6793  took : 8.608384609222412
====> Test loss: 1521.2909
iteration 0000: loss: 1516.170
iteration 0100: loss: 1517.283
iteration 0200: loss: 1519.457
iteration 0300: loss: 1518.292
iteration 0400: loss: 1517.267
iteration 0500: loss: 1522.083
iteration 0600: loss: 1520.363
iteration 0700: loss: 1517.583
iteration 0800: loss: 1519.689
iteration 0900: loss: 1517.107
====> Epoch: 012 Train loss: 1518.3137  took : 8.562718868255615
====> Test loss: 1520.8022
iteration 0000: loss: 1517.192
iteration 0100: loss: 1517.210
iteration 0200: loss: 1519.539
iteration 0300: loss: 1516.705
iteration 0400: loss: 1516.034
iteration 0500: loss: 1517.090
iteration 0600: loss: 1520.948
iteration 0700: loss: 1517.617
iteration 0800: loss: 1517.848
iteration 0900: loss: 1516.763
====> Epoch: 013 Train loss: 1518.0400  took : 8.536885261535645
====> Test loss: 1520.9447
iteration 0000: loss: 1519.062
iteration 0100: loss: 1518.809
iteration 0200: loss: 1518.418
iteration 0300: loss: 1515.744
iteration 0400: loss: 1517.176
iteration 0500: loss: 1512.396
iteration 0600: loss: 1517.171
iteration 0700: loss: 1517.185
iteration 0800: loss: 1518.145
iteration 0900: loss: 1521.110
====> Epoch: 014 Train loss: 1517.7836  took : 8.534066200256348
====> Test loss: 1520.3187
iteration 0000: loss: 1518.748
iteration 0100: loss: 1513.487
iteration 0200: loss: 1517.468
iteration 0300: loss: 1521.940
iteration 0400: loss: 1519.864
iteration 0500: loss: 1517.077
iteration 0600: loss: 1516.498
iteration 0700: loss: 1516.689
iteration 0800: loss: 1516.459
iteration 0900: loss: 1516.116
====> Epoch: 015 Train loss: 1517.4903  took : 8.569622993469238
====> Test loss: 1520.2238
iteration 0000: loss: 1519.691
iteration 0100: loss: 1515.710
iteration 0200: loss: 1518.445
iteration 0300: loss: 1516.228
iteration 0400: loss: 1518.067
iteration 0500: loss: 1518.277
iteration 0600: loss: 1516.183
iteration 0700: loss: 1518.112
iteration 0800: loss: 1515.810
iteration 0900: loss: 1521.247
====> Epoch: 016 Train loss: 1517.2978  took : 8.544474363327026
====> Test loss: 1519.9943
iteration 0000: loss: 1515.762
iteration 0100: loss: 1519.361
iteration 0200: loss: 1515.439
iteration 0300: loss: 1515.666
iteration 0400: loss: 1519.878
iteration 0500: loss: 1516.886
iteration 0600: loss: 1520.348
iteration 0700: loss: 1516.744
iteration 0800: loss: 1518.057
iteration 0900: loss: 1516.139
====> Epoch: 017 Train loss: 1517.0561  took : 8.629181385040283
====> Test loss: 1519.8521
iteration 0000: loss: 1516.859
iteration 0100: loss: 1521.618
iteration 0200: loss: 1517.292
iteration 0300: loss: 1518.451
iteration 0400: loss: 1518.174
iteration 0500: loss: 1517.677
iteration 0600: loss: 1515.842
iteration 0700: loss: 1519.010
iteration 0800: loss: 1513.784
iteration 0900: loss: 1517.611
====> Epoch: 018 Train loss: 1516.9051  took : 8.617393493652344
====> Test loss: 1519.7831
iteration 0000: loss: 1515.979
iteration 0100: loss: 1519.545
iteration 0200: loss: 1517.712
iteration 0300: loss: 1516.199
iteration 0400: loss: 1517.045
iteration 0500: loss: 1517.558
iteration 0600: loss: 1518.856
iteration 0700: loss: 1518.773
iteration 0800: loss: 1517.692
iteration 0900: loss: 1516.922
====> Epoch: 019 Train loss: 1516.6983  took : 8.522393226623535
====> Test loss: 1519.5195
iteration 0000: loss: 1515.371
iteration 0100: loss: 1520.217
iteration 0200: loss: 1515.627
iteration 0300: loss: 1515.048
iteration 0400: loss: 1516.563
iteration 0500: loss: 1517.150
iteration 0600: loss: 1516.495
iteration 0700: loss: 1518.140
iteration 0800: loss: 1516.815
iteration 0900: loss: 1516.858
====> Epoch: 020 Train loss: 1516.5189  took : 8.516091346740723
====> Test loss: 1519.4515
iteration 0000: loss: 1518.229
iteration 0100: loss: 1516.707
iteration 0200: loss: 1517.592
iteration 0300: loss: 1515.876
iteration 0400: loss: 1513.050
iteration 0500: loss: 1512.651
iteration 0600: loss: 1517.923
iteration 0700: loss: 1514.391
iteration 0800: loss: 1516.345
iteration 0900: loss: 1514.116
====> Epoch: 021 Train loss: 1516.3998  took : 8.541185140609741
====> Test loss: 1519.2559
iteration 0000: loss: 1517.456
iteration 0100: loss: 1516.283
iteration 0200: loss: 1517.495
iteration 0300: loss: 1518.248
iteration 0400: loss: 1514.599
iteration 0500: loss: 1516.296
iteration 0600: loss: 1513.752
iteration 0700: loss: 1513.008
iteration 0800: loss: 1517.513
iteration 0900: loss: 1515.302
====> Epoch: 022 Train loss: 1516.2470  took : 8.595499038696289
====> Test loss: 1519.1303
iteration 0000: loss: 1516.016
iteration 0100: loss: 1516.527
iteration 0200: loss: 1515.164
iteration 0300: loss: 1516.552
iteration 0400: loss: 1515.634
iteration 0500: loss: 1515.914
iteration 0600: loss: 1514.985
iteration 0700: loss: 1518.456
iteration 0800: loss: 1516.140
iteration 0900: loss: 1516.052
====> Epoch: 023 Train loss: 1516.1182  took : 8.51103949546814
====> Test loss: 1519.1030
iteration 0000: loss: 1517.015
iteration 0100: loss: 1518.011
iteration 0200: loss: 1515.743
iteration 0300: loss: 1521.584
iteration 0400: loss: 1517.166
iteration 0500: loss: 1516.979
iteration 0600: loss: 1515.385
iteration 0700: loss: 1516.015
iteration 0800: loss: 1515.113
iteration 0900: loss: 1515.749
====> Epoch: 024 Train loss: 1515.9792  took : 8.509967565536499
====> Test loss: 1518.8945
iteration 0000: loss: 1512.608
iteration 0100: loss: 1518.843
iteration 0200: loss: 1516.100
iteration 0300: loss: 1513.490
iteration 0400: loss: 1514.890
iteration 0500: loss: 1515.868
iteration 0600: loss: 1516.743
iteration 0700: loss: 1513.626
iteration 0800: loss: 1518.482
iteration 0900: loss: 1515.216
====> Epoch: 025 Train loss: 1515.9447  took : 8.547503232955933
====> Test loss: 1519.0609
iteration 0000: loss: 1518.861
iteration 0100: loss: 1514.142
iteration 0200: loss: 1515.278
iteration 0300: loss: 1512.412
iteration 0400: loss: 1518.858
iteration 0500: loss: 1517.819
iteration 0600: loss: 1518.307
iteration 0700: loss: 1513.978
iteration 0800: loss: 1514.956
iteration 0900: loss: 1516.243
====> Epoch: 026 Train loss: 1515.7551  took : 8.614405632019043
====> Test loss: 1518.8520
iteration 0000: loss: 1513.317
iteration 0100: loss: 1514.167
iteration 0200: loss: 1511.179
iteration 0300: loss: 1515.886
iteration 0400: loss: 1516.447
iteration 0500: loss: 1514.538
iteration 0600: loss: 1515.156
iteration 0700: loss: 1514.291
iteration 0800: loss: 1516.347
iteration 0900: loss: 1516.714
====> Epoch: 027 Train loss: 1515.6507  took : 8.581003665924072
====> Test loss: 1518.8501
iteration 0000: loss: 1514.356
iteration 0100: loss: 1515.365
iteration 0200: loss: 1514.982
iteration 0300: loss: 1514.163
iteration 0400: loss: 1512.945
iteration 0500: loss: 1515.509
iteration 0600: loss: 1514.467
iteration 0700: loss: 1517.275
iteration 0800: loss: 1514.194
iteration 0900: loss: 1517.314
====> Epoch: 028 Train loss: 1515.5491  took : 8.551077365875244
====> Test loss: 1518.6501
iteration 0000: loss: 1513.471
iteration 0100: loss: 1516.387
iteration 0200: loss: 1514.849
iteration 0300: loss: 1513.806
iteration 0400: loss: 1518.343
iteration 0500: loss: 1514.805
iteration 0600: loss: 1516.383
iteration 0700: loss: 1513.948
iteration 0800: loss: 1515.635
iteration 0900: loss: 1513.715
====> Epoch: 029 Train loss: 1515.5323  took : 8.591710567474365
====> Test loss: 1518.7112
iteration 0000: loss: 1514.057
iteration 0100: loss: 1516.017
iteration 0200: loss: 1517.926
iteration 0300: loss: 1512.229
iteration 0400: loss: 1515.868
iteration 0500: loss: 1514.066
iteration 0600: loss: 1515.268
iteration 0700: loss: 1517.869
iteration 0800: loss: 1514.351
iteration 0900: loss: 1516.145
====> Epoch: 030 Train loss: 1515.4770  took : 8.615182399749756
====> Test loss: 1518.6114
iteration 0000: loss: 1514.623
iteration 0100: loss: 1517.153
iteration 0200: loss: 1514.892
iteration 0300: loss: 1515.495
iteration 0400: loss: 1516.462
iteration 0500: loss: 1514.659
iteration 0600: loss: 1516.159
iteration 0700: loss: 1518.030
iteration 0800: loss: 1517.381
iteration 0900: loss: 1516.902
====> Epoch: 031 Train loss: 1515.3215  took : 8.622205972671509
====> Test loss: 1518.4886
iteration 0000: loss: 1518.934
iteration 0100: loss: 1516.713
iteration 0200: loss: 1513.591
iteration 0300: loss: 1517.279
iteration 0400: loss: 1514.185
iteration 0500: loss: 1515.475
iteration 0600: loss: 1516.755
iteration 0700: loss: 1516.595
iteration 0800: loss: 1515.976
iteration 0900: loss: 1513.426
====> Epoch: 032 Train loss: 1515.2817  took : 8.616357803344727
====> Test loss: 1518.4597
iteration 0000: loss: 1517.012
iteration 0100: loss: 1515.346
iteration 0200: loss: 1513.752
iteration 0300: loss: 1515.726
iteration 0400: loss: 1515.061
iteration 0500: loss: 1514.471
iteration 0600: loss: 1515.433
iteration 0700: loss: 1515.527
iteration 0800: loss: 1514.830
iteration 0900: loss: 1515.456
====> Epoch: 033 Train loss: 1515.1417  took : 8.550838947296143
====> Test loss: 1518.3995
iteration 0000: loss: 1514.800
iteration 0100: loss: 1515.251
iteration 0200: loss: 1513.426
iteration 0300: loss: 1514.067
iteration 0400: loss: 1515.240
iteration 0500: loss: 1516.803
iteration 0600: loss: 1513.337
iteration 0700: loss: 1521.321
iteration 0800: loss: 1514.917
iteration 0900: loss: 1515.382
====> Epoch: 034 Train loss: 1515.1196  took : 8.613581657409668
====> Test loss: 1518.3199
iteration 0000: loss: 1516.282
iteration 0100: loss: 1515.537
iteration 0200: loss: 1516.944
iteration 0300: loss: 1514.089
iteration 0400: loss: 1515.259
iteration 0500: loss: 1516.766
iteration 0600: loss: 1515.256
iteration 0700: loss: 1515.659
iteration 0800: loss: 1514.789
iteration 0900: loss: 1517.080
====> Epoch: 035 Train loss: 1515.0485  took : 8.599828720092773
====> Test loss: 1518.3197
iteration 0000: loss: 1511.883
iteration 0100: loss: 1518.276
iteration 0200: loss: 1512.749
iteration 0300: loss: 1514.390
iteration 0400: loss: 1512.997
iteration 0500: loss: 1513.994
iteration 0600: loss: 1516.448
iteration 0700: loss: 1514.957
iteration 0800: loss: 1513.468
iteration 0900: loss: 1513.591
====> Epoch: 036 Train loss: 1514.9459  took : 8.66258430480957
====> Test loss: 1518.2439
iteration 0000: loss: 1513.115
iteration 0100: loss: 1513.896
iteration 0200: loss: 1514.659
iteration 0300: loss: 1513.916
iteration 0400: loss: 1515.211
iteration 0500: loss: 1513.947
iteration 0600: loss: 1512.573
iteration 0700: loss: 1514.329
iteration 0800: loss: 1515.464
iteration 0900: loss: 1515.160
====> Epoch: 037 Train loss: 1514.8706  took : 8.575618028640747
====> Test loss: 1518.1082
iteration 0000: loss: 1515.272
iteration 0100: loss: 1513.924
iteration 0200: loss: 1514.988
iteration 0300: loss: 1515.874
iteration 0400: loss: 1513.315
iteration 0500: loss: 1512.380
iteration 0600: loss: 1514.764
iteration 0700: loss: 1516.479
iteration 0800: loss: 1515.370
iteration 0900: loss: 1514.516
====> Epoch: 038 Train loss: 1514.8721  took : 8.585726976394653
====> Test loss: 1518.1061
iteration 0000: loss: 1515.845
iteration 0100: loss: 1512.744
iteration 0200: loss: 1514.514
iteration 0300: loss: 1516.084
iteration 0400: loss: 1514.409
iteration 0500: loss: 1514.797
iteration 0600: loss: 1514.864
iteration 0700: loss: 1516.650
iteration 0800: loss: 1514.558
iteration 0900: loss: 1516.993
====> Epoch: 039 Train loss: 1514.7814  took : 8.49618148803711
====> Test loss: 1518.6479
iteration 0000: loss: 1516.851
iteration 0100: loss: 1514.164
iteration 0200: loss: 1514.969
iteration 0300: loss: 1514.848
iteration 0400: loss: 1515.363
iteration 0500: loss: 1513.799
iteration 0600: loss: 1515.439
iteration 0700: loss: 1514.412
iteration 0800: loss: 1515.042
iteration 0900: loss: 1515.054
====> Epoch: 040 Train loss: 1514.7643  took : 8.500402450561523
====> Test loss: 1518.0222
iteration 0000: loss: 1513.236
iteration 0100: loss: 1514.477
iteration 0200: loss: 1517.068
iteration 0300: loss: 1517.001
iteration 0400: loss: 1517.000
iteration 0500: loss: 1514.856
iteration 0600: loss: 1513.262
iteration 0700: loss: 1516.253
iteration 0800: loss: 1516.850
iteration 0900: loss: 1514.064
====> Epoch: 041 Train loss: 1514.6534  took : 8.54651951789856
====> Test loss: 1518.1600
iteration 0000: loss: 1514.205
iteration 0100: loss: 1514.046
iteration 0200: loss: 1513.931
iteration 0300: loss: 1515.329
iteration 0400: loss: 1516.219
iteration 0500: loss: 1513.501
iteration 0600: loss: 1516.001
iteration 0700: loss: 1516.711
iteration 0800: loss: 1514.707
iteration 0900: loss: 1513.717
====> Epoch: 042 Train loss: 1514.5148  took : 8.61057734489441
====> Test loss: 1517.8783
iteration 0000: loss: 1513.560
iteration 0100: loss: 1514.034
iteration 0200: loss: 1515.386
iteration 0300: loss: 1513.813
iteration 0400: loss: 1515.560
iteration 0500: loss: 1514.783
iteration 0600: loss: 1515.049
iteration 0700: loss: 1512.608
iteration 0800: loss: 1512.376
iteration 0900: loss: 1512.344
====> Epoch: 043 Train loss: 1514.4797  took : 8.571029424667358
====> Test loss: 1518.0465
iteration 0000: loss: 1513.719
iteration 0100: loss: 1514.967
iteration 0200: loss: 1511.988
iteration 0300: loss: 1515.764
iteration 0400: loss: 1514.754
iteration 0500: loss: 1516.607
iteration 0600: loss: 1515.694
iteration 0700: loss: 1513.432
iteration 0800: loss: 1514.044
iteration 0900: loss: 1515.293
====> Epoch: 044 Train loss: 1514.4993  took : 8.616170883178711
====> Test loss: 1518.0651
iteration 0000: loss: 1514.741
iteration 0100: loss: 1512.642
iteration 0200: loss: 1513.437
iteration 0300: loss: 1516.859
iteration 0400: loss: 1516.698
iteration 0500: loss: 1513.881
iteration 0600: loss: 1513.904
iteration 0700: loss: 1518.090
iteration 0800: loss: 1515.191
iteration 0900: loss: 1513.886
====> Epoch: 045 Train loss: 1514.3819  took : 8.57766056060791
====> Test loss: 1517.8716
iteration 0000: loss: 1513.417
iteration 0100: loss: 1514.970
iteration 0200: loss: 1514.937
iteration 0300: loss: 1515.911
iteration 0400: loss: 1514.167
iteration 0500: loss: 1516.357
iteration 0600: loss: 1515.016
iteration 0700: loss: 1514.714
iteration 0800: loss: 1515.487
iteration 0900: loss: 1513.219
====> Epoch: 046 Train loss: 1514.3391  took : 8.502955675125122
====> Test loss: 1518.5317
iteration 0000: loss: 1514.455
iteration 0100: loss: 1513.291
iteration 0200: loss: 1514.792
iteration 0300: loss: 1513.366
iteration 0400: loss: 1515.620
iteration 0500: loss: 1514.382
iteration 0600: loss: 1514.253
iteration 0700: loss: 1512.665
iteration 0800: loss: 1512.952
iteration 0900: loss: 1512.989
====> Epoch: 047 Train loss: 1514.2977  took : 8.62213659286499
====> Test loss: 1517.9114
iteration 0000: loss: 1516.460
iteration 0100: loss: 1513.333
iteration 0200: loss: 1514.323
iteration 0300: loss: 1513.847
iteration 0400: loss: 1514.408
iteration 0500: loss: 1514.403
iteration 0600: loss: 1512.855
iteration 0700: loss: 1512.742
iteration 0800: loss: 1513.444
iteration 0900: loss: 1514.934
====> Epoch: 048 Train loss: 1514.2666  took : 8.630546808242798
====> Test loss: 1517.7879
iteration 0000: loss: 1512.761
iteration 0100: loss: 1511.428
iteration 0200: loss: 1512.561
iteration 0300: loss: 1514.171
iteration 0400: loss: 1513.612
iteration 0500: loss: 1516.553
iteration 0600: loss: 1514.281
iteration 0700: loss: 1513.185
iteration 0800: loss: 1514.543
iteration 0900: loss: 1513.436
====> Epoch: 049 Train loss: 1514.1927  took : 8.61357569694519
====> Test loss: 1517.9297
iteration 0000: loss: 1514.568
iteration 0100: loss: 1514.282
iteration 0200: loss: 1514.093
iteration 0300: loss: 1511.631
iteration 0400: loss: 1513.771
iteration 0500: loss: 1514.058
iteration 0600: loss: 1516.449
iteration 0700: loss: 1515.708
iteration 0800: loss: 1511.890
iteration 0900: loss: 1516.448
====> Epoch: 050 Train loss: 1514.1912  took : 8.475161075592041
====> Test loss: 1518.2368
====> [MM-VAE] Time: 511.980s or 00:08:31
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  7
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_7
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_7
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.773
iteration 0100: loss: 2126.211
iteration 0200: loss: 2054.472
iteration 0300: loss: 2017.472
iteration 0400: loss: 2000.569
iteration 0500: loss: 2000.455
iteration 0600: loss: 1995.623
iteration 0700: loss: 1997.039
iteration 0800: loss: 1991.523
iteration 0900: loss: 1992.308
====> Epoch: 001 Train loss: 2023.5669  took : 13.388333082199097
====> Test loss: 1992.6271
iteration 0000: loss: 1991.550
iteration 0100: loss: 1985.113
iteration 0200: loss: 1982.700
iteration 0300: loss: 1987.493
iteration 0400: loss: 1981.712
iteration 0500: loss: 1979.864
iteration 0600: loss: 1981.183
iteration 0700: loss: 1978.403
iteration 0800: loss: 1975.761
iteration 0900: loss: 1973.641
====> Epoch: 002 Train loss: 1982.1479  took : 13.818333864212036
====> Test loss: 1976.8794
iteration 0000: loss: 1980.601
iteration 0100: loss: 1969.969
iteration 0200: loss: 1979.251
iteration 0300: loss: 1975.843
iteration 0400: loss: 1969.481
iteration 0500: loss: 1965.564
iteration 0600: loss: 1965.516
iteration 0700: loss: 1967.396
iteration 0800: loss: 1960.788
iteration 0900: loss: 1961.984
====> Epoch: 003 Train loss: 1968.6347  took : 13.22553277015686
====> Test loss: 1965.9862
iteration 0000: loss: 1967.259
iteration 0100: loss: 1962.456
iteration 0200: loss: 1962.759
iteration 0300: loss: 1960.586
iteration 0400: loss: 1959.730
iteration 0500: loss: 1956.063
iteration 0600: loss: 1961.276
iteration 0700: loss: 1958.386
iteration 0800: loss: 1960.368
iteration 0900: loss: 1960.206
====> Epoch: 004 Train loss: 1960.7845  took : 12.492171049118042
====> Test loss: 1960.1482
iteration 0000: loss: 1956.083
iteration 0100: loss: 1957.955
iteration 0200: loss: 1960.094
iteration 0300: loss: 1956.181
iteration 0400: loss: 1955.518
iteration 0500: loss: 1961.284
iteration 0600: loss: 1955.114
iteration 0700: loss: 1959.253
iteration 0800: loss: 1954.787
iteration 0900: loss: 1957.490
====> Epoch: 005 Train loss: 1957.3152  took : 13.408220767974854
====> Test loss: 1959.2739
iteration 0000: loss: 1958.155
iteration 0100: loss: 1956.107
iteration 0200: loss: 1955.174
iteration 0300: loss: 1959.101
iteration 0400: loss: 1954.842
iteration 0500: loss: 1957.783
iteration 0600: loss: 1954.725
iteration 0700: loss: 1952.498
iteration 0800: loss: 1952.033
iteration 0900: loss: 1955.550
====> Epoch: 006 Train loss: 1955.4280  took : 12.321971654891968
====> Test loss: 1957.5446
iteration 0000: loss: 1954.230
iteration 0100: loss: 1958.916
iteration 0200: loss: 1955.173
iteration 0300: loss: 1954.244
iteration 0400: loss: 1955.945
iteration 0500: loss: 1953.877
iteration 0600: loss: 1953.350
iteration 0700: loss: 1950.226
iteration 0800: loss: 1949.837
iteration 0900: loss: 1953.978
====> Epoch: 007 Train loss: 1953.7426  took : 13.263247013092041
====> Test loss: 1955.7781
iteration 0000: loss: 1953.101
iteration 0100: loss: 1952.021
iteration 0200: loss: 1955.163
iteration 0300: loss: 1950.555
iteration 0400: loss: 1953.449
iteration 0500: loss: 1954.366
iteration 0600: loss: 1954.004
iteration 0700: loss: 1953.402
iteration 0800: loss: 1954.525
iteration 0900: loss: 1953.224
====> Epoch: 008 Train loss: 1952.6564  took : 12.472617387771606
====> Test loss: 1954.7991
iteration 0000: loss: 1950.170
iteration 0100: loss: 1950.386
iteration 0200: loss: 1951.747
iteration 0300: loss: 1950.508
iteration 0400: loss: 1952.913
iteration 0500: loss: 1949.454
iteration 0600: loss: 1951.523
iteration 0700: loss: 1951.483
iteration 0800: loss: 1950.750
iteration 0900: loss: 1954.307
====> Epoch: 009 Train loss: 1951.8932  took : 12.724562168121338
====> Test loss: 1953.8634
iteration 0000: loss: 1951.261
iteration 0100: loss: 1952.796
iteration 0200: loss: 1951.493
iteration 0300: loss: 1950.010
iteration 0400: loss: 1949.466
iteration 0500: loss: 1950.006
iteration 0600: loss: 1952.229
iteration 0700: loss: 1949.813
iteration 0800: loss: 1951.391
iteration 0900: loss: 1950.172
====> Epoch: 010 Train loss: 1951.0644  took : 13.356623411178589
====> Test loss: 1952.9743
iteration 0000: loss: 1951.132
iteration 0100: loss: 1949.455
iteration 0200: loss: 1949.562
iteration 0300: loss: 1949.935
iteration 0400: loss: 1950.265
iteration 0500: loss: 1950.238
iteration 0600: loss: 1951.137
iteration 0700: loss: 1949.492
iteration 0800: loss: 1950.961
iteration 0900: loss: 1949.083
====> Epoch: 011 Train loss: 1950.4533  took : 13.405737400054932
====> Test loss: 1952.4141
iteration 0000: loss: 1949.938
iteration 0100: loss: 1949.846
iteration 0200: loss: 1950.631
iteration 0300: loss: 1950.193
iteration 0400: loss: 1951.141
iteration 0500: loss: 1949.510
iteration 0600: loss: 1949.587
iteration 0700: loss: 1951.574
iteration 0800: loss: 1949.338
iteration 0900: loss: 1949.788
====> Epoch: 012 Train loss: 1950.1514  took : 12.416660070419312
====> Test loss: 1952.0755
iteration 0000: loss: 1950.574
iteration 0100: loss: 1949.579
iteration 0200: loss: 1950.803
iteration 0300: loss: 1948.790
iteration 0400: loss: 1951.844
iteration 0500: loss: 1950.034
iteration 0600: loss: 1948.615
iteration 0700: loss: 1949.882
iteration 0800: loss: 1950.684
iteration 0900: loss: 1950.064
====> Epoch: 013 Train loss: 1949.8540  took : 12.505598545074463
====> Test loss: 1951.7001
iteration 0000: loss: 1949.951
iteration 0100: loss: 1949.482
iteration 0200: loss: 1948.641
iteration 0300: loss: 1949.324
iteration 0400: loss: 1950.410
iteration 0500: loss: 1950.098
iteration 0600: loss: 1950.638
iteration 0700: loss: 1949.590
iteration 0800: loss: 1949.133
iteration 0900: loss: 1947.929
====> Epoch: 014 Train loss: 1949.6563  took : 11.874329328536987
====> Test loss: 1951.8308
iteration 0000: loss: 1950.093
iteration 0100: loss: 1949.594
iteration 0200: loss: 1948.434
iteration 0300: loss: 1949.010
iteration 0400: loss: 1949.673
iteration 0500: loss: 1949.398
iteration 0600: loss: 1947.747
iteration 0700: loss: 1951.131
iteration 0800: loss: 1949.352
iteration 0900: loss: 1949.354
====> Epoch: 015 Train loss: 1949.4198  took : 12.97757363319397
====> Test loss: 1951.4772
iteration 0000: loss: 1949.480
iteration 0100: loss: 1951.096
iteration 0200: loss: 1949.503
iteration 0300: loss: 1948.816
iteration 0400: loss: 1950.318
iteration 0500: loss: 1950.725
iteration 0600: loss: 1950.547
iteration 0700: loss: 1951.013
iteration 0800: loss: 1948.325
iteration 0900: loss: 1950.395
====> Epoch: 016 Train loss: 1949.5339  took : 12.301456928253174
====> Test loss: 1950.9697
iteration 0000: loss: 1948.237
iteration 0100: loss: 1949.871
iteration 0200: loss: 1948.023
iteration 0300: loss: 1947.909
iteration 0400: loss: 1949.855
iteration 0500: loss: 1948.155
iteration 0600: loss: 1949.893
iteration 0700: loss: 1948.984
iteration 0800: loss: 1948.515
iteration 0900: loss: 1947.611
====> Epoch: 017 Train loss: 1949.0254  took : 12.931371927261353
====> Test loss: 1950.8941
iteration 0000: loss: 1948.580
iteration 0100: loss: 1949.150
iteration 0200: loss: 1947.592
iteration 0300: loss: 1948.807
iteration 0400: loss: 1949.779
iteration 0500: loss: 1949.118
iteration 0600: loss: 1949.373
iteration 0700: loss: 1947.430
iteration 0800: loss: 1949.337
iteration 0900: loss: 1949.116
====> Epoch: 018 Train loss: 1949.0123  took : 13.2811439037323
====> Test loss: 1950.8119
iteration 0000: loss: 1948.254
iteration 0100: loss: 1948.283
iteration 0200: loss: 1948.428
iteration 0300: loss: 1950.064
iteration 0400: loss: 1948.626
iteration 0500: loss: 1948.289
iteration 0600: loss: 1948.864
iteration 0700: loss: 1947.413
iteration 0800: loss: 1948.457
iteration 0900: loss: 1947.515
====> Epoch: 019 Train loss: 1948.5719  took : 12.499024391174316
====> Test loss: 1950.7223
iteration 0000: loss: 1947.410
iteration 0100: loss: 1947.635
iteration 0200: loss: 1949.178
iteration 0300: loss: 1949.089
iteration 0400: loss: 1947.702
iteration 0500: loss: 1948.332
iteration 0600: loss: 1948.866
iteration 0700: loss: 1948.507
iteration 0800: loss: 1950.243
iteration 0900: loss: 1947.292
====> Epoch: 020 Train loss: 1948.4345  took : 12.815544128417969
====> Test loss: 1950.2910
iteration 0000: loss: 1947.614
iteration 0100: loss: 1948.416
iteration 0200: loss: 1948.403
iteration 0300: loss: 1948.184
iteration 0400: loss: 1948.155
iteration 0500: loss: 1948.856
iteration 0600: loss: 1948.921
iteration 0700: loss: 1948.015
iteration 0800: loss: 1949.442
iteration 0900: loss: 1948.248
====> Epoch: 021 Train loss: 1948.2679  took : 13.190528392791748
====> Test loss: 1950.3580
iteration 0000: loss: 1949.155
iteration 0100: loss: 1947.722
iteration 0200: loss: 1948.143
iteration 0300: loss: 1948.353
iteration 0400: loss: 1948.080
iteration 0500: loss: 1947.857
iteration 0600: loss: 1947.103
iteration 0700: loss: 1947.973
iteration 0800: loss: 1948.178
iteration 0900: loss: 1947.744
====> Epoch: 022 Train loss: 1948.1090  took : 12.543339252471924
====> Test loss: 1949.6732
iteration 0000: loss: 1947.528
iteration 0100: loss: 1948.372
iteration 0200: loss: 1948.324
iteration 0300: loss: 1947.197
iteration 0400: loss: 1947.471
iteration 0500: loss: 1947.438
iteration 0600: loss: 1947.658
iteration 0700: loss: 1948.579
iteration 0800: loss: 1948.031
iteration 0900: loss: 1947.185
====> Epoch: 023 Train loss: 1948.0391  took : 12.003584861755371
====> Test loss: 1949.7840
iteration 0000: loss: 1947.782
iteration 0100: loss: 1948.709
iteration 0200: loss: 1947.371
iteration 0300: loss: 1948.767
iteration 0400: loss: 1947.663
iteration 0500: loss: 1947.128
iteration 0600: loss: 1948.221
iteration 0700: loss: 1949.484
iteration 0800: loss: 1947.357
iteration 0900: loss: 1947.939
====> Epoch: 024 Train loss: 1947.9716  took : 12.201782464981079
====> Test loss: 1949.8305
iteration 0000: loss: 1947.678
iteration 0100: loss: 1948.184
iteration 0200: loss: 1947.947
iteration 0300: loss: 1949.023
iteration 0400: loss: 1947.452
iteration 0500: loss: 1948.622
iteration 0600: loss: 1947.456
iteration 0700: loss: 1949.056
iteration 0800: loss: 1948.561
iteration 0900: loss: 1948.341
====> Epoch: 025 Train loss: 1947.9498  took : 12.318065881729126
====> Test loss: 1949.7767
iteration 0000: loss: 1947.566
iteration 0100: loss: 1947.753
iteration 0200: loss: 1947.609
iteration 0300: loss: 1948.561
iteration 0400: loss: 1947.178
iteration 0500: loss: 1948.198
iteration 0600: loss: 1947.100
iteration 0700: loss: 1947.948
iteration 0800: loss: 1947.164
iteration 0900: loss: 1947.744
====> Epoch: 026 Train loss: 1947.9441  took : 12.085648775100708
====> Test loss: 1949.8133
iteration 0000: loss: 1948.131
iteration 0100: loss: 1947.849
iteration 0200: loss: 1946.769
iteration 0300: loss: 1948.047
iteration 0400: loss: 1947.167
iteration 0500: loss: 1947.419
iteration 0600: loss: 1947.784
iteration 0700: loss: 1947.602
iteration 0800: loss: 1949.753
iteration 0900: loss: 1948.437
====> Epoch: 027 Train loss: 1947.8186  took : 12.922457695007324
====> Test loss: 1949.9339
iteration 0000: loss: 1947.908
iteration 0100: loss: 1947.655
iteration 0200: loss: 1947.870
iteration 0300: loss: 1947.798
iteration 0400: loss: 1948.521
iteration 0500: loss: 1948.126
iteration 0600: loss: 1947.262
iteration 0700: loss: 1947.903
iteration 0800: loss: 1948.576
iteration 0900: loss: 1947.596
====> Epoch: 028 Train loss: 1947.8177  took : 12.579957008361816
====> Test loss: 1949.5878
iteration 0000: loss: 1947.350
iteration 0100: loss: 1948.448
iteration 0200: loss: 1948.797
iteration 0300: loss: 1947.520
iteration 0400: loss: 1948.369
iteration 0500: loss: 1946.891
iteration 0600: loss: 1947.198
iteration 0700: loss: 1948.574
iteration 0800: loss: 1947.645
iteration 0900: loss: 1947.375
====> Epoch: 029 Train loss: 1947.7528  took : 12.609333038330078
====> Test loss: 1949.7702
iteration 0000: loss: 1947.897
iteration 0100: loss: 1948.032
iteration 0200: loss: 1947.136
iteration 0300: loss: 1947.081
iteration 0400: loss: 1947.706
iteration 0500: loss: 1947.712
iteration 0600: loss: 1947.007
iteration 0700: loss: 1947.210
iteration 0800: loss: 1947.561
iteration 0900: loss: 1947.274
====> Epoch: 030 Train loss: 1947.6551  took : 12.71512746810913
====> Test loss: 1949.4731
iteration 0000: loss: 1947.226
iteration 0100: loss: 1947.250
iteration 0200: loss: 1947.533
iteration 0300: loss: 1948.073
iteration 0400: loss: 1947.734
iteration 0500: loss: 1946.807
iteration 0600: loss: 1948.118
iteration 0700: loss: 1948.050
iteration 0800: loss: 1949.690
iteration 0900: loss: 1948.940
====> Epoch: 031 Train loss: 1947.7485  took : 12.41561770439148
====> Test loss: 1949.7517
iteration 0000: loss: 1947.434
iteration 0100: loss: 1947.823
iteration 0200: loss: 1947.948
iteration 0300: loss: 1948.584
iteration 0400: loss: 1948.456
iteration 0500: loss: 1947.912
iteration 0600: loss: 1947.443
iteration 0700: loss: 1948.369
iteration 0800: loss: 1947.273
iteration 0900: loss: 1947.022
====> Epoch: 032 Train loss: 1947.7708  took : 11.923134565353394
====> Test loss: 1949.6745
iteration 0000: loss: 1947.122
iteration 0100: loss: 1947.914
iteration 0200: loss: 1948.353
iteration 0300: loss: 1947.218
iteration 0400: loss: 1946.678
iteration 0500: loss: 1947.125
iteration 0600: loss: 1947.694
iteration 0700: loss: 1946.952
iteration 0800: loss: 1947.477
iteration 0900: loss: 1947.257
====> Epoch: 033 Train loss: 1947.5439  took : 13.505827188491821
====> Test loss: 1949.2047
iteration 0000: loss: 1948.015
iteration 0100: loss: 1946.916
iteration 0200: loss: 1947.290
iteration 0300: loss: 1947.520
iteration 0400: loss: 1947.446
iteration 0500: loss: 1947.184
iteration 0600: loss: 1947.408
iteration 0700: loss: 1947.396
iteration 0800: loss: 1947.334
iteration 0900: loss: 1947.708
====> Epoch: 034 Train loss: 1947.5500  took : 13.480865240097046
====> Test loss: 1949.5583
iteration 0000: loss: 1948.486
iteration 0100: loss: 1946.792
iteration 0200: loss: 1947.826
iteration 0300: loss: 1947.641
iteration 0400: loss: 1947.603
iteration 0500: loss: 1947.351
iteration 0600: loss: 1948.739
iteration 0700: loss: 1946.836
iteration 0800: loss: 1947.125
iteration 0900: loss: 1946.889
====> Epoch: 035 Train loss: 1947.6071  took : 12.62326979637146
====> Test loss: 1949.2166
iteration 0000: loss: 1948.030
iteration 0100: loss: 1947.759
iteration 0200: loss: 1948.177
iteration 0300: loss: 1946.969
iteration 0400: loss: 1947.335
iteration 0500: loss: 1947.815
iteration 0600: loss: 1946.969
iteration 0700: loss: 1947.170
iteration 0800: loss: 1947.572
iteration 0900: loss: 1947.786
====> Epoch: 036 Train loss: 1947.4943  took : 12.886789321899414
====> Test loss: 1950.0482
iteration 0000: loss: 1948.123
iteration 0100: loss: 1946.910
iteration 0200: loss: 1948.916
iteration 0300: loss: 1947.086
iteration 0400: loss: 1948.360
iteration 0500: loss: 1947.180
iteration 0600: loss: 1947.690
iteration 0700: loss: 1947.477
iteration 0800: loss: 1948.095
iteration 0900: loss: 1946.824
====> Epoch: 037 Train loss: 1947.5497  took : 13.435322523117065
====> Test loss: 1949.9302
iteration 0000: loss: 1948.334
iteration 0100: loss: 1947.934
iteration 0200: loss: 1946.991
iteration 0300: loss: 1947.597
iteration 0400: loss: 1947.215
iteration 0500: loss: 1946.986
iteration 0600: loss: 1946.965
iteration 0700: loss: 1947.448
iteration 0800: loss: 1947.603
iteration 0900: loss: 1946.900
====> Epoch: 038 Train loss: 1947.4357  took : 12.669416666030884
====> Test loss: 1949.1784
iteration 0000: loss: 1946.706
iteration 0100: loss: 1948.154
iteration 0200: loss: 1948.086
iteration 0300: loss: 1946.812
iteration 0400: loss: 1946.823
iteration 0500: loss: 1947.349
iteration 0600: loss: 1946.916
iteration 0700: loss: 1946.757
iteration 0800: loss: 1946.948
iteration 0900: loss: 1947.218
====> Epoch: 039 Train loss: 1947.4229  took : 12.10630202293396
====> Test loss: 1949.1617
iteration 0000: loss: 1947.693
iteration 0100: loss: 1947.307
iteration 0200: loss: 1947.024
iteration 0300: loss: 1947.556
iteration 0400: loss: 1948.645
iteration 0500: loss: 1949.093
iteration 0600: loss: 1947.848
iteration 0700: loss: 1947.362
iteration 0800: loss: 1948.879
iteration 0900: loss: 1947.774
====> Epoch: 040 Train loss: 1947.5142  took : 12.354084253311157
====> Test loss: 1949.1653
iteration 0000: loss: 1947.326
iteration 0100: loss: 1946.679
iteration 0200: loss: 1947.180
iteration 0300: loss: 1947.099
iteration 0400: loss: 1947.702
iteration 0500: loss: 1948.088
iteration 0600: loss: 1947.131
iteration 0700: loss: 1947.450
iteration 0800: loss: 1947.071
iteration 0900: loss: 1947.003
====> Epoch: 041 Train loss: 1947.4108  took : 12.215466499328613
====> Test loss: 1949.0843
iteration 0000: loss: 1946.949
iteration 0100: loss: 1946.662
iteration 0200: loss: 1947.319
iteration 0300: loss: 1947.082
iteration 0400: loss: 1947.260
iteration 0500: loss: 1947.153
iteration 0600: loss: 1948.800
iteration 0700: loss: 1946.820
iteration 0800: loss: 1948.419
iteration 0900: loss: 1948.697
====> Epoch: 042 Train loss: 1947.3348  took : 12.875672340393066
====> Test loss: 1949.5577
iteration 0000: loss: 1948.385
iteration 0100: loss: 1947.483
iteration 0200: loss: 1946.809
iteration 0300: loss: 1947.582
iteration 0400: loss: 1946.948
iteration 0500: loss: 1947.443
iteration 0600: loss: 1947.465
iteration 0700: loss: 1946.996
iteration 0800: loss: 1946.942
iteration 0900: loss: 1946.722
====> Epoch: 043 Train loss: 1947.4320  took : 11.948190927505493
====> Test loss: 1949.3471
iteration 0000: loss: 1946.898
iteration 0100: loss: 1947.037
iteration 0200: loss: 1947.934
iteration 0300: loss: 1947.558
iteration 0400: loss: 1947.763
iteration 0500: loss: 1947.133
iteration 0600: loss: 1947.018
iteration 0700: loss: 1946.960
iteration 0800: loss: 1947.864
iteration 0900: loss: 1946.825
====> Epoch: 044 Train loss: 1947.4269  took : 12.494486093521118
====> Test loss: 1949.2152
iteration 0000: loss: 1947.575
iteration 0100: loss: 1947.138
iteration 0200: loss: 1947.918
iteration 0300: loss: 1947.655
iteration 0400: loss: 1947.250
iteration 0500: loss: 1948.008
iteration 0600: loss: 1947.852
iteration 0700: loss: 1947.201
iteration 0800: loss: 1947.274
iteration 0900: loss: 1946.760
====> Epoch: 045 Train loss: 1947.4288  took : 11.843586921691895
====> Test loss: 1949.5096
iteration 0000: loss: 1947.136
iteration 0100: loss: 1947.099
iteration 0200: loss: 1946.789
iteration 0300: loss: 1946.931
iteration 0400: loss: 1946.945
iteration 0500: loss: 1947.128
iteration 0600: loss: 1948.477
iteration 0700: loss: 1946.846
iteration 0800: loss: 1946.776
iteration 0900: loss: 1947.924
====> Epoch: 046 Train loss: 1947.2838  took : 11.365305662155151
====> Test loss: 1949.1168
iteration 0000: loss: 1947.522
iteration 0100: loss: 1946.735
iteration 0200: loss: 1948.007
iteration 0300: loss: 1948.299
iteration 0400: loss: 1947.213
iteration 0500: loss: 1949.924
iteration 0600: loss: 1947.118
iteration 0700: loss: 1948.199
iteration 0800: loss: 1948.528
iteration 0900: loss: 1946.949
====> Epoch: 047 Train loss: 1947.4905  took : 12.591540575027466
====> Test loss: 1949.1316
iteration 0000: loss: 1947.306
iteration 0100: loss: 1947.309
iteration 0200: loss: 1946.683
iteration 0300: loss: 1947.414
iteration 0400: loss: 1947.074
iteration 0500: loss: 1946.956
iteration 0600: loss: 1946.942
iteration 0700: loss: 1946.920
iteration 0800: loss: 1947.363
iteration 0900: loss: 1947.655
====> Epoch: 048 Train loss: 1947.3558  took : 13.57907485961914
====> Test loss: 1948.9290
iteration 0000: loss: 1946.990
iteration 0100: loss: 1946.821
iteration 0200: loss: 1948.632
iteration 0300: loss: 1947.154
iteration 0400: loss: 1946.641
iteration 0500: loss: 1946.726
iteration 0600: loss: 1946.714
iteration 0700: loss: 1946.874
iteration 0800: loss: 1947.845
iteration 0900: loss: 1948.694
====> Epoch: 049 Train loss: 1947.3086  took : 11.852466106414795
====> Test loss: 1949.5458
iteration 0000: loss: 1947.737
iteration 0100: loss: 1948.720
iteration 0200: loss: 1946.931
iteration 0300: loss: 1946.612
iteration 0400: loss: 1946.699
iteration 0500: loss: 1947.174
iteration 0600: loss: 1946.719
iteration 0700: loss: 1946.972
iteration 0800: loss: 1947.864
iteration 0900: loss: 1947.148
====> Epoch: 050 Train loss: 1947.3833  took : 12.165855407714844
====> Test loss: 1949.1585
====> [MM-VAE] Time: 703.453s or 00:11:43
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  7
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_7
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_7
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5218.910
iteration 0100: loss: 4129.987
iteration 0200: loss: 4073.424
iteration 0300: loss: 4048.336
iteration 0400: loss: 4014.667
iteration 0500: loss: 4013.824
iteration 0600: loss: 4009.456
iteration 0700: loss: 4025.402
iteration 0800: loss: 3994.752
iteration 0900: loss: 3996.921
iteration 1000: loss: 4006.249
iteration 1100: loss: 4007.974
iteration 1200: loss: 3998.758
iteration 1300: loss: 3990.613
iteration 1400: loss: 4002.637
iteration 1500: loss: 3999.812
iteration 1600: loss: 3979.255
iteration 1700: loss: 3999.507
iteration 1800: loss: 3985.997
====> Epoch: 001 Train loss: 4023.4074  took : 53.683326959609985
====> Test loss: 3991.8483
iteration 0000: loss: 3992.813
iteration 0100: loss: 3993.175
iteration 0200: loss: 3996.614
iteration 0300: loss: 3979.150
iteration 0400: loss: 3985.780
iteration 0500: loss: 3989.378
iteration 0600: loss: 3977.283
iteration 0700: loss: 3972.359
iteration 0800: loss: 3993.335
iteration 0900: loss: 3983.604
iteration 1000: loss: 3981.636
iteration 1100: loss: 3980.571
iteration 1200: loss: 3979.913
iteration 1300: loss: 3978.567
iteration 1400: loss: 3974.756
iteration 1500: loss: 3983.979
iteration 1600: loss: 3991.723
iteration 1700: loss: 3979.490
iteration 1800: loss: 3985.550
====> Epoch: 002 Train loss: 3982.3147  took : 53.34274506568909
====> Test loss: 3981.9933
iteration 0000: loss: 3972.116
iteration 0100: loss: 3986.585
iteration 0200: loss: 3978.265
iteration 0300: loss: 3987.398
iteration 0400: loss: 3970.059
iteration 0500: loss: 3977.501
iteration 0600: loss: 3986.322
iteration 0700: loss: 3979.950
iteration 0800: loss: 3978.679
iteration 0900: loss: 3974.708
iteration 1000: loss: 3976.425
iteration 1100: loss: 3974.155
iteration 1200: loss: 3963.770
iteration 1300: loss: 3973.024
iteration 1400: loss: 3967.694
iteration 1500: loss: 3968.292
iteration 1600: loss: 3977.161
iteration 1700: loss: 3970.498
iteration 1800: loss: 3966.125
====> Epoch: 003 Train loss: 3974.3422  took : 53.53315567970276
====> Test loss: 3976.4204
iteration 0000: loss: 3962.296
iteration 0100: loss: 3968.717
iteration 0200: loss: 3977.218
iteration 0300: loss: 3973.118
iteration 0400: loss: 3970.606
iteration 0500: loss: 3972.586
iteration 0600: loss: 3977.613
iteration 0700: loss: 3971.295
iteration 0800: loss: 3978.093
iteration 0900: loss: 3959.969
iteration 1000: loss: 3975.375
iteration 1100: loss: 3967.032
iteration 1200: loss: 3960.666
iteration 1300: loss: 3969.277
iteration 1400: loss: 3974.049
iteration 1500: loss: 3962.916
iteration 1600: loss: 3954.445
iteration 1700: loss: 3977.809
iteration 1800: loss: 3966.596
====> Epoch: 004 Train loss: 3970.1514  took : 53.23122596740723
====> Test loss: 3972.2830
iteration 0000: loss: 3969.291
iteration 0100: loss: 3964.051
iteration 0200: loss: 3960.347
iteration 0300: loss: 3956.604
iteration 0400: loss: 3969.403
iteration 0500: loss: 3970.523
iteration 0600: loss: 3978.226
iteration 0700: loss: 3960.022
iteration 0800: loss: 3962.236
iteration 0900: loss: 3981.574
iteration 1000: loss: 3962.631
iteration 1100: loss: 3967.861
iteration 1200: loss: 3970.551
iteration 1300: loss: 3976.468
iteration 1400: loss: 3973.014
iteration 1500: loss: 3963.350
iteration 1600: loss: 3960.919
iteration 1700: loss: 3970.703
iteration 1800: loss: 3954.000
====> Epoch: 005 Train loss: 3966.6919  took : 53.327818155288696
====> Test loss: 3970.0567
iteration 0000: loss: 3952.986
iteration 0100: loss: 3963.070
iteration 0200: loss: 3961.213
iteration 0300: loss: 3972.420
iteration 0400: loss: 3966.407
iteration 0500: loss: 3964.269
iteration 0600: loss: 3967.043
iteration 0700: loss: 3962.122
iteration 0800: loss: 3959.371
iteration 0900: loss: 3968.551
iteration 1000: loss: 3971.560
iteration 1100: loss: 3958.631
iteration 1200: loss: 3960.225
iteration 1300: loss: 3964.709
iteration 1400: loss: 3958.236
iteration 1500: loss: 3962.160
iteration 1600: loss: 3957.149
iteration 1700: loss: 3970.182
iteration 1800: loss: 3970.255
====> Epoch: 006 Train loss: 3964.0083  took : 53.51946496963501
====> Test loss: 3968.2453
iteration 0000: loss: 3959.899
iteration 0100: loss: 3964.979
iteration 0200: loss: 3964.964
iteration 0300: loss: 3958.359
iteration 0400: loss: 3964.450
iteration 0500: loss: 3958.157
iteration 0600: loss: 3963.426
iteration 0700: loss: 3956.793
iteration 0800: loss: 3968.430
iteration 0900: loss: 3956.969
iteration 1000: loss: 3961.378
iteration 1100: loss: 3972.734
iteration 1200: loss: 3960.626
iteration 1300: loss: 3960.362
iteration 1400: loss: 3959.047
iteration 1500: loss: 3959.600
iteration 1600: loss: 3962.996
iteration 1700: loss: 3957.446
iteration 1800: loss: 3963.267
====> Epoch: 007 Train loss: 3962.0268  took : 53.39743638038635
====> Test loss: 3965.6450
iteration 0000: loss: 3964.264
iteration 0100: loss: 3958.034
iteration 0200: loss: 3969.355
iteration 0300: loss: 3962.549
iteration 0400: loss: 3962.740
iteration 0500: loss: 3965.400
iteration 0600: loss: 3963.114
iteration 0700: loss: 3954.244
iteration 0800: loss: 3969.319
iteration 0900: loss: 3954.525
iteration 1000: loss: 3955.729
iteration 1100: loss: 3951.887
iteration 1200: loss: 3952.750
iteration 1300: loss: 3959.847
iteration 1400: loss: 3957.034
iteration 1500: loss: 3953.243
iteration 1600: loss: 3958.789
iteration 1700: loss: 3955.940
iteration 1800: loss: 3955.849
====> Epoch: 008 Train loss: 3960.1366  took : 53.48825812339783
====> Test loss: 3962.7991
iteration 0000: loss: 3957.977
iteration 0100: loss: 3955.243
iteration 0200: loss: 3953.490
iteration 0300: loss: 3961.520
iteration 0400: loss: 3957.890
iteration 0500: loss: 3952.875
iteration 0600: loss: 3960.018
iteration 0700: loss: 3958.722
iteration 0800: loss: 3955.094
iteration 0900: loss: 3959.510
iteration 1000: loss: 3948.156
iteration 1100: loss: 3973.397
iteration 1200: loss: 3950.732
iteration 1300: loss: 3953.020
iteration 1400: loss: 3958.996
iteration 1500: loss: 3946.698
iteration 1600: loss: 3947.269
iteration 1700: loss: 3954.316
iteration 1800: loss: 3951.623
====> Epoch: 009 Train loss: 3957.7226  took : 53.32353377342224
====> Test loss: 3961.6571
iteration 0000: loss: 3966.401
iteration 0100: loss: 3960.018
iteration 0200: loss: 3955.917
iteration 0300: loss: 3951.993
iteration 0400: loss: 3957.146
iteration 0500: loss: 3952.899
iteration 0600: loss: 3955.495
iteration 0700: loss: 3954.250
iteration 0800: loss: 3957.009
iteration 0900: loss: 3956.579
iteration 1000: loss: 3953.843
iteration 1100: loss: 3947.493
iteration 1200: loss: 3946.900
iteration 1300: loss: 3954.816
iteration 1400: loss: 3958.774
iteration 1500: loss: 3950.276
iteration 1600: loss: 3956.344
iteration 1700: loss: 3950.936
iteration 1800: loss: 3951.191
====> Epoch: 010 Train loss: 3955.6308  took : 53.4922776222229
====> Test loss: 3958.8225
iteration 0000: loss: 3962.011
iteration 0100: loss: 3944.680
iteration 0200: loss: 3954.026
iteration 0300: loss: 3951.007
iteration 0400: loss: 3964.594
iteration 0500: loss: 3953.965
iteration 0600: loss: 3945.948
iteration 0700: loss: 3954.896
iteration 0800: loss: 3958.222
iteration 0900: loss: 3948.628
iteration 1000: loss: 3948.143
iteration 1100: loss: 3964.273
iteration 1200: loss: 3944.698
iteration 1300: loss: 3947.485
iteration 1400: loss: 3952.684
iteration 1500: loss: 3963.964
iteration 1600: loss: 3951.623
iteration 1700: loss: 3953.408
iteration 1800: loss: 3945.872
====> Epoch: 011 Train loss: 3953.1373  took : 53.574413537979126
====> Test loss: 3956.0527
iteration 0000: loss: 3944.620
iteration 0100: loss: 3948.938
iteration 0200: loss: 3951.907
iteration 0300: loss: 3954.258
iteration 0400: loss: 3956.314
iteration 0500: loss: 3954.477
iteration 0600: loss: 3951.932
iteration 0700: loss: 3953.619
iteration 0800: loss: 3955.215
iteration 0900: loss: 3948.866
iteration 1000: loss: 3950.433
iteration 1100: loss: 3957.452
iteration 1200: loss: 3951.211
iteration 1300: loss: 3951.299
iteration 1400: loss: 3952.523
iteration 1500: loss: 3951.137
iteration 1600: loss: 3948.745
iteration 1700: loss: 3946.120
iteration 1800: loss: 3950.701
====> Epoch: 012 Train loss: 3950.9087  took : 53.36086845397949
====> Test loss: 3953.1895
iteration 0000: loss: 3946.581
iteration 0100: loss: 3952.809
iteration 0200: loss: 3944.581
iteration 0300: loss: 3957.585
iteration 0400: loss: 3948.548
iteration 0500: loss: 3960.870
iteration 0600: loss: 3954.409
iteration 0700: loss: 3944.799
iteration 0800: loss: 3949.649
iteration 0900: loss: 3947.961
iteration 1000: loss: 3946.248
iteration 1100: loss: 3937.965
iteration 1200: loss: 3947.644
iteration 1300: loss: 3945.579
iteration 1400: loss: 3953.899
iteration 1500: loss: 3943.916
iteration 1600: loss: 3953.484
iteration 1700: loss: 3943.462
iteration 1800: loss: 3951.197
====> Epoch: 013 Train loss: 3948.9274  took : 53.297330141067505
====> Test loss: 3952.1653
iteration 0000: loss: 3945.484
iteration 0100: loss: 3945.473
iteration 0200: loss: 3947.659
iteration 0300: loss: 3949.500
iteration 0400: loss: 3950.710
iteration 0500: loss: 3946.579
iteration 0600: loss: 3953.003
iteration 0700: loss: 3951.369
iteration 0800: loss: 3942.871
iteration 0900: loss: 3945.292
iteration 1000: loss: 3946.128
iteration 1100: loss: 3954.220
iteration 1200: loss: 3944.074
iteration 1300: loss: 3949.212
iteration 1400: loss: 3948.552
iteration 1500: loss: 3957.607
iteration 1600: loss: 3945.436
iteration 1700: loss: 3953.245
iteration 1800: loss: 3943.402
====> Epoch: 014 Train loss: 3947.6226  took : 53.29262638092041
====> Test loss: 3950.0030
iteration 0000: loss: 3945.135
iteration 0100: loss: 3942.358
iteration 0200: loss: 3949.125
iteration 0300: loss: 3955.852
iteration 0400: loss: 3950.546
iteration 0500: loss: 3949.575
iteration 0600: loss: 3948.076
iteration 0700: loss: 3947.252
iteration 0800: loss: 3937.807
iteration 0900: loss: 3949.156
iteration 1000: loss: 3943.053
iteration 1100: loss: 3948.229
iteration 1200: loss: 3942.350
iteration 1300: loss: 3944.957
iteration 1400: loss: 3950.013
iteration 1500: loss: 3943.926
iteration 1600: loss: 3936.163
iteration 1700: loss: 3945.924
iteration 1800: loss: 3943.331
====> Epoch: 015 Train loss: 3945.4671  took : 53.210065603256226
====> Test loss: 3949.5898
iteration 0000: loss: 3949.225
iteration 0100: loss: 3947.278
iteration 0200: loss: 3943.518
iteration 0300: loss: 3942.608
iteration 0400: loss: 3942.067
iteration 0500: loss: 3944.661
iteration 0600: loss: 3940.594
iteration 0700: loss: 3942.695
iteration 0800: loss: 3951.080
iteration 0900: loss: 3948.592
iteration 1000: loss: 3950.608
iteration 1100: loss: 3946.336
iteration 1200: loss: 3944.173
iteration 1300: loss: 3946.057
iteration 1400: loss: 3941.985
iteration 1500: loss: 3940.319
iteration 1600: loss: 3939.325
iteration 1700: loss: 3941.363
iteration 1800: loss: 3941.851
====> Epoch: 016 Train loss: 3944.0656  took : 53.20696711540222
====> Test loss: 3947.5126
iteration 0000: loss: 3940.918
iteration 0100: loss: 3942.766
iteration 0200: loss: 3937.090
iteration 0300: loss: 3942.009
iteration 0400: loss: 3935.612
iteration 0500: loss: 3946.430
iteration 0600: loss: 3948.295
iteration 0700: loss: 3936.426
iteration 0800: loss: 3955.234
iteration 0900: loss: 3944.710
iteration 1000: loss: 3946.369
iteration 1100: loss: 3940.643
iteration 1200: loss: 3942.648
iteration 1300: loss: 3941.979
iteration 1400: loss: 3942.925
iteration 1500: loss: 3944.069
iteration 1600: loss: 3949.147
iteration 1700: loss: 3947.482
iteration 1800: loss: 3943.153
====> Epoch: 017 Train loss: 3943.4991  took : 53.22884488105774
====> Test loss: 3947.3326
iteration 0000: loss: 3942.186
iteration 0100: loss: 3938.311
iteration 0200: loss: 3945.782
iteration 0300: loss: 3937.315
iteration 0400: loss: 3948.650
iteration 0500: loss: 3943.512
iteration 0600: loss: 3935.311
iteration 0700: loss: 3949.486
iteration 0800: loss: 3940.682
iteration 0900: loss: 3947.108
iteration 1000: loss: 3947.848
iteration 1100: loss: 3941.961
iteration 1200: loss: 3944.704
iteration 1300: loss: 3935.902
iteration 1400: loss: 3948.604
iteration 1500: loss: 3943.175
iteration 1600: loss: 3935.481
iteration 1700: loss: 3941.934
iteration 1800: loss: 3946.508
====> Epoch: 018 Train loss: 3942.8724  took : 53.31373929977417
====> Test loss: 3946.6179
iteration 0000: loss: 3943.801
iteration 0100: loss: 3940.498
iteration 0200: loss: 3941.268
iteration 0300: loss: 3942.132
iteration 0400: loss: 3942.060
iteration 0500: loss: 3937.639
iteration 0600: loss: 3935.242
iteration 0700: loss: 3940.740
iteration 0800: loss: 3942.261
iteration 0900: loss: 3942.839
iteration 1000: loss: 3940.205
iteration 1100: loss: 3943.560
iteration 1200: loss: 3940.757
iteration 1300: loss: 3941.129
iteration 1400: loss: 3936.825
iteration 1500: loss: 3945.526
iteration 1600: loss: 3940.668
iteration 1700: loss: 3942.420
iteration 1800: loss: 3949.516
====> Epoch: 019 Train loss: 3942.4321  took : 53.28913331031799
====> Test loss: 3947.5306
iteration 0000: loss: 3942.567
iteration 0100: loss: 3943.083
iteration 0200: loss: 3943.176
iteration 0300: loss: 3941.489
iteration 0400: loss: 3943.709
iteration 0500: loss: 3946.357
iteration 0600: loss: 3939.699
iteration 0700: loss: 3938.031
iteration 0800: loss: 3941.006
iteration 0900: loss: 3944.207
iteration 1000: loss: 3940.057
iteration 1100: loss: 3947.356
iteration 1200: loss: 3951.094
iteration 1300: loss: 3945.120
iteration 1400: loss: 3943.981
iteration 1500: loss: 3947.616
iteration 1600: loss: 3940.779
iteration 1700: loss: 3931.975
iteration 1800: loss: 3944.473
====> Epoch: 020 Train loss: 3941.8426  took : 53.26503229141235
====> Test loss: 3946.2359
iteration 0000: loss: 3944.584
iteration 0100: loss: 3943.754
iteration 0200: loss: 3940.137
iteration 0300: loss: 3938.967
iteration 0400: loss: 3945.472
iteration 0500: loss: 3938.521
iteration 0600: loss: 3942.727
iteration 0700: loss: 3938.664
iteration 0800: loss: 3945.881
iteration 0900: loss: 3944.467
iteration 1000: loss: 3940.063
iteration 1100: loss: 3945.189
iteration 1200: loss: 3942.690
iteration 1300: loss: 3937.325
iteration 1400: loss: 3941.424
iteration 1500: loss: 3935.948
iteration 1600: loss: 3942.439
iteration 1700: loss: 3943.810
iteration 1800: loss: 3941.340
====> Epoch: 021 Train loss: 3941.0097  took : 53.15786814689636
====> Test loss: 3945.0470
iteration 0000: loss: 3941.459
iteration 0100: loss: 3937.579
iteration 0200: loss: 3940.520
iteration 0300: loss: 3945.140
iteration 0400: loss: 3935.523
iteration 0500: loss: 3937.517
iteration 0600: loss: 3936.316
iteration 0700: loss: 3936.300
iteration 0800: loss: 3934.237
iteration 0900: loss: 3940.201
iteration 1000: loss: 3947.148
iteration 1100: loss: 3937.394
iteration 1200: loss: 3942.100
iteration 1300: loss: 3942.870
iteration 1400: loss: 3939.360
iteration 1500: loss: 3938.974
iteration 1600: loss: 3943.613
iteration 1700: loss: 3942.560
iteration 1800: loss: 3949.417
====> Epoch: 022 Train loss: 3940.7145  took : 53.136223793029785
====> Test loss: 3945.3656
iteration 0000: loss: 3938.350
iteration 0100: loss: 3945.230
iteration 0200: loss: 3934.746
iteration 0300: loss: 3942.393
iteration 0400: loss: 3943.334
iteration 0500: loss: 3938.504
iteration 0600: loss: 3935.673
iteration 0700: loss: 3938.947
iteration 0800: loss: 3935.418
iteration 0900: loss: 3942.264
iteration 1000: loss: 3935.636
iteration 1100: loss: 3941.869
iteration 1200: loss: 3950.732
iteration 1300: loss: 3936.885
iteration 1400: loss: 3943.390
iteration 1500: loss: 3943.225
iteration 1600: loss: 3941.243
iteration 1700: loss: 3939.875
iteration 1800: loss: 3939.477
====> Epoch: 023 Train loss: 3940.2197  took : 53.35773038864136
====> Test loss: 3944.5975
iteration 0000: loss: 3940.391
iteration 0100: loss: 3942.761
iteration 0200: loss: 3941.608
iteration 0300: loss: 3943.203
iteration 0400: loss: 3939.425
iteration 0500: loss: 3937.760
iteration 0600: loss: 3947.360
iteration 0700: loss: 3938.925
iteration 0800: loss: 3947.219
iteration 0900: loss: 3937.761
iteration 1000: loss: 3937.055
iteration 1100: loss: 3943.815
iteration 1200: loss: 3946.224
iteration 1300: loss: 3938.237
iteration 1400: loss: 3941.206
iteration 1500: loss: 3940.438
iteration 1600: loss: 3939.330
iteration 1700: loss: 3939.385
iteration 1800: loss: 3937.803
====> Epoch: 024 Train loss: 3940.1405  took : 53.18747591972351
====> Test loss: 3944.5897
iteration 0000: loss: 3940.110
iteration 0100: loss: 3942.674
iteration 0200: loss: 3942.377
iteration 0300: loss: 3943.307
iteration 0400: loss: 3942.608
iteration 0500: loss: 3935.063
iteration 0600: loss: 3940.577
iteration 0700: loss: 3945.363
iteration 0800: loss: 3942.665
iteration 0900: loss: 3938.976
iteration 1000: loss: 3939.240
iteration 1100: loss: 3947.123
iteration 1200: loss: 3936.601
iteration 1300: loss: 3941.276
iteration 1400: loss: 3937.990
iteration 1500: loss: 3941.311
iteration 1600: loss: 3943.946
iteration 1700: loss: 3937.301
iteration 1800: loss: 3934.410
====> Epoch: 025 Train loss: 3939.9786  took : 53.26115441322327
====> Test loss: 3944.0589
iteration 0000: loss: 3939.735
iteration 0100: loss: 3941.046
iteration 0200: loss: 3933.235
iteration 0300: loss: 3933.906
iteration 0400: loss: 3939.894
iteration 0500: loss: 3947.671
iteration 0600: loss: 3937.378
iteration 0700: loss: 3941.609
iteration 0800: loss: 3933.993
iteration 0900: loss: 3943.435
iteration 1000: loss: 3947.745
iteration 1100: loss: 3940.594
iteration 1200: loss: 3941.606
iteration 1300: loss: 3940.410
iteration 1400: loss: 3939.399
iteration 1500: loss: 3936.972
iteration 1600: loss: 3937.899
iteration 1700: loss: 3943.629
iteration 1800: loss: 3943.821
====> Epoch: 026 Train loss: 3939.4118  took : 53.096343755722046
====> Test loss: 3943.4344
iteration 0000: loss: 3934.269
iteration 0100: loss: 3940.892
iteration 0200: loss: 3934.187
iteration 0300: loss: 3943.034
iteration 0400: loss: 3937.772
iteration 0500: loss: 3938.387
iteration 0600: loss: 3940.635
iteration 0700: loss: 3933.220
iteration 0800: loss: 3938.532
iteration 0900: loss: 3939.420
iteration 1000: loss: 3939.423
iteration 1100: loss: 3937.621
iteration 1200: loss: 3934.500
iteration 1300: loss: 3936.115
iteration 1400: loss: 3938.845
iteration 1500: loss: 3936.647
iteration 1600: loss: 3940.366
iteration 1700: loss: 3943.094
iteration 1800: loss: 3936.512
====> Epoch: 027 Train loss: 3939.2399  took : 53.2011342048645
====> Test loss: 3943.4385
iteration 0000: loss: 3933.768
iteration 0100: loss: 3943.143
iteration 0200: loss: 3939.587
iteration 0300: loss: 3935.741
iteration 0400: loss: 3933.367
iteration 0500: loss: 3934.130
iteration 0600: loss: 3934.985
iteration 0700: loss: 3934.842
iteration 0800: loss: 3941.918
iteration 0900: loss: 3935.916
iteration 1000: loss: 3945.916
iteration 1100: loss: 3944.345
iteration 1200: loss: 3936.566
iteration 1300: loss: 3942.948
iteration 1400: loss: 3938.825
iteration 1500: loss: 3938.359
iteration 1600: loss: 3941.010
iteration 1700: loss: 3941.379
iteration 1800: loss: 3933.715
====> Epoch: 028 Train loss: 3939.1127  took : 53.05673837661743
====> Test loss: 3943.9257
iteration 0000: loss: 3933.214
iteration 0100: loss: 3939.748
iteration 0200: loss: 3936.500
iteration 0300: loss: 3937.736
iteration 0400: loss: 3934.711
iteration 0500: loss: 3942.506
iteration 0600: loss: 3935.172
iteration 0700: loss: 3945.047
iteration 0800: loss: 3934.032
iteration 0900: loss: 3938.416
iteration 1000: loss: 3938.392
iteration 1100: loss: 3938.189
iteration 1200: loss: 3943.812
iteration 1300: loss: 3946.084
iteration 1400: loss: 3942.499
iteration 1500: loss: 3939.999
iteration 1600: loss: 3936.245
iteration 1700: loss: 3937.944
iteration 1800: loss: 3939.719
====> Epoch: 029 Train loss: 3939.0478  took : 53.385000467300415
====> Test loss: 3943.1410
iteration 0000: loss: 3942.806
iteration 0100: loss: 3937.707
iteration 0200: loss: 3940.437
iteration 0300: loss: 3934.489
iteration 0400: loss: 3933.812
iteration 0500: loss: 3936.709
iteration 0600: loss: 3941.931
iteration 0700: loss: 3942.227
iteration 0800: loss: 3932.812
iteration 0900: loss: 3940.605
iteration 1000: loss: 3944.532
iteration 1100: loss: 3942.879
iteration 1200: loss: 3944.094
iteration 1300: loss: 3938.109
iteration 1400: loss: 3938.717
iteration 1500: loss: 3934.760
iteration 1600: loss: 3935.194
iteration 1700: loss: 3944.655
iteration 1800: loss: 3939.098
====> Epoch: 030 Train loss: 3938.9971  took : 53.161263942718506
====> Test loss: 3943.3495
iteration 0000: loss: 3937.864
iteration 0100: loss: 3934.043
iteration 0200: loss: 3941.819
iteration 0300: loss: 3939.724
iteration 0400: loss: 3941.387
iteration 0500: loss: 3938.000
iteration 0600: loss: 3936.124
iteration 0700: loss: 3939.678
iteration 0800: loss: 3938.828
iteration 0900: loss: 3942.113
iteration 1000: loss: 3934.273
iteration 1100: loss: 3939.748
iteration 1200: loss: 3939.875
iteration 1300: loss: 3935.228
iteration 1400: loss: 3940.717
iteration 1500: loss: 3936.159
iteration 1600: loss: 3940.453
iteration 1700: loss: 3939.557
iteration 1800: loss: 3935.425
====> Epoch: 031 Train loss: 3938.7855  took : 53.111512660980225
====> Test loss: 3943.7402
iteration 0000: loss: 3947.437
iteration 0100: loss: 3931.430
iteration 0200: loss: 3937.663
iteration 0300: loss: 3933.282
iteration 0400: loss: 3939.804
iteration 0500: loss: 3939.844
iteration 0600: loss: 3932.678
iteration 0700: loss: 3946.463
iteration 0800: loss: 3942.483
iteration 0900: loss: 3939.625
iteration 1000: loss: 3939.967
iteration 1100: loss: 3939.937
iteration 1200: loss: 3935.775
iteration 1300: loss: 3940.464
iteration 1400: loss: 3936.931
iteration 1500: loss: 3941.356
iteration 1600: loss: 3939.285
iteration 1700: loss: 3936.778
iteration 1800: loss: 3934.683
====> Epoch: 032 Train loss: 3938.7592  took : 52.934869050979614
====> Test loss: 3942.9924
iteration 0000: loss: 3944.984
iteration 0100: loss: 3934.732
iteration 0200: loss: 3934.588
iteration 0300: loss: 3936.365
iteration 0400: loss: 3940.050
iteration 0500: loss: 3944.802
iteration 0600: loss: 3937.546
iteration 0700: loss: 3939.276
iteration 0800: loss: 3942.603
iteration 0900: loss: 3932.942
iteration 1000: loss: 3939.453
iteration 1100: loss: 3943.059
iteration 1200: loss: 3940.878
iteration 1300: loss: 3942.810
iteration 1400: loss: 3945.609
iteration 1500: loss: 3934.729
iteration 1600: loss: 3932.517
iteration 1700: loss: 3939.117
iteration 1800: loss: 3940.612
====> Epoch: 033 Train loss: 3938.7545  took : 53.0554895401001
====> Test loss: 3944.0349
iteration 0000: loss: 3935.388
iteration 0100: loss: 3944.573
iteration 0200: loss: 3938.887
iteration 0300: loss: 3933.958
iteration 0400: loss: 3937.615
iteration 0500: loss: 3932.347
iteration 0600: loss: 3938.776
iteration 0700: loss: 3940.156
iteration 0800: loss: 3937.835
iteration 0900: loss: 3937.002
iteration 1000: loss: 3941.782
iteration 1100: loss: 3941.892
iteration 1200: loss: 3933.746
iteration 1300: loss: 3944.316
iteration 1400: loss: 3945.359
iteration 1500: loss: 3943.708
iteration 1600: loss: 3936.160
iteration 1700: loss: 3936.205
iteration 1800: loss: 3938.220
====> Epoch: 034 Train loss: 3938.6173  took : 53.18486785888672
====> Test loss: 3943.2546
iteration 0000: loss: 3939.500
iteration 0100: loss: 3940.676
iteration 0200: loss: 3937.921
iteration 0300: loss: 3934.661
iteration 0400: loss: 3931.983
iteration 0500: loss: 3935.324
iteration 0600: loss: 3938.285
iteration 0700: loss: 3939.085
iteration 0800: loss: 3941.528
iteration 0900: loss: 3936.175
iteration 1000: loss: 3935.641
iteration 1100: loss: 3935.474
iteration 1200: loss: 3934.071
iteration 1300: loss: 3936.392
iteration 1400: loss: 3940.426
iteration 1500: loss: 3935.993
iteration 1600: loss: 3933.358
iteration 1700: loss: 3942.530
iteration 1800: loss: 3936.724
====> Epoch: 035 Train loss: 3938.3465  took : 53.09997916221619
====> Test loss: 3942.8419
iteration 0000: loss: 3937.157
iteration 0100: loss: 3936.459
iteration 0200: loss: 3941.183
iteration 0300: loss: 3935.837
iteration 0400: loss: 3940.053
iteration 0500: loss: 3932.583
iteration 0600: loss: 3934.865
iteration 0700: loss: 3930.483
iteration 0800: loss: 3938.129
iteration 0900: loss: 3939.283
iteration 1000: loss: 3935.111
iteration 1100: loss: 3946.677
iteration 1200: loss: 3940.175
iteration 1300: loss: 3936.048
iteration 1400: loss: 3935.955
iteration 1500: loss: 3936.713
iteration 1600: loss: 3942.573
iteration 1700: loss: 3928.408
iteration 1800: loss: 3939.643
====> Epoch: 036 Train loss: 3938.2768  took : 53.103084325790405
====> Test loss: 3943.0960
iteration 0000: loss: 3933.158
iteration 0100: loss: 3940.912
iteration 0200: loss: 3932.012
iteration 0300: loss: 3937.159
iteration 0400: loss: 3936.174
iteration 0500: loss: 3936.912
iteration 0600: loss: 3936.460
iteration 0700: loss: 3944.359
iteration 0800: loss: 3938.441
iteration 0900: loss: 3941.125
iteration 1000: loss: 3942.748
iteration 1100: loss: 3937.205
iteration 1200: loss: 3938.255
iteration 1300: loss: 3934.911
iteration 1400: loss: 3933.536
iteration 1500: loss: 3935.001
iteration 1600: loss: 3936.571
iteration 1700: loss: 3934.996
iteration 1800: loss: 3939.151
====> Epoch: 037 Train loss: 3938.1180  took : 52.94116163253784
====> Test loss: 3942.7987
iteration 0000: loss: 3942.142
iteration 0100: loss: 3940.165
iteration 0200: loss: 3934.659
iteration 0300: loss: 3939.416
iteration 0400: loss: 3937.829
iteration 0500: loss: 3939.627
iteration 0600: loss: 3940.099
iteration 0700: loss: 3942.177
iteration 0800: loss: 3932.526
iteration 0900: loss: 3937.440
iteration 1000: loss: 3940.940
iteration 1100: loss: 3938.443
iteration 1200: loss: 3929.166
iteration 1300: loss: 3936.389
iteration 1400: loss: 3940.514
iteration 1500: loss: 3939.102
iteration 1600: loss: 3932.071
iteration 1700: loss: 3935.809
iteration 1800: loss: 3937.301
====> Epoch: 038 Train loss: 3938.0026  took : 53.13881301879883
====> Test loss: 3942.8882
iteration 0000: loss: 3940.042
iteration 0100: loss: 3941.612
iteration 0200: loss: 3939.670
iteration 0300: loss: 3934.089
iteration 0400: loss: 3934.699
iteration 0500: loss: 3938.603
iteration 0600: loss: 3938.064
iteration 0700: loss: 3943.834
iteration 0800: loss: 3943.738
iteration 0900: loss: 3937.826
iteration 1000: loss: 3933.466
iteration 1100: loss: 3940.124
iteration 1200: loss: 3932.715
iteration 1300: loss: 3936.172
iteration 1400: loss: 3947.331
iteration 1500: loss: 3936.169
iteration 1600: loss: 3946.345
iteration 1700: loss: 3935.033
iteration 1800: loss: 3931.080
====> Epoch: 039 Train loss: 3937.9095  took : 52.879554271698
====> Test loss: 3942.5954
iteration 0000: loss: 3936.828
iteration 0100: loss: 3941.921
iteration 0200: loss: 3934.116
iteration 0300: loss: 3938.997
iteration 0400: loss: 3939.992
iteration 0500: loss: 3934.177
iteration 0600: loss: 3936.725
iteration 0700: loss: 3937.517
iteration 0800: loss: 3938.157
iteration 0900: loss: 3934.504
iteration 1000: loss: 3951.921
iteration 1100: loss: 3939.078
iteration 1200: loss: 3938.111
iteration 1300: loss: 3941.327
iteration 1400: loss: 3936.878
iteration 1500: loss: 3935.398
iteration 1600: loss: 3938.746
iteration 1700: loss: 3935.335
iteration 1800: loss: 3934.403
====> Epoch: 040 Train loss: 3937.7658  took : 53.19224572181702
====> Test loss: 3942.0733
iteration 0000: loss: 3936.560
iteration 0100: loss: 3933.288
iteration 0200: loss: 3936.128
iteration 0300: loss: 3935.266
iteration 0400: loss: 3938.625
iteration 0500: loss: 3943.650
iteration 0600: loss: 3932.981
iteration 0700: loss: 3935.805
iteration 0800: loss: 3942.457
iteration 0900: loss: 3937.204
iteration 1000: loss: 3940.919
iteration 1100: loss: 3938.405
iteration 1200: loss: 3940.044
iteration 1300: loss: 3938.636
iteration 1400: loss: 3934.857
iteration 1500: loss: 3937.121
iteration 1600: loss: 3943.094
iteration 1700: loss: 3935.760
iteration 1800: loss: 3935.972
====> Epoch: 041 Train loss: 3937.9065  took : 53.34023904800415
====> Test loss: 3942.7184
iteration 0000: loss: 3935.328
iteration 0100: loss: 3934.456
iteration 0200: loss: 3939.568
iteration 0300: loss: 3935.154
iteration 0400: loss: 3939.686
iteration 0500: loss: 3937.584
iteration 0600: loss: 3936.259
iteration 0700: loss: 3941.351
iteration 0800: loss: 3935.833
iteration 0900: loss: 3932.292
iteration 1000: loss: 3935.497
iteration 1100: loss: 3938.417
iteration 1200: loss: 3940.504
iteration 1300: loss: 3939.666
iteration 1400: loss: 3937.296
iteration 1500: loss: 3936.966
iteration 1600: loss: 3932.969
iteration 1700: loss: 3936.487
iteration 1800: loss: 3941.535
====> Epoch: 042 Train loss: 3937.6568  took : 52.99492573738098
====> Test loss: 3941.9664
iteration 0000: loss: 3932.981
iteration 0100: loss: 3939.136
iteration 0200: loss: 3939.502
iteration 0300: loss: 3940.619
iteration 0400: loss: 3936.899
iteration 0500: loss: 3937.172
iteration 0600: loss: 3937.930
iteration 0700: loss: 3935.064
iteration 0800: loss: 3938.198
iteration 0900: loss: 3934.044
iteration 1000: loss: 3939.636
iteration 1100: loss: 3937.096
iteration 1200: loss: 3934.997
iteration 1300: loss: 3937.601
iteration 1400: loss: 3936.339
iteration 1500: loss: 3939.013
iteration 1600: loss: 3937.225
iteration 1700: loss: 3929.759
iteration 1800: loss: 3940.215
====> Epoch: 043 Train loss: 3937.7142  took : 53.03651213645935
====> Test loss: 3941.5098
iteration 0000: loss: 3943.620
iteration 0100: loss: 3938.303
iteration 0200: loss: 3940.502
iteration 0300: loss: 3930.427
iteration 0400: loss: 3933.456
iteration 0500: loss: 3932.624
iteration 0600: loss: 3937.747
iteration 0700: loss: 3943.169
iteration 0800: loss: 3935.948
iteration 0900: loss: 3937.517
iteration 1000: loss: 3935.851
iteration 1100: loss: 3943.480
iteration 1200: loss: 3939.202
iteration 1300: loss: 3932.434
iteration 1400: loss: 3941.872
iteration 1500: loss: 3940.199
iteration 1600: loss: 3936.518
iteration 1700: loss: 3937.700
iteration 1800: loss: 3931.980
====> Epoch: 044 Train loss: 3937.6737  took : 52.97263169288635
====> Test loss: 3942.8402
iteration 0000: loss: 3935.793
iteration 0100: loss: 3930.954
iteration 0200: loss: 3930.886
iteration 0300: loss: 3940.411
iteration 0400: loss: 3934.050
iteration 0500: loss: 3943.676
iteration 0600: loss: 3933.498
iteration 0700: loss: 3937.796
iteration 0800: loss: 3934.613
iteration 0900: loss: 3943.999
iteration 1000: loss: 3939.345
iteration 1100: loss: 3941.747
iteration 1200: loss: 3933.095
iteration 1300: loss: 3933.559
iteration 1400: loss: 3933.094
iteration 1500: loss: 3942.195
iteration 1600: loss: 3935.893
iteration 1700: loss: 3938.396
iteration 1800: loss: 3939.297
====> Epoch: 045 Train loss: 3937.7553  took : 53.25865411758423
====> Test loss: 3941.8597
iteration 0000: loss: 3931.685
iteration 0100: loss: 3938.491
iteration 0200: loss: 3938.540
iteration 0300: loss: 3935.726
iteration 0400: loss: 3937.196
iteration 0500: loss: 3937.761
iteration 0600: loss: 3933.016
iteration 0700: loss: 3933.079
iteration 0800: loss: 3941.128
iteration 0900: loss: 3938.136
iteration 1000: loss: 3940.570
iteration 1100: loss: 3937.531
iteration 1200: loss: 3935.436
iteration 1300: loss: 3943.919
iteration 1400: loss: 3930.160
iteration 1500: loss: 3938.835
iteration 1600: loss: 3937.843
iteration 1700: loss: 3934.743
iteration 1800: loss: 3942.608
====> Epoch: 046 Train loss: 3937.4328  took : 53.301756620407104
====> Test loss: 3941.6454
iteration 0000: loss: 3939.602
iteration 0100: loss: 3939.867
iteration 0200: loss: 3934.157
iteration 0300: loss: 3937.401
iteration 0400: loss: 3928.473
iteration 0500: loss: 3932.796
iteration 0600: loss: 3939.172
iteration 0700: loss: 3937.393
iteration 0800: loss: 3937.155
iteration 0900: loss: 3930.680
iteration 1000: loss: 3931.843
iteration 1100: loss: 3942.425
iteration 1200: loss: 3939.278
iteration 1300: loss: 3939.819
iteration 1400: loss: 3934.165
iteration 1500: loss: 3933.480
iteration 1600: loss: 3934.653
iteration 1700: loss: 3940.763
iteration 1800: loss: 3931.812
====> Epoch: 047 Train loss: 3937.3246  took : 53.27454090118408
====> Test loss: 3942.0759
iteration 0000: loss: 3939.040
iteration 0100: loss: 3934.029
iteration 0200: loss: 3941.523
iteration 0300: loss: 3937.219
iteration 0400: loss: 3936.695
iteration 0500: loss: 3934.403
iteration 0600: loss: 3936.374
iteration 0700: loss: 3937.034
iteration 0800: loss: 3932.311
iteration 0900: loss: 3941.740
iteration 1000: loss: 3934.552
iteration 1100: loss: 3935.116
iteration 1200: loss: 3936.569
iteration 1300: loss: 3940.030
iteration 1400: loss: 3934.421
iteration 1500: loss: 3939.110
iteration 1600: loss: 3937.100
iteration 1700: loss: 3934.666
iteration 1800: loss: 3936.754
====> Epoch: 048 Train loss: 3937.5005  took : 53.095014333724976
====> Test loss: 3941.7622
iteration 0000: loss: 3935.091
iteration 0100: loss: 3938.411
iteration 0200: loss: 3937.118
iteration 0300: loss: 3932.903
iteration 0400: loss: 3941.531
iteration 0500: loss: 3938.690
iteration 0600: loss: 3938.560
iteration 0700: loss: 3934.437
iteration 0800: loss: 3936.731
iteration 0900: loss: 3936.605
iteration 1000: loss: 3938.523
iteration 1100: loss: 3941.415
iteration 1200: loss: 3936.665
iteration 1300: loss: 3937.578
iteration 1400: loss: 3938.481
iteration 1500: loss: 3938.907
iteration 1600: loss: 3943.813
iteration 1700: loss: 3934.660
iteration 1800: loss: 3940.596
====> Epoch: 049 Train loss: 3937.6950  took : 53.557114601135254
====> Test loss: 3942.0026
iteration 0000: loss: 3931.018
iteration 0100: loss: 3937.535
iteration 0200: loss: 3933.542
iteration 0300: loss: 3936.376
iteration 0400: loss: 3936.738
iteration 0500: loss: 3935.788
iteration 0600: loss: 3941.951
iteration 0700: loss: 3934.418
iteration 0800: loss: 3936.290
iteration 0900: loss: 3935.718
iteration 1000: loss: 3932.205
iteration 1100: loss: 3933.056
iteration 1200: loss: 3937.178
iteration 1300: loss: 3934.115
iteration 1400: loss: 3937.502
iteration 1500: loss: 3937.092
iteration 1600: loss: 3936.674
iteration 1700: loss: 3937.269
iteration 1800: loss: 3932.794
====> Epoch: 050 Train loss: 3937.5351  took : 53.04585552215576
====> Test loss: 3941.8527
====> [MM-VAE] Time: 3183.381s or 00:53:03
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  8
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_8
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_8
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1989.572
iteration 0100: loss: 1577.277
iteration 0200: loss: 1567.468
iteration 0300: loss: 1552.168
iteration 0400: loss: 1540.914
iteration 0500: loss: 1544.622
iteration 0600: loss: 1545.673
iteration 0700: loss: 1539.999
iteration 0800: loss: 1538.569
iteration 0900: loss: 1536.458
====> Epoch: 001 Train loss: 1553.7325  took : 8.589348077774048
====> Test loss: 1537.0750
iteration 0000: loss: 1541.000
iteration 0100: loss: 1531.554
iteration 0200: loss: 1532.850
iteration 0300: loss: 1532.375
iteration 0400: loss: 1531.188
iteration 0500: loss: 1529.553
iteration 0600: loss: 1533.641
iteration 0700: loss: 1524.277
iteration 0800: loss: 1526.850
iteration 0900: loss: 1527.648
====> Epoch: 002 Train loss: 1530.7152  took : 8.524163961410522
====> Test loss: 1529.2834
iteration 0000: loss: 1528.409
iteration 0100: loss: 1524.827
iteration 0200: loss: 1529.072
iteration 0300: loss: 1529.119
iteration 0400: loss: 1527.873
iteration 0500: loss: 1529.568
iteration 0600: loss: 1526.611
iteration 0700: loss: 1525.697
iteration 0800: loss: 1525.903
iteration 0900: loss: 1523.131
====> Epoch: 003 Train loss: 1526.1012  took : 8.567819595336914
====> Test loss: 1526.9855
iteration 0000: loss: 1525.585
iteration 0100: loss: 1526.581
iteration 0200: loss: 1521.922
iteration 0300: loss: 1523.704
iteration 0400: loss: 1529.188
iteration 0500: loss: 1524.243
iteration 0600: loss: 1525.266
iteration 0700: loss: 1520.107
iteration 0800: loss: 1519.346
iteration 0900: loss: 1521.651
====> Epoch: 004 Train loss: 1524.0772  took : 8.595037937164307
====> Test loss: 1525.2185
iteration 0000: loss: 1524.590
iteration 0100: loss: 1526.066
iteration 0200: loss: 1523.755
iteration 0300: loss: 1520.222
iteration 0400: loss: 1522.968
iteration 0500: loss: 1520.719
iteration 0600: loss: 1521.420
iteration 0700: loss: 1522.313
iteration 0800: loss: 1522.949
iteration 0900: loss: 1525.152
====> Epoch: 005 Train loss: 1522.5040  took : 8.444756984710693
====> Test loss: 1523.9153
iteration 0000: loss: 1523.873
iteration 0100: loss: 1524.959
iteration 0200: loss: 1520.342
iteration 0300: loss: 1522.691
iteration 0400: loss: 1520.818
iteration 0500: loss: 1520.549
iteration 0600: loss: 1519.289
iteration 0700: loss: 1520.166
iteration 0800: loss: 1523.030
iteration 0900: loss: 1523.677
====> Epoch: 006 Train loss: 1521.4430  took : 8.540085792541504
====> Test loss: 1523.4086
iteration 0000: loss: 1520.287
iteration 0100: loss: 1523.783
iteration 0200: loss: 1520.073
iteration 0300: loss: 1523.316
iteration 0400: loss: 1519.673
iteration 0500: loss: 1520.957
iteration 0600: loss: 1518.844
iteration 0700: loss: 1522.577
iteration 0800: loss: 1516.732
iteration 0900: loss: 1519.029
====> Epoch: 007 Train loss: 1520.6463  took : 8.540621995925903
====> Test loss: 1522.7270
iteration 0000: loss: 1516.776
iteration 0100: loss: 1519.233
iteration 0200: loss: 1519.211
iteration 0300: loss: 1517.063
iteration 0400: loss: 1517.522
iteration 0500: loss: 1519.844
iteration 0600: loss: 1522.892
iteration 0700: loss: 1518.406
iteration 0800: loss: 1522.049
iteration 0900: loss: 1520.615
====> Epoch: 008 Train loss: 1520.0166  took : 8.486747980117798
====> Test loss: 1522.1509
iteration 0000: loss: 1516.561
iteration 0100: loss: 1520.942
iteration 0200: loss: 1520.705
iteration 0300: loss: 1520.682
iteration 0400: loss: 1522.234
iteration 0500: loss: 1518.240
iteration 0600: loss: 1518.640
iteration 0700: loss: 1519.925
iteration 0800: loss: 1520.437
iteration 0900: loss: 1518.865
====> Epoch: 009 Train loss: 1519.4799  took : 8.570440292358398
====> Test loss: 1521.8814
iteration 0000: loss: 1516.049
iteration 0100: loss: 1521.368
iteration 0200: loss: 1520.339
iteration 0300: loss: 1518.958
iteration 0400: loss: 1520.304
iteration 0500: loss: 1519.723
iteration 0600: loss: 1518.959
iteration 0700: loss: 1519.683
iteration 0800: loss: 1520.522
iteration 0900: loss: 1517.729
====> Epoch: 010 Train loss: 1519.0421  took : 8.526664972305298
====> Test loss: 1521.4780
iteration 0000: loss: 1524.003
iteration 0100: loss: 1518.577
iteration 0200: loss: 1518.119
iteration 0300: loss: 1518.841
iteration 0400: loss: 1518.075
iteration 0500: loss: 1519.451
iteration 0600: loss: 1516.087
iteration 0700: loss: 1515.449
iteration 0800: loss: 1517.094
iteration 0900: loss: 1523.034
====> Epoch: 011 Train loss: 1518.6452  took : 8.590664386749268
====> Test loss: 1521.2048
iteration 0000: loss: 1519.805
iteration 0100: loss: 1516.755
iteration 0200: loss: 1520.026
iteration 0300: loss: 1518.096
iteration 0400: loss: 1518.537
iteration 0500: loss: 1517.153
iteration 0600: loss: 1519.193
iteration 0700: loss: 1517.247
iteration 0800: loss: 1515.759
iteration 0900: loss: 1519.396
====> Epoch: 012 Train loss: 1518.3289  took : 8.552884578704834
====> Test loss: 1520.9162
iteration 0000: loss: 1516.999
iteration 0100: loss: 1518.009
iteration 0200: loss: 1515.451
iteration 0300: loss: 1518.200
iteration 0400: loss: 1519.066
iteration 0500: loss: 1520.264
iteration 0600: loss: 1517.535
iteration 0700: loss: 1517.725
iteration 0800: loss: 1520.738
iteration 0900: loss: 1518.892
====> Epoch: 013 Train loss: 1518.0070  took : 8.536778450012207
====> Test loss: 1520.5492
iteration 0000: loss: 1516.868
iteration 0100: loss: 1518.153
iteration 0200: loss: 1518.624
iteration 0300: loss: 1518.773
iteration 0400: loss: 1517.066
iteration 0500: loss: 1518.148
iteration 0600: loss: 1518.746
iteration 0700: loss: 1515.905
iteration 0800: loss: 1519.081
iteration 0900: loss: 1516.277
====> Epoch: 014 Train loss: 1517.6947  took : 8.5387442111969
====> Test loss: 1520.2505
iteration 0000: loss: 1517.074
iteration 0100: loss: 1516.150
iteration 0200: loss: 1518.564
iteration 0300: loss: 1518.016
iteration 0400: loss: 1519.672
iteration 0500: loss: 1522.670
iteration 0600: loss: 1517.078
iteration 0700: loss: 1517.125
iteration 0800: loss: 1518.845
iteration 0900: loss: 1516.026
====> Epoch: 015 Train loss: 1517.4806  took : 8.512906789779663
====> Test loss: 1520.2720
iteration 0000: loss: 1515.122
iteration 0100: loss: 1519.183
iteration 0200: loss: 1517.186
iteration 0300: loss: 1519.435
iteration 0400: loss: 1520.454
iteration 0500: loss: 1518.076
iteration 0600: loss: 1517.456
iteration 0700: loss: 1517.473
iteration 0800: loss: 1516.849
iteration 0900: loss: 1514.596
====> Epoch: 016 Train loss: 1517.2716  took : 8.568203687667847
====> Test loss: 1520.1378
iteration 0000: loss: 1518.071
iteration 0100: loss: 1515.416
iteration 0200: loss: 1517.943
iteration 0300: loss: 1514.505
iteration 0400: loss: 1515.290
iteration 0500: loss: 1517.750
iteration 0600: loss: 1516.482
iteration 0700: loss: 1518.529
iteration 0800: loss: 1519.141
iteration 0900: loss: 1518.558
====> Epoch: 017 Train loss: 1517.0786  took : 8.643850564956665
====> Test loss: 1519.9815
iteration 0000: loss: 1515.924
iteration 0100: loss: 1515.989
iteration 0200: loss: 1519.702
iteration 0300: loss: 1518.245
iteration 0400: loss: 1517.003
iteration 0500: loss: 1513.754
iteration 0600: loss: 1518.877
iteration 0700: loss: 1516.749
iteration 0800: loss: 1518.705
iteration 0900: loss: 1518.002
====> Epoch: 018 Train loss: 1516.9057  took : 8.478926181793213
====> Test loss: 1519.7454
iteration 0000: loss: 1513.861
iteration 0100: loss: 1520.385
iteration 0200: loss: 1516.983
iteration 0300: loss: 1516.957
iteration 0400: loss: 1519.283
iteration 0500: loss: 1514.209
iteration 0600: loss: 1516.978
iteration 0700: loss: 1516.795
iteration 0800: loss: 1517.484
iteration 0900: loss: 1518.102
====> Epoch: 019 Train loss: 1516.7525  took : 8.600852489471436
====> Test loss: 1519.7096
iteration 0000: loss: 1515.524
iteration 0100: loss: 1514.823
iteration 0200: loss: 1518.913
iteration 0300: loss: 1519.076
iteration 0400: loss: 1515.342
iteration 0500: loss: 1519.286
iteration 0600: loss: 1518.255
iteration 0700: loss: 1516.683
iteration 0800: loss: 1516.384
iteration 0900: loss: 1518.517
====> Epoch: 020 Train loss: 1516.6228  took : 8.545316219329834
====> Test loss: 1519.6147
iteration 0000: loss: 1515.570
iteration 0100: loss: 1518.406
iteration 0200: loss: 1517.966
iteration 0300: loss: 1514.365
iteration 0400: loss: 1516.406
iteration 0500: loss: 1517.811
iteration 0600: loss: 1518.086
iteration 0700: loss: 1517.661
iteration 0800: loss: 1515.733
iteration 0900: loss: 1515.957
====> Epoch: 021 Train loss: 1516.4960  took : 8.51404857635498
====> Test loss: 1519.6337
iteration 0000: loss: 1515.234
iteration 0100: loss: 1517.170
iteration 0200: loss: 1518.550
iteration 0300: loss: 1516.560
iteration 0400: loss: 1518.002
iteration 0500: loss: 1516.795
iteration 0600: loss: 1516.419
iteration 0700: loss: 1515.575
iteration 0800: loss: 1518.663
iteration 0900: loss: 1516.500
====> Epoch: 022 Train loss: 1516.3827  took : 8.433326244354248
====> Test loss: 1519.3734
iteration 0000: loss: 1516.170
iteration 0100: loss: 1517.254
iteration 0200: loss: 1516.296
iteration 0300: loss: 1518.044
iteration 0400: loss: 1514.016
iteration 0500: loss: 1514.618
iteration 0600: loss: 1516.777
iteration 0700: loss: 1517.251
iteration 0800: loss: 1515.785
iteration 0900: loss: 1515.207
====> Epoch: 023 Train loss: 1516.2801  took : 8.623311281204224
====> Test loss: 1519.2635
iteration 0000: loss: 1516.925
iteration 0100: loss: 1517.055
iteration 0200: loss: 1516.613
iteration 0300: loss: 1513.456
iteration 0400: loss: 1519.438
iteration 0500: loss: 1514.880
iteration 0600: loss: 1518.146
iteration 0700: loss: 1515.590
iteration 0800: loss: 1515.138
iteration 0900: loss: 1515.605
====> Epoch: 024 Train loss: 1516.1683  took : 8.607395887374878
====> Test loss: 1519.1898
iteration 0000: loss: 1514.647
iteration 0100: loss: 1512.862
iteration 0200: loss: 1514.704
iteration 0300: loss: 1515.083
iteration 0400: loss: 1517.215
iteration 0500: loss: 1515.627
iteration 0600: loss: 1517.460
iteration 0700: loss: 1513.954
iteration 0800: loss: 1516.919
iteration 0900: loss: 1515.292
====> Epoch: 025 Train loss: 1516.0660  took : 8.609284162521362
====> Test loss: 1519.0897
iteration 0000: loss: 1513.980
iteration 0100: loss: 1515.307
iteration 0200: loss: 1517.213
iteration 0300: loss: 1515.349
iteration 0400: loss: 1513.470
iteration 0500: loss: 1518.128
iteration 0600: loss: 1516.961
iteration 0700: loss: 1513.887
iteration 0800: loss: 1512.588
iteration 0900: loss: 1517.017
====> Epoch: 026 Train loss: 1516.0025  took : 8.610396146774292
====> Test loss: 1519.1541
iteration 0000: loss: 1517.670
iteration 0100: loss: 1514.129
iteration 0200: loss: 1517.417
iteration 0300: loss: 1515.621
iteration 0400: loss: 1517.188
iteration 0500: loss: 1516.653
iteration 0600: loss: 1514.344
iteration 0700: loss: 1516.327
iteration 0800: loss: 1513.969
iteration 0900: loss: 1514.288
====> Epoch: 027 Train loss: 1515.9157  took : 8.604218482971191
====> Test loss: 1519.0098
iteration 0000: loss: 1516.860
iteration 0100: loss: 1515.449
iteration 0200: loss: 1514.314
iteration 0300: loss: 1517.916
iteration 0400: loss: 1515.477
iteration 0500: loss: 1516.294
iteration 0600: loss: 1515.268
iteration 0700: loss: 1517.708
iteration 0800: loss: 1517.222
iteration 0900: loss: 1512.748
====> Epoch: 028 Train loss: 1515.8618  took : 8.542715072631836
====> Test loss: 1519.1544
iteration 0000: loss: 1515.142
iteration 0100: loss: 1517.332
iteration 0200: loss: 1515.192
iteration 0300: loss: 1517.326
iteration 0400: loss: 1514.173
iteration 0500: loss: 1513.531
iteration 0600: loss: 1517.624
iteration 0700: loss: 1514.925
iteration 0800: loss: 1516.961
iteration 0900: loss: 1516.444
====> Epoch: 029 Train loss: 1515.7419  took : 8.563034296035767
====> Test loss: 1518.8708
iteration 0000: loss: 1517.578
iteration 0100: loss: 1516.048
iteration 0200: loss: 1516.588
iteration 0300: loss: 1516.972
iteration 0400: loss: 1519.033
iteration 0500: loss: 1514.243
iteration 0600: loss: 1516.255
iteration 0700: loss: 1516.182
iteration 0800: loss: 1516.786
iteration 0900: loss: 1516.878
====> Epoch: 030 Train loss: 1515.6719  took : 8.531190395355225
====> Test loss: 1518.9107
iteration 0000: loss: 1512.690
iteration 0100: loss: 1514.835
iteration 0200: loss: 1513.262
iteration 0300: loss: 1515.399
iteration 0400: loss: 1515.711
iteration 0500: loss: 1513.635
iteration 0600: loss: 1515.880
iteration 0700: loss: 1516.155
iteration 0800: loss: 1515.949
iteration 0900: loss: 1517.654
====> Epoch: 031 Train loss: 1515.6187  took : 8.509058237075806
====> Test loss: 1518.7479
iteration 0000: loss: 1515.493
iteration 0100: loss: 1518.352
iteration 0200: loss: 1516.605
iteration 0300: loss: 1516.033
iteration 0400: loss: 1513.875
iteration 0500: loss: 1515.009
iteration 0600: loss: 1516.607
iteration 0700: loss: 1516.676
iteration 0800: loss: 1516.098
iteration 0900: loss: 1517.100
====> Epoch: 032 Train loss: 1515.5452  took : 8.60221242904663
====> Test loss: 1518.8367
iteration 0000: loss: 1515.963
iteration 0100: loss: 1517.291
iteration 0200: loss: 1516.318
iteration 0300: loss: 1516.688
iteration 0400: loss: 1514.743
iteration 0500: loss: 1515.769
iteration 0600: loss: 1514.179
iteration 0700: loss: 1515.231
iteration 0800: loss: 1519.476
iteration 0900: loss: 1517.149
====> Epoch: 033 Train loss: 1515.4758  took : 8.570810794830322
====> Test loss: 1518.7859
iteration 0000: loss: 1515.120
iteration 0100: loss: 1516.463
iteration 0200: loss: 1515.611
iteration 0300: loss: 1513.029
iteration 0400: loss: 1514.615
iteration 0500: loss: 1513.516
iteration 0600: loss: 1517.613
iteration 0700: loss: 1519.821
iteration 0800: loss: 1515.715
iteration 0900: loss: 1517.738
====> Epoch: 034 Train loss: 1515.4724  took : 8.606189250946045
====> Test loss: 1518.7665
iteration 0000: loss: 1517.342
iteration 0100: loss: 1514.252
iteration 0200: loss: 1515.222
iteration 0300: loss: 1516.331
iteration 0400: loss: 1512.839
iteration 0500: loss: 1515.808
iteration 0600: loss: 1513.960
iteration 0700: loss: 1514.341
iteration 0800: loss: 1515.564
iteration 0900: loss: 1515.889
====> Epoch: 035 Train loss: 1515.3759  took : 8.58938217163086
====> Test loss: 1519.0817
iteration 0000: loss: 1516.509
iteration 0100: loss: 1516.254
iteration 0200: loss: 1515.783
iteration 0300: loss: 1514.620
iteration 0400: loss: 1515.182
iteration 0500: loss: 1513.175
iteration 0600: loss: 1516.949
iteration 0700: loss: 1514.121
iteration 0800: loss: 1514.250
iteration 0900: loss: 1517.645
====> Epoch: 036 Train loss: 1515.3044  took : 8.53660774230957
====> Test loss: 1518.6030
iteration 0000: loss: 1516.923
iteration 0100: loss: 1514.363
iteration 0200: loss: 1516.132
iteration 0300: loss: 1517.032
iteration 0400: loss: 1513.248
iteration 0500: loss: 1516.112
iteration 0600: loss: 1518.711
iteration 0700: loss: 1514.657
iteration 0800: loss: 1513.893
iteration 0900: loss: 1517.136
====> Epoch: 037 Train loss: 1515.2073  took : 8.667166948318481
====> Test loss: 1518.5290
iteration 0000: loss: 1516.594
iteration 0100: loss: 1515.769
iteration 0200: loss: 1514.613
iteration 0300: loss: 1514.407
iteration 0400: loss: 1516.584
iteration 0500: loss: 1514.956
iteration 0600: loss: 1514.098
iteration 0700: loss: 1516.774
iteration 0800: loss: 1515.619
iteration 0900: loss: 1512.829
====> Epoch: 038 Train loss: 1515.1389  took : 8.60784363746643
====> Test loss: 1518.5341
iteration 0000: loss: 1512.950
iteration 0100: loss: 1514.039
iteration 0200: loss: 1515.711
iteration 0300: loss: 1514.522
iteration 0400: loss: 1517.807
iteration 0500: loss: 1512.416
iteration 0600: loss: 1515.339
iteration 0700: loss: 1516.471
iteration 0800: loss: 1517.180
iteration 0900: loss: 1516.176
====> Epoch: 039 Train loss: 1515.0945  took : 8.516555070877075
====> Test loss: 1518.4329
iteration 0000: loss: 1516.019
iteration 0100: loss: 1516.139
iteration 0200: loss: 1513.594
iteration 0300: loss: 1516.355
iteration 0400: loss: 1515.649
iteration 0500: loss: 1515.651
iteration 0600: loss: 1515.092
iteration 0700: loss: 1515.294
iteration 0800: loss: 1513.817
iteration 0900: loss: 1511.923
====> Epoch: 040 Train loss: 1515.0637  took : 8.50877070426941
====> Test loss: 1518.4672
iteration 0000: loss: 1516.047
iteration 0100: loss: 1516.936
iteration 0200: loss: 1514.474
iteration 0300: loss: 1513.463
iteration 0400: loss: 1512.688
iteration 0500: loss: 1516.272
iteration 0600: loss: 1514.055
iteration 0700: loss: 1511.911
iteration 0800: loss: 1515.245
iteration 0900: loss: 1512.424
====> Epoch: 041 Train loss: 1515.0033  took : 8.593666791915894
====> Test loss: 1518.4798
iteration 0000: loss: 1514.082
iteration 0100: loss: 1515.575
iteration 0200: loss: 1515.697
iteration 0300: loss: 1513.421
iteration 0400: loss: 1514.059
iteration 0500: loss: 1516.221
iteration 0600: loss: 1515.013
iteration 0700: loss: 1515.059
iteration 0800: loss: 1518.063
iteration 0900: loss: 1513.627
====> Epoch: 042 Train loss: 1514.9435  took : 8.526055335998535
====> Test loss: 1518.3483
iteration 0000: loss: 1514.116
iteration 0100: loss: 1515.811
iteration 0200: loss: 1513.342
iteration 0300: loss: 1515.550
iteration 0400: loss: 1513.381
iteration 0500: loss: 1515.722
iteration 0600: loss: 1514.016
iteration 0700: loss: 1513.422
iteration 0800: loss: 1516.192
iteration 0900: loss: 1515.663
====> Epoch: 043 Train loss: 1514.9316  took : 8.537828207015991
====> Test loss: 1518.5430
iteration 0000: loss: 1515.490
iteration 0100: loss: 1516.551
iteration 0200: loss: 1512.938
iteration 0300: loss: 1517.829
iteration 0400: loss: 1514.789
iteration 0500: loss: 1514.555
iteration 0600: loss: 1513.237
iteration 0700: loss: 1513.733
iteration 0800: loss: 1513.912
iteration 0900: loss: 1514.203
====> Epoch: 044 Train loss: 1514.9296  took : 8.608936548233032
====> Test loss: 1518.5335
iteration 0000: loss: 1515.612
iteration 0100: loss: 1515.174
iteration 0200: loss: 1512.997
iteration 0300: loss: 1512.852
iteration 0400: loss: 1518.047
iteration 0500: loss: 1511.970
iteration 0600: loss: 1514.208
iteration 0700: loss: 1515.149
iteration 0800: loss: 1514.980
iteration 0900: loss: 1516.713
====> Epoch: 045 Train loss: 1514.8634  took : 8.597342252731323
====> Test loss: 1518.5354
iteration 0000: loss: 1515.615
iteration 0100: loss: 1515.049
iteration 0200: loss: 1513.227
iteration 0300: loss: 1514.925
iteration 0400: loss: 1513.725
iteration 0500: loss: 1514.965
iteration 0600: loss: 1517.521
iteration 0700: loss: 1515.277
iteration 0800: loss: 1515.730
iteration 0900: loss: 1514.719
====> Epoch: 046 Train loss: 1514.8885  took : 8.605836868286133
====> Test loss: 1518.5372
iteration 0000: loss: 1513.636
iteration 0100: loss: 1514.125
iteration 0200: loss: 1515.846
iteration 0300: loss: 1513.226
iteration 0400: loss: 1512.694
iteration 0500: loss: 1513.932
iteration 0600: loss: 1516.319
iteration 0700: loss: 1513.211
iteration 0800: loss: 1516.573
iteration 0900: loss: 1515.758
====> Epoch: 047 Train loss: 1514.8251  took : 8.541070222854614
====> Test loss: 1518.4216
iteration 0000: loss: 1513.398
iteration 0100: loss: 1514.630
iteration 0200: loss: 1515.874
iteration 0300: loss: 1515.833
iteration 0400: loss: 1517.414
iteration 0500: loss: 1516.195
iteration 0600: loss: 1517.003
iteration 0700: loss: 1515.960
iteration 0800: loss: 1515.169
iteration 0900: loss: 1517.358
====> Epoch: 048 Train loss: 1514.7226  took : 8.51477336883545
====> Test loss: 1518.4460
iteration 0000: loss: 1515.265
iteration 0100: loss: 1516.153
iteration 0200: loss: 1514.952
iteration 0300: loss: 1515.475
iteration 0400: loss: 1513.601
iteration 0500: loss: 1514.558
iteration 0600: loss: 1516.240
iteration 0700: loss: 1513.545
iteration 0800: loss: 1513.453
iteration 0900: loss: 1512.864
====> Epoch: 049 Train loss: 1514.7140  took : 8.529534578323364
====> Test loss: 1518.4814
iteration 0000: loss: 1514.856
iteration 0100: loss: 1513.479
iteration 0200: loss: 1513.418
iteration 0300: loss: 1515.514
iteration 0400: loss: 1516.838
iteration 0500: loss: 1514.543
iteration 0600: loss: 1515.429
iteration 0700: loss: 1515.217
iteration 0800: loss: 1514.232
iteration 0900: loss: 1518.554
====> Epoch: 050 Train loss: 1514.6834  took : 8.520473718643188
====> Test loss: 1518.3814
====> [MM-VAE] Time: 511.504s or 00:08:31
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  8
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_8
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_8
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.988
iteration 0100: loss: 2078.398
iteration 0200: loss: 2043.421
iteration 0300: loss: 2019.037
iteration 0400: loss: 2006.303
iteration 0500: loss: 2004.876
iteration 0600: loss: 1990.702
iteration 0700: loss: 1993.060
iteration 0800: loss: 1993.117
iteration 0900: loss: 1986.773
====> Epoch: 001 Train loss: 2022.9910  took : 12.797495365142822
====> Test loss: 1990.7448
iteration 0000: loss: 1994.138
iteration 0100: loss: 1986.607
iteration 0200: loss: 1983.159
iteration 0300: loss: 1984.006
iteration 0400: loss: 1980.668
iteration 0500: loss: 1978.879
iteration 0600: loss: 1978.999
iteration 0700: loss: 1978.724
iteration 0800: loss: 1972.648
iteration 0900: loss: 1974.814
====> Epoch: 002 Train loss: 1979.0508  took : 12.048494577407837
====> Test loss: 1976.5711
iteration 0000: loss: 1977.813
iteration 0100: loss: 1968.366
iteration 0200: loss: 1966.531
iteration 0300: loss: 1969.911
iteration 0400: loss: 1969.662
iteration 0500: loss: 1968.716
iteration 0600: loss: 1967.344
iteration 0700: loss: 1974.138
iteration 0800: loss: 1965.651
iteration 0900: loss: 1969.368
====> Epoch: 003 Train loss: 1969.3809  took : 13.140295028686523
====> Test loss: 1969.4939
iteration 0000: loss: 1972.883
iteration 0100: loss: 1964.264
iteration 0200: loss: 1968.493
iteration 0300: loss: 1963.951
iteration 0400: loss: 1965.153
iteration 0500: loss: 1962.882
iteration 0600: loss: 1965.493
iteration 0700: loss: 1962.702
iteration 0800: loss: 1964.643
iteration 0900: loss: 1962.338
====> Epoch: 004 Train loss: 1964.4651  took : 12.054208755493164
====> Test loss: 1966.7371
iteration 0000: loss: 1966.855
iteration 0100: loss: 1962.292
iteration 0200: loss: 1963.436
iteration 0300: loss: 1960.655
iteration 0400: loss: 1963.747
iteration 0500: loss: 1968.505
iteration 0600: loss: 1962.393
iteration 0700: loss: 1962.163
iteration 0800: loss: 1963.320
iteration 0900: loss: 1959.027
====> Epoch: 005 Train loss: 1962.0531  took : 13.453431844711304
====> Test loss: 1964.6917
iteration 0000: loss: 1961.999
iteration 0100: loss: 1961.414
iteration 0200: loss: 1961.028
iteration 0300: loss: 1961.124
iteration 0400: loss: 1965.716
iteration 0500: loss: 1964.028
iteration 0600: loss: 1966.035
iteration 0700: loss: 1959.958
iteration 0800: loss: 1961.019
iteration 0900: loss: 1963.351
====> Epoch: 006 Train loss: 1960.7689  took : 12.689990043640137
====> Test loss: 1963.1193
iteration 0000: loss: 1960.722
iteration 0100: loss: 1958.558
iteration 0200: loss: 1961.568
iteration 0300: loss: 1956.882
iteration 0400: loss: 1957.328
iteration 0500: loss: 1955.359
iteration 0600: loss: 1958.320
iteration 0700: loss: 1954.609
iteration 0800: loss: 1958.533
iteration 0900: loss: 1958.557
====> Epoch: 007 Train loss: 1959.1396  took : 13.715122699737549
====> Test loss: 1960.6514
iteration 0000: loss: 1953.643
iteration 0100: loss: 1960.453
iteration 0200: loss: 1958.938
iteration 0300: loss: 1956.326
iteration 0400: loss: 1957.440
iteration 0500: loss: 1960.210
iteration 0600: loss: 1954.197
iteration 0700: loss: 1959.804
iteration 0800: loss: 1954.682
iteration 0900: loss: 1955.439
====> Epoch: 008 Train loss: 1957.8759  took : 13.32713770866394
====> Test loss: 1958.9401
iteration 0000: loss: 1959.609
iteration 0100: loss: 1958.130
iteration 0200: loss: 1956.948
iteration 0300: loss: 1953.793
iteration 0400: loss: 1955.443
iteration 0500: loss: 1954.108
iteration 0600: loss: 1956.694
iteration 0700: loss: 1958.887
iteration 0800: loss: 1952.931
iteration 0900: loss: 1956.975
====> Epoch: 009 Train loss: 1955.9270  took : 12.902765989303589
====> Test loss: 1957.3869
iteration 0000: loss: 1953.370
iteration 0100: loss: 1953.611
iteration 0200: loss: 1955.058
iteration 0300: loss: 1954.615
iteration 0400: loss: 1953.619
iteration 0500: loss: 1953.985
iteration 0600: loss: 1957.676
iteration 0700: loss: 1953.299
iteration 0800: loss: 1951.691
iteration 0900: loss: 1952.086
====> Epoch: 010 Train loss: 1954.1342  took : 13.39118218421936
====> Test loss: 1955.8560
iteration 0000: loss: 1954.058
iteration 0100: loss: 1957.155
iteration 0200: loss: 1951.649
iteration 0300: loss: 1952.434
iteration 0400: loss: 1952.672
iteration 0500: loss: 1953.424
iteration 0600: loss: 1951.718
iteration 0700: loss: 1951.794
iteration 0800: loss: 1950.261
iteration 0900: loss: 1952.929
====> Epoch: 011 Train loss: 1952.8516  took : 12.972835779190063
====> Test loss: 1954.3020
iteration 0000: loss: 1951.842
iteration 0100: loss: 1952.591
iteration 0200: loss: 1951.060
iteration 0300: loss: 1952.703
iteration 0400: loss: 1952.855
iteration 0500: loss: 1953.257
iteration 0600: loss: 1951.753
iteration 0700: loss: 1952.187
iteration 0800: loss: 1951.637
iteration 0900: loss: 1952.272
====> Epoch: 012 Train loss: 1952.4535  took : 12.843124628067017
====> Test loss: 1953.8760
iteration 0000: loss: 1953.516
iteration 0100: loss: 1953.708
iteration 0200: loss: 1950.787
iteration 0300: loss: 1949.843
iteration 0400: loss: 1952.502
iteration 0500: loss: 1950.156
iteration 0600: loss: 1953.872
iteration 0700: loss: 1952.658
iteration 0800: loss: 1950.001
iteration 0900: loss: 1951.345
====> Epoch: 013 Train loss: 1951.6765  took : 12.958290100097656
====> Test loss: 1953.0159
iteration 0000: loss: 1949.604
iteration 0100: loss: 1952.185
iteration 0200: loss: 1950.965
iteration 0300: loss: 1952.737
iteration 0400: loss: 1949.547
iteration 0500: loss: 1949.892
iteration 0600: loss: 1950.277
iteration 0700: loss: 1950.081
iteration 0800: loss: 1950.678
iteration 0900: loss: 1950.926
====> Epoch: 014 Train loss: 1950.9996  took : 13.277150392532349
====> Test loss: 1952.6090
iteration 0000: loss: 1949.493
iteration 0100: loss: 1949.541
iteration 0200: loss: 1951.773
iteration 0300: loss: 1949.633
iteration 0400: loss: 1951.384
iteration 0500: loss: 1949.840
iteration 0600: loss: 1950.782
iteration 0700: loss: 1950.526
iteration 0800: loss: 1949.273
iteration 0900: loss: 1950.499
====> Epoch: 015 Train loss: 1950.4403  took : 13.347346544265747
====> Test loss: 1952.8242
iteration 0000: loss: 1953.548
iteration 0100: loss: 1950.262
iteration 0200: loss: 1950.798
iteration 0300: loss: 1950.161
iteration 0400: loss: 1949.627
iteration 0500: loss: 1949.845
iteration 0600: loss: 1948.579
iteration 0700: loss: 1952.201
iteration 0800: loss: 1948.899
iteration 0900: loss: 1951.950
====> Epoch: 016 Train loss: 1950.1850  took : 12.799299955368042
====> Test loss: 1951.3570
iteration 0000: loss: 1948.793
iteration 0100: loss: 1949.576
iteration 0200: loss: 1950.045
iteration 0300: loss: 1948.080
iteration 0400: loss: 1949.171
iteration 0500: loss: 1950.229
iteration 0600: loss: 1948.769
iteration 0700: loss: 1950.069
iteration 0800: loss: 1951.539
iteration 0900: loss: 1948.736
====> Epoch: 017 Train loss: 1949.8498  took : 12.798975706100464
====> Test loss: 1951.4874
iteration 0000: loss: 1949.358
iteration 0100: loss: 1949.676
iteration 0200: loss: 1950.314
iteration 0300: loss: 1949.387
iteration 0400: loss: 1947.464
iteration 0500: loss: 1952.687
iteration 0600: loss: 1949.249
iteration 0700: loss: 1948.822
iteration 0800: loss: 1950.219
iteration 0900: loss: 1950.577
====> Epoch: 018 Train loss: 1949.5579  took : 12.246417999267578
====> Test loss: 1951.7711
iteration 0000: loss: 1950.099
iteration 0100: loss: 1950.635
iteration 0200: loss: 1949.073
iteration 0300: loss: 1950.330
iteration 0400: loss: 1949.072
iteration 0500: loss: 1948.809
iteration 0600: loss: 1948.808
iteration 0700: loss: 1949.612
iteration 0800: loss: 1950.057
iteration 0900: loss: 1948.296
====> Epoch: 019 Train loss: 1949.2348  took : 13.612916469573975
====> Test loss: 1951.0163
iteration 0000: loss: 1948.909
iteration 0100: loss: 1949.293
iteration 0200: loss: 1948.551
iteration 0300: loss: 1949.531
iteration 0400: loss: 1949.785
iteration 0500: loss: 1948.708
iteration 0600: loss: 1952.149
iteration 0700: loss: 1949.003
iteration 0800: loss: 1949.173
iteration 0900: loss: 1950.116
====> Epoch: 020 Train loss: 1949.2113  took : 12.32120418548584
====> Test loss: 1951.1087
iteration 0000: loss: 1948.505
iteration 0100: loss: 1948.354
iteration 0200: loss: 1948.576
iteration 0300: loss: 1948.892
iteration 0400: loss: 1947.801
iteration 0500: loss: 1948.970
iteration 0600: loss: 1947.548
iteration 0700: loss: 1948.167
iteration 0800: loss: 1949.301
iteration 0900: loss: 1948.365
====> Epoch: 021 Train loss: 1949.0549  took : 12.366002321243286
====> Test loss: 1951.2190
iteration 0000: loss: 1949.678
iteration 0100: loss: 1949.493
iteration 0200: loss: 1948.829
iteration 0300: loss: 1948.256
iteration 0400: loss: 1949.470
iteration 0500: loss: 1949.690
iteration 0600: loss: 1948.089
iteration 0700: loss: 1948.898
iteration 0800: loss: 1948.598
iteration 0900: loss: 1948.242
====> Epoch: 022 Train loss: 1948.8846  took : 12.996607780456543
====> Test loss: 1950.7169
iteration 0000: loss: 1947.869
iteration 0100: loss: 1949.977
iteration 0200: loss: 1949.083
iteration 0300: loss: 1948.524
iteration 0400: loss: 1948.409
iteration 0500: loss: 1947.563
iteration 0600: loss: 1947.638
iteration 0700: loss: 1949.294
iteration 0800: loss: 1948.323
iteration 0900: loss: 1949.721
====> Epoch: 023 Train loss: 1948.7543  took : 13.204513311386108
====> Test loss: 1950.5540
iteration 0000: loss: 1948.679
iteration 0100: loss: 1949.022
iteration 0200: loss: 1948.860
iteration 0300: loss: 1949.268
iteration 0400: loss: 1947.479
iteration 0500: loss: 1948.844
iteration 0600: loss: 1947.870
iteration 0700: loss: 1948.696
iteration 0800: loss: 1948.566
iteration 0900: loss: 1948.742
====> Epoch: 024 Train loss: 1948.6047  took : 12.40700364112854
====> Test loss: 1950.5417
iteration 0000: loss: 1948.366
iteration 0100: loss: 1947.543
iteration 0200: loss: 1949.385
iteration 0300: loss: 1948.698
iteration 0400: loss: 1948.047
iteration 0500: loss: 1947.871
iteration 0600: loss: 1947.885
iteration 0700: loss: 1948.574
iteration 0800: loss: 1949.045
iteration 0900: loss: 1948.125
====> Epoch: 025 Train loss: 1948.7893  took : 12.38218641281128
====> Test loss: 1950.6929
iteration 0000: loss: 1949.778
iteration 0100: loss: 1948.388
iteration 0200: loss: 1948.840
iteration 0300: loss: 1950.129
iteration 0400: loss: 1949.704
iteration 0500: loss: 1949.126
iteration 0600: loss: 1947.424
iteration 0700: loss: 1948.779
iteration 0800: loss: 1948.508
iteration 0900: loss: 1949.009
====> Epoch: 026 Train loss: 1948.7207  took : 12.632806777954102
====> Test loss: 1950.9951
iteration 0000: loss: 1948.767
iteration 0100: loss: 1948.710
iteration 0200: loss: 1947.811
iteration 0300: loss: 1949.148
iteration 0400: loss: 1947.679
iteration 0500: loss: 1949.760
iteration 0600: loss: 1951.650
iteration 0700: loss: 1949.323
iteration 0800: loss: 1949.872
iteration 0900: loss: 1948.961
====> Epoch: 027 Train loss: 1948.8693  took : 13.147213220596313
====> Test loss: 1951.1894
iteration 0000: loss: 1949.979
iteration 0100: loss: 1948.882
iteration 0200: loss: 1948.204
iteration 0300: loss: 1950.220
iteration 0400: loss: 1947.277
iteration 0500: loss: 1948.247
iteration 0600: loss: 1947.734
iteration 0700: loss: 1948.986
iteration 0800: loss: 1948.305
iteration 0900: loss: 1947.404
====> Epoch: 028 Train loss: 1948.6418  took : 12.752590894699097
====> Test loss: 1950.0980
iteration 0000: loss: 1948.847
iteration 0100: loss: 1948.234
iteration 0200: loss: 1950.263
iteration 0300: loss: 1950.068
iteration 0400: loss: 1947.973
iteration 0500: loss: 1949.187
iteration 0600: loss: 1948.932
iteration 0700: loss: 1947.588
iteration 0800: loss: 1948.224
iteration 0900: loss: 1947.928
====> Epoch: 029 Train loss: 1948.3622  took : 13.52053189277649
====> Test loss: 1950.2395
iteration 0000: loss: 1949.082
iteration 0100: loss: 1947.510
iteration 0200: loss: 1948.112
iteration 0300: loss: 1947.725
iteration 0400: loss: 1948.893
iteration 0500: loss: 1946.680
iteration 0600: loss: 1947.472
iteration 0700: loss: 1947.347
iteration 0800: loss: 1948.193
iteration 0900: loss: 1947.854
====> Epoch: 030 Train loss: 1948.4054  took : 13.382265090942383
====> Test loss: 1950.5113
iteration 0000: loss: 1948.075
iteration 0100: loss: 1948.488
iteration 0200: loss: 1948.316
iteration 0300: loss: 1947.466
iteration 0400: loss: 1948.377
iteration 0500: loss: 1947.887
iteration 0600: loss: 1948.870
iteration 0700: loss: 1948.376
iteration 0800: loss: 1947.888
iteration 0900: loss: 1947.921
====> Epoch: 031 Train loss: 1948.2942  took : 12.307506322860718
====> Test loss: 1950.2560
iteration 0000: loss: 1947.489
iteration 0100: loss: 1948.433
iteration 0200: loss: 1948.744
iteration 0300: loss: 1948.161
iteration 0400: loss: 1948.104
iteration 0500: loss: 1948.519
iteration 0600: loss: 1948.720
iteration 0700: loss: 1948.928
iteration 0800: loss: 1947.676
iteration 0900: loss: 1948.599
====> Epoch: 032 Train loss: 1948.2193  took : 13.188295602798462
====> Test loss: 1949.9907
iteration 0000: loss: 1949.410
iteration 0100: loss: 1950.529
iteration 0200: loss: 1948.653
iteration 0300: loss: 1950.254
iteration 0400: loss: 1947.232
iteration 0500: loss: 1947.577
iteration 0600: loss: 1947.927
iteration 0700: loss: 1949.516
iteration 0800: loss: 1947.676
iteration 0900: loss: 1948.418
====> Epoch: 033 Train loss: 1948.2537  took : 13.29996109008789
====> Test loss: 1950.2859
iteration 0000: loss: 1947.006
iteration 0100: loss: 1947.600
iteration 0200: loss: 1949.336
iteration 0300: loss: 1947.009
iteration 0400: loss: 1947.598
iteration 0500: loss: 1949.061
iteration 0600: loss: 1947.879
iteration 0700: loss: 1948.172
iteration 0800: loss: 1948.712
iteration 0900: loss: 1948.219
====> Epoch: 034 Train loss: 1948.5070  took : 12.789350032806396
====> Test loss: 1951.8492
iteration 0000: loss: 1949.340
iteration 0100: loss: 1948.987
iteration 0200: loss: 1948.402
iteration 0300: loss: 1948.800
iteration 0400: loss: 1948.204
iteration 0500: loss: 1950.714
iteration 0600: loss: 1949.539
iteration 0700: loss: 1947.403
iteration 0800: loss: 1948.117
iteration 0900: loss: 1949.570
====> Epoch: 035 Train loss: 1948.7783  took : 13.03791618347168
====> Test loss: 1950.2654
iteration 0000: loss: 1947.931
iteration 0100: loss: 1947.931
iteration 0200: loss: 1948.157
iteration 0300: loss: 1949.425
iteration 0400: loss: 1948.289
iteration 0500: loss: 1947.597
iteration 0600: loss: 1948.047
iteration 0700: loss: 1950.891
iteration 0800: loss: 1949.965
iteration 0900: loss: 1947.113
====> Epoch: 036 Train loss: 1948.4798  took : 12.643061876296997
====> Test loss: 1950.2159
iteration 0000: loss: 1948.510
iteration 0100: loss: 1948.281
iteration 0200: loss: 1947.960
iteration 0300: loss: 1948.042
iteration 0400: loss: 1948.518
iteration 0500: loss: 1947.360
iteration 0600: loss: 1948.911
iteration 0700: loss: 1947.991
iteration 0800: loss: 1947.659
iteration 0900: loss: 1948.659
====> Epoch: 037 Train loss: 1948.1816  took : 12.77273440361023
====> Test loss: 1950.3996
iteration 0000: loss: 1949.107
iteration 0100: loss: 1949.419
iteration 0200: loss: 1948.219
iteration 0300: loss: 1948.001
iteration 0400: loss: 1948.963
iteration 0500: loss: 1948.031
iteration 0600: loss: 1948.103
iteration 0700: loss: 1947.901
iteration 0800: loss: 1947.767
iteration 0900: loss: 1948.648
====> Epoch: 038 Train loss: 1948.1994  took : 13.422240972518921
====> Test loss: 1950.9758
iteration 0000: loss: 1949.332
iteration 0100: loss: 1947.933
iteration 0200: loss: 1948.121
iteration 0300: loss: 1949.554
iteration 0400: loss: 1947.845
iteration 0500: loss: 1948.301
iteration 0600: loss: 1947.697
iteration 0700: loss: 1947.652
iteration 0800: loss: 1948.274
iteration 0900: loss: 1947.539
====> Epoch: 039 Train loss: 1948.4178  took : 12.642342567443848
====> Test loss: 1949.7255
iteration 0000: loss: 1948.063
iteration 0100: loss: 1947.743
iteration 0200: loss: 1947.892
iteration 0300: loss: 1947.310
iteration 0400: loss: 1950.023
iteration 0500: loss: 1947.375
iteration 0600: loss: 1947.600
iteration 0700: loss: 1947.385
iteration 0800: loss: 1947.686
iteration 0900: loss: 1948.429
====> Epoch: 040 Train loss: 1948.2489  took : 12.356221675872803
====> Test loss: 1951.2929
iteration 0000: loss: 1948.473
iteration 0100: loss: 1950.146
iteration 0200: loss: 1948.729
iteration 0300: loss: 1948.457
iteration 0400: loss: 1948.785
iteration 0500: loss: 1948.390
iteration 0600: loss: 1947.183
iteration 0700: loss: 1947.645
iteration 0800: loss: 1948.029
iteration 0900: loss: 1948.072
====> Epoch: 041 Train loss: 1948.3434  took : 13.111297845840454
====> Test loss: 1950.2450
iteration 0000: loss: 1947.461
iteration 0100: loss: 1949.213
iteration 0200: loss: 1948.118
iteration 0300: loss: 1947.449
iteration 0400: loss: 1949.179
iteration 0500: loss: 1948.345
iteration 0600: loss: 1948.379
iteration 0700: loss: 1949.938
iteration 0800: loss: 1948.123
iteration 0900: loss: 1947.649
====> Epoch: 042 Train loss: 1948.2933  took : 13.528820753097534
====> Test loss: 1949.8409
iteration 0000: loss: 1947.727
iteration 0100: loss: 1948.461
iteration 0200: loss: 1948.925
iteration 0300: loss: 1948.251
iteration 0400: loss: 1948.168
iteration 0500: loss: 1948.570
iteration 0600: loss: 1948.050
iteration 0700: loss: 1948.054
iteration 0800: loss: 1948.264
iteration 0900: loss: 1948.367
====> Epoch: 043 Train loss: 1948.1503  took : 12.280726432800293
====> Test loss: 1950.0694
iteration 0000: loss: 1948.245
iteration 0100: loss: 1948.694
iteration 0200: loss: 1948.453
iteration 0300: loss: 1947.690
iteration 0400: loss: 1947.638
iteration 0500: loss: 1947.922
iteration 0600: loss: 1947.823
iteration 0700: loss: 1946.745
iteration 0800: loss: 1947.699
iteration 0900: loss: 1948.080
====> Epoch: 044 Train loss: 1948.1102  took : 13.268160581588745
====> Test loss: 1950.2449
iteration 0000: loss: 1948.267
iteration 0100: loss: 1947.294
iteration 0200: loss: 1948.431
iteration 0300: loss: 1950.206
iteration 0400: loss: 1948.909
iteration 0500: loss: 1947.376
iteration 0600: loss: 1947.651
iteration 0700: loss: 1947.486
iteration 0800: loss: 1947.535
iteration 0900: loss: 1947.899
====> Epoch: 045 Train loss: 1948.3065  took : 13.15817928314209
====> Test loss: 1950.0250
iteration 0000: loss: 1947.704
iteration 0100: loss: 1947.626
iteration 0200: loss: 1947.284
iteration 0300: loss: 1947.316
iteration 0400: loss: 1947.674
iteration 0500: loss: 1947.564
iteration 0600: loss: 1947.751
iteration 0700: loss: 1947.862
iteration 0800: loss: 1947.811
iteration 0900: loss: 1947.000
====> Epoch: 046 Train loss: 1948.1459  took : 13.339673042297363
====> Test loss: 1950.0279
iteration 0000: loss: 1948.600
iteration 0100: loss: 1948.503
iteration 0200: loss: 1948.209
iteration 0300: loss: 1948.240
iteration 0400: loss: 1949.708
iteration 0500: loss: 1950.262
iteration 0600: loss: 1947.736
iteration 0700: loss: 1948.045
iteration 0800: loss: 1948.048
iteration 0900: loss: 1947.924
====> Epoch: 047 Train loss: 1948.2058  took : 12.653296709060669
====> Test loss: 1950.4759
iteration 0000: loss: 1948.970
iteration 0100: loss: 1947.563
iteration 0200: loss: 1947.505
iteration 0300: loss: 1947.009
iteration 0400: loss: 1947.427
iteration 0500: loss: 1949.643
iteration 0600: loss: 1947.698
iteration 0700: loss: 1947.252
iteration 0800: loss: 1946.799
iteration 0900: loss: 1947.589
====> Epoch: 048 Train loss: 1948.1179  took : 12.098959922790527
====> Test loss: 1949.7485
iteration 0000: loss: 1948.215
iteration 0100: loss: 1948.686
iteration 0200: loss: 1948.474
iteration 0300: loss: 1947.826
iteration 0400: loss: 1948.390
iteration 0500: loss: 1947.715
iteration 0600: loss: 1947.146
iteration 0700: loss: 1948.780
iteration 0800: loss: 1948.018
iteration 0900: loss: 1947.564
====> Epoch: 049 Train loss: 1948.2155  took : 12.504804611206055
====> Test loss: 1950.0359
iteration 0000: loss: 1948.594
iteration 0100: loss: 1948.081
iteration 0200: loss: 1947.485
iteration 0300: loss: 1949.163
iteration 0400: loss: 1948.699
iteration 0500: loss: 1947.044
iteration 0600: loss: 1947.174
iteration 0700: loss: 1948.580
iteration 0800: loss: 1948.635
iteration 0900: loss: 1949.817
====> Epoch: 050 Train loss: 1948.0646  took : 13.589272499084473
====> Test loss: 1950.2195
====> [MM-VAE] Time: 717.572s or 00:11:57
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  8
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_8
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_8
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5220.088
iteration 0100: loss: 4149.412
iteration 0200: loss: 4072.346
iteration 0300: loss: 4045.481
iteration 0400: loss: 4004.573
iteration 0500: loss: 4020.947
iteration 0600: loss: 4015.322
iteration 0700: loss: 4016.781
iteration 0800: loss: 3999.297
iteration 0900: loss: 3994.001
iteration 1000: loss: 3991.573
iteration 1100: loss: 4004.360
iteration 1200: loss: 4001.000
iteration 1300: loss: 3992.011
iteration 1400: loss: 3992.236
iteration 1500: loss: 3990.719
iteration 1600: loss: 3985.020
iteration 1700: loss: 3982.583
iteration 1800: loss: 3981.636
====> Epoch: 001 Train loss: 4025.0587  took : 53.04363536834717
====> Test loss: 3987.9121
iteration 0000: loss: 3978.804
iteration 0100: loss: 3991.380
iteration 0200: loss: 3986.333
iteration 0300: loss: 3979.824
iteration 0400: loss: 3968.156
iteration 0500: loss: 3983.793
iteration 0600: loss: 3974.927
iteration 0700: loss: 3975.903
iteration 0800: loss: 3974.450
iteration 0900: loss: 3979.729
iteration 1000: loss: 3983.001
iteration 1100: loss: 3973.909
iteration 1200: loss: 3967.434
iteration 1300: loss: 3973.252
iteration 1400: loss: 3966.425
iteration 1500: loss: 3965.504
iteration 1600: loss: 3970.132
iteration 1700: loss: 3963.433
iteration 1800: loss: 3964.975
====> Epoch: 002 Train loss: 3973.2201  took : 53.108710050582886
====> Test loss: 3968.2206
iteration 0000: loss: 3962.301
iteration 0100: loss: 3960.680
iteration 0200: loss: 3972.856
iteration 0300: loss: 3960.223
iteration 0400: loss: 3961.566
iteration 0500: loss: 3950.654
iteration 0600: loss: 3957.803
iteration 0700: loss: 3948.879
iteration 0800: loss: 3954.019
iteration 0900: loss: 3958.137
iteration 1000: loss: 3970.462
iteration 1100: loss: 3961.211
iteration 1200: loss: 3959.631
iteration 1300: loss: 3952.629
iteration 1400: loss: 3957.758
iteration 1500: loss: 3956.001
iteration 1600: loss: 3953.869
iteration 1700: loss: 3961.395
iteration 1800: loss: 3960.292
====> Epoch: 003 Train loss: 3957.8917  took : 53.24664759635925
====> Test loss: 3956.5910
iteration 0000: loss: 3956.563
iteration 0100: loss: 3952.036
iteration 0200: loss: 3948.789
iteration 0300: loss: 3950.678
iteration 0400: loss: 3959.556
iteration 0500: loss: 3951.587
iteration 0600: loss: 3952.441
iteration 0700: loss: 3948.647
iteration 0800: loss: 3952.691
iteration 0900: loss: 3957.844
iteration 1000: loss: 3952.544
iteration 1100: loss: 3949.745
iteration 1200: loss: 3957.027
iteration 1300: loss: 3952.595
iteration 1400: loss: 3955.413
iteration 1500: loss: 3945.121
iteration 1600: loss: 3952.299
iteration 1700: loss: 3957.201
iteration 1800: loss: 3954.639
====> Epoch: 004 Train loss: 3952.1527  took : 52.958043336868286
====> Test loss: 3952.6397
iteration 0000: loss: 3946.593
iteration 0100: loss: 3949.625
iteration 0200: loss: 3946.022
iteration 0300: loss: 3943.885
iteration 0400: loss: 3943.343
iteration 0500: loss: 3952.472
iteration 0600: loss: 3952.979
iteration 0700: loss: 3954.143
iteration 0800: loss: 3959.514
iteration 0900: loss: 3950.083
iteration 1000: loss: 3956.088
iteration 1100: loss: 3951.213
iteration 1200: loss: 3949.429
iteration 1300: loss: 3953.498
iteration 1400: loss: 3954.437
iteration 1500: loss: 3943.775
iteration 1600: loss: 3947.105
iteration 1700: loss: 3951.098
iteration 1800: loss: 3951.637
====> Epoch: 005 Train loss: 3949.4540  took : 52.963247299194336
====> Test loss: 3951.3779
iteration 0000: loss: 3948.438
iteration 0100: loss: 3953.545
iteration 0200: loss: 3947.684
iteration 0300: loss: 3948.642
iteration 0400: loss: 3947.851
iteration 0500: loss: 3946.956
iteration 0600: loss: 3939.776
iteration 0700: loss: 3942.045
iteration 0800: loss: 3949.274
iteration 0900: loss: 3951.096
iteration 1000: loss: 3942.993
iteration 1100: loss: 3945.647
iteration 1200: loss: 3944.572
iteration 1300: loss: 3946.342
iteration 1400: loss: 3948.133
iteration 1500: loss: 3948.949
iteration 1600: loss: 3953.175
iteration 1700: loss: 3949.617
iteration 1800: loss: 3943.724
====> Epoch: 006 Train loss: 3947.7400  took : 53.179136753082275
====> Test loss: 3951.7478
iteration 0000: loss: 3950.339
iteration 0100: loss: 3951.844
iteration 0200: loss: 3951.188
iteration 0300: loss: 3946.229
iteration 0400: loss: 3953.328
iteration 0500: loss: 3939.509
iteration 0600: loss: 3950.843
iteration 0700: loss: 3945.278
iteration 0800: loss: 3943.676
iteration 0900: loss: 3947.571
iteration 1000: loss: 3947.762
iteration 1100: loss: 3938.096
iteration 1200: loss: 3946.609
iteration 1300: loss: 3943.916
iteration 1400: loss: 3951.494
iteration 1500: loss: 3943.222
iteration 1600: loss: 3948.682
iteration 1700: loss: 3943.437
iteration 1800: loss: 3948.678
====> Epoch: 007 Train loss: 3946.7211  took : 52.895177602767944
====> Test loss: 3948.8144
iteration 0000: loss: 3940.260
iteration 0100: loss: 3950.143
iteration 0200: loss: 3944.371
iteration 0300: loss: 3948.571
iteration 0400: loss: 3943.428
iteration 0500: loss: 3946.197
iteration 0600: loss: 3945.930
iteration 0700: loss: 3946.541
iteration 0800: loss: 3952.111
iteration 0900: loss: 3943.339
iteration 1000: loss: 3956.528
iteration 1100: loss: 3942.406
iteration 1200: loss: 3951.054
iteration 1300: loss: 3944.436
iteration 1400: loss: 3943.026
iteration 1500: loss: 3943.975
iteration 1600: loss: 3941.651
iteration 1700: loss: 3944.217
iteration 1800: loss: 3945.853
====> Epoch: 008 Train loss: 3945.7047  took : 52.952810764312744
====> Test loss: 3948.8070
iteration 0000: loss: 3949.024
iteration 0100: loss: 3948.790
iteration 0200: loss: 3944.143
iteration 0300: loss: 3938.616
iteration 0400: loss: 3946.833
iteration 0500: loss: 3945.586
iteration 0600: loss: 3942.227
iteration 0700: loss: 3951.692
iteration 0800: loss: 3938.105
iteration 0900: loss: 3948.915
iteration 1000: loss: 3944.842
iteration 1100: loss: 3946.850
iteration 1200: loss: 3947.528
iteration 1300: loss: 3941.487
iteration 1400: loss: 3945.260
iteration 1500: loss: 3944.249
iteration 1600: loss: 3945.087
iteration 1700: loss: 3946.486
iteration 1800: loss: 3946.416
====> Epoch: 009 Train loss: 3945.2326  took : 52.97344374656677
====> Test loss: 3948.7006
iteration 0000: loss: 3950.269
iteration 0100: loss: 3945.855
iteration 0200: loss: 3941.726
iteration 0300: loss: 3945.305
iteration 0400: loss: 3950.420
iteration 0500: loss: 3953.881
iteration 0600: loss: 3942.784
iteration 0700: loss: 3934.091
iteration 0800: loss: 3945.686
iteration 0900: loss: 3951.340
iteration 1000: loss: 3941.297
iteration 1100: loss: 3954.913
iteration 1200: loss: 3946.891
iteration 1300: loss: 3939.572
iteration 1400: loss: 3940.144
iteration 1500: loss: 3941.524
iteration 1600: loss: 3945.208
iteration 1700: loss: 3940.814
iteration 1800: loss: 3943.325
====> Epoch: 010 Train loss: 3944.7161  took : 52.98590612411499
====> Test loss: 3948.1973
iteration 0000: loss: 3940.730
iteration 0100: loss: 3939.877
iteration 0200: loss: 3942.825
iteration 0300: loss: 3939.139
iteration 0400: loss: 3942.401
iteration 0500: loss: 3940.979
iteration 0600: loss: 3944.645
iteration 0700: loss: 3943.077
iteration 0800: loss: 3948.052
iteration 0900: loss: 3938.357
iteration 1000: loss: 3949.373
iteration 1100: loss: 3941.679
iteration 1200: loss: 3943.396
iteration 1300: loss: 3946.393
iteration 1400: loss: 3945.437
iteration 1500: loss: 3939.343
iteration 1600: loss: 3949.170
iteration 1700: loss: 3944.055
iteration 1800: loss: 3952.388
====> Epoch: 011 Train loss: 3944.3830  took : 52.89656186103821
====> Test loss: 3948.5980
iteration 0000: loss: 3944.209
iteration 0100: loss: 3955.306
iteration 0200: loss: 3945.890
iteration 0300: loss: 3938.314
iteration 0400: loss: 3950.972
iteration 0500: loss: 3946.965
iteration 0600: loss: 3941.520
iteration 0700: loss: 3946.268
iteration 0800: loss: 3941.172
iteration 0900: loss: 3946.564
iteration 1000: loss: 3942.068
iteration 1100: loss: 3955.181
iteration 1200: loss: 3949.217
iteration 1300: loss: 3940.729
iteration 1400: loss: 3948.861
iteration 1500: loss: 3940.196
iteration 1600: loss: 3940.916
iteration 1700: loss: 3947.842
iteration 1800: loss: 3948.222
====> Epoch: 012 Train loss: 3944.1177  took : 53.17235040664673
====> Test loss: 3947.4499
iteration 0000: loss: 3948.193
iteration 0100: loss: 3943.187
iteration 0200: loss: 3948.252
iteration 0300: loss: 3936.380
iteration 0400: loss: 3940.447
iteration 0500: loss: 3938.635
iteration 0600: loss: 3937.254
iteration 0700: loss: 3946.875
iteration 0800: loss: 3947.365
iteration 0900: loss: 3941.157
iteration 1000: loss: 3944.110
iteration 1100: loss: 3940.420
iteration 1200: loss: 3946.428
iteration 1300: loss: 3940.062
iteration 1400: loss: 3935.898
iteration 1500: loss: 3948.831
iteration 1600: loss: 3945.483
iteration 1700: loss: 3944.967
iteration 1800: loss: 3946.590
====> Epoch: 013 Train loss: 3943.8761  took : 52.96292471885681
====> Test loss: 3947.5596
iteration 0000: loss: 3942.210
iteration 0100: loss: 3934.344
iteration 0200: loss: 3951.312
iteration 0300: loss: 3941.177
iteration 0400: loss: 3941.161
iteration 0500: loss: 3945.327
iteration 0600: loss: 3946.598
iteration 0700: loss: 3942.459
iteration 0800: loss: 3943.499
iteration 0900: loss: 3942.771
iteration 1000: loss: 3934.982
iteration 1100: loss: 3940.594
iteration 1200: loss: 3944.426
iteration 1300: loss: 3942.781
iteration 1400: loss: 3943.153
iteration 1500: loss: 3936.571
iteration 1600: loss: 3941.900
iteration 1700: loss: 3942.679
iteration 1800: loss: 3944.795
====> Epoch: 014 Train loss: 3943.7758  took : 52.83043098449707
====> Test loss: 3947.1712
iteration 0000: loss: 3946.343
iteration 0100: loss: 3944.274
iteration 0200: loss: 3941.710
iteration 0300: loss: 3945.063
iteration 0400: loss: 3950.944
iteration 0500: loss: 3944.216
iteration 0600: loss: 3950.229
iteration 0700: loss: 3942.499
iteration 0800: loss: 3941.209
iteration 0900: loss: 3939.001
iteration 1000: loss: 3939.799
iteration 1100: loss: 3936.034
iteration 1200: loss: 3948.108
iteration 1300: loss: 3942.562
iteration 1400: loss: 3946.790
iteration 1500: loss: 3943.970
iteration 1600: loss: 3947.210
iteration 1700: loss: 3949.627
iteration 1800: loss: 3940.252
====> Epoch: 015 Train loss: 3943.6446  took : 53.112733602523804
====> Test loss: 3946.8017
iteration 0000: loss: 3947.016
iteration 0100: loss: 3943.795
iteration 0200: loss: 3944.208
iteration 0300: loss: 3942.625
iteration 0400: loss: 3946.685
iteration 0500: loss: 3943.362
iteration 0600: loss: 3945.060
iteration 0700: loss: 3941.989
iteration 0800: loss: 3938.983
iteration 0900: loss: 3942.707
iteration 1000: loss: 3949.449
iteration 1100: loss: 3947.201
iteration 1200: loss: 3944.887
iteration 1300: loss: 3946.181
iteration 1400: loss: 3938.118
iteration 1500: loss: 3937.636
iteration 1600: loss: 3940.633
iteration 1700: loss: 3945.624
iteration 1800: loss: 3939.274
====> Epoch: 016 Train loss: 3943.6498  took : 53.014075756073
====> Test loss: 3947.0702
iteration 0000: loss: 3941.584
iteration 0100: loss: 3947.090
iteration 0200: loss: 3943.108
iteration 0300: loss: 3949.617
iteration 0400: loss: 3951.963
iteration 0500: loss: 3946.366
iteration 0600: loss: 3942.047
iteration 0700: loss: 3943.416
iteration 0800: loss: 3953.316
iteration 0900: loss: 3940.041
iteration 1000: loss: 3940.284
iteration 1100: loss: 3946.342
iteration 1200: loss: 3946.500
iteration 1300: loss: 3938.169
iteration 1400: loss: 3945.560
iteration 1500: loss: 3948.233
iteration 1600: loss: 3946.309
iteration 1700: loss: 3943.092
iteration 1800: loss: 3943.526
====> Epoch: 017 Train loss: 3943.3666  took : 52.85324144363403
====> Test loss: 3947.0619
iteration 0000: loss: 3943.449
iteration 0100: loss: 3936.546
iteration 0200: loss: 3941.972
iteration 0300: loss: 3942.329
iteration 0400: loss: 3941.622
iteration 0500: loss: 3939.440
iteration 0600: loss: 3938.979
iteration 0700: loss: 3943.959
iteration 0800: loss: 3941.429
iteration 0900: loss: 3944.133
iteration 1000: loss: 3938.746
iteration 1100: loss: 3943.028
iteration 1200: loss: 3943.324
iteration 1300: loss: 3950.212
iteration 1400: loss: 3946.861
iteration 1500: loss: 3948.397
iteration 1600: loss: 3947.765
iteration 1700: loss: 3947.670
iteration 1800: loss: 3943.451
====> Epoch: 018 Train loss: 3943.3945  took : 52.994142293930054
====> Test loss: 3947.4262
iteration 0000: loss: 3939.187
iteration 0100: loss: 3946.242
iteration 0200: loss: 3943.452
iteration 0300: loss: 3947.047
iteration 0400: loss: 3946.508
iteration 0500: loss: 3947.168
iteration 0600: loss: 3948.134
iteration 0700: loss: 3945.225
iteration 0800: loss: 3948.288
iteration 0900: loss: 3946.169
iteration 1000: loss: 3939.814
iteration 1100: loss: 3941.714
iteration 1200: loss: 3934.842
iteration 1300: loss: 3943.376
iteration 1400: loss: 3941.612
iteration 1500: loss: 3942.762
iteration 1600: loss: 3937.027
iteration 1700: loss: 3941.697
iteration 1800: loss: 3947.080
====> Epoch: 019 Train loss: 3943.1303  took : 52.87644624710083
====> Test loss: 3946.4390
iteration 0000: loss: 3938.634
iteration 0100: loss: 3944.533
iteration 0200: loss: 3942.187
iteration 0300: loss: 3946.021
iteration 0400: loss: 3948.794
iteration 0500: loss: 3941.699
iteration 0600: loss: 3940.863
iteration 0700: loss: 3947.555
iteration 0800: loss: 3939.567
iteration 0900: loss: 3945.890
iteration 1000: loss: 3939.400
iteration 1100: loss: 3944.545
iteration 1200: loss: 3941.429
iteration 1300: loss: 3946.602
iteration 1400: loss: 3942.640
iteration 1500: loss: 3942.774
iteration 1600: loss: 3942.057
iteration 1700: loss: 3946.340
iteration 1800: loss: 3940.173
====> Epoch: 020 Train loss: 3942.9926  took : 53.00607085227966
====> Test loss: 3946.2461
iteration 0000: loss: 3943.561
iteration 0100: loss: 3945.946
iteration 0200: loss: 3939.293
iteration 0300: loss: 3934.423
iteration 0400: loss: 3941.006
iteration 0500: loss: 3939.950
iteration 0600: loss: 3944.427
iteration 0700: loss: 3943.185
iteration 0800: loss: 3944.867
iteration 0900: loss: 3943.375
iteration 1000: loss: 3945.095
iteration 1100: loss: 3934.679
iteration 1200: loss: 3942.200
iteration 1300: loss: 3940.137
iteration 1400: loss: 3940.948
iteration 1500: loss: 3946.613
iteration 1600: loss: 3944.637
iteration 1700: loss: 3948.146
iteration 1800: loss: 3943.213
====> Epoch: 021 Train loss: 3942.9315  took : 52.835936069488525
====> Test loss: 3946.4238
iteration 0000: loss: 3943.580
iteration 0100: loss: 3943.828
iteration 0200: loss: 3938.312
iteration 0300: loss: 3943.979
iteration 0400: loss: 3944.827
iteration 0500: loss: 3940.168
iteration 0600: loss: 3945.008
iteration 0700: loss: 3941.664
iteration 0800: loss: 3940.517
iteration 0900: loss: 3940.839
iteration 1000: loss: 3943.907
iteration 1100: loss: 3941.760
iteration 1200: loss: 3945.487
iteration 1300: loss: 3938.008
iteration 1400: loss: 3943.769
iteration 1500: loss: 3951.257
iteration 1600: loss: 3940.758
iteration 1700: loss: 3941.068
iteration 1800: loss: 3943.302
====> Epoch: 022 Train loss: 3942.9235  took : 52.861685037612915
====> Test loss: 3946.8909
iteration 0000: loss: 3947.182
iteration 0100: loss: 3939.505
iteration 0200: loss: 3943.061
iteration 0300: loss: 3943.004
iteration 0400: loss: 3938.540
iteration 0500: loss: 3946.742
iteration 0600: loss: 3940.682
iteration 0700: loss: 3940.522
iteration 0800: loss: 3946.321
iteration 0900: loss: 3940.713
iteration 1000: loss: 3945.863
iteration 1100: loss: 3938.433
iteration 1200: loss: 3944.988
iteration 1300: loss: 3940.617
iteration 1400: loss: 3947.431
iteration 1500: loss: 3952.484
iteration 1600: loss: 3941.001
iteration 1700: loss: 3940.823
iteration 1800: loss: 3942.705
====> Epoch: 023 Train loss: 3942.7721  took : 52.96244692802429
====> Test loss: 3946.9420
iteration 0000: loss: 3944.313
iteration 0100: loss: 3941.068
iteration 0200: loss: 3940.697
iteration 0300: loss: 3941.258
iteration 0400: loss: 3946.444
iteration 0500: loss: 3941.513
iteration 0600: loss: 3944.781
iteration 0700: loss: 3946.880
iteration 0800: loss: 3940.279
iteration 0900: loss: 3942.813
iteration 1000: loss: 3945.423
iteration 1100: loss: 3942.826
iteration 1200: loss: 3942.324
iteration 1300: loss: 3943.659
iteration 1400: loss: 3945.344
iteration 1500: loss: 3934.578
iteration 1600: loss: 3943.553
iteration 1700: loss: 3943.966
iteration 1800: loss: 3937.397
====> Epoch: 024 Train loss: 3942.7091  took : 52.911606788635254
====> Test loss: 3946.3632
iteration 0000: loss: 3943.447
iteration 0100: loss: 3944.327
iteration 0200: loss: 3942.470
iteration 0300: loss: 3944.263
iteration 0400: loss: 3940.722
iteration 0500: loss: 3943.250
iteration 0600: loss: 3942.823
iteration 0700: loss: 3952.047
iteration 0800: loss: 3938.624
iteration 0900: loss: 3943.314
iteration 1000: loss: 3943.269
iteration 1100: loss: 3941.618
iteration 1200: loss: 3945.369
iteration 1300: loss: 3937.881
iteration 1400: loss: 3946.060
iteration 1500: loss: 3941.539
iteration 1600: loss: 3953.333
iteration 1700: loss: 3942.999
iteration 1800: loss: 3944.968
====> Epoch: 025 Train loss: 3942.6907  took : 52.83114957809448
====> Test loss: 3946.2540
iteration 0000: loss: 3936.917
iteration 0100: loss: 3945.129
iteration 0200: loss: 3942.007
iteration 0300: loss: 3943.210
iteration 0400: loss: 3936.716
iteration 0500: loss: 3941.710
iteration 0600: loss: 3946.287
iteration 0700: loss: 3944.716
iteration 0800: loss: 3941.349
iteration 0900: loss: 3946.790
iteration 1000: loss: 3940.414
iteration 1100: loss: 3942.102
iteration 1200: loss: 3945.089
iteration 1300: loss: 3940.736
iteration 1400: loss: 3947.805
iteration 1500: loss: 3942.421
iteration 1600: loss: 3944.330
iteration 1700: loss: 3945.941
iteration 1800: loss: 3935.975
====> Epoch: 026 Train loss: 3942.7000  took : 53.12433981895447
====> Test loss: 3946.7905
iteration 0000: loss: 3944.053
iteration 0100: loss: 3938.977
iteration 0200: loss: 3942.761
iteration 0300: loss: 3942.646
iteration 0400: loss: 3940.266
iteration 0500: loss: 3940.021
iteration 0600: loss: 3946.523
iteration 0700: loss: 3941.649
iteration 0800: loss: 3941.509
iteration 0900: loss: 3938.868
iteration 1000: loss: 3945.576
iteration 1100: loss: 3941.291
iteration 1200: loss: 3944.429
iteration 1300: loss: 3941.792
iteration 1400: loss: 3945.216
iteration 1500: loss: 3944.321
iteration 1600: loss: 3948.208
iteration 1700: loss: 3939.118
iteration 1800: loss: 3937.531
====> Epoch: 027 Train loss: 3942.6516  took : 53.2245569229126
====> Test loss: 3946.4242
iteration 0000: loss: 3945.785
iteration 0100: loss: 3946.757
iteration 0200: loss: 3946.077
iteration 0300: loss: 3945.217
iteration 0400: loss: 3940.009
iteration 0500: loss: 3941.777
iteration 0600: loss: 3946.051
iteration 0700: loss: 3941.495
iteration 0800: loss: 3943.256
iteration 0900: loss: 3940.072
iteration 1000: loss: 3941.570
iteration 1100: loss: 3934.605
iteration 1200: loss: 3942.047
iteration 1300: loss: 3949.551
iteration 1400: loss: 3939.864
iteration 1500: loss: 3941.006
iteration 1600: loss: 3941.763
iteration 1700: loss: 3940.535
iteration 1800: loss: 3948.775
====> Epoch: 028 Train loss: 3942.7185  took : 52.824262619018555
====> Test loss: 3946.0608
iteration 0000: loss: 3943.507
iteration 0100: loss: 3935.993
iteration 0200: loss: 3943.856
iteration 0300: loss: 3943.322
iteration 0400: loss: 3940.721
iteration 0500: loss: 3943.935
iteration 0600: loss: 3945.594
iteration 0700: loss: 3941.345
iteration 0800: loss: 3944.975
iteration 0900: loss: 3945.110
iteration 1000: loss: 3944.630
iteration 1100: loss: 3944.284
iteration 1200: loss: 3941.055
iteration 1300: loss: 3941.060
iteration 1400: loss: 3944.229
iteration 1500: loss: 3947.472
iteration 1600: loss: 3939.533
iteration 1700: loss: 3943.897
iteration 1800: loss: 3945.568
====> Epoch: 029 Train loss: 3942.5446  took : 52.80712413787842
====> Test loss: 3945.9322
iteration 0000: loss: 3948.598
iteration 0100: loss: 3940.155
iteration 0200: loss: 3940.883
iteration 0300: loss: 3944.147
iteration 0400: loss: 3939.474
iteration 0500: loss: 3935.865
iteration 0600: loss: 3943.654
iteration 0700: loss: 3940.437
iteration 0800: loss: 3943.108
iteration 0900: loss: 3941.023
iteration 1000: loss: 3941.394
iteration 1100: loss: 3950.937
iteration 1200: loss: 3949.481
iteration 1300: loss: 3943.012
iteration 1400: loss: 3943.013
iteration 1500: loss: 3947.646
iteration 1600: loss: 3937.566
iteration 1700: loss: 3939.755
iteration 1800: loss: 3940.650
====> Epoch: 030 Train loss: 3942.4083  took : 52.77873373031616
====> Test loss: 3945.8622
iteration 0000: loss: 3941.792
iteration 0100: loss: 3944.434
iteration 0200: loss: 3941.709
iteration 0300: loss: 3944.731
iteration 0400: loss: 3942.407
iteration 0500: loss: 3942.558
iteration 0600: loss: 3941.584
iteration 0700: loss: 3943.492
iteration 0800: loss: 3943.942
iteration 0900: loss: 3947.904
iteration 1000: loss: 3944.048
iteration 1100: loss: 3942.428
iteration 1200: loss: 3937.369
iteration 1300: loss: 3945.230
iteration 1400: loss: 3940.239
iteration 1500: loss: 3936.526
iteration 1600: loss: 3945.698
iteration 1700: loss: 3939.550
iteration 1800: loss: 3939.840
====> Epoch: 031 Train loss: 3942.3061  took : 52.80298948287964
====> Test loss: 3946.0871
iteration 0000: loss: 3937.471
iteration 0100: loss: 3940.729
iteration 0200: loss: 3947.775
iteration 0300: loss: 3939.341
iteration 0400: loss: 3941.954
iteration 0500: loss: 3944.635
iteration 0600: loss: 3949.774
iteration 0700: loss: 3941.970
iteration 0800: loss: 3943.212
iteration 0900: loss: 3938.962
iteration 1000: loss: 3946.969
iteration 1100: loss: 3943.094
iteration 1200: loss: 3941.074
iteration 1300: loss: 3941.292
iteration 1400: loss: 3942.510
iteration 1500: loss: 3942.228
iteration 1600: loss: 3941.477
iteration 1700: loss: 3935.529
iteration 1800: loss: 3947.521
====> Epoch: 032 Train loss: 3942.4560  took : 52.74100661277771
====> Test loss: 3945.6124
iteration 0000: loss: 3942.713
iteration 0100: loss: 3937.061
iteration 0200: loss: 3943.556
iteration 0300: loss: 3940.312
iteration 0400: loss: 3940.821
iteration 0500: loss: 3947.231
iteration 0600: loss: 3943.581
iteration 0700: loss: 3950.519
iteration 0800: loss: 3946.325
iteration 0900: loss: 3946.283
iteration 1000: loss: 3944.568
iteration 1100: loss: 3941.724
iteration 1200: loss: 3943.537
iteration 1300: loss: 3938.134
iteration 1400: loss: 3937.229
iteration 1500: loss: 3938.667
iteration 1600: loss: 3940.878
iteration 1700: loss: 3943.260
iteration 1800: loss: 3947.022
====> Epoch: 033 Train loss: 3942.2316  took : 52.97399067878723
====> Test loss: 3945.6480
iteration 0000: loss: 3940.524
iteration 0100: loss: 3932.958
iteration 0200: loss: 3943.471
iteration 0300: loss: 3941.376
iteration 0400: loss: 3947.461
iteration 0500: loss: 3942.516
iteration 0600: loss: 3939.034
iteration 0700: loss: 3943.132
iteration 0800: loss: 3942.586
iteration 0900: loss: 3943.732
iteration 1000: loss: 3935.645
iteration 1100: loss: 3947.371
iteration 1200: loss: 3940.101
iteration 1300: loss: 3934.310
iteration 1400: loss: 3945.472
iteration 1500: loss: 3947.970
iteration 1600: loss: 3943.698
iteration 1700: loss: 3938.647
iteration 1800: loss: 3945.221
====> Epoch: 034 Train loss: 3942.2522  took : 53.08317947387695
====> Test loss: 3945.9216
iteration 0000: loss: 3939.363
iteration 0100: loss: 3940.511
iteration 0200: loss: 3942.362
iteration 0300: loss: 3938.798
iteration 0400: loss: 3941.530
iteration 0500: loss: 3941.499
iteration 0600: loss: 3942.514
iteration 0700: loss: 3945.761
iteration 0800: loss: 3938.766
iteration 0900: loss: 3951.312
iteration 1000: loss: 3939.109
iteration 1100: loss: 3944.812
iteration 1200: loss: 3942.447
iteration 1300: loss: 3947.677
iteration 1400: loss: 3943.326
iteration 1500: loss: 3936.155
iteration 1600: loss: 3936.810
iteration 1700: loss: 3944.870
iteration 1800: loss: 3944.150
====> Epoch: 035 Train loss: 3942.1611  took : 52.919859886169434
====> Test loss: 3945.9199
iteration 0000: loss: 3942.162
iteration 0100: loss: 3935.575
iteration 0200: loss: 3948.293
iteration 0300: loss: 3945.094
iteration 0400: loss: 3945.484
iteration 0500: loss: 3935.045
iteration 0600: loss: 3943.861
iteration 0700: loss: 3943.273
iteration 0800: loss: 3948.368
iteration 0900: loss: 3940.465
iteration 1000: loss: 3940.640
iteration 1100: loss: 3940.572
iteration 1200: loss: 3938.033
iteration 1300: loss: 3945.003
iteration 1400: loss: 3937.572
iteration 1500: loss: 3943.749
iteration 1600: loss: 3940.068
iteration 1700: loss: 3937.537
iteration 1800: loss: 3935.061
====> Epoch: 036 Train loss: 3942.2398  took : 52.58703422546387
====> Test loss: 3945.3797
iteration 0000: loss: 3946.733
iteration 0100: loss: 3939.166
iteration 0200: loss: 3941.053
iteration 0300: loss: 3945.913
iteration 0400: loss: 3943.898
iteration 0500: loss: 3940.692
iteration 0600: loss: 3939.041
iteration 0700: loss: 3940.845
iteration 0800: loss: 3940.690
iteration 0900: loss: 3940.439
iteration 1000: loss: 3941.328
iteration 1100: loss: 3942.987
iteration 1200: loss: 3936.031
iteration 1300: loss: 3947.055
iteration 1400: loss: 3939.699
iteration 1500: loss: 3944.210
iteration 1600: loss: 3940.480
iteration 1700: loss: 3941.898
iteration 1800: loss: 3945.176
====> Epoch: 037 Train loss: 3942.1180  took : 52.849647521972656
====> Test loss: 3945.5475
iteration 0000: loss: 3941.769
iteration 0100: loss: 3936.606
iteration 0200: loss: 3941.735
iteration 0300: loss: 3940.490
iteration 0400: loss: 3945.868
iteration 0500: loss: 3946.508
iteration 0600: loss: 3943.558
iteration 0700: loss: 3945.318
iteration 0800: loss: 3934.923
iteration 0900: loss: 3944.161
iteration 1000: loss: 3935.677
iteration 1100: loss: 3936.344
iteration 1200: loss: 3944.906
iteration 1300: loss: 3943.183
iteration 1400: loss: 3944.783
iteration 1500: loss: 3951.163
iteration 1600: loss: 3945.314
iteration 1700: loss: 3939.286
iteration 1800: loss: 3946.411
====> Epoch: 038 Train loss: 3942.0628  took : 52.7587456703186
====> Test loss: 3945.4939
iteration 0000: loss: 3941.684
iteration 0100: loss: 3945.957
iteration 0200: loss: 3938.082
iteration 0300: loss: 3945.820
iteration 0400: loss: 3947.380
iteration 0500: loss: 3938.595
iteration 0600: loss: 3939.867
iteration 0700: loss: 3941.358
iteration 0800: loss: 3938.539
iteration 0900: loss: 3943.065
iteration 1000: loss: 3942.222
iteration 1100: loss: 3943.577
iteration 1200: loss: 3942.449
iteration 1300: loss: 3942.254
iteration 1400: loss: 3938.461
iteration 1500: loss: 3942.700
iteration 1600: loss: 3937.485
iteration 1700: loss: 3940.370
iteration 1800: loss: 3935.952
====> Epoch: 039 Train loss: 3942.0352  took : 52.88021659851074
====> Test loss: 3946.2768
iteration 0000: loss: 3943.670
iteration 0100: loss: 3943.061
iteration 0200: loss: 3941.024
iteration 0300: loss: 3935.147
iteration 0400: loss: 3936.019
iteration 0500: loss: 3944.062
iteration 0600: loss: 3948.734
iteration 0700: loss: 3944.718
iteration 0800: loss: 3940.172
iteration 0900: loss: 3942.403
iteration 1000: loss: 3941.536
iteration 1100: loss: 3937.546
iteration 1200: loss: 3949.459
iteration 1300: loss: 3944.688
iteration 1400: loss: 3946.033
iteration 1500: loss: 3941.018
iteration 1600: loss: 3940.014
iteration 1700: loss: 3939.257
iteration 1800: loss: 3942.604
====> Epoch: 040 Train loss: 3942.0494  took : 53.01428174972534
====> Test loss: 3945.9141
iteration 0000: loss: 3942.370
iteration 0100: loss: 3939.108
iteration 0200: loss: 3939.685
iteration 0300: loss: 3943.066
iteration 0400: loss: 3945.582
iteration 0500: loss: 3939.852
iteration 0600: loss: 3939.740
iteration 0700: loss: 3941.380
iteration 0800: loss: 3940.277
iteration 0900: loss: 3944.118
iteration 1000: loss: 3931.620
iteration 1100: loss: 3944.223
iteration 1200: loss: 3939.726
iteration 1300: loss: 3945.065
iteration 1400: loss: 3951.020
iteration 1500: loss: 3935.223
iteration 1600: loss: 3940.859
iteration 1700: loss: 3935.583
iteration 1800: loss: 3942.625
====> Epoch: 041 Train loss: 3942.0789  took : 52.9521119594574
====> Test loss: 3945.8700
iteration 0000: loss: 3941.586
iteration 0100: loss: 3933.738
iteration 0200: loss: 3940.270
iteration 0300: loss: 3938.908
iteration 0400: loss: 3942.828
iteration 0500: loss: 3941.051
iteration 0600: loss: 3937.733
iteration 0700: loss: 3940.011
iteration 0800: loss: 3943.680
iteration 0900: loss: 3944.199
iteration 1000: loss: 3943.617
iteration 1100: loss: 3944.934
iteration 1200: loss: 3937.567
iteration 1300: loss: 3935.694
iteration 1400: loss: 3942.472
iteration 1500: loss: 3939.646
iteration 1600: loss: 3942.549
iteration 1700: loss: 3944.970
iteration 1800: loss: 3941.591
====> Epoch: 042 Train loss: 3942.0887  took : 52.785232067108154
====> Test loss: 3945.2822
iteration 0000: loss: 3945.070
iteration 0100: loss: 3945.095
iteration 0200: loss: 3942.539
iteration 0300: loss: 3943.466
iteration 0400: loss: 3949.150
iteration 0500: loss: 3936.592
iteration 0600: loss: 3940.353
iteration 0700: loss: 3942.743
iteration 0800: loss: 3944.304
iteration 0900: loss: 3944.346
iteration 1000: loss: 3938.734
iteration 1100: loss: 3944.288
iteration 1200: loss: 3943.038
iteration 1300: loss: 3936.527
iteration 1400: loss: 3945.787
iteration 1500: loss: 3947.293
iteration 1600: loss: 3943.416
iteration 1700: loss: 3942.328
iteration 1800: loss: 3948.183
====> Epoch: 043 Train loss: 3941.9762  took : 52.91006803512573
====> Test loss: 3945.5283
iteration 0000: loss: 3939.640
iteration 0100: loss: 3941.879
iteration 0200: loss: 3945.758
iteration 0300: loss: 3940.596
iteration 0400: loss: 3945.148
iteration 0500: loss: 3940.241
iteration 0600: loss: 3944.507
iteration 0700: loss: 3941.586
iteration 0800: loss: 3941.734
iteration 0900: loss: 3944.944
iteration 1000: loss: 3937.747
iteration 1100: loss: 3941.865
iteration 1200: loss: 3941.301
iteration 1300: loss: 3940.219
iteration 1400: loss: 3941.836
iteration 1500: loss: 3937.499
iteration 1600: loss: 3943.199
iteration 1700: loss: 3942.206
iteration 1800: loss: 3941.476
====> Epoch: 044 Train loss: 3941.8647  took : 52.752171993255615
====> Test loss: 3945.5844
iteration 0000: loss: 3942.434
iteration 0100: loss: 3944.028
iteration 0200: loss: 3937.523
iteration 0300: loss: 3939.470
iteration 0400: loss: 3942.496
iteration 0500: loss: 3941.279
iteration 0600: loss: 3942.524
iteration 0700: loss: 3938.337
iteration 0800: loss: 3941.491
iteration 0900: loss: 3940.841
iteration 1000: loss: 3940.946
iteration 1100: loss: 3943.823
iteration 1200: loss: 3940.907
iteration 1300: loss: 3942.030
iteration 1400: loss: 3945.323
iteration 1500: loss: 3939.178
iteration 1600: loss: 3940.260
iteration 1700: loss: 3943.711
iteration 1800: loss: 3941.206
====> Epoch: 045 Train loss: 3941.9438  took : 52.728044509887695
====> Test loss: 3944.9876
iteration 0000: loss: 3942.558
iteration 0100: loss: 3940.595
iteration 0200: loss: 3938.066
iteration 0300: loss: 3945.876
iteration 0400: loss: 3941.879
iteration 0500: loss: 3941.284
iteration 0600: loss: 3941.460
iteration 0700: loss: 3935.039
iteration 0800: loss: 3941.487
iteration 0900: loss: 3945.027
iteration 1000: loss: 3937.234
iteration 1100: loss: 3945.152
iteration 1200: loss: 3939.953
iteration 1300: loss: 3938.421
iteration 1400: loss: 3940.525
iteration 1500: loss: 3943.688
iteration 1600: loss: 3945.789
iteration 1700: loss: 3945.908
iteration 1800: loss: 3946.208
====> Epoch: 046 Train loss: 3941.6252  took : 52.75623965263367
====> Test loss: 3945.7824
iteration 0000: loss: 3943.802
iteration 0100: loss: 3931.551
iteration 0200: loss: 3938.174
iteration 0300: loss: 3943.991
iteration 0400: loss: 3941.771
iteration 0500: loss: 3942.639
iteration 0600: loss: 3952.093
iteration 0700: loss: 3941.140
iteration 0800: loss: 3940.686
iteration 0900: loss: 3943.041
iteration 1000: loss: 3939.352
iteration 1100: loss: 3946.188
iteration 1200: loss: 3946.783
iteration 1300: loss: 3940.891
iteration 1400: loss: 3939.864
iteration 1500: loss: 3943.155
iteration 1600: loss: 3944.176
iteration 1700: loss: 3946.839
iteration 1800: loss: 3939.189
====> Epoch: 047 Train loss: 3941.6867  took : 52.96436381340027
====> Test loss: 3944.9112
iteration 0000: loss: 3937.608
iteration 0100: loss: 3940.518
iteration 0200: loss: 3945.681
iteration 0300: loss: 3937.068
iteration 0400: loss: 3938.296
iteration 0500: loss: 3942.019
iteration 0600: loss: 3940.350
iteration 0700: loss: 3941.482
iteration 0800: loss: 3943.491
iteration 0900: loss: 3945.134
iteration 1000: loss: 3947.274
iteration 1100: loss: 3936.468
iteration 1200: loss: 3935.904
iteration 1300: loss: 3942.703
iteration 1400: loss: 3935.866
iteration 1500: loss: 3950.283
iteration 1600: loss: 3942.490
iteration 1700: loss: 3935.392
iteration 1800: loss: 3945.819
====> Epoch: 048 Train loss: 3941.3901  took : 53.132919788360596
====> Test loss: 3944.5422
iteration 0000: loss: 3944.193
iteration 0100: loss: 3935.730
iteration 0200: loss: 3938.451
iteration 0300: loss: 3940.910
iteration 0400: loss: 3947.406
iteration 0500: loss: 3942.687
iteration 0600: loss: 3942.249
iteration 0700: loss: 3942.218
iteration 0800: loss: 3935.049
iteration 0900: loss: 3940.366
iteration 1000: loss: 3947.421
iteration 1100: loss: 3945.974
iteration 1200: loss: 3942.514
iteration 1300: loss: 3943.549
iteration 1400: loss: 3944.062
iteration 1500: loss: 3941.213
iteration 1600: loss: 3938.806
iteration 1700: loss: 3942.457
iteration 1800: loss: 3943.397
====> Epoch: 049 Train loss: 3941.2460  took : 53.0171320438385
====> Test loss: 3945.0731
iteration 0000: loss: 3937.165
iteration 0100: loss: 3941.155
iteration 0200: loss: 3941.485
iteration 0300: loss: 3936.116
iteration 0400: loss: 3943.708
iteration 0500: loss: 3942.880
iteration 0600: loss: 3941.022
iteration 0700: loss: 3944.818
iteration 0800: loss: 3939.164
iteration 0900: loss: 3943.436
iteration 1000: loss: 3939.151
iteration 1100: loss: 3937.362
iteration 1200: loss: 3939.370
iteration 1300: loss: 3936.815
iteration 1400: loss: 3945.630
iteration 1500: loss: 3942.973
iteration 1600: loss: 3943.516
iteration 1700: loss: 3945.032
iteration 1800: loss: 3938.664
====> Epoch: 050 Train loss: 3941.1507  took : 52.698798418045044
====> Test loss: 3944.9652
====> [MM-VAE] Time: 3134.606s or 00:52:14
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  9
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_9
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_9
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1987.437
iteration 0100: loss: 1567.457
iteration 0200: loss: 1565.897
iteration 0300: loss: 1569.314
iteration 0400: loss: 1553.159
iteration 0500: loss: 1549.311
iteration 0600: loss: 1541.416
iteration 0700: loss: 1542.524
iteration 0800: loss: 1541.641
iteration 0900: loss: 1535.859
====> Epoch: 001 Train loss: 1555.2885  took : 8.501060485839844
====> Test loss: 1538.1192
iteration 0000: loss: 1539.478
iteration 0100: loss: 1533.679
iteration 0200: loss: 1532.021
iteration 0300: loss: 1529.917
iteration 0400: loss: 1529.326
iteration 0500: loss: 1528.973
iteration 0600: loss: 1528.542
iteration 0700: loss: 1528.026
iteration 0800: loss: 1527.861
iteration 0900: loss: 1525.651
====> Epoch: 002 Train loss: 1531.3446  took : 8.482714891433716
====> Test loss: 1529.3172
iteration 0000: loss: 1527.131
iteration 0100: loss: 1529.131
iteration 0200: loss: 1526.053
iteration 0300: loss: 1526.946
iteration 0400: loss: 1524.868
iteration 0500: loss: 1528.382
iteration 0600: loss: 1523.742
iteration 0700: loss: 1529.576
iteration 0800: loss: 1522.924
iteration 0900: loss: 1522.645
====> Epoch: 003 Train loss: 1525.9468  took : 8.409690856933594
====> Test loss: 1526.8766
iteration 0000: loss: 1526.049
iteration 0100: loss: 1524.255
iteration 0200: loss: 1522.843
iteration 0300: loss: 1523.628
iteration 0400: loss: 1525.941
iteration 0500: loss: 1524.212
iteration 0600: loss: 1528.085
iteration 0700: loss: 1523.985
iteration 0800: loss: 1521.234
iteration 0900: loss: 1527.484
====> Epoch: 004 Train loss: 1523.5208  took : 8.650548934936523
====> Test loss: 1525.0207
iteration 0000: loss: 1523.388
iteration 0100: loss: 1521.992
iteration 0200: loss: 1523.161
iteration 0300: loss: 1521.838
iteration 0400: loss: 1523.554
iteration 0500: loss: 1521.054
iteration 0600: loss: 1521.669
iteration 0700: loss: 1521.239
iteration 0800: loss: 1519.447
iteration 0900: loss: 1522.051
====> Epoch: 005 Train loss: 1521.9968  took : 8.515933513641357
====> Test loss: 1523.7693
iteration 0000: loss: 1524.446
iteration 0100: loss: 1517.973
iteration 0200: loss: 1523.452
iteration 0300: loss: 1522.190
iteration 0400: loss: 1517.516
iteration 0500: loss: 1523.709
iteration 0600: loss: 1521.535
iteration 0700: loss: 1522.674
iteration 0800: loss: 1518.506
iteration 0900: loss: 1521.564
====> Epoch: 006 Train loss: 1520.9581  took : 8.460496187210083
====> Test loss: 1523.0642
iteration 0000: loss: 1521.397
iteration 0100: loss: 1520.093
iteration 0200: loss: 1520.041
iteration 0300: loss: 1520.536
iteration 0400: loss: 1517.817
iteration 0500: loss: 1519.752
iteration 0600: loss: 1520.618
iteration 0700: loss: 1516.986
iteration 0800: loss: 1518.027
iteration 0900: loss: 1518.705
====> Epoch: 007 Train loss: 1520.1039  took : 8.50385570526123
====> Test loss: 1522.1593
iteration 0000: loss: 1521.586
iteration 0100: loss: 1519.748
iteration 0200: loss: 1521.715
iteration 0300: loss: 1523.516
iteration 0400: loss: 1522.003
iteration 0500: loss: 1521.429
iteration 0600: loss: 1522.246
iteration 0700: loss: 1518.368
iteration 0800: loss: 1518.926
iteration 0900: loss: 1518.141
====> Epoch: 008 Train loss: 1519.4201  took : 8.479615688323975
====> Test loss: 1521.8962
iteration 0000: loss: 1521.946
iteration 0100: loss: 1518.705
iteration 0200: loss: 1521.227
iteration 0300: loss: 1519.383
iteration 0400: loss: 1520.837
iteration 0500: loss: 1513.678
iteration 0600: loss: 1519.044
iteration 0700: loss: 1518.742
iteration 0800: loss: 1521.600
iteration 0900: loss: 1517.274
====> Epoch: 009 Train loss: 1518.8281  took : 8.473466873168945
====> Test loss: 1521.3481
iteration 0000: loss: 1518.461
iteration 0100: loss: 1519.262
iteration 0200: loss: 1517.217
iteration 0300: loss: 1519.464
iteration 0400: loss: 1519.704
iteration 0500: loss: 1520.398
iteration 0600: loss: 1518.817
iteration 0700: loss: 1520.211
iteration 0800: loss: 1520.603
iteration 0900: loss: 1516.458
====> Epoch: 010 Train loss: 1518.3280  took : 8.452517986297607
====> Test loss: 1521.1426
iteration 0000: loss: 1516.814
iteration 0100: loss: 1518.689
iteration 0200: loss: 1518.599
iteration 0300: loss: 1521.348
iteration 0400: loss: 1518.978
iteration 0500: loss: 1514.622
iteration 0600: loss: 1516.506
iteration 0700: loss: 1516.617
iteration 0800: loss: 1516.885
iteration 0900: loss: 1518.667
====> Epoch: 011 Train loss: 1517.9514  took : 8.45087718963623
====> Test loss: 1520.6970
iteration 0000: loss: 1517.738
iteration 0100: loss: 1518.212
iteration 0200: loss: 1517.006
iteration 0300: loss: 1520.700
iteration 0400: loss: 1517.993
iteration 0500: loss: 1516.474
iteration 0600: loss: 1517.896
iteration 0700: loss: 1516.788
iteration 0800: loss: 1516.697
iteration 0900: loss: 1519.265
====> Epoch: 012 Train loss: 1517.6214  took : 8.416935205459595
====> Test loss: 1520.3995
iteration 0000: loss: 1516.168
iteration 0100: loss: 1515.613
iteration 0200: loss: 1518.396
iteration 0300: loss: 1515.106
iteration 0400: loss: 1519.347
iteration 0500: loss: 1516.846
iteration 0600: loss: 1517.626
iteration 0700: loss: 1516.663
iteration 0800: loss: 1518.300
iteration 0900: loss: 1517.652
====> Epoch: 013 Train loss: 1517.3126  took : 8.419731616973877
====> Test loss: 1520.0959
iteration 0000: loss: 1518.678
iteration 0100: loss: 1515.350
iteration 0200: loss: 1517.910
iteration 0300: loss: 1516.739
iteration 0400: loss: 1516.460
iteration 0500: loss: 1516.850
iteration 0600: loss: 1519.188
iteration 0700: loss: 1521.491
iteration 0800: loss: 1520.430
iteration 0900: loss: 1515.259
====> Epoch: 014 Train loss: 1517.1254  took : 8.525475978851318
====> Test loss: 1520.0499
iteration 0000: loss: 1518.827
iteration 0100: loss: 1516.461
iteration 0200: loss: 1519.716
iteration 0300: loss: 1518.134
iteration 0400: loss: 1517.576
iteration 0500: loss: 1516.067
iteration 0600: loss: 1512.920
iteration 0700: loss: 1517.551
iteration 0800: loss: 1515.573
iteration 0900: loss: 1514.879
====> Epoch: 015 Train loss: 1516.8549  took : 8.429139137268066
====> Test loss: 1519.8422
iteration 0000: loss: 1516.799
iteration 0100: loss: 1514.931
iteration 0200: loss: 1514.810
iteration 0300: loss: 1515.410
iteration 0400: loss: 1517.038
iteration 0500: loss: 1516.618
iteration 0600: loss: 1515.038
iteration 0700: loss: 1514.033
iteration 0800: loss: 1514.873
iteration 0900: loss: 1519.052
====> Epoch: 016 Train loss: 1516.6983  took : 8.527721643447876
====> Test loss: 1519.6089
iteration 0000: loss: 1516.085
iteration 0100: loss: 1516.560
iteration 0200: loss: 1515.128
iteration 0300: loss: 1515.118
iteration 0400: loss: 1515.094
iteration 0500: loss: 1515.319
iteration 0600: loss: 1517.133
iteration 0700: loss: 1518.499
iteration 0800: loss: 1517.108
iteration 0900: loss: 1517.536
====> Epoch: 017 Train loss: 1516.5325  took : 8.575441837310791
====> Test loss: 1519.3536
iteration 0000: loss: 1515.244
iteration 0100: loss: 1520.667
iteration 0200: loss: 1515.969
iteration 0300: loss: 1512.323
iteration 0400: loss: 1519.290
iteration 0500: loss: 1516.624
iteration 0600: loss: 1514.231
iteration 0700: loss: 1517.280
iteration 0800: loss: 1514.277
iteration 0900: loss: 1516.744
====> Epoch: 018 Train loss: 1516.3174  took : 8.415297508239746
====> Test loss: 1519.2673
iteration 0000: loss: 1514.807
iteration 0100: loss: 1518.287
iteration 0200: loss: 1516.677
iteration 0300: loss: 1517.137
iteration 0400: loss: 1517.091
iteration 0500: loss: 1515.228
iteration 0600: loss: 1515.733
iteration 0700: loss: 1516.297
iteration 0800: loss: 1519.321
iteration 0900: loss: 1516.028
====> Epoch: 019 Train loss: 1516.1522  took : 8.51690411567688
====> Test loss: 1519.3939
iteration 0000: loss: 1516.857
iteration 0100: loss: 1515.314
iteration 0200: loss: 1517.979
iteration 0300: loss: 1515.229
iteration 0400: loss: 1516.978
iteration 0500: loss: 1516.567
iteration 0600: loss: 1515.870
iteration 0700: loss: 1516.213
iteration 0800: loss: 1514.958
iteration 0900: loss: 1516.644
====> Epoch: 020 Train loss: 1516.0094  took : 8.503455400466919
====> Test loss: 1519.2978
iteration 0000: loss: 1516.489
iteration 0100: loss: 1517.648
iteration 0200: loss: 1517.624
iteration 0300: loss: 1516.715
iteration 0400: loss: 1517.768
iteration 0500: loss: 1519.657
iteration 0600: loss: 1517.312
iteration 0700: loss: 1515.001
iteration 0800: loss: 1514.379
iteration 0900: loss: 1516.947
====> Epoch: 021 Train loss: 1515.8768  took : 8.538747310638428
====> Test loss: 1519.0403
iteration 0000: loss: 1515.138
iteration 0100: loss: 1515.705
iteration 0200: loss: 1514.512
iteration 0300: loss: 1517.505
iteration 0400: loss: 1514.154
iteration 0500: loss: 1515.435
iteration 0600: loss: 1519.089
iteration 0700: loss: 1518.557
iteration 0800: loss: 1513.974
iteration 0900: loss: 1519.087
====> Epoch: 022 Train loss: 1515.8018  took : 8.507795095443726
====> Test loss: 1518.9751
iteration 0000: loss: 1514.421
iteration 0100: loss: 1515.819
iteration 0200: loss: 1518.516
iteration 0300: loss: 1511.437
iteration 0400: loss: 1516.174
iteration 0500: loss: 1514.900
iteration 0600: loss: 1516.569
iteration 0700: loss: 1515.202
iteration 0800: loss: 1515.648
iteration 0900: loss: 1513.352
====> Epoch: 023 Train loss: 1515.6882  took : 8.506040811538696
====> Test loss: 1518.8034
iteration 0000: loss: 1512.397
iteration 0100: loss: 1514.955
iteration 0200: loss: 1515.555
iteration 0300: loss: 1514.893
iteration 0400: loss: 1513.928
iteration 0500: loss: 1519.774
iteration 0600: loss: 1518.744
iteration 0700: loss: 1519.806
iteration 0800: loss: 1514.756
iteration 0900: loss: 1515.792
====> Epoch: 024 Train loss: 1515.5539  took : 8.51101279258728
====> Test loss: 1518.8307
iteration 0000: loss: 1514.763
iteration 0100: loss: 1515.278
iteration 0200: loss: 1514.102
iteration 0300: loss: 1512.986
iteration 0400: loss: 1514.236
iteration 0500: loss: 1518.484
iteration 0600: loss: 1513.955
iteration 0700: loss: 1517.936
iteration 0800: loss: 1515.620
iteration 0900: loss: 1516.189
====> Epoch: 025 Train loss: 1515.4663  took : 8.566969871520996
====> Test loss: 1518.9149
iteration 0000: loss: 1516.565
iteration 0100: loss: 1515.003
iteration 0200: loss: 1514.206
iteration 0300: loss: 1515.188
iteration 0400: loss: 1517.797
iteration 0500: loss: 1516.766
iteration 0600: loss: 1519.850
iteration 0700: loss: 1514.108
iteration 0800: loss: 1515.963
iteration 0900: loss: 1514.919
====> Epoch: 026 Train loss: 1515.5008  took : 8.53090500831604
====> Test loss: 1518.8870
iteration 0000: loss: 1514.408
iteration 0100: loss: 1517.529
iteration 0200: loss: 1513.190
iteration 0300: loss: 1514.046
iteration 0400: loss: 1515.550
iteration 0500: loss: 1515.698
iteration 0600: loss: 1516.298
iteration 0700: loss: 1514.298
iteration 0800: loss: 1513.075
iteration 0900: loss: 1513.631
====> Epoch: 027 Train loss: 1515.3324  took : 8.482253074645996
====> Test loss: 1518.5953
iteration 0000: loss: 1515.611
iteration 0100: loss: 1514.516
iteration 0200: loss: 1512.937
iteration 0300: loss: 1514.432
iteration 0400: loss: 1514.562
iteration 0500: loss: 1514.616
iteration 0600: loss: 1513.672
iteration 0700: loss: 1511.339
iteration 0800: loss: 1515.725
iteration 0900: loss: 1517.609
====> Epoch: 028 Train loss: 1515.2065  took : 8.548659324645996
====> Test loss: 1518.5710
iteration 0000: loss: 1515.041
iteration 0100: loss: 1515.877
iteration 0200: loss: 1515.812
iteration 0300: loss: 1513.645
iteration 0400: loss: 1514.679
iteration 0500: loss: 1513.706
iteration 0600: loss: 1517.246
iteration 0700: loss: 1517.609
iteration 0800: loss: 1514.846
iteration 0900: loss: 1515.122
====> Epoch: 029 Train loss: 1515.1255  took : 8.420788288116455
====> Test loss: 1518.5475
iteration 0000: loss: 1516.050
iteration 0100: loss: 1515.004
iteration 0200: loss: 1516.057
iteration 0300: loss: 1514.045
iteration 0400: loss: 1513.663
iteration 0500: loss: 1513.967
iteration 0600: loss: 1514.479
iteration 0700: loss: 1519.798
iteration 0800: loss: 1517.877
iteration 0900: loss: 1515.281
====> Epoch: 030 Train loss: 1515.0513  took : 8.54259729385376
====> Test loss: 1518.4858
iteration 0000: loss: 1513.942
iteration 0100: loss: 1517.116
iteration 0200: loss: 1511.408
iteration 0300: loss: 1512.794
iteration 0400: loss: 1513.855
iteration 0500: loss: 1516.262
iteration 0600: loss: 1516.051
iteration 0700: loss: 1514.493
iteration 0800: loss: 1512.934
iteration 0900: loss: 1516.094
====> Epoch: 031 Train loss: 1515.0138  took : 8.451392650604248
====> Test loss: 1518.5555
iteration 0000: loss: 1514.622
iteration 0100: loss: 1513.970
iteration 0200: loss: 1520.309
iteration 0300: loss: 1511.417
iteration 0400: loss: 1514.564
iteration 0500: loss: 1513.560
iteration 0600: loss: 1516.865
iteration 0700: loss: 1515.375
iteration 0800: loss: 1516.980
iteration 0900: loss: 1512.518
====> Epoch: 032 Train loss: 1514.9584  took : 8.49321699142456
====> Test loss: 1518.4478
iteration 0000: loss: 1517.163
iteration 0100: loss: 1516.323
iteration 0200: loss: 1514.954
iteration 0300: loss: 1515.667
iteration 0400: loss: 1514.021
iteration 0500: loss: 1517.289
iteration 0600: loss: 1514.275
iteration 0700: loss: 1512.561
iteration 0800: loss: 1513.985
iteration 0900: loss: 1513.540
====> Epoch: 033 Train loss: 1514.8637  took : 8.552085638046265
====> Test loss: 1518.5125
iteration 0000: loss: 1513.201
iteration 0100: loss: 1514.123
iteration 0200: loss: 1515.008
iteration 0300: loss: 1516.033
iteration 0400: loss: 1514.510
iteration 0500: loss: 1517.954
iteration 0600: loss: 1517.578
iteration 0700: loss: 1514.766
iteration 0800: loss: 1518.599
iteration 0900: loss: 1515.204
====> Epoch: 034 Train loss: 1514.8224  took : 8.442639589309692
====> Test loss: 1518.3581
iteration 0000: loss: 1513.368
iteration 0100: loss: 1517.001
iteration 0200: loss: 1512.319
iteration 0300: loss: 1515.000
iteration 0400: loss: 1516.306
iteration 0500: loss: 1516.707
iteration 0600: loss: 1515.604
iteration 0700: loss: 1514.714
iteration 0800: loss: 1515.189
iteration 0900: loss: 1516.338
====> Epoch: 035 Train loss: 1514.8337  took : 8.427379369735718
====> Test loss: 1518.5822
iteration 0000: loss: 1515.316
iteration 0100: loss: 1515.781
iteration 0200: loss: 1517.143
iteration 0300: loss: 1518.314
iteration 0400: loss: 1516.144
iteration 0500: loss: 1514.936
iteration 0600: loss: 1513.957
iteration 0700: loss: 1514.288
iteration 0800: loss: 1513.332
iteration 0900: loss: 1514.063
====> Epoch: 036 Train loss: 1514.7216  took : 8.467897415161133
====> Test loss: 1518.3456
iteration 0000: loss: 1511.406
iteration 0100: loss: 1513.004
iteration 0200: loss: 1511.635
iteration 0300: loss: 1513.907
iteration 0400: loss: 1513.752
iteration 0500: loss: 1514.878
iteration 0600: loss: 1511.667
iteration 0700: loss: 1514.535
iteration 0800: loss: 1515.007
iteration 0900: loss: 1514.584
====> Epoch: 037 Train loss: 1514.6412  took : 8.480434894561768
====> Test loss: 1518.3827
iteration 0000: loss: 1513.459
iteration 0100: loss: 1513.396
iteration 0200: loss: 1515.235
iteration 0300: loss: 1515.567
iteration 0400: loss: 1515.681
iteration 0500: loss: 1516.047
iteration 0600: loss: 1515.985
iteration 0700: loss: 1513.328
iteration 0800: loss: 1516.185
iteration 0900: loss: 1516.161
====> Epoch: 038 Train loss: 1514.5789  took : 8.498097896575928
====> Test loss: 1518.3455
iteration 0000: loss: 1514.855
iteration 0100: loss: 1517.258
iteration 0200: loss: 1515.527
iteration 0300: loss: 1516.310
iteration 0400: loss: 1515.387
iteration 0500: loss: 1514.738
iteration 0600: loss: 1514.566
iteration 0700: loss: 1514.233
iteration 0800: loss: 1515.412
iteration 0900: loss: 1514.571
====> Epoch: 039 Train loss: 1514.6098  took : 8.547698497772217
====> Test loss: 1518.2318
iteration 0000: loss: 1516.092
iteration 0100: loss: 1512.298
iteration 0200: loss: 1513.567
iteration 0300: loss: 1515.402
iteration 0400: loss: 1512.616
iteration 0500: loss: 1514.672
iteration 0600: loss: 1514.650
iteration 0700: loss: 1513.231
iteration 0800: loss: 1515.268
iteration 0900: loss: 1512.996
====> Epoch: 040 Train loss: 1514.4841  took : 8.407052278518677
====> Test loss: 1518.2051
iteration 0000: loss: 1515.080
iteration 0100: loss: 1514.546
iteration 0200: loss: 1515.114
iteration 0300: loss: 1516.283
iteration 0400: loss: 1511.560
iteration 0500: loss: 1514.399
iteration 0600: loss: 1515.819
iteration 0700: loss: 1515.998
iteration 0800: loss: 1513.826
iteration 0900: loss: 1512.389
====> Epoch: 041 Train loss: 1514.4475  took : 8.427956819534302
====> Test loss: 1518.3556
iteration 0000: loss: 1515.181
iteration 0100: loss: 1513.230
iteration 0200: loss: 1515.655
iteration 0300: loss: 1514.206
iteration 0400: loss: 1515.389
iteration 0500: loss: 1512.471
iteration 0600: loss: 1514.236
iteration 0700: loss: 1513.724
iteration 0800: loss: 1514.175
iteration 0900: loss: 1513.558
====> Epoch: 042 Train loss: 1514.3778  took : 8.405904054641724
====> Test loss: 1518.2905
iteration 0000: loss: 1512.975
iteration 0100: loss: 1511.697
iteration 0200: loss: 1512.400
iteration 0300: loss: 1517.604
iteration 0400: loss: 1514.071
iteration 0500: loss: 1512.231
iteration 0600: loss: 1516.562
iteration 0700: loss: 1516.465
iteration 0800: loss: 1514.333
iteration 0900: loss: 1515.968
====> Epoch: 043 Train loss: 1514.3195  took : 8.532223463058472
====> Test loss: 1518.2010
iteration 0000: loss: 1512.067
iteration 0100: loss: 1514.880
iteration 0200: loss: 1515.558
iteration 0300: loss: 1513.486
iteration 0400: loss: 1514.861
iteration 0500: loss: 1513.173
iteration 0600: loss: 1515.689
iteration 0700: loss: 1513.675
iteration 0800: loss: 1512.516
iteration 0900: loss: 1514.081
====> Epoch: 044 Train loss: 1514.2842  took : 8.457345485687256
====> Test loss: 1518.1821
iteration 0000: loss: 1511.406
iteration 0100: loss: 1513.720
iteration 0200: loss: 1516.717
iteration 0300: loss: 1514.745
iteration 0400: loss: 1518.117
iteration 0500: loss: 1512.349
iteration 0600: loss: 1514.893
iteration 0700: loss: 1515.529
iteration 0800: loss: 1515.718
iteration 0900: loss: 1515.547
====> Epoch: 045 Train loss: 1514.2648  took : 8.462633609771729
====> Test loss: 1518.0902
iteration 0000: loss: 1512.149
iteration 0100: loss: 1512.977
iteration 0200: loss: 1513.782
iteration 0300: loss: 1513.357
iteration 0400: loss: 1513.820
iteration 0500: loss: 1512.637
iteration 0600: loss: 1513.702
iteration 0700: loss: 1516.590
iteration 0800: loss: 1512.689
iteration 0900: loss: 1516.085
====> Epoch: 046 Train loss: 1514.1974  took : 8.47609567642212
====> Test loss: 1518.0433
iteration 0000: loss: 1512.996
iteration 0100: loss: 1513.345
iteration 0200: loss: 1515.170
iteration 0300: loss: 1515.136
iteration 0400: loss: 1515.195
iteration 0500: loss: 1511.975
iteration 0600: loss: 1515.364
iteration 0700: loss: 1513.347
iteration 0800: loss: 1513.284
iteration 0900: loss: 1510.387
====> Epoch: 047 Train loss: 1514.1542  took : 8.421558380126953
====> Test loss: 1518.2989
iteration 0000: loss: 1514.641
iteration 0100: loss: 1515.149
iteration 0200: loss: 1516.710
iteration 0300: loss: 1513.819
iteration 0400: loss: 1512.617
iteration 0500: loss: 1511.755
iteration 0600: loss: 1514.064
iteration 0700: loss: 1514.447
iteration 0800: loss: 1514.464
iteration 0900: loss: 1514.974
====> Epoch: 048 Train loss: 1514.1998  took : 8.490475177764893
====> Test loss: 1518.1344
iteration 0000: loss: 1517.443
iteration 0100: loss: 1514.786
iteration 0200: loss: 1514.685
iteration 0300: loss: 1514.097
iteration 0400: loss: 1514.645
iteration 0500: loss: 1514.029
iteration 0600: loss: 1513.223
iteration 0700: loss: 1516.161
iteration 0800: loss: 1513.060
iteration 0900: loss: 1512.331
====> Epoch: 049 Train loss: 1514.2360  took : 8.513434410095215
====> Test loss: 1518.2662
iteration 0000: loss: 1515.396
iteration 0100: loss: 1511.813
iteration 0200: loss: 1515.069
iteration 0300: loss: 1516.619
iteration 0400: loss: 1514.865
iteration 0500: loss: 1513.476
iteration 0600: loss: 1515.335
iteration 0700: loss: 1514.255
iteration 0800: loss: 1516.592
iteration 0900: loss: 1513.613
====> Epoch: 050 Train loss: 1514.0755  took : 8.552937507629395
====> Test loss: 1518.1149
====> [MM-VAE] Time: 510.412s or 00:08:30
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  9
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_9
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_9
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.511
iteration 0100: loss: 2109.958
iteration 0200: loss: 2033.979
iteration 0300: loss: 2018.874
iteration 0400: loss: 2006.231
iteration 0500: loss: 1998.475
iteration 0600: loss: 1999.829
iteration 0700: loss: 1994.802
iteration 0800: loss: 1995.959
iteration 0900: loss: 1993.434
====> Epoch: 001 Train loss: 2020.1147  took : 12.734755039215088
====> Test loss: 1993.0394
iteration 0000: loss: 1988.069
iteration 0100: loss: 1988.225
iteration 0200: loss: 1987.328
iteration 0300: loss: 1986.759
iteration 0400: loss: 1989.692
iteration 0500: loss: 1985.496
iteration 0600: loss: 1981.625
iteration 0700: loss: 1983.251
iteration 0800: loss: 1979.053
iteration 0900: loss: 1978.041
====> Epoch: 002 Train loss: 1981.8043  took : 12.817496061325073
====> Test loss: 1977.4197
iteration 0000: loss: 1977.772
iteration 0100: loss: 1971.340
iteration 0200: loss: 1969.352
iteration 0300: loss: 1970.197
iteration 0400: loss: 1970.539
iteration 0500: loss: 1974.046
iteration 0600: loss: 1971.937
iteration 0700: loss: 1970.678
iteration 0800: loss: 1967.531
iteration 0900: loss: 1964.915
====> Epoch: 003 Train loss: 1969.6652  took : 12.660759687423706
====> Test loss: 1967.0760
iteration 0000: loss: 1963.235
iteration 0100: loss: 1963.940
iteration 0200: loss: 1966.077
iteration 0300: loss: 1965.393
iteration 0400: loss: 1965.825
iteration 0500: loss: 1963.336
iteration 0600: loss: 1962.154
iteration 0700: loss: 1960.643
iteration 0800: loss: 1965.694
iteration 0900: loss: 1957.998
====> Epoch: 004 Train loss: 1963.0085  took : 12.42062520980835
====> Test loss: 1962.1602
iteration 0000: loss: 1962.675
iteration 0100: loss: 1958.698
iteration 0200: loss: 1958.695
iteration 0300: loss: 1961.006
iteration 0400: loss: 1961.152
iteration 0500: loss: 1959.587
iteration 0600: loss: 1955.577
iteration 0700: loss: 1958.769
iteration 0800: loss: 1958.923
iteration 0900: loss: 1958.219
====> Epoch: 005 Train loss: 1958.5315  took : 13.080833673477173
====> Test loss: 1959.9120
iteration 0000: loss: 1955.696
iteration 0100: loss: 1958.191
iteration 0200: loss: 1956.801
iteration 0300: loss: 1954.540
iteration 0400: loss: 1955.323
iteration 0500: loss: 1956.205
iteration 0600: loss: 1956.334
iteration 0700: loss: 1954.884
iteration 0800: loss: 1956.125
iteration 0900: loss: 1954.872
====> Epoch: 006 Train loss: 1955.8157  took : 12.173309803009033
====> Test loss: 1956.9710
iteration 0000: loss: 1954.788
iteration 0100: loss: 1956.930
iteration 0200: loss: 1955.614
iteration 0300: loss: 1953.841
iteration 0400: loss: 1955.521
iteration 0500: loss: 1952.966
iteration 0600: loss: 1952.711
iteration 0700: loss: 1955.448
iteration 0800: loss: 1956.174
iteration 0900: loss: 1953.738
====> Epoch: 007 Train loss: 1954.0401  took : 12.16421890258789
====> Test loss: 1955.9438
iteration 0000: loss: 1952.595
iteration 0100: loss: 1954.773
iteration 0200: loss: 1953.655
iteration 0300: loss: 1952.681
iteration 0400: loss: 1951.652
iteration 0500: loss: 1952.932
iteration 0600: loss: 1951.330
iteration 0700: loss: 1951.760
iteration 0800: loss: 1951.167
iteration 0900: loss: 1953.139
====> Epoch: 008 Train loss: 1952.8070  took : 12.046486139297485
====> Test loss: 1955.3228
iteration 0000: loss: 1952.679
iteration 0100: loss: 1952.874
iteration 0200: loss: 1952.632
iteration 0300: loss: 1951.196
iteration 0400: loss: 1952.096
iteration 0500: loss: 1953.587
iteration 0600: loss: 1950.353
iteration 0700: loss: 1950.513
iteration 0800: loss: 1949.456
iteration 0900: loss: 1951.807
====> Epoch: 009 Train loss: 1951.7773  took : 11.961766242980957
====> Test loss: 1953.3123
iteration 0000: loss: 1951.320
iteration 0100: loss: 1950.678
iteration 0200: loss: 1949.816
iteration 0300: loss: 1952.484
iteration 0400: loss: 1951.412
iteration 0500: loss: 1949.083
iteration 0600: loss: 1951.097
iteration 0700: loss: 1952.791
iteration 0800: loss: 1949.671
iteration 0900: loss: 1948.738
====> Epoch: 010 Train loss: 1951.0688  took : 12.084318399429321
====> Test loss: 1952.5092
iteration 0000: loss: 1949.830
iteration 0100: loss: 1950.703
iteration 0200: loss: 1950.803
iteration 0300: loss: 1952.536
iteration 0400: loss: 1950.281
iteration 0500: loss: 1950.694
iteration 0600: loss: 1950.315
iteration 0700: loss: 1949.657
iteration 0800: loss: 1949.320
iteration 0900: loss: 1949.801
====> Epoch: 011 Train loss: 1950.3614  took : 11.929551124572754
====> Test loss: 1952.8427
iteration 0000: loss: 1949.370
iteration 0100: loss: 1949.657
iteration 0200: loss: 1950.087
iteration 0300: loss: 1949.580
iteration 0400: loss: 1951.665
iteration 0500: loss: 1950.497
iteration 0600: loss: 1948.856
iteration 0700: loss: 1951.428
iteration 0800: loss: 1949.603
iteration 0900: loss: 1950.463
====> Epoch: 012 Train loss: 1950.0097  took : 13.094771385192871
====> Test loss: 1951.8919
iteration 0000: loss: 1948.893
iteration 0100: loss: 1949.634
iteration 0200: loss: 1949.329
iteration 0300: loss: 1950.452
iteration 0400: loss: 1950.262
iteration 0500: loss: 1949.211
iteration 0600: loss: 1948.918
iteration 0700: loss: 1949.625
iteration 0800: loss: 1950.088
iteration 0900: loss: 1949.655
====> Epoch: 013 Train loss: 1949.5629  took : 11.821768522262573
====> Test loss: 1951.6848
iteration 0000: loss: 1949.833
iteration 0100: loss: 1947.488
iteration 0200: loss: 1948.784
iteration 0300: loss: 1950.000
iteration 0400: loss: 1949.674
iteration 0500: loss: 1949.807
iteration 0600: loss: 1948.983
iteration 0700: loss: 1949.132
iteration 0800: loss: 1949.031
iteration 0900: loss: 1949.537
====> Epoch: 014 Train loss: 1949.2366  took : 12.191569566726685
====> Test loss: 1951.1193
iteration 0000: loss: 1948.662
iteration 0100: loss: 1948.057
iteration 0200: loss: 1948.448
iteration 0300: loss: 1948.029
iteration 0400: loss: 1948.262
iteration 0500: loss: 1948.793
iteration 0600: loss: 1948.542
iteration 0700: loss: 1950.315
iteration 0800: loss: 1949.376
iteration 0900: loss: 1948.250
====> Epoch: 015 Train loss: 1948.9302  took : 12.842049360275269
====> Test loss: 1950.7232
iteration 0000: loss: 1948.817
iteration 0100: loss: 1947.572
iteration 0200: loss: 1948.925
iteration 0300: loss: 1949.915
iteration 0400: loss: 1948.692
iteration 0500: loss: 1948.776
iteration 0600: loss: 1949.041
iteration 0700: loss: 1948.265
iteration 0800: loss: 1948.255
iteration 0900: loss: 1948.910
====> Epoch: 016 Train loss: 1948.7128  took : 12.484426975250244
====> Test loss: 1950.4231
iteration 0000: loss: 1948.703
iteration 0100: loss: 1948.987
iteration 0200: loss: 1948.402
iteration 0300: loss: 1949.186
iteration 0400: loss: 1948.501
iteration 0500: loss: 1948.821
iteration 0600: loss: 1947.625
iteration 0700: loss: 1948.159
iteration 0800: loss: 1948.635
iteration 0900: loss: 1947.670
====> Epoch: 017 Train loss: 1948.6389  took : 12.764530420303345
====> Test loss: 1950.9906
iteration 0000: loss: 1949.076
iteration 0100: loss: 1949.510
iteration 0200: loss: 1948.535
iteration 0300: loss: 1947.784
iteration 0400: loss: 1949.149
iteration 0500: loss: 1947.503
iteration 0600: loss: 1948.762
iteration 0700: loss: 1948.822
iteration 0800: loss: 1947.567
iteration 0900: loss: 1948.302
====> Epoch: 018 Train loss: 1948.7073  took : 12.94631028175354
====> Test loss: 1950.9346
iteration 0000: loss: 1948.373
iteration 0100: loss: 1947.645
iteration 0200: loss: 1948.434
iteration 0300: loss: 1948.369
iteration 0400: loss: 1948.189
iteration 0500: loss: 1948.495
iteration 0600: loss: 1948.086
iteration 0700: loss: 1948.381
iteration 0800: loss: 1948.403
iteration 0900: loss: 1949.300
====> Epoch: 019 Train loss: 1948.5609  took : 11.995689153671265
====> Test loss: 1950.6124
iteration 0000: loss: 1947.266
iteration 0100: loss: 1950.442
iteration 0200: loss: 1947.747
iteration 0300: loss: 1949.421
iteration 0400: loss: 1947.916
iteration 0500: loss: 1948.889
iteration 0600: loss: 1948.196
iteration 0700: loss: 1947.841
iteration 0800: loss: 1947.645
iteration 0900: loss: 1948.554
====> Epoch: 020 Train loss: 1948.2914  took : 11.940143346786499
====> Test loss: 1950.5016
iteration 0000: loss: 1949.169
iteration 0100: loss: 1949.209
iteration 0200: loss: 1947.944
iteration 0300: loss: 1949.062
iteration 0400: loss: 1948.676
iteration 0500: loss: 1948.409
iteration 0600: loss: 1947.596
iteration 0700: loss: 1950.778
iteration 0800: loss: 1947.692
iteration 0900: loss: 1947.497
====> Epoch: 021 Train loss: 1948.2675  took : 11.805554866790771
====> Test loss: 1950.4243
iteration 0000: loss: 1947.417
iteration 0100: loss: 1947.955
iteration 0200: loss: 1948.623
iteration 0300: loss: 1947.720
iteration 0400: loss: 1947.833
iteration 0500: loss: 1948.319
iteration 0600: loss: 1948.218
iteration 0700: loss: 1948.701
iteration 0800: loss: 1948.594
iteration 0900: loss: 1948.017
====> Epoch: 022 Train loss: 1948.3126  took : 11.960497617721558
====> Test loss: 1950.0673
iteration 0000: loss: 1947.610
iteration 0100: loss: 1947.884
iteration 0200: loss: 1947.843
iteration 0300: loss: 1948.453
iteration 0400: loss: 1947.823
iteration 0500: loss: 1949.557
iteration 0600: loss: 1947.766
iteration 0700: loss: 1948.098
iteration 0800: loss: 1948.932
iteration 0900: loss: 1948.873
====> Epoch: 023 Train loss: 1948.1691  took : 11.810725688934326
====> Test loss: 1950.1153
iteration 0000: loss: 1947.920
iteration 0100: loss: 1948.048
iteration 0200: loss: 1947.176
iteration 0300: loss: 1947.976
iteration 0400: loss: 1947.627
iteration 0500: loss: 1948.209
iteration 0600: loss: 1948.270
iteration 0700: loss: 1947.719
iteration 0800: loss: 1947.474
iteration 0900: loss: 1948.222
====> Epoch: 024 Train loss: 1948.0554  took : 13.382249116897583
====> Test loss: 1950.1180
iteration 0000: loss: 1948.015
iteration 0100: loss: 1947.986
iteration 0200: loss: 1947.905
iteration 0300: loss: 1948.311
iteration 0400: loss: 1948.413
iteration 0500: loss: 1947.681
iteration 0600: loss: 1947.905
iteration 0700: loss: 1948.277
iteration 0800: loss: 1947.518
iteration 0900: loss: 1947.705
====> Epoch: 025 Train loss: 1948.0264  took : 12.134974002838135
====> Test loss: 1949.9684
iteration 0000: loss: 1948.383
iteration 0100: loss: 1947.749
iteration 0200: loss: 1947.867
iteration 0300: loss: 1948.622
iteration 0400: loss: 1947.165
iteration 0500: loss: 1948.340
iteration 0600: loss: 1947.579
iteration 0700: loss: 1947.282
iteration 0800: loss: 1948.634
iteration 0900: loss: 1947.625
====> Epoch: 026 Train loss: 1947.7656  took : 13.214181900024414
====> Test loss: 1949.7216
iteration 0000: loss: 1947.947
iteration 0100: loss: 1948.719
iteration 0200: loss: 1947.438
iteration 0300: loss: 1947.630
iteration 0400: loss: 1948.204
iteration 0500: loss: 1948.544
iteration 0600: loss: 1947.451
iteration 0700: loss: 1947.301
iteration 0800: loss: 1948.035
iteration 0900: loss: 1947.238
====> Epoch: 027 Train loss: 1947.9104  took : 12.479374885559082
====> Test loss: 1949.8743
iteration 0000: loss: 1947.699
iteration 0100: loss: 1949.328
iteration 0200: loss: 1948.307
iteration 0300: loss: 1947.908
iteration 0400: loss: 1948.242
iteration 0500: loss: 1947.203
iteration 0600: loss: 1947.619
iteration 0700: loss: 1947.254
iteration 0800: loss: 1947.973
iteration 0900: loss: 1949.640
====> Epoch: 028 Train loss: 1947.9462  took : 12.337583780288696
====> Test loss: 1950.5058
iteration 0000: loss: 1948.718
iteration 0100: loss: 1948.258
iteration 0200: loss: 1949.128
iteration 0300: loss: 1947.884
iteration 0400: loss: 1947.493
iteration 0500: loss: 1947.129
iteration 0600: loss: 1947.615
iteration 0700: loss: 1947.353
iteration 0800: loss: 1947.488
iteration 0900: loss: 1947.162
====> Epoch: 029 Train loss: 1947.7394  took : 13.048423767089844
====> Test loss: 1949.9895
iteration 0000: loss: 1947.852
iteration 0100: loss: 1947.816
iteration 0200: loss: 1947.378
iteration 0300: loss: 1947.126
iteration 0400: loss: 1947.165
iteration 0500: loss: 1947.359
iteration 0600: loss: 1947.625
iteration 0700: loss: 1947.903
iteration 0800: loss: 1947.339
iteration 0900: loss: 1947.883
====> Epoch: 030 Train loss: 1947.6930  took : 12.233038425445557
====> Test loss: 1949.7404
iteration 0000: loss: 1949.184
iteration 0100: loss: 1946.939
iteration 0200: loss: 1947.222
iteration 0300: loss: 1947.231
iteration 0400: loss: 1946.576
iteration 0500: loss: 1947.953
iteration 0600: loss: 1947.333
iteration 0700: loss: 1947.718
iteration 0800: loss: 1947.638
iteration 0900: loss: 1947.692
====> Epoch: 031 Train loss: 1947.6838  took : 13.06259536743164
====> Test loss: 1949.4532
iteration 0000: loss: 1947.496
iteration 0100: loss: 1947.444
iteration 0200: loss: 1947.531
iteration 0300: loss: 1947.377
iteration 0400: loss: 1947.684
iteration 0500: loss: 1947.758
iteration 0600: loss: 1948.298
iteration 0700: loss: 1947.741
iteration 0800: loss: 1947.518
iteration 0900: loss: 1947.689
====> Epoch: 032 Train loss: 1947.7072  took : 12.778264284133911
====> Test loss: 1949.5830
iteration 0000: loss: 1947.971
iteration 0100: loss: 1946.556
iteration 0200: loss: 1948.224
iteration 0300: loss: 1948.394
iteration 0400: loss: 1947.234
iteration 0500: loss: 1947.459
iteration 0600: loss: 1947.119
iteration 0700: loss: 1947.399
iteration 0800: loss: 1946.935
iteration 0900: loss: 1946.905
====> Epoch: 033 Train loss: 1947.5365  took : 11.756929159164429
====> Test loss: 1949.5383
iteration 0000: loss: 1947.582
iteration 0100: loss: 1947.733
iteration 0200: loss: 1947.330
iteration 0300: loss: 1947.856
iteration 0400: loss: 1947.447
iteration 0500: loss: 1946.982
iteration 0600: loss: 1946.955
iteration 0700: loss: 1947.207
iteration 0800: loss: 1948.658
iteration 0900: loss: 1947.075
====> Epoch: 034 Train loss: 1947.6274  took : 12.528578758239746
====> Test loss: 1949.2283
iteration 0000: loss: 1946.989
iteration 0100: loss: 1947.201
iteration 0200: loss: 1949.037
iteration 0300: loss: 1947.730
iteration 0400: loss: 1946.841
iteration 0500: loss: 1948.127
iteration 0600: loss: 1947.461
iteration 0700: loss: 1947.830
iteration 0800: loss: 1947.624
iteration 0900: loss: 1948.833
====> Epoch: 035 Train loss: 1947.7493  took : 12.728860855102539
====> Test loss: 1949.6397
iteration 0000: loss: 1947.398
iteration 0100: loss: 1947.338
iteration 0200: loss: 1947.512
iteration 0300: loss: 1947.862
iteration 0400: loss: 1947.374
iteration 0500: loss: 1947.151
iteration 0600: loss: 1947.650
iteration 0700: loss: 1947.705
iteration 0800: loss: 1946.781
iteration 0900: loss: 1946.790
====> Epoch: 036 Train loss: 1947.4791  took : 12.175555229187012
====> Test loss: 1949.4420
iteration 0000: loss: 1946.811
iteration 0100: loss: 1947.411
iteration 0200: loss: 1947.276
iteration 0300: loss: 1947.992
iteration 0400: loss: 1947.176
iteration 0500: loss: 1946.771
iteration 0600: loss: 1947.030
iteration 0700: loss: 1947.269
iteration 0800: loss: 1946.923
iteration 0900: loss: 1947.447
====> Epoch: 037 Train loss: 1947.4889  took : 12.226236581802368
====> Test loss: 1949.4128
iteration 0000: loss: 1947.094
iteration 0100: loss: 1947.544
iteration 0200: loss: 1948.329
iteration 0300: loss: 1948.597
iteration 0400: loss: 1947.589
iteration 0500: loss: 1947.445
iteration 0600: loss: 1946.751
iteration 0700: loss: 1947.285
iteration 0800: loss: 1947.139
iteration 0900: loss: 1947.985
====> Epoch: 038 Train loss: 1947.6943  took : 12.682151317596436
====> Test loss: 1950.6348
iteration 0000: loss: 1948.399
iteration 0100: loss: 1947.321
iteration 0200: loss: 1947.328
iteration 0300: loss: 1947.334
iteration 0400: loss: 1948.307
iteration 0500: loss: 1947.805
iteration 0600: loss: 1948.103
iteration 0700: loss: 1946.753
iteration 0800: loss: 1947.074
iteration 0900: loss: 1947.315
====> Epoch: 039 Train loss: 1947.6146  took : 12.421511888504028
====> Test loss: 1949.2535
iteration 0000: loss: 1947.287
iteration 0100: loss: 1947.555
iteration 0200: loss: 1947.176
iteration 0300: loss: 1947.587
iteration 0400: loss: 1947.460
iteration 0500: loss: 1947.415
iteration 0600: loss: 1946.910
iteration 0700: loss: 1946.856
iteration 0800: loss: 1946.879
iteration 0900: loss: 1946.842
====> Epoch: 040 Train loss: 1947.5493  took : 12.473591327667236
====> Test loss: 1949.3663
iteration 0000: loss: 1947.136
iteration 0100: loss: 1946.595
iteration 0200: loss: 1948.018
iteration 0300: loss: 1948.019
iteration 0400: loss: 1947.463
iteration 0500: loss: 1947.932
iteration 0600: loss: 1947.374
iteration 0700: loss: 1946.951
iteration 0800: loss: 1947.896
iteration 0900: loss: 1948.529
====> Epoch: 041 Train loss: 1947.5910  took : 12.226516008377075
====> Test loss: 1949.8349
iteration 0000: loss: 1948.153
iteration 0100: loss: 1946.596
iteration 0200: loss: 1946.868
iteration 0300: loss: 1946.605
iteration 0400: loss: 1946.990
iteration 0500: loss: 1947.769
iteration 0600: loss: 1946.856
iteration 0700: loss: 1946.893
iteration 0800: loss: 1947.734
iteration 0900: loss: 1947.257
====> Epoch: 042 Train loss: 1947.4199  took : 12.060624122619629
====> Test loss: 1949.6818
iteration 0000: loss: 1947.808
iteration 0100: loss: 1947.210
iteration 0200: loss: 1947.566
iteration 0300: loss: 1947.444
iteration 0400: loss: 1947.542
iteration 0500: loss: 1947.481
iteration 0600: loss: 1946.888
iteration 0700: loss: 1948.814
iteration 0800: loss: 1946.975
iteration 0900: loss: 1946.976
====> Epoch: 043 Train loss: 1947.4587  took : 12.353582382202148
====> Test loss: 1949.4210
iteration 0000: loss: 1946.951
iteration 0100: loss: 1947.208
iteration 0200: loss: 1947.308
iteration 0300: loss: 1947.434
iteration 0400: loss: 1947.074
iteration 0500: loss: 1948.490
iteration 0600: loss: 1947.686
iteration 0700: loss: 1948.242
iteration 0800: loss: 1947.406
iteration 0900: loss: 1946.919
====> Epoch: 044 Train loss: 1947.4465  took : 12.48829436302185
====> Test loss: 1949.1138
iteration 0000: loss: 1947.070
iteration 0100: loss: 1947.299
iteration 0200: loss: 1947.480
iteration 0300: loss: 1947.245
iteration 0400: loss: 1946.964
iteration 0500: loss: 1947.471
iteration 0600: loss: 1947.189
iteration 0700: loss: 1948.773
iteration 0800: loss: 1948.163
iteration 0900: loss: 1947.643
====> Epoch: 045 Train loss: 1947.4469  took : 12.010496854782104
====> Test loss: 1949.4032
iteration 0000: loss: 1947.124
iteration 0100: loss: 1947.318
iteration 0200: loss: 1947.422
iteration 0300: loss: 1947.839
iteration 0400: loss: 1946.794
iteration 0500: loss: 1946.782
iteration 0600: loss: 1947.892
iteration 0700: loss: 1947.546
iteration 0800: loss: 1947.567
iteration 0900: loss: 1948.020
====> Epoch: 046 Train loss: 1947.5286  took : 12.106921672821045
====> Test loss: 1949.4725
iteration 0000: loss: 1948.058
iteration 0100: loss: 1946.881
iteration 0200: loss: 1947.356
iteration 0300: loss: 1947.048
iteration 0400: loss: 1947.340
iteration 0500: loss: 1947.490
iteration 0600: loss: 1948.115
iteration 0700: loss: 1947.221
iteration 0800: loss: 1946.802
iteration 0900: loss: 1946.974
====> Epoch: 047 Train loss: 1947.4597  took : 13.028554916381836
====> Test loss: 1949.6356
iteration 0000: loss: 1948.286
iteration 0100: loss: 1946.938
iteration 0200: loss: 1947.328
iteration 0300: loss: 1948.298
iteration 0400: loss: 1947.445
iteration 0500: loss: 1947.610
iteration 0600: loss: 1948.349
iteration 0700: loss: 1947.851
iteration 0800: loss: 1947.281
iteration 0900: loss: 1947.503
====> Epoch: 048 Train loss: 1947.4511  took : 12.16807508468628
====> Test loss: 1949.1937
iteration 0000: loss: 1946.807
iteration 0100: loss: 1947.723
iteration 0200: loss: 1946.767
iteration 0300: loss: 1947.858
iteration 0400: loss: 1947.046
iteration 0500: loss: 1947.368
iteration 0600: loss: 1947.068
iteration 0700: loss: 1947.687
iteration 0800: loss: 1947.195
iteration 0900: loss: 1947.089
====> Epoch: 049 Train loss: 1947.4431  took : 11.426955938339233
====> Test loss: 1949.4054
iteration 0000: loss: 1947.522
iteration 0100: loss: 1947.569
iteration 0200: loss: 1947.361
iteration 0300: loss: 1947.431
iteration 0400: loss: 1948.121
iteration 0500: loss: 1947.489
iteration 0600: loss: 1947.242
iteration 0700: loss: 1946.507
iteration 0800: loss: 1947.304
iteration 0900: loss: 1946.876
====> Epoch: 050 Train loss: 1947.3284  took : 12.987637758255005
====> Test loss: 1949.8854
====> [MM-VAE] Time: 694.972s or 00:11:34
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  9
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_9
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_9
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5219.763
iteration 0100: loss: 4215.701
iteration 0200: loss: 4174.511
iteration 0300: loss: 4201.444
iteration 0400: loss: 4187.074
iteration 0500: loss: 4202.278
iteration 0600: loss: 4126.603
iteration 0700: loss: 4132.888
iteration 0800: loss: 4141.396
iteration 0900: loss: 4110.376
iteration 1000: loss: 4096.047
iteration 1100: loss: 4128.073
iteration 1200: loss: 4107.518
iteration 1300: loss: 4088.885
iteration 1400: loss: 4091.483
iteration 1500: loss: 4070.592
iteration 1600: loss: 4128.151
iteration 1700: loss: 4092.677
iteration 1800: loss: 4131.847
====> Epoch: 001 Train loss: 4140.6516  took : 53.09714412689209
====> Test loss: 4103.6702
iteration 0000: loss: 4133.049
iteration 0100: loss: 4103.758
iteration 0200: loss: 4115.374
iteration 0300: loss: 4113.180
iteration 0400: loss: 4118.988
iteration 0500: loss: 4093.801
iteration 0600: loss: 4121.181
iteration 0700: loss: 4089.888
iteration 0800: loss: 4098.405
iteration 0900: loss: 4128.234
iteration 1000: loss: 4094.645
iteration 1100: loss: 4115.180
iteration 1200: loss: 4084.991
iteration 1300: loss: 4098.225
iteration 1400: loss: 4104.906
iteration 1500: loss: 4090.476
iteration 1600: loss: 4086.630
iteration 1700: loss: 4115.389
iteration 1800: loss: 4132.969
====> Epoch: 002 Train loss: 4101.3639  took : 53.13719987869263
====> Test loss: 4094.5323
iteration 0000: loss: 4097.499
iteration 0100: loss: 4113.332
iteration 0200: loss: 4116.001
iteration 0300: loss: 4098.708
iteration 0400: loss: 4082.349
iteration 0500: loss: 4107.905
iteration 0600: loss: 4084.430
iteration 0700: loss: 4094.868
iteration 0800: loss: 4099.041
iteration 0900: loss: 4115.213
iteration 1000: loss: 4075.590
iteration 1100: loss: 4085.374
iteration 1200: loss: 4094.478
iteration 1300: loss: 4108.485
iteration 1400: loss: 4110.913
iteration 1500: loss: 4110.754
iteration 1600: loss: 4092.592
iteration 1700: loss: 4112.148
iteration 1800: loss: 4081.980
====> Epoch: 003 Train loss: 4096.2220  took : 53.04429531097412
====> Test loss: 4091.9553
iteration 0000: loss: 4084.625
iteration 0100: loss: 4098.417
iteration 0200: loss: 4131.631
iteration 0300: loss: 4081.017
iteration 0400: loss: 4071.306
iteration 0500: loss: 4068.643
iteration 0600: loss: 4127.278
iteration 0700: loss: 4072.683
iteration 0800: loss: 4072.806
iteration 0900: loss: 4107.845
iteration 1000: loss: 4117.793
iteration 1100: loss: 4108.021
iteration 1200: loss: 4116.566
iteration 1300: loss: 4099.612
iteration 1400: loss: 4119.242
iteration 1500: loss: 4089.716
iteration 1600: loss: 4105.820
iteration 1700: loss: 4057.567
iteration 1800: loss: 4076.692
====> Epoch: 004 Train loss: 4094.3788  took : 53.08666253089905
====> Test loss: 4090.8278
iteration 0000: loss: 4073.111
iteration 0100: loss: 4103.368
iteration 0200: loss: 4070.905
iteration 0300: loss: 4089.126
iteration 0400: loss: 4074.517
iteration 0500: loss: 4071.633
iteration 0600: loss: 4140.205
iteration 0700: loss: 4111.469
iteration 0800: loss: 4109.589
iteration 0900: loss: 4095.062
iteration 1000: loss: 4078.195
iteration 1100: loss: 4069.205
iteration 1200: loss: 4065.331
iteration 1300: loss: 4100.060
iteration 1400: loss: 4063.203
iteration 1500: loss: 4057.532
iteration 1600: loss: 4093.329
iteration 1700: loss: 4066.738
iteration 1800: loss: 4102.318
====> Epoch: 005 Train loss: 4092.8636  took : 53.083172082901
====> Test loss: 4089.4874
iteration 0000: loss: 4076.398
iteration 0100: loss: 4073.370
iteration 0200: loss: 4107.450
iteration 0300: loss: 4102.011
iteration 0400: loss: 4094.552
iteration 0500: loss: 4117.479
iteration 0600: loss: 4068.624
iteration 0700: loss: 4112.699
iteration 0800: loss: 4073.393
iteration 0900: loss: 4092.812
iteration 1000: loss: 4081.415
iteration 1100: loss: 4110.767
iteration 1200: loss: 4086.992
iteration 1300: loss: 4088.828
iteration 1400: loss: 4089.442
iteration 1500: loss: 4115.684
iteration 1600: loss: 4066.703
iteration 1700: loss: 4097.511
iteration 1800: loss: 4073.037
====> Epoch: 006 Train loss: 4091.5658  took : 53.066991567611694
====> Test loss: 4088.3971
iteration 0000: loss: 4053.419
iteration 0100: loss: 4094.963
iteration 0200: loss: 4095.106
iteration 0300: loss: 4114.729
iteration 0400: loss: 4099.374
iteration 0500: loss: 4095.037
iteration 0600: loss: 4110.800
iteration 0700: loss: 4094.282
iteration 0800: loss: 4097.229
iteration 0900: loss: 4108.494
iteration 1000: loss: 4068.389
iteration 1100: loss: 4078.788
iteration 1200: loss: 4066.164
iteration 1300: loss: 4077.880
iteration 1400: loss: 4114.196
iteration 1500: loss: 4099.592
iteration 1600: loss: 4079.745
iteration 1700: loss: 4118.549
iteration 1800: loss: 4120.290
====> Epoch: 007 Train loss: 4090.5562  took : 53.03577637672424
====> Test loss: 4087.6036
iteration 0000: loss: 4104.562
iteration 0100: loss: 4093.190
iteration 0200: loss: 4078.207
iteration 0300: loss: 4063.708
iteration 0400: loss: 4076.657
iteration 0500: loss: 4085.681
iteration 0600: loss: 4092.694
iteration 0700: loss: 4097.036
iteration 0800: loss: 4094.771
iteration 0900: loss: 4071.163
iteration 1000: loss: 4054.994
iteration 1100: loss: 4125.910
iteration 1200: loss: 4055.162
iteration 1300: loss: 4077.164
iteration 1400: loss: 4108.414
iteration 1500: loss: 4071.401
iteration 1600: loss: 4123.538
iteration 1700: loss: 4095.018
iteration 1800: loss: 4084.247
====> Epoch: 008 Train loss: 4089.6500  took : 53.13959074020386
====> Test loss: 4086.8499
iteration 0000: loss: 4075.812
iteration 0100: loss: 4099.306
iteration 0200: loss: 4096.750
iteration 0300: loss: 4061.759
iteration 0400: loss: 4085.216
iteration 0500: loss: 4072.977
iteration 0600: loss: 4112.113
iteration 0700: loss: 4086.675
iteration 0800: loss: 4075.052
iteration 0900: loss: 4114.042
iteration 1000: loss: 4096.497
iteration 1100: loss: 4085.075
iteration 1200: loss: 4068.402
iteration 1300: loss: 4090.220
iteration 1400: loss: 4087.337
iteration 1500: loss: 4086.701
iteration 1600: loss: 4085.726
iteration 1700: loss: 4094.794
iteration 1800: loss: 4083.866
====> Epoch: 009 Train loss: 4088.8743  took : 52.98355412483215
====> Test loss: 4086.3118
iteration 0000: loss: 4081.168
iteration 0100: loss: 4071.625
iteration 0200: loss: 4069.368
iteration 0300: loss: 4075.712
iteration 0400: loss: 4114.390
iteration 0500: loss: 4075.125
iteration 0600: loss: 4059.321
iteration 0700: loss: 4072.080
iteration 0800: loss: 4099.769
iteration 0900: loss: 4113.483
iteration 1000: loss: 4078.904
iteration 1100: loss: 4086.450
iteration 1200: loss: 4095.968
iteration 1300: loss: 4075.818
iteration 1400: loss: 4080.182
iteration 1500: loss: 4069.859
iteration 1600: loss: 4093.556
iteration 1700: loss: 4088.092
iteration 1800: loss: 4089.747
====> Epoch: 010 Train loss: 4088.3879  took : 53.14846897125244
====> Test loss: 4086.1775
iteration 0000: loss: 4100.926
iteration 0100: loss: 4094.120
iteration 0200: loss: 4071.038
iteration 0300: loss: 4114.377
iteration 0400: loss: 4109.707
iteration 0500: loss: 4073.048
iteration 0600: loss: 4093.713
iteration 0700: loss: 4072.865
iteration 0800: loss: 4082.918
iteration 0900: loss: 4075.430
iteration 1000: loss: 4099.687
iteration 1100: loss: 4080.219
iteration 1200: loss: 4104.522
iteration 1300: loss: 4079.659
iteration 1400: loss: 4096.469
iteration 1500: loss: 4060.563
iteration 1600: loss: 4086.161
iteration 1700: loss: 4097.591
iteration 1800: loss: 4099.994
====> Epoch: 011 Train loss: 4088.4861  took : 53.17218828201294
====> Test loss: 4086.6717
iteration 0000: loss: 4096.280
iteration 0100: loss: 4093.549
iteration 0200: loss: 4100.777
iteration 0300: loss: 4081.625
iteration 0400: loss: 4078.930
iteration 0500: loss: 4059.262
iteration 0600: loss: 4077.951
iteration 0700: loss: 4072.308
iteration 0800: loss: 4090.310
iteration 0900: loss: 4097.795
iteration 1000: loss: 4136.141
iteration 1100: loss: 4065.859
iteration 1200: loss: 4086.819
iteration 1300: loss: 4099.596
iteration 1400: loss: 4073.146
iteration 1500: loss: 4099.821
iteration 1600: loss: 4096.557
iteration 1700: loss: 4096.994
iteration 1800: loss: 4103.346
====> Epoch: 012 Train loss: 4088.4724  took : 53.02257490158081
====> Test loss: 4086.5235
iteration 0000: loss: 4074.161
iteration 0100: loss: 4094.095
iteration 0200: loss: 4095.951
iteration 0300: loss: 4071.669
iteration 0400: loss: 4095.775
iteration 0500: loss: 4092.250
iteration 0600: loss: 4099.907
iteration 0700: loss: 4113.094
iteration 0800: loss: 4064.070
iteration 0900: loss: 4069.122
iteration 1000: loss: 4080.872
iteration 1100: loss: 4070.365
iteration 1200: loss: 4089.639
iteration 1300: loss: 4087.173
iteration 1400: loss: 4084.986
iteration 1500: loss: 4100.642
iteration 1600: loss: 4082.816
iteration 1700: loss: 4085.726
iteration 1800: loss: 4090.743
====> Epoch: 013 Train loss: 4088.9048  took : 52.889914989471436
====> Test loss: 4086.7367
iteration 0000: loss: 4083.242
iteration 0100: loss: 4101.428
iteration 0200: loss: 4089.754
iteration 0300: loss: 4095.175
iteration 0400: loss: 4089.718
iteration 0500: loss: 4090.699
iteration 0600: loss: 4088.761
iteration 0700: loss: 4106.986
iteration 0800: loss: 4119.881
iteration 0900: loss: 4092.206
iteration 1000: loss: 4120.850
iteration 1100: loss: 4065.221
iteration 1200: loss: 4076.345
iteration 1300: loss: 4094.227
iteration 1400: loss: 4079.645
iteration 1500: loss: 4085.133
iteration 1600: loss: 4089.973
iteration 1700: loss: 4065.512
iteration 1800: loss: 4114.488
====> Epoch: 014 Train loss: 4088.8160  took : 53.00476956367493
====> Test loss: 4086.6496
iteration 0000: loss: 4088.401
iteration 0100: loss: 4063.711
iteration 0200: loss: 4077.846
iteration 0300: loss: 4085.628
iteration 0400: loss: 4091.157
iteration 0500: loss: 4065.401
iteration 0600: loss: 4083.442
iteration 0700: loss: 4089.521
iteration 0800: loss: 4074.929
iteration 0900: loss: 4087.531
iteration 1000: loss: 4063.100
iteration 1100: loss: 4105.840
iteration 1200: loss: 4066.075
iteration 1300: loss: 4080.462
iteration 1400: loss: 4079.657
iteration 1500: loss: 4111.436
iteration 1600: loss: 4077.988
iteration 1700: loss: 4093.878
iteration 1800: loss: 4082.081
====> Epoch: 015 Train loss: 4088.5621  took : 53.03232717514038
====> Test loss: 4086.6331
iteration 0000: loss: 4083.587
iteration 0100: loss: 4106.854
iteration 0200: loss: 4064.547
iteration 0300: loss: 4078.801
iteration 0400: loss: 4104.763
iteration 0500: loss: 4073.388
iteration 0600: loss: 4085.726
iteration 0700: loss: 4085.638
iteration 0800: loss: 4100.755
iteration 0900: loss: 4068.956
iteration 1000: loss: 4083.105
iteration 1100: loss: 4115.290
iteration 1200: loss: 4072.757
iteration 1300: loss: 4089.696
iteration 1400: loss: 4088.198
iteration 1500: loss: 4105.803
iteration 1600: loss: 4089.706
iteration 1700: loss: 4104.006
iteration 1800: loss: 4070.543
====> Epoch: 016 Train loss: 4088.3271  took : 53.18369197845459
====> Test loss: 4086.4459
iteration 0000: loss: 4085.721
iteration 0100: loss: 4090.574
iteration 0200: loss: 4072.893
iteration 0300: loss: 4074.027
iteration 0400: loss: 4089.560
iteration 0500: loss: 4095.858
iteration 0600: loss: 4079.053
iteration 0700: loss: 4090.222
iteration 0800: loss: 4085.333
iteration 0900: loss: 4087.490
iteration 1000: loss: 4104.041
iteration 1100: loss: 4073.172
iteration 1200: loss: 4096.382
iteration 1300: loss: 4077.893
iteration 1400: loss: 4103.114
iteration 1500: loss: 4078.673
iteration 1600: loss: 4092.771
iteration 1700: loss: 4078.847
iteration 1800: loss: 4083.151
====> Epoch: 017 Train loss: 4088.4679  took : 53.30729937553406
====> Test loss: 4086.4808
iteration 0000: loss: 4073.335
iteration 0100: loss: 4075.704
iteration 0200: loss: 4098.623
iteration 0300: loss: 4103.003
iteration 0400: loss: 4066.675
iteration 0500: loss: 4092.213
iteration 0600: loss: 4107.153
iteration 0700: loss: 4079.568
iteration 0800: loss: 4063.805
iteration 0900: loss: 4112.464
iteration 1000: loss: 4101.815
iteration 1100: loss: 4113.765
iteration 1200: loss: 4091.867
iteration 1300: loss: 4083.988
iteration 1400: loss: 4101.860
iteration 1500: loss: 4092.351
iteration 1600: loss: 4094.033
iteration 1700: loss: 4099.536
iteration 1800: loss: 4098.218
====> Epoch: 018 Train loss: 4088.3437  took : 53.262516260147095
====> Test loss: 4086.3570
iteration 0000: loss: 4108.748
iteration 0100: loss: 4097.488
iteration 0200: loss: 4078.078
iteration 0300: loss: 4077.376
iteration 0400: loss: 4077.706
iteration 0500: loss: 4084.223
iteration 0600: loss: 4088.396
iteration 0700: loss: 4100.088
iteration 0800: loss: 4093.147
iteration 0900: loss: 4103.458
iteration 1000: loss: 4094.446
iteration 1100: loss: 4053.902
iteration 1200: loss: 4083.126
iteration 1300: loss: 4097.141
iteration 1400: loss: 4088.949
iteration 1500: loss: 4066.972
iteration 1600: loss: 4089.212
iteration 1700: loss: 4093.996
iteration 1800: loss: 4068.657
====> Epoch: 019 Train loss: 4088.1767  took : 53.227861642837524
====> Test loss: 4086.5710
iteration 0000: loss: 4081.623
iteration 0100: loss: 4067.046
iteration 0200: loss: 4089.618
iteration 0300: loss: 4086.533
iteration 0400: loss: 4091.821
iteration 0500: loss: 4113.918
iteration 0600: loss: 4080.064
iteration 0700: loss: 4090.877
iteration 0800: loss: 4076.901
iteration 0900: loss: 4074.323
iteration 1000: loss: 4105.068
iteration 1100: loss: 4098.383
iteration 1200: loss: 4075.452
iteration 1300: loss: 4094.220
iteration 1400: loss: 4081.436
iteration 1500: loss: 4079.892
iteration 1600: loss: 4093.870
iteration 1700: loss: 4090.530
iteration 1800: loss: 4055.151
====> Epoch: 020 Train loss: 4088.0133  took : 53.1562864780426
====> Test loss: 4086.3341
iteration 0000: loss: 4078.502
iteration 0100: loss: 4109.674
iteration 0200: loss: 4099.707
iteration 0300: loss: 4077.208
iteration 0400: loss: 4082.082
iteration 0500: loss: 4078.979
iteration 0600: loss: 4067.841
iteration 0700: loss: 4088.808
iteration 0800: loss: 4076.204
iteration 0900: loss: 4095.072
iteration 1000: loss: 4078.876
iteration 1100: loss: 4078.347
iteration 1200: loss: 4092.171
iteration 1300: loss: 4109.347
iteration 1400: loss: 4109.924
iteration 1500: loss: 4097.837
iteration 1600: loss: 4083.175
iteration 1700: loss: 4095.324
iteration 1800: loss: 4092.656
====> Epoch: 021 Train loss: 4087.8701  took : 53.24571251869202
====> Test loss: 4086.1364
iteration 0000: loss: 4066.970
iteration 0100: loss: 4087.180
iteration 0200: loss: 4086.248
iteration 0300: loss: 4099.515
iteration 0400: loss: 4103.832
iteration 0500: loss: 4073.556
iteration 0600: loss: 4085.825
iteration 0700: loss: 4091.675
iteration 0800: loss: 4106.874
iteration 0900: loss: 4080.928
iteration 1000: loss: 4066.771
iteration 1100: loss: 4093.656
iteration 1200: loss: 4075.339
iteration 1300: loss: 4073.311
iteration 1400: loss: 4077.946
iteration 1500: loss: 4115.329
iteration 1600: loss: 4088.273
iteration 1700: loss: 4110.965
iteration 1800: loss: 4074.978
====> Epoch: 022 Train loss: 4087.6910  took : 53.042020082473755
====> Test loss: 4086.0098
iteration 0000: loss: 4094.054
iteration 0100: loss: 4083.098
iteration 0200: loss: 4077.576
iteration 0300: loss: 4068.043
iteration 0400: loss: 4078.306
iteration 0500: loss: 4078.660
iteration 0600: loss: 4096.555
iteration 0700: loss: 4118.947
iteration 0800: loss: 4087.487
iteration 0900: loss: 4091.461
iteration 1000: loss: 4088.041
iteration 1100: loss: 4082.155
iteration 1200: loss: 4096.986
iteration 1300: loss: 4077.675
iteration 1400: loss: 4065.494
iteration 1500: loss: 4099.130
iteration 1600: loss: 4111.508
iteration 1700: loss: 4064.739
iteration 1800: loss: 4108.076
====> Epoch: 023 Train loss: 4087.6029  took : 53.18061828613281
====> Test loss: 4086.0011
iteration 0000: loss: 4076.779
iteration 0100: loss: 4076.756
iteration 0200: loss: 4115.024
iteration 0300: loss: 4075.595
iteration 0400: loss: 4116.567
iteration 0500: loss: 4086.088
iteration 0600: loss: 4106.646
iteration 0700: loss: 4090.824
iteration 0800: loss: 4065.750
iteration 0900: loss: 4090.982
iteration 1000: loss: 4085.843
iteration 1100: loss: 4097.021
iteration 1200: loss: 4086.870
iteration 1300: loss: 4077.397
iteration 1400: loss: 4075.090
iteration 1500: loss: 4123.406
iteration 1600: loss: 4071.401
iteration 1700: loss: 4079.133
iteration 1800: loss: 4081.583
====> Epoch: 024 Train loss: 4087.4037  took : 52.95339632034302
====> Test loss: 4085.8044
iteration 0000: loss: 4081.903
iteration 0100: loss: 4083.844
iteration 0200: loss: 4092.089
iteration 0300: loss: 4069.203
iteration 0400: loss: 4073.488
iteration 0500: loss: 4058.540
iteration 0600: loss: 4092.278
iteration 0700: loss: 4097.272
iteration 0800: loss: 4089.477
iteration 0900: loss: 4102.101
iteration 1000: loss: 4111.682
iteration 1100: loss: 4090.508
iteration 1200: loss: 4068.368
iteration 1300: loss: 4086.322
iteration 1400: loss: 4104.913
iteration 1500: loss: 4067.918
iteration 1600: loss: 4109.187
iteration 1700: loss: 4083.249
iteration 1800: loss: 4085.096
====> Epoch: 025 Train loss: 4087.2776  took : 53.40237092971802
====> Test loss: 4085.7686
iteration 0000: loss: 4094.518
iteration 0100: loss: 4077.813
iteration 0200: loss: 4074.300
iteration 0300: loss: 4095.646
iteration 0400: loss: 4072.772
iteration 0500: loss: 4092.339
iteration 0600: loss: 4116.212
iteration 0700: loss: 4067.543
iteration 0800: loss: 4106.299
iteration 0900: loss: 4079.263
iteration 1000: loss: 4067.595
iteration 1100: loss: 4084.230
iteration 1200: loss: 4083.747
iteration 1300: loss: 4106.486
iteration 1400: loss: 4083.182
iteration 1500: loss: 4106.731
iteration 1600: loss: 4065.146
iteration 1700: loss: 4093.371
iteration 1800: loss: 4090.365
====> Epoch: 026 Train loss: 4087.1740  took : 53.0470290184021
====> Test loss: 4085.6105
iteration 0000: loss: 4112.501
iteration 0100: loss: 4118.689
iteration 0200: loss: 4118.752
iteration 0300: loss: 4079.899
iteration 0400: loss: 4078.851
iteration 0500: loss: 4092.859
iteration 0600: loss: 4115.896
iteration 0700: loss: 4078.339
iteration 0800: loss: 4095.542
iteration 0900: loss: 4096.830
iteration 1000: loss: 4073.956
iteration 1100: loss: 4070.410
iteration 1200: loss: 4076.963
iteration 1300: loss: 4088.521
iteration 1400: loss: 4096.917
iteration 1500: loss: 4088.062
iteration 1600: loss: 4101.891
iteration 1700: loss: 4073.700
iteration 1800: loss: 4088.120
====> Epoch: 027 Train loss: 4087.0821  took : 53.03196907043457
====> Test loss: 4085.6559
iteration 0000: loss: 4090.442
iteration 0100: loss: 4076.287
iteration 0200: loss: 4072.217
iteration 0300: loss: 4076.764
iteration 0400: loss: 4080.228
iteration 0500: loss: 4082.018
iteration 0600: loss: 4088.703
iteration 0700: loss: 4070.491
iteration 0800: loss: 4076.007
iteration 0900: loss: 4073.792
iteration 1000: loss: 4073.300
iteration 1100: loss: 4067.126
iteration 1200: loss: 4104.932
iteration 1300: loss: 4099.094
iteration 1400: loss: 4096.881
iteration 1500: loss: 4109.060
iteration 1600: loss: 4069.916
iteration 1700: loss: 4086.570
iteration 1800: loss: 4113.789
====> Epoch: 028 Train loss: 4086.9219  took : 53.06193161010742
====> Test loss: 4085.8782
iteration 0000: loss: 4089.403
iteration 0100: loss: 4077.483
iteration 0200: loss: 4095.572
iteration 0300: loss: 4096.770
iteration 0400: loss: 4065.220
iteration 0500: loss: 4109.928
iteration 0600: loss: 4074.417
iteration 0700: loss: 4084.243
iteration 0800: loss: 4094.410
iteration 0900: loss: 4112.195
iteration 1000: loss: 4066.421
iteration 1100: loss: 4104.956
iteration 1200: loss: 4083.162
iteration 1300: loss: 4087.651
iteration 1400: loss: 4090.690
iteration 1500: loss: 4087.175
iteration 1600: loss: 4094.415
iteration 1700: loss: 4091.303
iteration 1800: loss: 4116.580
====> Epoch: 029 Train loss: 4086.8266  took : 52.92767024040222
====> Test loss: 4085.2820
iteration 0000: loss: 4068.122
iteration 0100: loss: 4075.664
iteration 0200: loss: 4093.119
iteration 0300: loss: 4117.644
iteration 0400: loss: 4051.046
iteration 0500: loss: 4076.346
iteration 0600: loss: 4088.770
iteration 0700: loss: 4067.471
iteration 0800: loss: 4075.282
iteration 0900: loss: 4078.108
iteration 1000: loss: 4109.194
iteration 1100: loss: 4091.637
iteration 1200: loss: 4086.404
iteration 1300: loss: 4092.226
iteration 1400: loss: 4074.702
iteration 1500: loss: 4083.756
iteration 1600: loss: 4087.103
iteration 1700: loss: 4111.335
iteration 1800: loss: 4104.603
====> Epoch: 030 Train loss: 4086.7282  took : 53.267104387283325
====> Test loss: 4085.4155
iteration 0000: loss: 4065.433
iteration 0100: loss: 4079.159
iteration 0200: loss: 4092.573
iteration 0300: loss: 4094.704
iteration 0400: loss: 4095.859
iteration 0500: loss: 4073.687
iteration 0600: loss: 4093.890
iteration 0700: loss: 4075.497
iteration 0800: loss: 4105.980
iteration 0900: loss: 4109.970
iteration 1000: loss: 4087.849
iteration 1100: loss: 4078.185
iteration 1200: loss: 4095.436
iteration 1300: loss: 4107.086
iteration 1400: loss: 4106.122
iteration 1500: loss: 4113.980
iteration 1600: loss: 4093.176
iteration 1700: loss: 4082.190
iteration 1800: loss: 4090.570
====> Epoch: 031 Train loss: 4087.0791  took : 53.01548147201538
====> Test loss: 4085.9067
iteration 0000: loss: 4087.084
iteration 0100: loss: 4077.504
iteration 0200: loss: 4088.175
iteration 0300: loss: 4076.335
iteration 0400: loss: 4071.552
iteration 0500: loss: 4110.470
iteration 0600: loss: 4086.447
iteration 0700: loss: 4075.033
iteration 0800: loss: 4063.878
iteration 0900: loss: 4068.016
iteration 1000: loss: 4082.687
iteration 1100: loss: 4081.481
iteration 1200: loss: 4115.270
iteration 1300: loss: 4098.571
iteration 1400: loss: 4086.772
iteration 1500: loss: 4077.301
iteration 1600: loss: 4075.664
iteration 1700: loss: 4077.945
iteration 1800: loss: 4085.512
====> Epoch: 032 Train loss: 4087.1049  took : 52.95301342010498
====> Test loss: 4085.8085
iteration 0000: loss: 4068.470
iteration 0100: loss: 4071.340
iteration 0200: loss: 4080.965
iteration 0300: loss: 4082.688
iteration 0400: loss: 4082.080
iteration 0500: loss: 4065.057
iteration 0600: loss: 4096.498
iteration 0700: loss: 4073.589
iteration 0800: loss: 4077.880
iteration 0900: loss: 4087.167
iteration 1000: loss: 4083.770
iteration 1100: loss: 4101.118
iteration 1200: loss: 4092.192
iteration 1300: loss: 4083.975
iteration 1400: loss: 4094.697
iteration 1500: loss: 4084.006
iteration 1600: loss: 4083.155
iteration 1700: loss: 4068.759
iteration 1800: loss: 4073.263
====> Epoch: 033 Train loss: 4087.0448  took : 52.894360303878784
====> Test loss: 4085.8329
iteration 0000: loss: 4109.739
iteration 0100: loss: 4092.249
iteration 0200: loss: 4093.895
iteration 0300: loss: 4074.471
iteration 0400: loss: 4078.814
iteration 0500: loss: 4114.161
iteration 0600: loss: 4080.930
iteration 0700: loss: 4086.533
iteration 0800: loss: 4083.034
iteration 0900: loss: 4112.810
iteration 1000: loss: 4096.879
iteration 1100: loss: 4114.972
iteration 1200: loss: 4098.921
iteration 1300: loss: 4080.378
iteration 1400: loss: 4103.102
iteration 1500: loss: 4054.114
iteration 1600: loss: 4063.752
iteration 1700: loss: 4083.871
iteration 1800: loss: 4090.879
====> Epoch: 034 Train loss: 4087.0500  took : 52.87887763977051
====> Test loss: 4086.3144
iteration 0000: loss: 4090.369
iteration 0100: loss: 4077.233
iteration 0200: loss: 4094.909
iteration 0300: loss: 4066.493
iteration 0400: loss: 4072.043
iteration 0500: loss: 4095.925
iteration 0600: loss: 4084.731
iteration 0700: loss: 4083.113
iteration 0800: loss: 4072.902
iteration 0900: loss: 4086.547
iteration 1000: loss: 4075.639
iteration 1100: loss: 4106.516
iteration 1200: loss: 4089.847
iteration 1300: loss: 4105.196
iteration 1400: loss: 4110.568
iteration 1500: loss: 4109.570
iteration 1600: loss: 4058.885
iteration 1700: loss: 4089.516
iteration 1800: loss: 4083.761
====> Epoch: 035 Train loss: 4087.4433  took : 52.99910092353821
====> Test loss: 4086.2164
iteration 0000: loss: 4073.192
iteration 0100: loss: 4078.502
iteration 0200: loss: 4063.817
iteration 0300: loss: 4097.249
iteration 0400: loss: 4070.099
iteration 0500: loss: 4113.589
iteration 0600: loss: 4091.826
iteration 0700: loss: 4102.751
iteration 0800: loss: 4084.168
iteration 0900: loss: 4064.526
iteration 1000: loss: 4062.470
iteration 1100: loss: 4071.090
iteration 1200: loss: 4119.365
iteration 1300: loss: 4091.320
iteration 1400: loss: 4095.556
iteration 1500: loss: 4096.473
iteration 1600: loss: 4080.858
iteration 1700: loss: 4096.873
iteration 1800: loss: 4104.834
====> Epoch: 036 Train loss: 4087.3629  took : 53.05576801300049
====> Test loss: 4086.2709
iteration 0000: loss: 4068.825
iteration 0100: loss: 4122.263
iteration 0200: loss: 4078.784
iteration 0300: loss: 4095.117
iteration 0400: loss: 4082.547
iteration 0500: loss: 4087.890
iteration 0600: loss: 4087.558
iteration 0700: loss: 4092.517
iteration 0800: loss: 4090.062
iteration 0900: loss: 4077.084
iteration 1000: loss: 4104.878
iteration 1100: loss: 4076.635
iteration 1200: loss: 4091.186
iteration 1300: loss: 4101.265
iteration 1400: loss: 4086.963
iteration 1500: loss: 4098.544
iteration 1600: loss: 4097.027
iteration 1700: loss: 4111.720
iteration 1800: loss: 4074.993
====> Epoch: 037 Train loss: 4087.2866  took : 52.870248556137085
====> Test loss: 4086.1359
iteration 0000: loss: 4090.229
iteration 0100: loss: 4105.432
iteration 0200: loss: 4081.452
iteration 0300: loss: 4088.384
iteration 0400: loss: 4078.304
iteration 0500: loss: 4066.418
iteration 0600: loss: 4066.489
iteration 0700: loss: 4096.693
iteration 0800: loss: 4083.459
iteration 0900: loss: 4099.684
iteration 1000: loss: 4091.459
iteration 1100: loss: 4082.235
iteration 1200: loss: 4080.174
iteration 1300: loss: 4084.132
iteration 1400: loss: 4104.656
iteration 1500: loss: 4084.912
iteration 1600: loss: 4076.756
iteration 1700: loss: 4090.093
iteration 1800: loss: 4107.003
====> Epoch: 038 Train loss: 4087.2061  took : 52.990310192108154
====> Test loss: 4086.3653
iteration 0000: loss: 4074.830
iteration 0100: loss: 4080.187
iteration 0200: loss: 4094.689
iteration 0300: loss: 4078.349
iteration 0400: loss: 4097.728
iteration 0500: loss: 4088.302
iteration 0600: loss: 4070.255
iteration 0700: loss: 4072.658
iteration 0800: loss: 4060.408
iteration 0900: loss: 4089.497
iteration 1000: loss: 4083.417
iteration 1100: loss: 4079.564
iteration 1200: loss: 4127.091
iteration 1300: loss: 4099.177
iteration 1400: loss: 4106.042
iteration 1500: loss: 4087.424
iteration 1600: loss: 4084.536
iteration 1700: loss: 4090.097
iteration 1800: loss: 4109.279
====> Epoch: 039 Train loss: 4087.1935  took : 52.940101623535156
====> Test loss: 4085.9872
iteration 0000: loss: 4088.776
iteration 0100: loss: 4074.863
iteration 0200: loss: 4116.924
iteration 0300: loss: 4108.519
iteration 0400: loss: 4088.618
iteration 0500: loss: 4084.726
iteration 0600: loss: 4086.130
iteration 0700: loss: 4092.346
iteration 0800: loss: 4093.187
iteration 0900: loss: 4095.716
iteration 1000: loss: 4094.120
iteration 1100: loss: 4052.083
iteration 1200: loss: 4064.627
iteration 1300: loss: 4073.634
iteration 1400: loss: 4103.089
iteration 1500: loss: 4095.069
iteration 1600: loss: 4077.666
iteration 1700: loss: 4092.782
iteration 1800: loss: 4083.938
====> Epoch: 040 Train loss: 4087.0221  took : 52.932944536209106
====> Test loss: 4086.2394
iteration 0000: loss: 4086.233
iteration 0100: loss: 4064.517
iteration 0200: loss: 4074.061
iteration 0300: loss: 4081.204
iteration 0400: loss: 4082.392
iteration 0500: loss: 4098.835
iteration 0600: loss: 4081.377
iteration 0700: loss: 4069.485
iteration 0800: loss: 4107.292
iteration 0900: loss: 4093.374
iteration 1000: loss: 4072.872
iteration 1100: loss: 4088.397
iteration 1200: loss: 4082.386
iteration 1300: loss: 4094.713
iteration 1400: loss: 4087.204
iteration 1500: loss: 4098.587
iteration 1600: loss: 4119.438
iteration 1700: loss: 4114.417
iteration 1800: loss: 4095.171
====> Epoch: 041 Train loss: 4086.9749  took : 53.06671142578125
====> Test loss: 4086.0208
iteration 0000: loss: 4082.817
iteration 0100: loss: 4074.486
iteration 0200: loss: 4108.264
iteration 0300: loss: 4072.590
iteration 0400: loss: 4065.817
iteration 0500: loss: 4061.755
iteration 0600: loss: 4087.685
iteration 0700: loss: 4102.364
iteration 0800: loss: 4086.579
iteration 0900: loss: 4078.956
iteration 1000: loss: 4101.497
iteration 1100: loss: 4098.004
iteration 1200: loss: 4075.095
iteration 1300: loss: 4097.480
iteration 1400: loss: 4093.907
iteration 1500: loss: 4102.614
iteration 1600: loss: 4076.699
iteration 1700: loss: 4090.752
iteration 1800: loss: 4075.983
====> Epoch: 042 Train loss: 4087.0019  took : 52.786296367645264
====> Test loss: 4086.5876
iteration 0000: loss: 4091.473
iteration 0100: loss: 4077.250
iteration 0200: loss: 4093.669
iteration 0300: loss: 4113.918
iteration 0400: loss: 4077.351
iteration 0500: loss: 4120.278
iteration 0600: loss: 4065.518
iteration 0700: loss: 4098.681
iteration 0800: loss: 4064.250
iteration 0900: loss: 4081.720
iteration 1000: loss: 4099.505
iteration 1100: loss: 4093.719
iteration 1200: loss: 4105.183
iteration 1300: loss: 4070.625
iteration 1400: loss: 4099.598
iteration 1500: loss: 4091.769
iteration 1600: loss: 4072.371
iteration 1700: loss: 4097.685
iteration 1800: loss: 4086.504
====> Epoch: 043 Train loss: 4087.3799  took : 53.0560257434845
====> Test loss: 4086.4648
iteration 0000: loss: 4083.318
iteration 0100: loss: 4116.603
iteration 0200: loss: 4069.512
iteration 0300: loss: 4072.899
iteration 0400: loss: 4079.141
iteration 0500: loss: 4114.379
iteration 0600: loss: 4072.938
iteration 0700: loss: 4086.053
iteration 0800: loss: 4103.359
iteration 0900: loss: 4107.478
iteration 1000: loss: 4081.493
iteration 1100: loss: 4085.063
iteration 1200: loss: 4079.303
iteration 1300: loss: 4062.182
iteration 1400: loss: 4089.587
iteration 1500: loss: 4075.110
iteration 1600: loss: 4071.584
iteration 1700: loss: 4059.608
iteration 1800: loss: 4075.794
====> Epoch: 044 Train loss: 4087.8627  took : 53.070061445236206
====> Test loss: 4087.0408
iteration 0000: loss: 4088.281
iteration 0100: loss: 4082.834
iteration 0200: loss: 4105.887
iteration 0300: loss: 4083.938
iteration 0400: loss: 4112.787
iteration 0500: loss: 4088.271
iteration 0600: loss: 4103.979
iteration 0700: loss: 4108.247
iteration 0800: loss: 4072.572
iteration 0900: loss: 4089.178
iteration 1000: loss: 4075.918
iteration 1100: loss: 4083.044
iteration 1200: loss: 4114.207
iteration 1300: loss: 4067.857
iteration 1400: loss: 4091.787
iteration 1500: loss: 4110.346
iteration 1600: loss: 4072.848
iteration 1700: loss: 4083.309
iteration 1800: loss: 4094.129
====> Epoch: 045 Train loss: 4088.3131  took : 53.11792802810669
====> Test loss: 4088.1168
iteration 0000: loss: 4088.364
iteration 0100: loss: 4091.808
iteration 0200: loss: 4078.441
iteration 0300: loss: 4111.999
iteration 0400: loss: 4100.369
iteration 0500: loss: 4091.736
iteration 0600: loss: 4106.524
iteration 0700: loss: 4061.939
iteration 0800: loss: 4102.310
iteration 0900: loss: 4100.131
iteration 1000: loss: 4112.573
iteration 1100: loss: 4079.901
iteration 1200: loss: 4108.720
iteration 1300: loss: 4086.892
iteration 1400: loss: 4106.825
iteration 1500: loss: 4102.023
iteration 1600: loss: 4083.731
iteration 1700: loss: 4092.086
iteration 1800: loss: 4083.812
====> Epoch: 046 Train loss: 4088.8769  took : 53.10964345932007
====> Test loss: 4088.2314
iteration 0000: loss: 4091.090
iteration 0100: loss: 4080.885
iteration 0200: loss: 4078.924
iteration 0300: loss: 4069.972
iteration 0400: loss: 4087.732
iteration 0500: loss: 4104.837
iteration 0600: loss: 4105.937
iteration 0700: loss: 4105.175
iteration 0800: loss: 4088.712
iteration 0900: loss: 4125.033
iteration 1000: loss: 4088.960
iteration 1100: loss: 4077.229
iteration 1200: loss: 4090.351
iteration 1300: loss: 4089.627
iteration 1400: loss: 4113.071
iteration 1500: loss: 4071.864
iteration 1600: loss: 4085.407
iteration 1700: loss: 4098.597
iteration 1800: loss: 4090.428
====> Epoch: 047 Train loss: 4088.8096  took : 53.048205614089966
====> Test loss: 4088.0797
iteration 0000: loss: 4079.099
iteration 0100: loss: 4107.196
iteration 0200: loss: 4100.482
iteration 0300: loss: 4113.617
iteration 0400: loss: 4056.872
iteration 0500: loss: 4065.584
iteration 0600: loss: 4077.271
iteration 0700: loss: 4074.171
iteration 0800: loss: 4089.470
iteration 0900: loss: 4091.767
iteration 1000: loss: 4076.474
iteration 1100: loss: 4111.952
iteration 1200: loss: 4097.774
iteration 1300: loss: 4096.691
iteration 1400: loss: 4093.250
iteration 1500: loss: 4100.358
iteration 1600: loss: 4075.818
iteration 1700: loss: 4101.191
iteration 1800: loss: 4071.028
====> Epoch: 048 Train loss: 4088.7262  took : 52.96395492553711
====> Test loss: 4088.0885
iteration 0000: loss: 4087.545
iteration 0100: loss: 4078.834
iteration 0200: loss: 4096.283
iteration 0300: loss: 4103.933
iteration 0400: loss: 4057.782
iteration 0500: loss: 4053.132
iteration 0600: loss: 4075.012
iteration 0700: loss: 4087.881
iteration 0800: loss: 4093.216
iteration 0900: loss: 4106.542
iteration 1000: loss: 4114.062
iteration 1100: loss: 4079.542
iteration 1200: loss: 4057.937
iteration 1300: loss: 4070.630
iteration 1400: loss: 4092.488
iteration 1500: loss: 4102.188
iteration 1600: loss: 4088.271
iteration 1700: loss: 4092.299
iteration 1800: loss: 4098.486
====> Epoch: 049 Train loss: 4088.8130  took : 53.00561261177063
====> Test loss: 4088.6646
iteration 0000: loss: 4114.499
iteration 0100: loss: 4085.764
iteration 0200: loss: 4056.100
iteration 0300: loss: 4092.382
iteration 0400: loss: 4102.428
iteration 0500: loss: 4101.641
iteration 0600: loss: 4075.660
iteration 0700: loss: 4107.132
iteration 0800: loss: 4081.733
iteration 0900: loss: 4076.208
iteration 1000: loss: 4075.768
iteration 1100: loss: 4107.680
iteration 1200: loss: 4066.306
iteration 1300: loss: 4096.195
iteration 1400: loss: 4071.087
iteration 1500: loss: 4077.852
iteration 1600: loss: 4095.146
iteration 1700: loss: 4079.157
iteration 1800: loss: 4081.203
====> Epoch: 050 Train loss: 4089.2060  took : 52.90003180503845
====> Test loss: 4088.5496
====> [MM-VAE] Time: 3118.304s or 00:51:58
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  10
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_10
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_10
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1989.104
iteration 0100: loss: 1561.388
iteration 0200: loss: 1565.844
iteration 0300: loss: 1559.834
iteration 0400: loss: 1550.384
iteration 0500: loss: 1553.346
iteration 0600: loss: 1536.098
iteration 0700: loss: 1543.392
iteration 0800: loss: 1537.809
iteration 0900: loss: 1540.562
====> Epoch: 001 Train loss: 1555.2917  took : 8.386852502822876
====> Test loss: 1538.8056
iteration 0000: loss: 1537.084
iteration 0100: loss: 1534.476
iteration 0200: loss: 1535.603
iteration 0300: loss: 1539.011
iteration 0400: loss: 1533.038
iteration 0500: loss: 1533.413
iteration 0600: loss: 1533.998
iteration 0700: loss: 1532.411
iteration 0800: loss: 1528.855
iteration 0900: loss: 1532.683
====> Epoch: 002 Train loss: 1533.4070  took : 8.564659833908081
====> Test loss: 1531.9311
iteration 0000: loss: 1526.963
iteration 0100: loss: 1526.703
iteration 0200: loss: 1531.655
iteration 0300: loss: 1526.851
iteration 0400: loss: 1525.497
iteration 0500: loss: 1527.162
iteration 0600: loss: 1522.148
iteration 0700: loss: 1526.579
iteration 0800: loss: 1525.709
iteration 0900: loss: 1527.053
====> Epoch: 003 Train loss: 1527.8517  took : 8.462004661560059
====> Test loss: 1528.1988
iteration 0000: loss: 1529.755
iteration 0100: loss: 1527.165
iteration 0200: loss: 1523.451
iteration 0300: loss: 1526.900
iteration 0400: loss: 1526.661
iteration 0500: loss: 1522.473
iteration 0600: loss: 1521.466
iteration 0700: loss: 1523.283
iteration 0800: loss: 1523.025
iteration 0900: loss: 1524.234
====> Epoch: 004 Train loss: 1525.0819  took : 8.430540561676025
====> Test loss: 1526.2327
iteration 0000: loss: 1518.745
iteration 0100: loss: 1520.350
iteration 0200: loss: 1522.388
iteration 0300: loss: 1524.463
iteration 0400: loss: 1525.502
iteration 0500: loss: 1524.350
iteration 0600: loss: 1524.949
iteration 0700: loss: 1520.852
iteration 0800: loss: 1521.503
iteration 0900: loss: 1522.910
====> Epoch: 005 Train loss: 1523.4200  took : 8.446993827819824
====> Test loss: 1525.2612
iteration 0000: loss: 1520.538
iteration 0100: loss: 1525.403
iteration 0200: loss: 1525.815
iteration 0300: loss: 1524.003
iteration 0400: loss: 1522.916
iteration 0500: loss: 1525.008
iteration 0600: loss: 1523.379
iteration 0700: loss: 1518.156
iteration 0800: loss: 1523.493
iteration 0900: loss: 1521.637
====> Epoch: 006 Train loss: 1522.2840  took : 8.510911226272583
====> Test loss: 1524.2672
iteration 0000: loss: 1523.810
iteration 0100: loss: 1524.326
iteration 0200: loss: 1523.782
iteration 0300: loss: 1522.882
iteration 0400: loss: 1521.599
iteration 0500: loss: 1524.403
iteration 0600: loss: 1521.191
iteration 0700: loss: 1517.010
iteration 0800: loss: 1524.054
iteration 0900: loss: 1526.445
====> Epoch: 007 Train loss: 1521.4239  took : 8.422796249389648
====> Test loss: 1523.7406
iteration 0000: loss: 1522.122
iteration 0100: loss: 1519.694
iteration 0200: loss: 1524.884
iteration 0300: loss: 1523.083
iteration 0400: loss: 1522.627
iteration 0500: loss: 1520.372
iteration 0600: loss: 1520.194
iteration 0700: loss: 1520.812
iteration 0800: loss: 1517.933
iteration 0900: loss: 1522.099
====> Epoch: 008 Train loss: 1520.7001  took : 8.444641590118408
====> Test loss: 1522.9036
iteration 0000: loss: 1520.374
iteration 0100: loss: 1516.859
iteration 0200: loss: 1518.619
iteration 0300: loss: 1519.161
iteration 0400: loss: 1523.660
iteration 0500: loss: 1520.864
iteration 0600: loss: 1517.595
iteration 0700: loss: 1520.058
iteration 0800: loss: 1519.412
iteration 0900: loss: 1518.446
====> Epoch: 009 Train loss: 1520.1319  took : 8.458997249603271
====> Test loss: 1522.4132
iteration 0000: loss: 1521.554
iteration 0100: loss: 1516.890
iteration 0200: loss: 1522.197
iteration 0300: loss: 1517.584
iteration 0400: loss: 1517.910
iteration 0500: loss: 1520.003
iteration 0600: loss: 1520.416
iteration 0700: loss: 1521.711
iteration 0800: loss: 1514.083
iteration 0900: loss: 1519.595
====> Epoch: 010 Train loss: 1519.6394  took : 8.547005414962769
====> Test loss: 1522.1654
iteration 0000: loss: 1521.334
iteration 0100: loss: 1518.477
iteration 0200: loss: 1520.015
iteration 0300: loss: 1520.423
iteration 0400: loss: 1518.015
iteration 0500: loss: 1519.252
iteration 0600: loss: 1518.325
iteration 0700: loss: 1520.258
iteration 0800: loss: 1519.559
iteration 0900: loss: 1517.437
====> Epoch: 011 Train loss: 1519.2514  took : 8.467651844024658
====> Test loss: 1521.7468
iteration 0000: loss: 1516.732
iteration 0100: loss: 1519.977
iteration 0200: loss: 1515.557
iteration 0300: loss: 1517.316
iteration 0400: loss: 1517.906
iteration 0500: loss: 1517.295
iteration 0600: loss: 1518.433
iteration 0700: loss: 1523.305
iteration 0800: loss: 1521.174
iteration 0900: loss: 1517.302
====> Epoch: 012 Train loss: 1518.9054  took : 8.354804754257202
====> Test loss: 1521.6570
iteration 0000: loss: 1518.549
iteration 0100: loss: 1518.203
iteration 0200: loss: 1518.093
iteration 0300: loss: 1520.552
iteration 0400: loss: 1518.685
iteration 0500: loss: 1521.329
iteration 0600: loss: 1517.288
iteration 0700: loss: 1519.289
iteration 0800: loss: 1516.331
iteration 0900: loss: 1517.874
====> Epoch: 013 Train loss: 1518.6263  took : 8.456981420516968
====> Test loss: 1521.1213
iteration 0000: loss: 1515.769
iteration 0100: loss: 1515.234
iteration 0200: loss: 1516.334
iteration 0300: loss: 1517.634
iteration 0400: loss: 1517.470
iteration 0500: loss: 1518.265
iteration 0600: loss: 1517.340
iteration 0700: loss: 1516.770
iteration 0800: loss: 1519.653
iteration 0900: loss: 1516.122
====> Epoch: 014 Train loss: 1518.3916  took : 8.428512811660767
====> Test loss: 1521.1240
iteration 0000: loss: 1518.928
iteration 0100: loss: 1518.337
iteration 0200: loss: 1517.407
iteration 0300: loss: 1519.435
iteration 0400: loss: 1518.895
iteration 0500: loss: 1519.742
iteration 0600: loss: 1516.596
iteration 0700: loss: 1518.151
iteration 0800: loss: 1516.151
iteration 0900: loss: 1516.160
====> Epoch: 015 Train loss: 1518.1351  took : 8.503567695617676
====> Test loss: 1520.8528
iteration 0000: loss: 1517.526
iteration 0100: loss: 1517.561
iteration 0200: loss: 1515.488
iteration 0300: loss: 1515.786
iteration 0400: loss: 1517.095
iteration 0500: loss: 1518.059
iteration 0600: loss: 1522.105
iteration 0700: loss: 1517.228
iteration 0800: loss: 1516.326
iteration 0900: loss: 1516.013
====> Epoch: 016 Train loss: 1517.9454  took : 8.393422842025757
====> Test loss: 1520.7459
iteration 0000: loss: 1519.054
iteration 0100: loss: 1516.152
iteration 0200: loss: 1517.592
iteration 0300: loss: 1517.025
iteration 0400: loss: 1519.175
iteration 0500: loss: 1519.234
iteration 0600: loss: 1516.778
iteration 0700: loss: 1518.490
iteration 0800: loss: 1517.975
iteration 0900: loss: 1517.679
====> Epoch: 017 Train loss: 1517.7429  took : 8.50613784790039
====> Test loss: 1520.6017
iteration 0000: loss: 1517.757
iteration 0100: loss: 1516.445
iteration 0200: loss: 1519.843
iteration 0300: loss: 1517.220
iteration 0400: loss: 1519.463
iteration 0500: loss: 1516.237
iteration 0600: loss: 1514.323
iteration 0700: loss: 1519.136
iteration 0800: loss: 1516.051
iteration 0900: loss: 1518.934
====> Epoch: 018 Train loss: 1517.5576  took : 8.501365184783936
====> Test loss: 1520.5081
iteration 0000: loss: 1517.956
iteration 0100: loss: 1516.447
iteration 0200: loss: 1514.663
iteration 0300: loss: 1516.886
iteration 0400: loss: 1516.050
iteration 0500: loss: 1516.577
iteration 0600: loss: 1517.637
iteration 0700: loss: 1515.476
iteration 0800: loss: 1515.110
iteration 0900: loss: 1516.345
====> Epoch: 019 Train loss: 1517.3932  took : 8.517930746078491
====> Test loss: 1520.3671
iteration 0000: loss: 1516.831
iteration 0100: loss: 1520.766
iteration 0200: loss: 1519.457
iteration 0300: loss: 1516.775
iteration 0400: loss: 1518.037
iteration 0500: loss: 1517.201
iteration 0600: loss: 1518.938
iteration 0700: loss: 1518.900
iteration 0800: loss: 1518.577
iteration 0900: loss: 1517.033
====> Epoch: 020 Train loss: 1517.2770  took : 8.514305353164673
====> Test loss: 1520.1364
iteration 0000: loss: 1518.027
iteration 0100: loss: 1518.507
iteration 0200: loss: 1515.294
iteration 0300: loss: 1517.779
iteration 0400: loss: 1513.878
iteration 0500: loss: 1518.308
iteration 0600: loss: 1515.987
iteration 0700: loss: 1516.584
iteration 0800: loss: 1518.206
iteration 0900: loss: 1518.224
====> Epoch: 021 Train loss: 1517.0995  took : 8.421169519424438
====> Test loss: 1520.3500
iteration 0000: loss: 1517.233
iteration 0100: loss: 1518.414
iteration 0200: loss: 1516.146
iteration 0300: loss: 1514.158
iteration 0400: loss: 1516.090
iteration 0500: loss: 1514.911
iteration 0600: loss: 1515.362
iteration 0700: loss: 1518.988
iteration 0800: loss: 1517.158
iteration 0900: loss: 1517.116
====> Epoch: 022 Train loss: 1516.9731  took : 8.47510576248169
====> Test loss: 1519.8169
iteration 0000: loss: 1520.456
iteration 0100: loss: 1518.699
iteration 0200: loss: 1519.024
iteration 0300: loss: 1517.710
iteration 0400: loss: 1516.586
iteration 0500: loss: 1515.807
iteration 0600: loss: 1516.365
iteration 0700: loss: 1514.403
iteration 0800: loss: 1516.091
iteration 0900: loss: 1516.596
====> Epoch: 023 Train loss: 1516.8393  took : 8.513699769973755
====> Test loss: 1519.7945
iteration 0000: loss: 1514.975
iteration 0100: loss: 1515.292
iteration 0200: loss: 1516.109
iteration 0300: loss: 1516.979
iteration 0400: loss: 1515.172
iteration 0500: loss: 1516.661
iteration 0600: loss: 1516.927
iteration 0700: loss: 1515.820
iteration 0800: loss: 1517.962
iteration 0900: loss: 1515.730
====> Epoch: 024 Train loss: 1516.7634  took : 8.412386894226074
====> Test loss: 1519.9665
iteration 0000: loss: 1517.279
iteration 0100: loss: 1517.480
iteration 0200: loss: 1516.479
iteration 0300: loss: 1519.250
iteration 0400: loss: 1518.610
iteration 0500: loss: 1517.570
iteration 0600: loss: 1516.789
iteration 0700: loss: 1516.398
iteration 0800: loss: 1515.946
iteration 0900: loss: 1516.193
====> Epoch: 025 Train loss: 1516.6608  took : 8.481165647506714
====> Test loss: 1519.6390
iteration 0000: loss: 1518.972
iteration 0100: loss: 1515.470
iteration 0200: loss: 1515.495
iteration 0300: loss: 1516.681
iteration 0400: loss: 1517.198
iteration 0500: loss: 1515.244
iteration 0600: loss: 1516.230
iteration 0700: loss: 1516.640
iteration 0800: loss: 1515.123
iteration 0900: loss: 1515.423
====> Epoch: 026 Train loss: 1516.5594  took : 8.451807975769043
====> Test loss: 1519.6022
iteration 0000: loss: 1518.548
iteration 0100: loss: 1519.245
iteration 0200: loss: 1516.735
iteration 0300: loss: 1520.134
iteration 0400: loss: 1518.010
iteration 0500: loss: 1516.195
iteration 0600: loss: 1513.932
iteration 0700: loss: 1517.172
iteration 0800: loss: 1514.004
iteration 0900: loss: 1514.465
====> Epoch: 027 Train loss: 1516.4508  took : 8.544735431671143
====> Test loss: 1519.5295
iteration 0000: loss: 1515.174
iteration 0100: loss: 1514.629
iteration 0200: loss: 1514.135
iteration 0300: loss: 1516.035
iteration 0400: loss: 1517.684
iteration 0500: loss: 1518.233
iteration 0600: loss: 1516.566
iteration 0700: loss: 1519.030
iteration 0800: loss: 1517.422
iteration 0900: loss: 1514.201
====> Epoch: 028 Train loss: 1516.3829  took : 8.41728138923645
====> Test loss: 1519.4490
iteration 0000: loss: 1519.902
iteration 0100: loss: 1514.614
iteration 0200: loss: 1516.188
iteration 0300: loss: 1516.044
iteration 0400: loss: 1516.219
iteration 0500: loss: 1515.528
iteration 0600: loss: 1517.922
iteration 0700: loss: 1517.550
iteration 0800: loss: 1517.480
iteration 0900: loss: 1518.198
====> Epoch: 029 Train loss: 1516.2562  took : 8.523025035858154
====> Test loss: 1519.4414
iteration 0000: loss: 1512.586
iteration 0100: loss: 1517.131
iteration 0200: loss: 1515.099
iteration 0300: loss: 1515.286
iteration 0400: loss: 1515.929
iteration 0500: loss: 1517.351
iteration 0600: loss: 1512.795
iteration 0700: loss: 1516.587
iteration 0800: loss: 1516.578
iteration 0900: loss: 1517.599
====> Epoch: 030 Train loss: 1516.1883  took : 8.422001838684082
====> Test loss: 1519.3769
iteration 0000: loss: 1518.675
iteration 0100: loss: 1516.982
iteration 0200: loss: 1518.237
iteration 0300: loss: 1515.405
iteration 0400: loss: 1515.329
iteration 0500: loss: 1516.334
iteration 0600: loss: 1518.780
iteration 0700: loss: 1514.548
iteration 0800: loss: 1519.821
iteration 0900: loss: 1515.632
====> Epoch: 031 Train loss: 1516.0901  took : 8.450654983520508
====> Test loss: 1519.2446
iteration 0000: loss: 1517.384
iteration 0100: loss: 1515.667
iteration 0200: loss: 1514.369
iteration 0300: loss: 1515.892
iteration 0400: loss: 1518.665
iteration 0500: loss: 1516.974
iteration 0600: loss: 1518.353
iteration 0700: loss: 1514.588
iteration 0800: loss: 1515.823
iteration 0900: loss: 1517.282
====> Epoch: 032 Train loss: 1516.0394  took : 8.480827331542969
====> Test loss: 1519.2603
iteration 0000: loss: 1513.229
iteration 0100: loss: 1514.547
iteration 0200: loss: 1517.670
iteration 0300: loss: 1513.594
iteration 0400: loss: 1516.743
iteration 0500: loss: 1515.530
iteration 0600: loss: 1514.183
iteration 0700: loss: 1515.693
iteration 0800: loss: 1518.333
iteration 0900: loss: 1515.643
====> Epoch: 033 Train loss: 1515.9758  took : 8.552673816680908
====> Test loss: 1519.1414
iteration 0000: loss: 1516.229
iteration 0100: loss: 1518.931
iteration 0200: loss: 1513.937
iteration 0300: loss: 1515.077
iteration 0400: loss: 1518.391
iteration 0500: loss: 1514.820
iteration 0600: loss: 1521.971
iteration 0700: loss: 1514.911
iteration 0800: loss: 1515.793
iteration 0900: loss: 1513.548
====> Epoch: 034 Train loss: 1515.8976  took : 8.330171823501587
====> Test loss: 1519.0890
iteration 0000: loss: 1512.644
iteration 0100: loss: 1514.200
iteration 0200: loss: 1516.235
iteration 0300: loss: 1514.661
iteration 0400: loss: 1514.190
iteration 0500: loss: 1514.515
iteration 0600: loss: 1514.623
iteration 0700: loss: 1518.486
iteration 0800: loss: 1515.892
iteration 0900: loss: 1515.775
====> Epoch: 035 Train loss: 1515.8489  took : 8.497946739196777
====> Test loss: 1519.0286
iteration 0000: loss: 1519.287
iteration 0100: loss: 1518.700
iteration 0200: loss: 1519.246
iteration 0300: loss: 1515.855
iteration 0400: loss: 1516.166
iteration 0500: loss: 1517.848
iteration 0600: loss: 1515.018
iteration 0700: loss: 1513.916
iteration 0800: loss: 1515.699
iteration 0900: loss: 1516.031
====> Epoch: 036 Train loss: 1515.8386  took : 8.471119403839111
====> Test loss: 1519.1210
iteration 0000: loss: 1512.563
iteration 0100: loss: 1514.194
iteration 0200: loss: 1516.984
iteration 0300: loss: 1514.250
iteration 0400: loss: 1517.266
iteration 0500: loss: 1514.390
iteration 0600: loss: 1513.935
iteration 0700: loss: 1519.831
iteration 0800: loss: 1517.064
iteration 0900: loss: 1515.319
====> Epoch: 037 Train loss: 1515.7323  took : 8.454751014709473
====> Test loss: 1519.0428
iteration 0000: loss: 1516.288
iteration 0100: loss: 1516.226
iteration 0200: loss: 1516.570
iteration 0300: loss: 1514.621
iteration 0400: loss: 1517.440
iteration 0500: loss: 1514.438
iteration 0600: loss: 1515.341
iteration 0700: loss: 1514.374
iteration 0800: loss: 1516.033
iteration 0900: loss: 1517.085
====> Epoch: 038 Train loss: 1515.6542  took : 8.466331720352173
====> Test loss: 1518.9864
iteration 0000: loss: 1515.025
iteration 0100: loss: 1515.035
iteration 0200: loss: 1515.229
iteration 0300: loss: 1513.911
iteration 0400: loss: 1519.248
iteration 0500: loss: 1517.687
iteration 0600: loss: 1519.594
iteration 0700: loss: 1512.667
iteration 0800: loss: 1517.161
iteration 0900: loss: 1514.191
====> Epoch: 039 Train loss: 1515.5867  took : 8.475212335586548
====> Test loss: 1518.8532
iteration 0000: loss: 1515.021
iteration 0100: loss: 1516.666
iteration 0200: loss: 1514.224
iteration 0300: loss: 1516.844
iteration 0400: loss: 1514.937
iteration 0500: loss: 1514.022
iteration 0600: loss: 1516.693
iteration 0700: loss: 1516.385
iteration 0800: loss: 1514.671
iteration 0900: loss: 1516.255
====> Epoch: 040 Train loss: 1515.5711  took : 8.54703974723816
====> Test loss: 1519.0364
iteration 0000: loss: 1515.513
iteration 0100: loss: 1516.830
iteration 0200: loss: 1516.366
iteration 0300: loss: 1514.674
iteration 0400: loss: 1515.600
iteration 0500: loss: 1516.412
iteration 0600: loss: 1512.989
iteration 0700: loss: 1518.683
iteration 0800: loss: 1517.302
iteration 0900: loss: 1518.041
====> Epoch: 041 Train loss: 1515.5844  took : 8.522160768508911
====> Test loss: 1519.0345
iteration 0000: loss: 1515.134
iteration 0100: loss: 1519.073
iteration 0200: loss: 1516.399
iteration 0300: loss: 1514.020
iteration 0400: loss: 1514.916
iteration 0500: loss: 1517.211
iteration 0600: loss: 1513.499
iteration 0700: loss: 1514.874
iteration 0800: loss: 1515.917
iteration 0900: loss: 1514.722
====> Epoch: 042 Train loss: 1515.4682  took : 8.486669063568115
====> Test loss: 1519.0396
iteration 0000: loss: 1515.735
iteration 0100: loss: 1514.677
iteration 0200: loss: 1517.121
iteration 0300: loss: 1514.790
iteration 0400: loss: 1515.516
iteration 0500: loss: 1515.221
iteration 0600: loss: 1513.937
iteration 0700: loss: 1517.105
iteration 0800: loss: 1516.773
iteration 0900: loss: 1513.286
====> Epoch: 043 Train loss: 1515.4138  took : 8.52520203590393
====> Test loss: 1518.8936
iteration 0000: loss: 1513.796
iteration 0100: loss: 1516.855
iteration 0200: loss: 1512.802
iteration 0300: loss: 1515.773
iteration 0400: loss: 1513.276
iteration 0500: loss: 1513.588
iteration 0600: loss: 1514.448
iteration 0700: loss: 1514.931
iteration 0800: loss: 1515.224
iteration 0900: loss: 1515.538
====> Epoch: 044 Train loss: 1515.3784  took : 8.44615387916565
====> Test loss: 1519.1032
iteration 0000: loss: 1518.159
iteration 0100: loss: 1516.881
iteration 0200: loss: 1518.420
iteration 0300: loss: 1515.253
iteration 0400: loss: 1514.449
iteration 0500: loss: 1514.513
iteration 0600: loss: 1512.941
iteration 0700: loss: 1514.575
iteration 0800: loss: 1517.081
iteration 0900: loss: 1513.154
====> Epoch: 045 Train loss: 1515.4034  took : 8.383724927902222
====> Test loss: 1518.8578
iteration 0000: loss: 1517.900
iteration 0100: loss: 1515.876
iteration 0200: loss: 1516.323
iteration 0300: loss: 1513.992
iteration 0400: loss: 1514.949
iteration 0500: loss: 1516.267
iteration 0600: loss: 1516.855
iteration 0700: loss: 1516.611
iteration 0800: loss: 1516.158
iteration 0900: loss: 1513.889
====> Epoch: 046 Train loss: 1515.2840  took : 8.532403707504272
====> Test loss: 1518.8056
iteration 0000: loss: 1514.518
iteration 0100: loss: 1514.220
iteration 0200: loss: 1513.291
iteration 0300: loss: 1516.295
iteration 0400: loss: 1513.393
iteration 0500: loss: 1516.870
iteration 0600: loss: 1515.479
iteration 0700: loss: 1513.295
iteration 0800: loss: 1514.584
iteration 0900: loss: 1515.379
====> Epoch: 047 Train loss: 1515.2181  took : 8.572100400924683
====> Test loss: 1518.6606
iteration 0000: loss: 1515.358
iteration 0100: loss: 1515.586
iteration 0200: loss: 1515.788
iteration 0300: loss: 1514.274
iteration 0400: loss: 1512.314
iteration 0500: loss: 1513.126
iteration 0600: loss: 1513.580
iteration 0700: loss: 1517.921
iteration 0800: loss: 1511.724
iteration 0900: loss: 1514.065
====> Epoch: 048 Train loss: 1515.1911  took : 8.37374496459961
====> Test loss: 1518.7181
iteration 0000: loss: 1515.361
iteration 0100: loss: 1514.987
iteration 0200: loss: 1514.422
iteration 0300: loss: 1515.298
iteration 0400: loss: 1515.035
iteration 0500: loss: 1515.509
iteration 0600: loss: 1513.177
iteration 0700: loss: 1519.518
iteration 0800: loss: 1513.861
iteration 0900: loss: 1517.008
====> Epoch: 049 Train loss: 1515.1493  took : 8.422938108444214
====> Test loss: 1518.6218
iteration 0000: loss: 1512.987
iteration 0100: loss: 1516.652
iteration 0200: loss: 1513.583
iteration 0300: loss: 1515.006
iteration 0400: loss: 1515.381
iteration 0500: loss: 1515.086
iteration 0600: loss: 1515.283
iteration 0700: loss: 1516.532
iteration 0800: loss: 1514.821
iteration 0900: loss: 1517.178
====> Epoch: 050 Train loss: 1515.1075  took : 8.541150331497192
====> Test loss: 1518.5775
====> [MM-VAE] Time: 506.536s or 00:08:26
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  10
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_10
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_10
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.963
iteration 0100: loss: 2081.796
iteration 0200: loss: 2049.497
iteration 0300: loss: 2002.332
iteration 0400: loss: 1995.494
iteration 0500: loss: 1987.093
iteration 0600: loss: 1991.164
iteration 0700: loss: 1988.973
iteration 0800: loss: 1983.190
iteration 0900: loss: 1982.801
====> Epoch: 001 Train loss: 2018.1969  took : 11.585818767547607
====> Test loss: 1987.2814
iteration 0000: loss: 1991.841
iteration 0100: loss: 1981.212
iteration 0200: loss: 1980.448
iteration 0300: loss: 1978.698
iteration 0400: loss: 1975.105
iteration 0500: loss: 1986.460
iteration 0600: loss: 1978.046
iteration 0700: loss: 1975.844
iteration 0800: loss: 1976.106
iteration 0900: loss: 1975.065
====> Epoch: 002 Train loss: 1977.7212  took : 12.852277278900146
====> Test loss: 1975.9963
iteration 0000: loss: 1971.624
iteration 0100: loss: 1971.837
iteration 0200: loss: 1967.282
iteration 0300: loss: 1974.323
iteration 0400: loss: 1970.817
iteration 0500: loss: 1965.752
iteration 0600: loss: 1966.486
iteration 0700: loss: 1971.184
iteration 0800: loss: 1967.402
iteration 0900: loss: 1968.833
====> Epoch: 003 Train loss: 1969.1190  took : 13.397919178009033
====> Test loss: 1969.7726
iteration 0000: loss: 1970.222
iteration 0100: loss: 1964.762
iteration 0200: loss: 1966.499
iteration 0300: loss: 1960.628
iteration 0400: loss: 1967.966
iteration 0500: loss: 1965.214
iteration 0600: loss: 1968.996
iteration 0700: loss: 1968.633
iteration 0800: loss: 1965.842
iteration 0900: loss: 1962.496
====> Epoch: 004 Train loss: 1965.2207  took : 12.936803579330444
====> Test loss: 1966.6305
iteration 0000: loss: 1965.203
iteration 0100: loss: 1961.481
iteration 0200: loss: 1965.973
iteration 0300: loss: 1962.923
iteration 0400: loss: 1961.403
iteration 0500: loss: 1961.732
iteration 0600: loss: 1960.385
iteration 0700: loss: 1961.604
iteration 0800: loss: 1959.525
iteration 0900: loss: 1959.058
====> Epoch: 005 Train loss: 1961.9253  took : 12.88349437713623
====> Test loss: 1962.9049
iteration 0000: loss: 1957.029
iteration 0100: loss: 1958.430
iteration 0200: loss: 1959.474
iteration 0300: loss: 1957.714
iteration 0400: loss: 1961.024
iteration 0500: loss: 1957.995
iteration 0600: loss: 1957.394
iteration 0700: loss: 1955.003
iteration 0800: loss: 1958.535
iteration 0900: loss: 1956.800
====> Epoch: 006 Train loss: 1958.8592  took : 13.326321840286255
====> Test loss: 1960.6576
iteration 0000: loss: 1956.922
iteration 0100: loss: 1955.348
iteration 0200: loss: 1956.921
iteration 0300: loss: 1958.565
iteration 0400: loss: 1957.784
iteration 0500: loss: 1954.156
iteration 0600: loss: 1959.524
iteration 0700: loss: 1956.021
iteration 0800: loss: 1955.947
iteration 0900: loss: 1954.925
====> Epoch: 007 Train loss: 1956.2594  took : 12.621249198913574
====> Test loss: 1959.0371
iteration 0000: loss: 1955.201
iteration 0100: loss: 1952.671
iteration 0200: loss: 1954.208
iteration 0300: loss: 1958.109
iteration 0400: loss: 1958.091
iteration 0500: loss: 1952.361
iteration 0600: loss: 1953.239
iteration 0700: loss: 1953.190
iteration 0800: loss: 1956.148
iteration 0900: loss: 1954.450
====> Epoch: 008 Train loss: 1954.8805  took : 12.840238332748413
====> Test loss: 1956.6641
iteration 0000: loss: 1951.993
iteration 0100: loss: 1955.590
iteration 0200: loss: 1955.621
iteration 0300: loss: 1953.053
iteration 0400: loss: 1951.663
iteration 0500: loss: 1955.319
iteration 0600: loss: 1954.050
iteration 0700: loss: 1953.291
iteration 0800: loss: 1951.422
iteration 0900: loss: 1954.998
====> Epoch: 009 Train loss: 1953.7740  took : 12.963561296463013
====> Test loss: 1955.7897
iteration 0000: loss: 1953.705
iteration 0100: loss: 1951.757
iteration 0200: loss: 1954.326
iteration 0300: loss: 1954.551
iteration 0400: loss: 1953.746
iteration 0500: loss: 1953.434
iteration 0600: loss: 1950.658
iteration 0700: loss: 1952.626
iteration 0800: loss: 1952.308
iteration 0900: loss: 1953.519
====> Epoch: 010 Train loss: 1953.0901  took : 13.128692388534546
====> Test loss: 1955.8307
iteration 0000: loss: 1956.916
iteration 0100: loss: 1952.586
iteration 0200: loss: 1953.689
iteration 0300: loss: 1952.962
iteration 0400: loss: 1955.270
iteration 0500: loss: 1950.664
iteration 0600: loss: 1952.325
iteration 0700: loss: 1951.685
iteration 0800: loss: 1952.468
iteration 0900: loss: 1951.582
====> Epoch: 011 Train loss: 1952.5372  took : 12.501307725906372
====> Test loss: 1954.8326
iteration 0000: loss: 1953.182
iteration 0100: loss: 1953.150
iteration 0200: loss: 1955.721
iteration 0300: loss: 1955.460
iteration 0400: loss: 1952.237
iteration 0500: loss: 1952.164
iteration 0600: loss: 1951.411
iteration 0700: loss: 1953.291
iteration 0800: loss: 1953.080
iteration 0900: loss: 1950.199
====> Epoch: 012 Train loss: 1952.1616  took : 12.928116083145142
====> Test loss: 1954.2141
iteration 0000: loss: 1951.996
iteration 0100: loss: 1951.412
iteration 0200: loss: 1952.049
iteration 0300: loss: 1950.464
iteration 0400: loss: 1951.570
iteration 0500: loss: 1952.226
iteration 0600: loss: 1952.238
iteration 0700: loss: 1951.998
iteration 0800: loss: 1952.210
iteration 0900: loss: 1951.897
====> Epoch: 013 Train loss: 1951.7191  took : 12.296506404876709
====> Test loss: 1954.0829
iteration 0000: loss: 1951.750
iteration 0100: loss: 1952.943
iteration 0200: loss: 1952.251
iteration 0300: loss: 1953.362
iteration 0400: loss: 1954.685
iteration 0500: loss: 1951.130
iteration 0600: loss: 1950.809
iteration 0700: loss: 1950.596
iteration 0800: loss: 1951.691
iteration 0900: loss: 1949.202
====> Epoch: 014 Train loss: 1951.3200  took : 12.186258554458618
====> Test loss: 1953.4338
iteration 0000: loss: 1949.762
iteration 0100: loss: 1952.049
iteration 0200: loss: 1952.560
iteration 0300: loss: 1951.492
iteration 0400: loss: 1950.418
iteration 0500: loss: 1950.298
iteration 0600: loss: 1950.465
iteration 0700: loss: 1952.804
iteration 0800: loss: 1949.903
iteration 0900: loss: 1951.162
====> Epoch: 015 Train loss: 1951.1606  took : 12.847442865371704
====> Test loss: 1953.1178
iteration 0000: loss: 1949.969
iteration 0100: loss: 1950.814
iteration 0200: loss: 1951.183
iteration 0300: loss: 1950.382
iteration 0400: loss: 1952.072
iteration 0500: loss: 1951.247
iteration 0600: loss: 1949.409
iteration 0700: loss: 1949.965
iteration 0800: loss: 1950.444
iteration 0900: loss: 1950.304
====> Epoch: 016 Train loss: 1950.8867  took : 12.387778282165527
====> Test loss: 1953.0432
iteration 0000: loss: 1951.454
iteration 0100: loss: 1952.353
iteration 0200: loss: 1950.878
iteration 0300: loss: 1951.691
iteration 0400: loss: 1950.570
iteration 0500: loss: 1950.018
iteration 0600: loss: 1948.724
iteration 0700: loss: 1948.273
iteration 0800: loss: 1952.119
iteration 0900: loss: 1949.516
====> Epoch: 017 Train loss: 1950.6041  took : 12.354891061782837
====> Test loss: 1953.5599
iteration 0000: loss: 1952.253
iteration 0100: loss: 1951.047
iteration 0200: loss: 1951.628
iteration 0300: loss: 1951.221
iteration 0400: loss: 1951.628
iteration 0500: loss: 1950.151
iteration 0600: loss: 1949.770
iteration 0700: loss: 1947.248
iteration 0800: loss: 1950.035
iteration 0900: loss: 1951.569
====> Epoch: 018 Train loss: 1950.4485  took : 12.464725971221924
====> Test loss: 1952.6322
iteration 0000: loss: 1950.172
iteration 0100: loss: 1952.082
iteration 0200: loss: 1950.064
iteration 0300: loss: 1951.058
iteration 0400: loss: 1953.206
iteration 0500: loss: 1948.167
iteration 0600: loss: 1949.170
iteration 0700: loss: 1949.334
iteration 0800: loss: 1948.517
iteration 0900: loss: 1949.897
====> Epoch: 019 Train loss: 1950.0686  took : 13.278144121170044
====> Test loss: 1951.8804
iteration 0000: loss: 1947.987
iteration 0100: loss: 1949.855
iteration 0200: loss: 1947.976
iteration 0300: loss: 1948.389
iteration 0400: loss: 1948.935
iteration 0500: loss: 1950.907
iteration 0600: loss: 1949.271
iteration 0700: loss: 1948.836
iteration 0800: loss: 1949.566
iteration 0900: loss: 1949.759
====> Epoch: 020 Train loss: 1949.8590  took : 13.019688844680786
====> Test loss: 1952.2719
iteration 0000: loss: 1949.194
iteration 0100: loss: 1949.805
iteration 0200: loss: 1948.836
iteration 0300: loss: 1952.744
iteration 0400: loss: 1949.149
iteration 0500: loss: 1948.515
iteration 0600: loss: 1948.257
iteration 0700: loss: 1949.541
iteration 0800: loss: 1949.924
iteration 0900: loss: 1949.509
====> Epoch: 021 Train loss: 1949.7067  took : 12.970632314682007
====> Test loss: 1951.8383
iteration 0000: loss: 1949.670
iteration 0100: loss: 1952.039
iteration 0200: loss: 1949.750
iteration 0300: loss: 1948.398
iteration 0400: loss: 1950.480
iteration 0500: loss: 1949.938
iteration 0600: loss: 1950.728
iteration 0700: loss: 1950.204
iteration 0800: loss: 1948.769
iteration 0900: loss: 1948.179
====> Epoch: 022 Train loss: 1949.6861  took : 11.673261165618896
====> Test loss: 1952.1683
iteration 0000: loss: 1948.815
iteration 0100: loss: 1948.952
iteration 0200: loss: 1949.861
iteration 0300: loss: 1949.788
iteration 0400: loss: 1948.789
iteration 0500: loss: 1949.179
iteration 0600: loss: 1950.246
iteration 0700: loss: 1948.941
iteration 0800: loss: 1950.410
iteration 0900: loss: 1949.019
====> Epoch: 023 Train loss: 1949.5832  took : 12.477578163146973
====> Test loss: 1951.6030
iteration 0000: loss: 1948.860
iteration 0100: loss: 1949.653
iteration 0200: loss: 1949.567
iteration 0300: loss: 1949.055
iteration 0400: loss: 1949.597
iteration 0500: loss: 1949.439
iteration 0600: loss: 1949.749
iteration 0700: loss: 1948.466
iteration 0800: loss: 1947.301
iteration 0900: loss: 1950.451
====> Epoch: 024 Train loss: 1949.3729  took : 11.940723896026611
====> Test loss: 1951.1637
iteration 0000: loss: 1949.003
iteration 0100: loss: 1949.007
iteration 0200: loss: 1948.489
iteration 0300: loss: 1948.696
iteration 0400: loss: 1947.668
iteration 0500: loss: 1948.284
iteration 0600: loss: 1948.646
iteration 0700: loss: 1950.584
iteration 0800: loss: 1948.496
iteration 0900: loss: 1948.682
====> Epoch: 025 Train loss: 1949.0457  took : 12.07572317123413
====> Test loss: 1950.9104
iteration 0000: loss: 1947.627
iteration 0100: loss: 1949.035
iteration 0200: loss: 1949.407
iteration 0300: loss: 1948.202
iteration 0400: loss: 1948.520
iteration 0500: loss: 1948.872
iteration 0600: loss: 1948.792
iteration 0700: loss: 1947.215
iteration 0800: loss: 1949.110
iteration 0900: loss: 1947.717
====> Epoch: 026 Train loss: 1948.8265  took : 12.09163212776184
====> Test loss: 1951.1031
iteration 0000: loss: 1950.976
iteration 0100: loss: 1950.053
iteration 0200: loss: 1948.325
iteration 0300: loss: 1948.975
iteration 0400: loss: 1949.224
iteration 0500: loss: 1949.008
iteration 0600: loss: 1950.172
iteration 0700: loss: 1948.250
iteration 0800: loss: 1948.686
iteration 0900: loss: 1949.397
====> Epoch: 027 Train loss: 1948.8800  took : 12.283458232879639
====> Test loss: 1950.6652
iteration 0000: loss: 1949.768
iteration 0100: loss: 1948.948
iteration 0200: loss: 1948.423
iteration 0300: loss: 1948.574
iteration 0400: loss: 1948.680
iteration 0500: loss: 1949.173
iteration 0600: loss: 1947.966
iteration 0700: loss: 1948.692
iteration 0800: loss: 1946.866
iteration 0900: loss: 1949.493
====> Epoch: 028 Train loss: 1948.7582  took : 12.881425619125366
====> Test loss: 1951.1344
iteration 0000: loss: 1947.167
iteration 0100: loss: 1947.780
iteration 0200: loss: 1948.233
iteration 0300: loss: 1947.512
iteration 0400: loss: 1947.929
iteration 0500: loss: 1947.870
iteration 0600: loss: 1949.545
iteration 0700: loss: 1948.167
iteration 0800: loss: 1948.227
iteration 0900: loss: 1948.301
====> Epoch: 029 Train loss: 1948.6562  took : 13.00114393234253
====> Test loss: 1950.6738
iteration 0000: loss: 1947.682
iteration 0100: loss: 1948.416
iteration 0200: loss: 1949.681
iteration 0300: loss: 1948.516
iteration 0400: loss: 1948.088
iteration 0500: loss: 1947.675
iteration 0600: loss: 1947.985
iteration 0700: loss: 1949.030
iteration 0800: loss: 1947.536
iteration 0900: loss: 1949.595
====> Epoch: 030 Train loss: 1948.5852  took : 12.95083212852478
====> Test loss: 1950.5905
iteration 0000: loss: 1950.467
iteration 0100: loss: 1948.436
iteration 0200: loss: 1948.214
iteration 0300: loss: 1948.274
iteration 0400: loss: 1948.769
iteration 0500: loss: 1948.528
iteration 0600: loss: 1948.003
iteration 0700: loss: 1947.780
iteration 0800: loss: 1947.526
iteration 0900: loss: 1947.895
====> Epoch: 031 Train loss: 1948.3862  took : 12.473951578140259
====> Test loss: 1950.6069
iteration 0000: loss: 1947.583
iteration 0100: loss: 1947.630
iteration 0200: loss: 1947.204
iteration 0300: loss: 1949.477
iteration 0400: loss: 1949.170
iteration 0500: loss: 1947.773
iteration 0600: loss: 1948.927
iteration 0700: loss: 1948.327
iteration 0800: loss: 1948.874
iteration 0900: loss: 1948.461
====> Epoch: 032 Train loss: 1948.3756  took : 12.758084058761597
====> Test loss: 1950.2272
iteration 0000: loss: 1949.489
iteration 0100: loss: 1948.852
iteration 0200: loss: 1949.585
iteration 0300: loss: 1948.628
iteration 0400: loss: 1948.314
iteration 0500: loss: 1948.615
iteration 0600: loss: 1950.012
iteration 0700: loss: 1946.941
iteration 0800: loss: 1947.764
iteration 0900: loss: 1948.028
====> Epoch: 033 Train loss: 1948.4420  took : 12.270110368728638
====> Test loss: 1950.6586
iteration 0000: loss: 1948.850
iteration 0100: loss: 1947.392
iteration 0200: loss: 1949.087
iteration 0300: loss: 1948.241
iteration 0400: loss: 1949.211
iteration 0500: loss: 1947.411
iteration 0600: loss: 1948.076
iteration 0700: loss: 1947.791
iteration 0800: loss: 1949.957
iteration 0900: loss: 1948.213
====> Epoch: 034 Train loss: 1948.4482  took : 12.84253740310669
====> Test loss: 1950.5824
iteration 0000: loss: 1947.821
iteration 0100: loss: 1946.963
iteration 0200: loss: 1949.353
iteration 0300: loss: 1947.718
iteration 0400: loss: 1948.811
iteration 0500: loss: 1947.570
iteration 0600: loss: 1947.869
iteration 0700: loss: 1948.250
iteration 0800: loss: 1949.571
iteration 0900: loss: 1947.463
====> Epoch: 035 Train loss: 1948.4338  took : 12.340590715408325
====> Test loss: 1950.3835
iteration 0000: loss: 1948.787
iteration 0100: loss: 1947.466
iteration 0200: loss: 1948.550
iteration 0300: loss: 1947.440
iteration 0400: loss: 1948.021
iteration 0500: loss: 1949.246
iteration 0600: loss: 1948.705
iteration 0700: loss: 1948.917
iteration 0800: loss: 1949.998
iteration 0900: loss: 1948.113
====> Epoch: 036 Train loss: 1948.3644  took : 12.92483377456665
====> Test loss: 1950.3149
iteration 0000: loss: 1947.499
iteration 0100: loss: 1947.828
iteration 0200: loss: 1947.958
iteration 0300: loss: 1947.530
iteration 0400: loss: 1948.375
iteration 0500: loss: 1947.187
iteration 0600: loss: 1947.749
iteration 0700: loss: 1947.846
iteration 0800: loss: 1947.162
iteration 0900: loss: 1950.964
====> Epoch: 037 Train loss: 1948.3268  took : 11.843239545822144
====> Test loss: 1950.5052
iteration 0000: loss: 1949.109
iteration 0100: loss: 1948.350
iteration 0200: loss: 1948.152
iteration 0300: loss: 1948.075
iteration 0400: loss: 1948.554
iteration 0500: loss: 1947.839
iteration 0600: loss: 1948.212
iteration 0700: loss: 1947.663
iteration 0800: loss: 1947.053
iteration 0900: loss: 1950.323
====> Epoch: 038 Train loss: 1948.2404  took : 12.653075933456421
====> Test loss: 1950.4106
iteration 0000: loss: 1948.432
iteration 0100: loss: 1949.875
iteration 0200: loss: 1949.651
iteration 0300: loss: 1948.781
iteration 0400: loss: 1949.190
iteration 0500: loss: 1949.297
iteration 0600: loss: 1949.565
iteration 0700: loss: 1949.208
iteration 0800: loss: 1948.114
iteration 0900: loss: 1948.070
====> Epoch: 039 Train loss: 1948.2044  took : 12.741024494171143
====> Test loss: 1950.1506
iteration 0000: loss: 1947.967
iteration 0100: loss: 1947.247
iteration 0200: loss: 1948.129
iteration 0300: loss: 1947.769
iteration 0400: loss: 1946.668
iteration 0500: loss: 1947.202
iteration 0600: loss: 1948.185
iteration 0700: loss: 1948.466
iteration 0800: loss: 1947.283
iteration 0900: loss: 1947.486
====> Epoch: 040 Train loss: 1948.0334  took : 12.426786661148071
====> Test loss: 1949.8898
iteration 0000: loss: 1948.344
iteration 0100: loss: 1947.193
iteration 0200: loss: 1947.929
iteration 0300: loss: 1947.299
iteration 0400: loss: 1947.872
iteration 0500: loss: 1948.521
iteration 0600: loss: 1947.233
iteration 0700: loss: 1948.329
iteration 0800: loss: 1947.929
iteration 0900: loss: 1947.488
====> Epoch: 041 Train loss: 1948.1739  took : 13.209376573562622
====> Test loss: 1950.1488
iteration 0000: loss: 1946.947
iteration 0100: loss: 1947.807
iteration 0200: loss: 1947.852
iteration 0300: loss: 1948.231
iteration 0400: loss: 1948.342
iteration 0500: loss: 1948.649
iteration 0600: loss: 1947.892
iteration 0700: loss: 1947.899
iteration 0800: loss: 1947.994
iteration 0900: loss: 1950.039
====> Epoch: 042 Train loss: 1948.0572  took : 12.508529901504517
====> Test loss: 1949.9616
iteration 0000: loss: 1948.104
iteration 0100: loss: 1947.486
iteration 0200: loss: 1947.875
iteration 0300: loss: 1949.640
iteration 0400: loss: 1948.156
iteration 0500: loss: 1947.545
iteration 0600: loss: 1947.602
iteration 0700: loss: 1947.283
iteration 0800: loss: 1947.610
iteration 0900: loss: 1947.284
====> Epoch: 043 Train loss: 1947.9935  took : 12.427963495254517
====> Test loss: 1950.0463
iteration 0000: loss: 1947.305
iteration 0100: loss: 1948.035
iteration 0200: loss: 1948.262
iteration 0300: loss: 1950.122
iteration 0400: loss: 1947.801
iteration 0500: loss: 1948.253
iteration 0600: loss: 1947.651
iteration 0700: loss: 1947.216
iteration 0800: loss: 1947.439
iteration 0900: loss: 1948.164
====> Epoch: 044 Train loss: 1948.2697  took : 12.876352310180664
====> Test loss: 1950.2669
iteration 0000: loss: 1947.001
iteration 0100: loss: 1947.601
iteration 0200: loss: 1947.323
iteration 0300: loss: 1948.247
iteration 0400: loss: 1948.068
iteration 0500: loss: 1947.639
iteration 0600: loss: 1948.893
iteration 0700: loss: 1948.101
iteration 0800: loss: 1948.546
iteration 0900: loss: 1948.005
====> Epoch: 045 Train loss: 1948.1506  took : 12.925284624099731
====> Test loss: 1950.1206
iteration 0000: loss: 1948.591
iteration 0100: loss: 1947.737
iteration 0200: loss: 1947.962
iteration 0300: loss: 1947.260
iteration 0400: loss: 1947.580
iteration 0500: loss: 1949.222
iteration 0600: loss: 1948.643
iteration 0700: loss: 1949.198
iteration 0800: loss: 1948.167
iteration 0900: loss: 1947.967
====> Epoch: 046 Train loss: 1948.1544  took : 13.229308605194092
====> Test loss: 1950.2364
iteration 0000: loss: 1948.315
iteration 0100: loss: 1948.106
iteration 0200: loss: 1947.833
iteration 0300: loss: 1948.262
iteration 0400: loss: 1947.965
iteration 0500: loss: 1947.223
iteration 0600: loss: 1948.111
iteration 0700: loss: 1948.814
iteration 0800: loss: 1949.986
iteration 0900: loss: 1948.182
====> Epoch: 047 Train loss: 1948.0714  took : 12.831356763839722
====> Test loss: 1949.8081
iteration 0000: loss: 1947.333
iteration 0100: loss: 1947.630
iteration 0200: loss: 1947.398
iteration 0300: loss: 1948.070
iteration 0400: loss: 1947.792
iteration 0500: loss: 1948.640
iteration 0600: loss: 1948.417
iteration 0700: loss: 1947.064
iteration 0800: loss: 1948.264
iteration 0900: loss: 1948.188
====> Epoch: 048 Train loss: 1948.0130  took : 12.158997774124146
====> Test loss: 1949.8285
iteration 0000: loss: 1947.169
iteration 0100: loss: 1949.213
iteration 0200: loss: 1948.053
iteration 0300: loss: 1947.654
iteration 0400: loss: 1948.213
iteration 0500: loss: 1948.133
iteration 0600: loss: 1948.501
iteration 0700: loss: 1947.815
iteration 0800: loss: 1948.355
iteration 0900: loss: 1948.396
====> Epoch: 049 Train loss: 1948.0787  took : 12.250800848007202
====> Test loss: 1950.1105
iteration 0000: loss: 1948.570
iteration 0100: loss: 1948.216
iteration 0200: loss: 1948.900
iteration 0300: loss: 1948.068
iteration 0400: loss: 1947.082
iteration 0500: loss: 1948.067
iteration 0600: loss: 1947.810
iteration 0700: loss: 1947.543
iteration 0800: loss: 1948.731
iteration 0900: loss: 1948.583
====> Epoch: 050 Train loss: 1948.1411  took : 13.118880033493042
====> Test loss: 1949.9699
====> [MM-VAE] Time: 702.455s or 00:11:42
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  10
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_10
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_10
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5220.348
iteration 0100: loss: 4132.213
iteration 0200: loss: 4092.662
iteration 0300: loss: 4059.159
iteration 0400: loss: 4035.854
iteration 0500: loss: 4013.398
iteration 0600: loss: 4024.967
iteration 0700: loss: 4022.249
iteration 0800: loss: 4009.179
iteration 0900: loss: 3998.070
iteration 1000: loss: 4009.406
iteration 1100: loss: 3989.357
iteration 1200: loss: 4008.796
iteration 1300: loss: 3991.560
iteration 1400: loss: 4005.326
iteration 1500: loss: 3996.066
iteration 1600: loss: 3992.259
iteration 1700: loss: 4013.405
iteration 1800: loss: 3984.545
====> Epoch: 001 Train loss: 4026.2623  took : 53.279263734817505
====> Test loss: 3994.4564
iteration 0000: loss: 3992.119
iteration 0100: loss: 3999.309
iteration 0200: loss: 3993.088
iteration 0300: loss: 3990.348
iteration 0400: loss: 3991.062
iteration 0500: loss: 3997.214
iteration 0600: loss: 3982.689
iteration 0700: loss: 3987.992
iteration 0800: loss: 3984.296
iteration 0900: loss: 3983.589
iteration 1000: loss: 3979.872
iteration 1100: loss: 3978.585
iteration 1200: loss: 3979.588
iteration 1300: loss: 3978.914
iteration 1400: loss: 3980.075
iteration 1500: loss: 3986.089
iteration 1600: loss: 3974.327
iteration 1700: loss: 3971.669
iteration 1800: loss: 3980.321
====> Epoch: 002 Train loss: 3983.4369  took : 53.62547159194946
====> Test loss: 3979.7535
iteration 0000: loss: 3990.239
iteration 0100: loss: 3973.928
iteration 0200: loss: 3980.225
iteration 0300: loss: 3966.505
iteration 0400: loss: 3974.435
iteration 0500: loss: 3971.673
iteration 0600: loss: 3973.167
iteration 0700: loss: 3975.350
iteration 0800: loss: 3968.810
iteration 0900: loss: 3967.228
iteration 1000: loss: 3971.141
iteration 1100: loss: 3966.794
iteration 1200: loss: 3964.673
iteration 1300: loss: 3969.474
iteration 1400: loss: 3963.339
iteration 1500: loss: 3963.492
iteration 1600: loss: 3964.500
iteration 1700: loss: 3967.029
iteration 1800: loss: 3962.231
====> Epoch: 003 Train loss: 3970.7099  took : 53.27744770050049
====> Test loss: 3968.5169
iteration 0000: loss: 3959.054
iteration 0100: loss: 3965.940
iteration 0200: loss: 3961.126
iteration 0300: loss: 3959.759
iteration 0400: loss: 3962.810
iteration 0500: loss: 3959.735
iteration 0600: loss: 3962.607
iteration 0700: loss: 3962.460
iteration 0800: loss: 3966.949
iteration 0900: loss: 3959.934
iteration 1000: loss: 3951.627
iteration 1100: loss: 3950.532
iteration 1200: loss: 3962.440
iteration 1300: loss: 3964.066
iteration 1400: loss: 3965.077
iteration 1500: loss: 3960.600
iteration 1600: loss: 3958.100
iteration 1700: loss: 3947.700
iteration 1800: loss: 3947.029
====> Epoch: 004 Train loss: 3960.1983  took : 53.276320695877075
====> Test loss: 3958.4099
iteration 0000: loss: 3956.693
iteration 0100: loss: 3951.140
iteration 0200: loss: 3952.868
iteration 0300: loss: 3955.108
iteration 0400: loss: 3956.420
iteration 0500: loss: 3959.976
iteration 0600: loss: 3958.140
iteration 0700: loss: 3953.645
iteration 0800: loss: 3953.573
iteration 0900: loss: 3952.992
iteration 1000: loss: 3965.334
iteration 1100: loss: 3950.838
iteration 1200: loss: 3951.617
iteration 1300: loss: 3955.022
iteration 1400: loss: 3950.520
iteration 1500: loss: 3956.083
iteration 1600: loss: 3954.903
iteration 1700: loss: 3949.608
iteration 1800: loss: 3953.844
====> Epoch: 005 Train loss: 3953.3799  took : 53.332762241363525
====> Test loss: 3953.0511
iteration 0000: loss: 3952.755
iteration 0100: loss: 3955.053
iteration 0200: loss: 3949.627
iteration 0300: loss: 3947.505
iteration 0400: loss: 3952.014
iteration 0500: loss: 3946.606
iteration 0600: loss: 3946.578
iteration 0700: loss: 3947.538
iteration 0800: loss: 3950.613
iteration 0900: loss: 3955.995
iteration 1000: loss: 3954.183
iteration 1100: loss: 3951.476
iteration 1200: loss: 3945.611
iteration 1300: loss: 3948.073
iteration 1400: loss: 3948.767
iteration 1500: loss: 3943.337
iteration 1600: loss: 3945.996
iteration 1700: loss: 3950.133
iteration 1800: loss: 3944.350
====> Epoch: 006 Train loss: 3949.0574  took : 53.04882764816284
====> Test loss: 3951.0686
iteration 0000: loss: 3946.000
iteration 0100: loss: 3951.451
iteration 0200: loss: 3944.618
iteration 0300: loss: 3944.062
iteration 0400: loss: 3952.448
iteration 0500: loss: 3955.458
iteration 0600: loss: 3946.334
iteration 0700: loss: 3938.101
iteration 0800: loss: 3947.037
iteration 0900: loss: 3946.809
iteration 1000: loss: 3949.903
iteration 1100: loss: 3946.427
iteration 1200: loss: 3950.804
iteration 1300: loss: 3950.213
iteration 1400: loss: 3942.502
iteration 1500: loss: 3947.346
iteration 1600: loss: 3943.131
iteration 1700: loss: 3941.618
iteration 1800: loss: 3946.369
====> Epoch: 007 Train loss: 3946.4429  took : 53.08497905731201
====> Test loss: 3948.6577
iteration 0000: loss: 3944.577
iteration 0100: loss: 3949.707
iteration 0200: loss: 3952.378
iteration 0300: loss: 3949.783
iteration 0400: loss: 3952.504
iteration 0500: loss: 3951.635
iteration 0600: loss: 3943.183
iteration 0700: loss: 3947.505
iteration 0800: loss: 3940.605
iteration 0900: loss: 3948.048
iteration 1000: loss: 3936.335
iteration 1100: loss: 3942.458
iteration 1200: loss: 3938.446
iteration 1300: loss: 3945.606
iteration 1400: loss: 3944.467
iteration 1500: loss: 3939.082
iteration 1600: loss: 3951.132
iteration 1700: loss: 3947.192
iteration 1800: loss: 3944.339
====> Epoch: 008 Train loss: 3944.7784  took : 53.32958507537842
====> Test loss: 3948.2839
iteration 0000: loss: 3937.979
iteration 0100: loss: 3943.661
iteration 0200: loss: 3945.065
iteration 0300: loss: 3946.795
iteration 0400: loss: 3945.798
iteration 0500: loss: 3943.057
iteration 0600: loss: 3951.985
iteration 0700: loss: 3942.118
iteration 0800: loss: 3937.449
iteration 0900: loss: 3944.630
iteration 1000: loss: 3944.681
iteration 1100: loss: 3940.954
iteration 1200: loss: 3949.338
iteration 1300: loss: 3941.451
iteration 1400: loss: 3938.877
iteration 1500: loss: 3945.610
iteration 1600: loss: 3940.666
iteration 1700: loss: 3940.694
iteration 1800: loss: 3946.614
====> Epoch: 009 Train loss: 3943.9011  took : 53.01928400993347
====> Test loss: 3947.4306
iteration 0000: loss: 3939.253
iteration 0100: loss: 3950.598
iteration 0200: loss: 3948.140
iteration 0300: loss: 3950.674
iteration 0400: loss: 3946.744
iteration 0500: loss: 3939.868
iteration 0600: loss: 3943.227
iteration 0700: loss: 3944.504
iteration 0800: loss: 3943.971
iteration 0900: loss: 3937.699
iteration 1000: loss: 3940.817
iteration 1100: loss: 3940.713
iteration 1200: loss: 3944.066
iteration 1300: loss: 3942.643
iteration 1400: loss: 3948.168
iteration 1500: loss: 3940.935
iteration 1600: loss: 3941.593
iteration 1700: loss: 3947.812
iteration 1800: loss: 3942.929
====> Epoch: 010 Train loss: 3943.4191  took : 53.126707553863525
====> Test loss: 3946.9273
iteration 0000: loss: 3938.142
iteration 0100: loss: 3944.288
iteration 0200: loss: 3943.740
iteration 0300: loss: 3942.280
iteration 0400: loss: 3945.914
iteration 0500: loss: 3944.299
iteration 0600: loss: 3939.992
iteration 0700: loss: 3940.254
iteration 0800: loss: 3949.484
iteration 0900: loss: 3936.830
iteration 1000: loss: 3936.349
iteration 1100: loss: 3940.640
iteration 1200: loss: 3938.732
iteration 1300: loss: 3940.460
iteration 1400: loss: 3944.567
iteration 1500: loss: 3947.130
iteration 1600: loss: 3942.541
iteration 1700: loss: 3941.375
iteration 1800: loss: 3940.977
====> Epoch: 011 Train loss: 3942.9412  took : 52.93703889846802
====> Test loss: 3946.1258
iteration 0000: loss: 3941.009
iteration 0100: loss: 3940.156
iteration 0200: loss: 3947.955
iteration 0300: loss: 3941.976
iteration 0400: loss: 3942.489
iteration 0500: loss: 3939.396
iteration 0600: loss: 3948.724
iteration 0700: loss: 3945.286
iteration 0800: loss: 3932.844
iteration 0900: loss: 3950.913
iteration 1000: loss: 3945.490
iteration 1100: loss: 3940.426
iteration 1200: loss: 3939.926
iteration 1300: loss: 3936.627
iteration 1400: loss: 3944.868
iteration 1500: loss: 3939.742
iteration 1600: loss: 3940.913
iteration 1700: loss: 3944.983
iteration 1800: loss: 3940.167
====> Epoch: 012 Train loss: 3942.4848  took : 52.83071041107178
====> Test loss: 3945.9873
iteration 0000: loss: 3944.976
iteration 0100: loss: 3938.971
iteration 0200: loss: 3944.451
iteration 0300: loss: 3939.880
iteration 0400: loss: 3941.161
iteration 0500: loss: 3942.250
iteration 0600: loss: 3945.239
iteration 0700: loss: 3945.079
iteration 0800: loss: 3941.599
iteration 0900: loss: 3941.291
iteration 1000: loss: 3948.642
iteration 1100: loss: 3944.287
iteration 1200: loss: 3940.534
iteration 1300: loss: 3936.856
iteration 1400: loss: 3945.128
iteration 1500: loss: 3941.831
iteration 1600: loss: 3939.583
iteration 1700: loss: 3944.328
iteration 1800: loss: 3942.336
====> Epoch: 013 Train loss: 3941.9760  took : 53.24275016784668
====> Test loss: 3945.7973
iteration 0000: loss: 3949.013
iteration 0100: loss: 3941.694
iteration 0200: loss: 3938.373
iteration 0300: loss: 3947.241
iteration 0400: loss: 3945.364
iteration 0500: loss: 3941.118
iteration 0600: loss: 3937.700
iteration 0700: loss: 3943.871
iteration 0800: loss: 3943.025
iteration 0900: loss: 3941.117
iteration 1000: loss: 3941.656
iteration 1100: loss: 3952.055
iteration 1200: loss: 3945.356
iteration 1300: loss: 3938.728
iteration 1400: loss: 3942.536
iteration 1500: loss: 3937.974
iteration 1600: loss: 3941.574
iteration 1700: loss: 3946.660
iteration 1800: loss: 3942.741
====> Epoch: 014 Train loss: 3941.6574  took : 52.93167757987976
====> Test loss: 3945.4183
iteration 0000: loss: 3937.754
iteration 0100: loss: 3940.349
iteration 0200: loss: 3943.206
iteration 0300: loss: 3943.245
iteration 0400: loss: 3938.470
iteration 0500: loss: 3945.774
iteration 0600: loss: 3940.496
iteration 0700: loss: 3938.192
iteration 0800: loss: 3931.627
iteration 0900: loss: 3941.936
iteration 1000: loss: 3938.827
iteration 1100: loss: 3937.381
iteration 1200: loss: 3941.902
iteration 1300: loss: 3938.266
iteration 1400: loss: 3939.051
iteration 1500: loss: 3940.239
iteration 1600: loss: 3944.192
iteration 1700: loss: 3944.438
iteration 1800: loss: 3944.914
====> Epoch: 015 Train loss: 3941.3045  took : 53.12430810928345
====> Test loss: 3945.1549
iteration 0000: loss: 3938.744
iteration 0100: loss: 3939.319
iteration 0200: loss: 3947.857
iteration 0300: loss: 3943.376
iteration 0400: loss: 3946.929
iteration 0500: loss: 3942.271
iteration 0600: loss: 3936.165
iteration 0700: loss: 3936.464
iteration 0800: loss: 3939.001
iteration 0900: loss: 3936.091
iteration 1000: loss: 3942.193
iteration 1100: loss: 3931.325
iteration 1200: loss: 3945.001
iteration 1300: loss: 3940.972
iteration 1400: loss: 3939.291
iteration 1500: loss: 3945.294
iteration 1600: loss: 3945.644
iteration 1700: loss: 3942.926
iteration 1800: loss: 3948.071
====> Epoch: 016 Train loss: 3941.2145  took : 52.97652077674866
====> Test loss: 3944.8346
iteration 0000: loss: 3942.140
iteration 0100: loss: 3939.602
iteration 0200: loss: 3941.553
iteration 0300: loss: 3938.970
iteration 0400: loss: 3943.203
iteration 0500: loss: 3944.686
iteration 0600: loss: 3936.246
iteration 0700: loss: 3941.149
iteration 0800: loss: 3942.046
iteration 0900: loss: 3942.428
iteration 1000: loss: 3937.293
iteration 1100: loss: 3939.767
iteration 1200: loss: 3940.825
iteration 1300: loss: 3935.327
iteration 1400: loss: 3940.146
iteration 1500: loss: 3947.790
iteration 1600: loss: 3938.437
iteration 1700: loss: 3947.558
iteration 1800: loss: 3929.020
====> Epoch: 017 Train loss: 3940.9899  took : 53.090373277664185
====> Test loss: 3944.2723
iteration 0000: loss: 3945.075
iteration 0100: loss: 3937.102
iteration 0200: loss: 3942.087
iteration 0300: loss: 3941.579
iteration 0400: loss: 3940.987
iteration 0500: loss: 3937.722
iteration 0600: loss: 3952.452
iteration 0700: loss: 3944.971
iteration 0800: loss: 3940.044
iteration 0900: loss: 3939.737
iteration 1000: loss: 3941.532
iteration 1100: loss: 3943.825
iteration 1200: loss: 3946.532
iteration 1300: loss: 3940.841
iteration 1400: loss: 3945.747
iteration 1500: loss: 3939.398
iteration 1600: loss: 3942.523
iteration 1700: loss: 3941.881
iteration 1800: loss: 3934.713
====> Epoch: 018 Train loss: 3941.1403  took : 52.887336015701294
====> Test loss: 3945.3735
iteration 0000: loss: 3941.461
iteration 0100: loss: 3938.808
iteration 0200: loss: 3937.512
iteration 0300: loss: 3943.966
iteration 0400: loss: 3940.565
iteration 0500: loss: 3939.300
iteration 0600: loss: 3938.264
iteration 0700: loss: 3938.331
iteration 0800: loss: 3939.127
iteration 0900: loss: 3944.557
iteration 1000: loss: 3935.585
iteration 1100: loss: 3940.989
iteration 1200: loss: 3940.725
iteration 1300: loss: 3940.830
iteration 1400: loss: 3936.033
iteration 1500: loss: 3939.899
iteration 1600: loss: 3942.380
iteration 1700: loss: 3937.165
iteration 1800: loss: 3941.302
====> Epoch: 019 Train loss: 3940.6332  took : 53.104493618011475
====> Test loss: 3944.2267
iteration 0000: loss: 3943.277
iteration 0100: loss: 3935.185
iteration 0200: loss: 3941.137
iteration 0300: loss: 3940.957
iteration 0400: loss: 3943.257
iteration 0500: loss: 3940.809
iteration 0600: loss: 3945.392
iteration 0700: loss: 3942.437
iteration 0800: loss: 3941.390
iteration 0900: loss: 3937.474
iteration 1000: loss: 3943.047
iteration 1100: loss: 3934.334
iteration 1200: loss: 3942.806
iteration 1300: loss: 3941.387
iteration 1400: loss: 3935.322
iteration 1500: loss: 3942.253
iteration 1600: loss: 3938.456
iteration 1700: loss: 3935.563
iteration 1800: loss: 3947.848
====> Epoch: 020 Train loss: 3940.4104  took : 52.995107650756836
====> Test loss: 3944.2868
iteration 0000: loss: 3942.570
iteration 0100: loss: 3937.971
iteration 0200: loss: 3945.744
iteration 0300: loss: 3937.528
iteration 0400: loss: 3938.961
iteration 0500: loss: 3944.347
iteration 0600: loss: 3938.936
iteration 0700: loss: 3941.683
iteration 0800: loss: 3948.210
iteration 0900: loss: 3939.788
iteration 1000: loss: 3938.293
iteration 1100: loss: 3940.203
iteration 1200: loss: 3942.160
iteration 1300: loss: 3940.609
iteration 1400: loss: 3935.095
iteration 1500: loss: 3939.945
iteration 1600: loss: 3940.034
iteration 1700: loss: 3936.745
iteration 1800: loss: 3944.859
====> Epoch: 021 Train loss: 3940.3212  took : 53.127556800842285
====> Test loss: 3943.6947
iteration 0000: loss: 3943.830
iteration 0100: loss: 3936.647
iteration 0200: loss: 3947.729
iteration 0300: loss: 3937.118
iteration 0400: loss: 3937.882
iteration 0500: loss: 3939.271
iteration 0600: loss: 3945.935
iteration 0700: loss: 3936.323
iteration 0800: loss: 3940.885
iteration 0900: loss: 3943.396
iteration 1000: loss: 3936.969
iteration 1100: loss: 3940.236
iteration 1200: loss: 3936.651
iteration 1300: loss: 3940.511
iteration 1400: loss: 3931.972
iteration 1500: loss: 3939.262
iteration 1600: loss: 3943.905
iteration 1700: loss: 3941.594
iteration 1800: loss: 3934.529
====> Epoch: 022 Train loss: 3940.3287  took : 53.07296562194824
====> Test loss: 3944.1061
iteration 0000: loss: 3943.417
iteration 0100: loss: 3936.811
iteration 0200: loss: 3939.011
iteration 0300: loss: 3942.990
iteration 0400: loss: 3933.276
iteration 0500: loss: 3942.549
iteration 0600: loss: 3941.758
iteration 0700: loss: 3933.082
iteration 0800: loss: 3937.462
iteration 0900: loss: 3940.398
iteration 1000: loss: 3937.555
iteration 1100: loss: 3937.147
iteration 1200: loss: 3941.364
iteration 1300: loss: 3938.753
iteration 1400: loss: 3937.367
iteration 1500: loss: 3929.241
iteration 1600: loss: 3938.565
iteration 1700: loss: 3939.286
iteration 1800: loss: 3941.157
====> Epoch: 023 Train loss: 3939.9543  took : 52.97670340538025
====> Test loss: 3944.4756
iteration 0000: loss: 3934.778
iteration 0100: loss: 3937.809
iteration 0200: loss: 3944.160
iteration 0300: loss: 3940.866
iteration 0400: loss: 3946.614
iteration 0500: loss: 3936.413
iteration 0600: loss: 3940.571
iteration 0700: loss: 3942.415
iteration 0800: loss: 3931.284
iteration 0900: loss: 3933.506
iteration 1000: loss: 3940.306
iteration 1100: loss: 3939.398
iteration 1200: loss: 3935.974
iteration 1300: loss: 3938.705
iteration 1400: loss: 3937.456
iteration 1500: loss: 3943.287
iteration 1600: loss: 3940.730
iteration 1700: loss: 3943.865
iteration 1800: loss: 3941.600
====> Epoch: 024 Train loss: 3939.8908  took : 53.050326108932495
====> Test loss: 3943.9893
iteration 0000: loss: 3941.880
iteration 0100: loss: 3935.789
iteration 0200: loss: 3937.773
iteration 0300: loss: 3935.899
iteration 0400: loss: 3946.182
iteration 0500: loss: 3941.268
iteration 0600: loss: 3946.314
iteration 0700: loss: 3938.372
iteration 0800: loss: 3940.181
iteration 0900: loss: 3945.815
iteration 1000: loss: 3945.386
iteration 1100: loss: 3945.011
iteration 1200: loss: 3949.544
iteration 1300: loss: 3940.515
iteration 1400: loss: 3943.970
iteration 1500: loss: 3939.823
iteration 1600: loss: 3939.029
iteration 1700: loss: 3943.626
iteration 1800: loss: 3939.721
====> Epoch: 025 Train loss: 3940.0039  took : 52.927114963531494
====> Test loss: 3943.6218
iteration 0000: loss: 3934.414
iteration 0100: loss: 3937.811
iteration 0200: loss: 3938.600
iteration 0300: loss: 3941.510
iteration 0400: loss: 3941.403
iteration 0500: loss: 3949.032
iteration 0600: loss: 3936.827
iteration 0700: loss: 3936.085
iteration 0800: loss: 3939.250
iteration 0900: loss: 3941.251
iteration 1000: loss: 3934.793
iteration 1100: loss: 3942.298
iteration 1200: loss: 3941.478
iteration 1300: loss: 3946.307
iteration 1400: loss: 3937.393
iteration 1500: loss: 3942.200
iteration 1600: loss: 3942.694
iteration 1700: loss: 3939.703
iteration 1800: loss: 3940.824
====> Epoch: 026 Train loss: 3940.0982  took : 53.01451849937439
====> Test loss: 3943.9853
iteration 0000: loss: 3940.565
iteration 0100: loss: 3938.196
iteration 0200: loss: 3941.096
iteration 0300: loss: 3943.709
iteration 0400: loss: 3938.294
iteration 0500: loss: 3941.145
iteration 0600: loss: 3935.863
iteration 0700: loss: 3931.917
iteration 0800: loss: 3942.953
iteration 0900: loss: 3939.110
iteration 1000: loss: 3935.986
iteration 1100: loss: 3942.801
iteration 1200: loss: 3940.493
iteration 1300: loss: 3935.962
iteration 1400: loss: 3938.551
iteration 1500: loss: 3943.868
iteration 1600: loss: 3944.636
iteration 1700: loss: 3942.223
iteration 1800: loss: 3940.276
====> Epoch: 027 Train loss: 3939.6211  took : 53.32969522476196
====> Test loss: 3943.7155
iteration 0000: loss: 3940.902
iteration 0100: loss: 3939.439
iteration 0200: loss: 3939.234
iteration 0300: loss: 3939.349
iteration 0400: loss: 3938.912
iteration 0500: loss: 3935.884
iteration 0600: loss: 3936.055
iteration 0700: loss: 3933.349
iteration 0800: loss: 3932.640
iteration 0900: loss: 3939.877
iteration 1000: loss: 3940.522
iteration 1100: loss: 3933.795
iteration 1200: loss: 3930.732
iteration 1300: loss: 3940.541
iteration 1400: loss: 3941.371
iteration 1500: loss: 3941.613
iteration 1600: loss: 3936.275
iteration 1700: loss: 3938.677
iteration 1800: loss: 3936.361
====> Epoch: 028 Train loss: 3939.6267  took : 53.22031283378601
====> Test loss: 3944.7727
iteration 0000: loss: 3938.015
iteration 0100: loss: 3936.088
iteration 0200: loss: 3939.151
iteration 0300: loss: 3940.789
iteration 0400: loss: 3935.589
iteration 0500: loss: 3947.667
iteration 0600: loss: 3936.928
iteration 0700: loss: 3935.628
iteration 0800: loss: 3940.118
iteration 0900: loss: 3933.492
iteration 1000: loss: 3942.330
iteration 1100: loss: 3937.488
iteration 1200: loss: 3938.382
iteration 1300: loss: 3937.287
iteration 1400: loss: 3941.124
iteration 1500: loss: 3940.096
iteration 1600: loss: 3934.019
iteration 1700: loss: 3940.007
iteration 1800: loss: 3938.227
====> Epoch: 029 Train loss: 3939.3789  took : 53.02486038208008
====> Test loss: 3943.3869
iteration 0000: loss: 3937.263
iteration 0100: loss: 3945.309
iteration 0200: loss: 3938.980
iteration 0300: loss: 3932.317
iteration 0400: loss: 3939.295
iteration 0500: loss: 3937.539
iteration 0600: loss: 3940.857
iteration 0700: loss: 3940.325
iteration 0800: loss: 3938.776
iteration 0900: loss: 3936.731
iteration 1000: loss: 3938.281
iteration 1100: loss: 3937.775
iteration 1200: loss: 3939.077
iteration 1300: loss: 3937.439
iteration 1400: loss: 3932.895
iteration 1500: loss: 3939.920
iteration 1600: loss: 3931.331
iteration 1700: loss: 3940.233
iteration 1800: loss: 3942.889
====> Epoch: 030 Train loss: 3939.2841  took : 53.081334829330444
====> Test loss: 3943.3501
iteration 0000: loss: 3942.012
iteration 0100: loss: 3940.208
iteration 0200: loss: 3938.462
iteration 0300: loss: 3939.988
iteration 0400: loss: 3945.570
iteration 0500: loss: 3944.995
iteration 0600: loss: 3938.732
iteration 0700: loss: 3947.332
iteration 0800: loss: 3933.664
iteration 0900: loss: 3938.840
iteration 1000: loss: 3938.264
iteration 1100: loss: 3936.273
iteration 1200: loss: 3937.958
iteration 1300: loss: 3936.204
iteration 1400: loss: 3937.099
iteration 1500: loss: 3938.143
iteration 1600: loss: 3938.103
iteration 1700: loss: 3943.236
iteration 1800: loss: 3932.091
====> Epoch: 031 Train loss: 3939.3628  took : 52.875176191329956
====> Test loss: 3943.5169
iteration 0000: loss: 3935.472
iteration 0100: loss: 3945.700
iteration 0200: loss: 3939.793
iteration 0300: loss: 3942.178
iteration 0400: loss: 3946.081
iteration 0500: loss: 3940.514
iteration 0600: loss: 3938.847
iteration 0700: loss: 3943.944
iteration 0800: loss: 3947.377
iteration 0900: loss: 3942.593
iteration 1000: loss: 3937.169
iteration 1100: loss: 3942.401
iteration 1200: loss: 3942.382
iteration 1300: loss: 3941.564
iteration 1400: loss: 3939.317
iteration 1500: loss: 3935.016
iteration 1600: loss: 3940.536
iteration 1700: loss: 3932.605
iteration 1800: loss: 3936.764
====> Epoch: 032 Train loss: 3939.3157  took : 52.8556809425354
====> Test loss: 3943.2806
iteration 0000: loss: 3937.096
iteration 0100: loss: 3946.101
iteration 0200: loss: 3943.654
iteration 0300: loss: 3940.180
iteration 0400: loss: 3943.588
iteration 0500: loss: 3939.181
iteration 0600: loss: 3934.880
iteration 0700: loss: 3942.339
iteration 0800: loss: 3937.859
iteration 0900: loss: 3950.292
iteration 1000: loss: 3938.034
iteration 1100: loss: 3938.800
iteration 1200: loss: 3939.143
iteration 1300: loss: 3938.596
iteration 1400: loss: 3934.565
iteration 1500: loss: 3936.618
iteration 1600: loss: 3937.285
iteration 1700: loss: 3941.199
iteration 1800: loss: 3938.960
====> Epoch: 033 Train loss: 3939.1169  took : 53.05468678474426
====> Test loss: 3943.6244
iteration 0000: loss: 3947.091
iteration 0100: loss: 3939.485
iteration 0200: loss: 3940.660
iteration 0300: loss: 3935.120
iteration 0400: loss: 3937.393
iteration 0500: loss: 3932.414
iteration 0600: loss: 3934.206
iteration 0700: loss: 3936.700
iteration 0800: loss: 3940.276
iteration 0900: loss: 3935.892
iteration 1000: loss: 3937.797
iteration 1100: loss: 3936.068
iteration 1200: loss: 3942.162
iteration 1300: loss: 3938.245
iteration 1400: loss: 3938.029
iteration 1500: loss: 3937.839
iteration 1600: loss: 3941.703
iteration 1700: loss: 3937.797
iteration 1800: loss: 3943.790
====> Epoch: 034 Train loss: 3939.1944  took : 52.978410959243774
====> Test loss: 3943.7397
iteration 0000: loss: 3933.956
iteration 0100: loss: 3939.380
iteration 0200: loss: 3935.923
iteration 0300: loss: 3938.773
iteration 0400: loss: 3943.072
iteration 0500: loss: 3943.092
iteration 0600: loss: 3937.087
iteration 0700: loss: 3936.131
iteration 0800: loss: 3939.762
iteration 0900: loss: 3934.117
iteration 1000: loss: 3942.915
iteration 1100: loss: 3939.696
iteration 1200: loss: 3937.353
iteration 1300: loss: 3938.884
iteration 1400: loss: 3937.739
iteration 1500: loss: 3939.008
iteration 1600: loss: 3933.492
iteration 1700: loss: 3934.715
iteration 1800: loss: 3942.933
====> Epoch: 035 Train loss: 3939.0150  took : 53.06939935684204
====> Test loss: 3943.2233
iteration 0000: loss: 3936.561
iteration 0100: loss: 3937.876
iteration 0200: loss: 3942.992
iteration 0300: loss: 3941.598
iteration 0400: loss: 3939.675
iteration 0500: loss: 3940.438
iteration 0600: loss: 3943.541
iteration 0700: loss: 3942.028
iteration 0800: loss: 3938.145
iteration 0900: loss: 3941.892
iteration 1000: loss: 3936.896
iteration 1100: loss: 3936.495
iteration 1200: loss: 3934.512
iteration 1300: loss: 3942.996
iteration 1400: loss: 3937.471
iteration 1500: loss: 3937.070
iteration 1600: loss: 3934.678
iteration 1700: loss: 3942.180
iteration 1800: loss: 3932.037
====> Epoch: 036 Train loss: 3938.8779  took : 52.96491360664368
====> Test loss: 3943.5007
iteration 0000: loss: 3935.260
iteration 0100: loss: 3942.175
iteration 0200: loss: 3935.899
iteration 0300: loss: 3934.194
iteration 0400: loss: 3943.299
iteration 0500: loss: 3940.279
iteration 0600: loss: 3939.732
iteration 0700: loss: 3945.560
iteration 0800: loss: 3944.081
iteration 0900: loss: 3943.439
iteration 1000: loss: 3944.753
iteration 1100: loss: 3942.032
iteration 1200: loss: 3938.212
iteration 1300: loss: 3938.536
iteration 1400: loss: 3934.802
iteration 1500: loss: 3943.441
iteration 1600: loss: 3940.545
iteration 1700: loss: 3938.101
iteration 1800: loss: 3937.579
====> Epoch: 037 Train loss: 3939.1256  took : 53.23705840110779
====> Test loss: 3942.9565
iteration 0000: loss: 3934.945
iteration 0100: loss: 3943.779
iteration 0200: loss: 3934.857
iteration 0300: loss: 3940.340
iteration 0400: loss: 3942.050
iteration 0500: loss: 3938.002
iteration 0600: loss: 3935.494
iteration 0700: loss: 3936.987
iteration 0800: loss: 3939.992
iteration 0900: loss: 3932.936
iteration 1000: loss: 3935.384
iteration 1100: loss: 3938.941
iteration 1200: loss: 3939.479
iteration 1300: loss: 3934.802
iteration 1400: loss: 3933.960
iteration 1500: loss: 3942.869
iteration 1600: loss: 3939.579
iteration 1700: loss: 3938.682
iteration 1800: loss: 3938.748
====> Epoch: 038 Train loss: 3939.0170  took : 53.184014320373535
====> Test loss: 3945.1339
iteration 0000: loss: 3941.773
iteration 0100: loss: 3937.866
iteration 0200: loss: 3938.221
iteration 0300: loss: 3932.731
iteration 0400: loss: 3937.534
iteration 0500: loss: 3941.789
iteration 0600: loss: 3942.167
iteration 0700: loss: 3937.253
iteration 0800: loss: 3929.213
iteration 0900: loss: 3938.225
iteration 1000: loss: 3937.520
iteration 1100: loss: 3939.667
iteration 1200: loss: 3938.021
iteration 1300: loss: 3934.989
iteration 1400: loss: 3940.805
iteration 1500: loss: 3938.303
iteration 1600: loss: 3945.940
iteration 1700: loss: 3935.885
iteration 1800: loss: 3940.763
====> Epoch: 039 Train loss: 3938.9368  took : 53.062196254730225
====> Test loss: 3943.1146
iteration 0000: loss: 3936.836
iteration 0100: loss: 3939.794
iteration 0200: loss: 3940.537
iteration 0300: loss: 3936.944
iteration 0400: loss: 3937.869
iteration 0500: loss: 3942.738
iteration 0600: loss: 3938.522
iteration 0700: loss: 3940.432
iteration 0800: loss: 3936.874
iteration 0900: loss: 3936.173
iteration 1000: loss: 3940.670
iteration 1100: loss: 3943.024
iteration 1200: loss: 3940.097
iteration 1300: loss: 3940.323
iteration 1400: loss: 3935.176
iteration 1500: loss: 3941.710
iteration 1600: loss: 3939.427
iteration 1700: loss: 3931.980
iteration 1800: loss: 3941.493
====> Epoch: 040 Train loss: 3938.7836  took : 52.978668451309204
====> Test loss: 3942.6213
iteration 0000: loss: 3936.354
iteration 0100: loss: 3938.932
iteration 0200: loss: 3938.326
iteration 0300: loss: 3935.071
iteration 0400: loss: 3935.557
iteration 0500: loss: 3944.230
iteration 0600: loss: 3947.287
iteration 0700: loss: 3941.853
iteration 0800: loss: 3934.476
iteration 0900: loss: 3934.987
iteration 1000: loss: 3940.879
iteration 1100: loss: 3941.407
iteration 1200: loss: 3941.743
iteration 1300: loss: 3931.485
iteration 1400: loss: 3943.402
iteration 1500: loss: 3936.617
iteration 1600: loss: 3936.041
iteration 1700: loss: 3941.153
iteration 1800: loss: 3939.856
====> Epoch: 041 Train loss: 3938.8425  took : 53.00626015663147
====> Test loss: 3942.6533
iteration 0000: loss: 3938.511
iteration 0100: loss: 3942.628
iteration 0200: loss: 3939.892
iteration 0300: loss: 3937.200
iteration 0400: loss: 3936.200
iteration 0500: loss: 3937.454
iteration 0600: loss: 3937.752
iteration 0700: loss: 3937.507
iteration 0800: loss: 3932.650
iteration 0900: loss: 3935.469
iteration 1000: loss: 3940.751
iteration 1100: loss: 3942.274
iteration 1200: loss: 3937.214
iteration 1300: loss: 3939.351
iteration 1400: loss: 3932.923
iteration 1500: loss: 3942.413
iteration 1600: loss: 3937.743
iteration 1700: loss: 3934.754
iteration 1800: loss: 3936.690
====> Epoch: 042 Train loss: 3938.5845  took : 53.09830904006958
====> Test loss: 3942.9677
iteration 0000: loss: 3937.533
iteration 0100: loss: 3938.615
iteration 0200: loss: 3936.280
iteration 0300: loss: 3938.811
iteration 0400: loss: 3942.841
iteration 0500: loss: 3943.766
iteration 0600: loss: 3940.885
iteration 0700: loss: 3933.984
iteration 0800: loss: 3935.685
iteration 0900: loss: 3933.401
iteration 1000: loss: 3934.806
iteration 1100: loss: 3940.621
iteration 1200: loss: 3944.923
iteration 1300: loss: 3942.828
iteration 1400: loss: 3944.411
iteration 1500: loss: 3938.064
iteration 1600: loss: 3938.554
iteration 1700: loss: 3935.633
iteration 1800: loss: 3939.016
====> Epoch: 043 Train loss: 3938.5440  took : 53.14974784851074
====> Test loss: 3942.6968
iteration 0000: loss: 3937.150
iteration 0100: loss: 3938.043
iteration 0200: loss: 3933.646
iteration 0300: loss: 3937.879
iteration 0400: loss: 3935.754
iteration 0500: loss: 3936.964
iteration 0600: loss: 3936.760
iteration 0700: loss: 3945.720
iteration 0800: loss: 3932.672
iteration 0900: loss: 3938.833
iteration 1000: loss: 3940.294
iteration 1100: loss: 3941.217
iteration 1200: loss: 3934.704
iteration 1300: loss: 3942.686
iteration 1400: loss: 3940.354
iteration 1500: loss: 3942.111
iteration 1600: loss: 3939.088
iteration 1700: loss: 3934.060
iteration 1800: loss: 3942.159
====> Epoch: 044 Train loss: 3938.4494  took : 52.846293687820435
====> Test loss: 3943.0464
iteration 0000: loss: 3933.085
iteration 0100: loss: 3933.164
iteration 0200: loss: 3932.980
iteration 0300: loss: 3938.386
iteration 0400: loss: 3939.917
iteration 0500: loss: 3940.049
iteration 0600: loss: 3939.366
iteration 0700: loss: 3935.377
iteration 0800: loss: 3937.791
iteration 0900: loss: 3940.383
iteration 1000: loss: 3944.276
iteration 1100: loss: 3939.713
iteration 1200: loss: 3940.060
iteration 1300: loss: 3939.752
iteration 1400: loss: 3938.431
iteration 1500: loss: 3936.028
iteration 1600: loss: 3934.650
iteration 1700: loss: 3941.569
iteration 1800: loss: 3946.543
====> Epoch: 045 Train loss: 3938.4210  took : 52.977636098861694
====> Test loss: 3943.7658
iteration 0000: loss: 3940.007
iteration 0100: loss: 3935.540
iteration 0200: loss: 3935.954
iteration 0300: loss: 3941.915
iteration 0400: loss: 3936.627
iteration 0500: loss: 3938.198
iteration 0600: loss: 3937.865
iteration 0700: loss: 3933.651
iteration 0800: loss: 3934.189
iteration 0900: loss: 3938.010
iteration 1000: loss: 3935.446
iteration 1100: loss: 3932.920
iteration 1200: loss: 3935.073
iteration 1300: loss: 3939.807
iteration 1400: loss: 3941.162
iteration 1500: loss: 3933.449
iteration 1600: loss: 3938.199
iteration 1700: loss: 3946.584
iteration 1800: loss: 3938.769
====> Epoch: 046 Train loss: 3938.4988  took : 52.80625677108765
====> Test loss: 3943.6679
iteration 0000: loss: 3946.713
iteration 0100: loss: 3930.468
iteration 0200: loss: 3938.929
iteration 0300: loss: 3934.953
iteration 0400: loss: 3935.401
iteration 0500: loss: 3941.354
iteration 0600: loss: 3934.126
iteration 0700: loss: 3939.317
iteration 0800: loss: 3942.178
iteration 0900: loss: 3933.825
iteration 1000: loss: 3938.191
iteration 1100: loss: 3945.854
iteration 1200: loss: 3932.747
iteration 1300: loss: 3938.405
iteration 1400: loss: 3931.394
iteration 1500: loss: 3933.166
iteration 1600: loss: 3937.324
iteration 1700: loss: 3934.402
iteration 1800: loss: 3938.529
====> Epoch: 047 Train loss: 3938.7690  took : 53.13454270362854
====> Test loss: 3942.9303
iteration 0000: loss: 3939.244
iteration 0100: loss: 3941.704
iteration 0200: loss: 3943.834
iteration 0300: loss: 3936.064
iteration 0400: loss: 3945.277
iteration 0500: loss: 3937.866
iteration 0600: loss: 3931.226
iteration 0700: loss: 3937.661
iteration 0800: loss: 3938.489
iteration 0900: loss: 3942.503
iteration 1000: loss: 3942.679
iteration 1100: loss: 3938.754
iteration 1200: loss: 3942.673
iteration 1300: loss: 3939.271
iteration 1400: loss: 3938.466
iteration 1500: loss: 3931.023
iteration 1600: loss: 3938.957
iteration 1700: loss: 3938.410
iteration 1800: loss: 3940.579
====> Epoch: 048 Train loss: 3938.4009  took : 52.990028619766235
====> Test loss: 3942.8171
iteration 0000: loss: 3936.998
iteration 0100: loss: 3938.361
iteration 0200: loss: 3936.142
iteration 0300: loss: 3937.773
iteration 0400: loss: 3938.290
iteration 0500: loss: 3940.942
iteration 0600: loss: 3939.039
iteration 0700: loss: 3933.033
iteration 0800: loss: 3942.645
iteration 0900: loss: 3931.183
iteration 1000: loss: 3938.443
iteration 1100: loss: 3938.101
iteration 1200: loss: 3943.200
iteration 1300: loss: 3943.588
iteration 1400: loss: 3937.601
iteration 1500: loss: 3937.573
iteration 1600: loss: 3939.491
iteration 1700: loss: 3938.758
iteration 1800: loss: 3946.778
====> Epoch: 049 Train loss: 3938.3938  took : 52.91080617904663
====> Test loss: 3943.1601
iteration 0000: loss: 3940.557
iteration 0100: loss: 3935.699
iteration 0200: loss: 3943.350
iteration 0300: loss: 3941.326
iteration 0400: loss: 3937.336
iteration 0500: loss: 3938.002
iteration 0600: loss: 3938.212
iteration 0700: loss: 3933.430
iteration 0800: loss: 3938.270
iteration 0900: loss: 3936.273
iteration 1000: loss: 3937.809
iteration 1100: loss: 3937.290
iteration 1200: loss: 3933.328
iteration 1300: loss: 3933.702
iteration 1400: loss: 3936.938
iteration 1500: loss: 3938.627
iteration 1600: loss: 3939.611
iteration 1700: loss: 3933.964
iteration 1800: loss: 3945.734
====> Epoch: 050 Train loss: 3938.3411  took : 52.79156279563904
====> Test loss: 3942.7249
====> [MM-VAE] Time: 3169.540s or 00:52:49
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  11
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_11
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_11
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1988.057
iteration 0100: loss: 1570.859
iteration 0200: loss: 1564.029
iteration 0300: loss: 1561.714
iteration 0400: loss: 1546.611
iteration 0500: loss: 1545.590
iteration 0600: loss: 1540.951
iteration 0700: loss: 1540.482
iteration 0800: loss: 1539.230
iteration 0900: loss: 1532.031
====> Epoch: 001 Train loss: 1555.3121  took : 8.464780807495117
====> Test loss: 1537.9942
iteration 0000: loss: 1535.409
iteration 0100: loss: 1534.593
iteration 0200: loss: 1533.243
iteration 0300: loss: 1528.525
iteration 0400: loss: 1532.553
iteration 0500: loss: 1530.900
iteration 0600: loss: 1532.873
iteration 0700: loss: 1531.778
iteration 0800: loss: 1524.094
iteration 0900: loss: 1527.286
====> Epoch: 002 Train loss: 1530.8891  took : 8.427883863449097
====> Test loss: 1528.7274
iteration 0000: loss: 1524.823
iteration 0100: loss: 1528.869
iteration 0200: loss: 1521.693
iteration 0300: loss: 1526.362
iteration 0400: loss: 1524.516
iteration 0500: loss: 1524.551
iteration 0600: loss: 1525.951
iteration 0700: loss: 1527.572
iteration 0800: loss: 1522.650
iteration 0900: loss: 1525.688
====> Epoch: 003 Train loss: 1525.3722  took : 8.508000135421753
====> Test loss: 1526.1167
iteration 0000: loss: 1525.871
iteration 0100: loss: 1524.709
iteration 0200: loss: 1524.973
iteration 0300: loss: 1525.798
iteration 0400: loss: 1524.739
iteration 0500: loss: 1521.399
iteration 0600: loss: 1525.221
iteration 0700: loss: 1520.848
iteration 0800: loss: 1524.502
iteration 0900: loss: 1523.746
====> Epoch: 004 Train loss: 1523.1949  took : 8.510350465774536
====> Test loss: 1524.7864
iteration 0000: loss: 1525.471
iteration 0100: loss: 1522.491
iteration 0200: loss: 1524.078
iteration 0300: loss: 1520.761
iteration 0400: loss: 1519.751
iteration 0500: loss: 1519.963
iteration 0600: loss: 1523.911
iteration 0700: loss: 1524.999
iteration 0800: loss: 1523.116
iteration 0900: loss: 1522.882
====> Epoch: 005 Train loss: 1521.8501  took : 8.490360260009766
====> Test loss: 1523.4951
iteration 0000: loss: 1518.885
iteration 0100: loss: 1523.542
iteration 0200: loss: 1521.771
iteration 0300: loss: 1519.046
iteration 0400: loss: 1519.284
iteration 0500: loss: 1521.958
iteration 0600: loss: 1522.892
iteration 0700: loss: 1522.210
iteration 0800: loss: 1518.696
iteration 0900: loss: 1520.435
====> Epoch: 006 Train loss: 1520.8405  took : 8.41271424293518
====> Test loss: 1522.9231
iteration 0000: loss: 1519.971
iteration 0100: loss: 1518.460
iteration 0200: loss: 1519.066
iteration 0300: loss: 1522.974
iteration 0400: loss: 1517.989
iteration 0500: loss: 1516.915
iteration 0600: loss: 1516.726
iteration 0700: loss: 1520.538
iteration 0800: loss: 1522.124
iteration 0900: loss: 1520.118
====> Epoch: 007 Train loss: 1520.0849  took : 8.396718502044678
====> Test loss: 1522.3637
iteration 0000: loss: 1519.620
iteration 0100: loss: 1522.932
iteration 0200: loss: 1521.771
iteration 0300: loss: 1517.619
iteration 0400: loss: 1518.198
iteration 0500: loss: 1520.001
iteration 0600: loss: 1520.833
iteration 0700: loss: 1518.453
iteration 0800: loss: 1517.135
iteration 0900: loss: 1521.293
====> Epoch: 008 Train loss: 1519.5008  took : 8.459648132324219
====> Test loss: 1521.8697
iteration 0000: loss: 1521.250
iteration 0100: loss: 1516.920
iteration 0200: loss: 1518.713
iteration 0300: loss: 1518.178
iteration 0400: loss: 1515.890
iteration 0500: loss: 1518.839
iteration 0600: loss: 1518.893
iteration 0700: loss: 1518.604
iteration 0800: loss: 1519.521
iteration 0900: loss: 1517.848
====> Epoch: 009 Train loss: 1518.9903  took : 8.462239503860474
====> Test loss: 1521.5863
iteration 0000: loss: 1518.144
iteration 0100: loss: 1521.228
iteration 0200: loss: 1520.961
iteration 0300: loss: 1521.521
iteration 0400: loss: 1519.672
iteration 0500: loss: 1520.635
iteration 0600: loss: 1520.734
iteration 0700: loss: 1518.249
iteration 0800: loss: 1520.253
iteration 0900: loss: 1519.405
====> Epoch: 010 Train loss: 1518.5822  took : 8.511954069137573
====> Test loss: 1521.0324
iteration 0000: loss: 1518.624
iteration 0100: loss: 1518.534
iteration 0200: loss: 1518.922
iteration 0300: loss: 1516.470
iteration 0400: loss: 1515.331
iteration 0500: loss: 1516.872
iteration 0600: loss: 1516.608
iteration 0700: loss: 1515.539
iteration 0800: loss: 1518.104
iteration 0900: loss: 1518.481
====> Epoch: 011 Train loss: 1518.2054  took : 8.535358428955078
====> Test loss: 1520.7486
iteration 0000: loss: 1522.301
iteration 0100: loss: 1519.300
iteration 0200: loss: 1519.429
iteration 0300: loss: 1516.944
iteration 0400: loss: 1519.493
iteration 0500: loss: 1516.357
iteration 0600: loss: 1518.597
iteration 0700: loss: 1518.016
iteration 0800: loss: 1514.686
iteration 0900: loss: 1518.850
====> Epoch: 012 Train loss: 1517.9373  took : 8.524337768554688
====> Test loss: 1520.5642
iteration 0000: loss: 1518.852
iteration 0100: loss: 1516.118
iteration 0200: loss: 1516.096
iteration 0300: loss: 1520.300
iteration 0400: loss: 1514.091
iteration 0500: loss: 1517.307
iteration 0600: loss: 1518.265
iteration 0700: loss: 1518.345
iteration 0800: loss: 1519.211
iteration 0900: loss: 1519.439
====> Epoch: 013 Train loss: 1517.6427  took : 8.417402982711792
====> Test loss: 1520.3011
iteration 0000: loss: 1515.341
iteration 0100: loss: 1519.301
iteration 0200: loss: 1514.899
iteration 0300: loss: 1517.250
iteration 0400: loss: 1517.776
iteration 0500: loss: 1517.085
iteration 0600: loss: 1517.969
iteration 0700: loss: 1518.612
iteration 0800: loss: 1517.913
iteration 0900: loss: 1517.164
====> Epoch: 014 Train loss: 1517.3903  took : 8.418914794921875
====> Test loss: 1520.1876
iteration 0000: loss: 1518.667
iteration 0100: loss: 1518.702
iteration 0200: loss: 1518.370
iteration 0300: loss: 1517.418
iteration 0400: loss: 1518.068
iteration 0500: loss: 1515.998
iteration 0600: loss: 1519.742
iteration 0700: loss: 1516.834
iteration 0800: loss: 1516.276
iteration 0900: loss: 1515.807
====> Epoch: 015 Train loss: 1517.2350  took : 8.515382766723633
====> Test loss: 1520.0735
iteration 0000: loss: 1518.874
iteration 0100: loss: 1515.460
iteration 0200: loss: 1519.908
iteration 0300: loss: 1520.751
iteration 0400: loss: 1517.496
iteration 0500: loss: 1516.806
iteration 0600: loss: 1517.672
iteration 0700: loss: 1516.737
iteration 0800: loss: 1515.722
iteration 0900: loss: 1518.460
====> Epoch: 016 Train loss: 1516.9983  took : 8.516385793685913
====> Test loss: 1520.1116
iteration 0000: loss: 1515.903
iteration 0100: loss: 1517.450
iteration 0200: loss: 1517.951
iteration 0300: loss: 1518.357
iteration 0400: loss: 1515.884
iteration 0500: loss: 1516.148
iteration 0600: loss: 1517.916
iteration 0700: loss: 1514.773
iteration 0800: loss: 1516.362
iteration 0900: loss: 1517.686
====> Epoch: 017 Train loss: 1516.7970  took : 8.386377334594727
====> Test loss: 1519.7716
iteration 0000: loss: 1516.719
iteration 0100: loss: 1517.333
iteration 0200: loss: 1516.796
iteration 0300: loss: 1514.984
iteration 0400: loss: 1513.662
iteration 0500: loss: 1518.446
iteration 0600: loss: 1516.483
iteration 0700: loss: 1515.980
iteration 0800: loss: 1515.216
iteration 0900: loss: 1515.194
====> Epoch: 018 Train loss: 1516.6358  took : 8.473692655563354
====> Test loss: 1519.5451
iteration 0000: loss: 1518.287
iteration 0100: loss: 1516.260
iteration 0200: loss: 1514.240
iteration 0300: loss: 1517.585
iteration 0400: loss: 1518.236
iteration 0500: loss: 1520.277
iteration 0600: loss: 1516.756
iteration 0700: loss: 1514.358
iteration 0800: loss: 1517.060
iteration 0900: loss: 1516.008
====> Epoch: 019 Train loss: 1516.4580  took : 8.402129173278809
====> Test loss: 1519.4245
iteration 0000: loss: 1514.662
iteration 0100: loss: 1513.184
iteration 0200: loss: 1515.278
iteration 0300: loss: 1517.096
iteration 0400: loss: 1516.157
iteration 0500: loss: 1518.598
iteration 0600: loss: 1517.223
iteration 0700: loss: 1517.342
iteration 0800: loss: 1515.359
iteration 0900: loss: 1517.050
====> Epoch: 020 Train loss: 1516.3277  took : 8.431872367858887
====> Test loss: 1519.4015
iteration 0000: loss: 1515.793
iteration 0100: loss: 1518.279
iteration 0200: loss: 1514.931
iteration 0300: loss: 1514.780
iteration 0400: loss: 1516.094
iteration 0500: loss: 1514.648
iteration 0600: loss: 1516.980
iteration 0700: loss: 1515.625
iteration 0800: loss: 1516.035
iteration 0900: loss: 1514.058
====> Epoch: 021 Train loss: 1516.2242  took : 8.516597032546997
====> Test loss: 1519.3133
iteration 0000: loss: 1517.263
iteration 0100: loss: 1516.277
iteration 0200: loss: 1517.132
iteration 0300: loss: 1516.191
iteration 0400: loss: 1516.513
iteration 0500: loss: 1516.559
iteration 0600: loss: 1513.623
iteration 0700: loss: 1516.770
iteration 0800: loss: 1514.616
iteration 0900: loss: 1519.918
====> Epoch: 022 Train loss: 1516.1023  took : 8.426291465759277
====> Test loss: 1519.3326
iteration 0000: loss: 1515.165
iteration 0100: loss: 1517.228
iteration 0200: loss: 1516.557
iteration 0300: loss: 1514.263
iteration 0400: loss: 1515.402
iteration 0500: loss: 1517.884
iteration 0600: loss: 1514.189
iteration 0700: loss: 1514.874
iteration 0800: loss: 1518.758
iteration 0900: loss: 1515.828
====> Epoch: 023 Train loss: 1516.0305  took : 8.477405071258545
====> Test loss: 1519.1167
iteration 0000: loss: 1515.310
iteration 0100: loss: 1512.512
iteration 0200: loss: 1514.320
iteration 0300: loss: 1517.935
iteration 0400: loss: 1516.714
iteration 0500: loss: 1515.696
iteration 0600: loss: 1516.583
iteration 0700: loss: 1520.991
iteration 0800: loss: 1515.832
iteration 0900: loss: 1517.607
====> Epoch: 024 Train loss: 1515.8888  took : 8.522621631622314
====> Test loss: 1519.2785
iteration 0000: loss: 1516.325
iteration 0100: loss: 1515.419
iteration 0200: loss: 1519.321
iteration 0300: loss: 1515.273
iteration 0400: loss: 1515.654
iteration 0500: loss: 1513.685
iteration 0600: loss: 1514.453
iteration 0700: loss: 1514.951
iteration 0800: loss: 1514.769
iteration 0900: loss: 1517.309
====> Epoch: 025 Train loss: 1515.7870  took : 8.440572738647461
====> Test loss: 1519.0168
iteration 0000: loss: 1512.655
iteration 0100: loss: 1515.640
iteration 0200: loss: 1514.962
iteration 0300: loss: 1513.172
iteration 0400: loss: 1515.491
iteration 0500: loss: 1515.295
iteration 0600: loss: 1516.547
iteration 0700: loss: 1515.360
iteration 0800: loss: 1514.523
iteration 0900: loss: 1513.013
====> Epoch: 026 Train loss: 1515.7061  took : 8.413443565368652
====> Test loss: 1518.9905
iteration 0000: loss: 1515.667
iteration 0100: loss: 1514.272
iteration 0200: loss: 1514.249
iteration 0300: loss: 1514.734
iteration 0400: loss: 1514.780
iteration 0500: loss: 1517.241
iteration 0600: loss: 1515.272
iteration 0700: loss: 1515.790
iteration 0800: loss: 1517.352
iteration 0900: loss: 1512.757
====> Epoch: 027 Train loss: 1515.6551  took : 8.49293041229248
====> Test loss: 1518.9734
iteration 0000: loss: 1516.253
iteration 0100: loss: 1513.648
iteration 0200: loss: 1512.956
iteration 0300: loss: 1514.845
iteration 0400: loss: 1515.185
iteration 0500: loss: 1516.963
iteration 0600: loss: 1515.244
iteration 0700: loss: 1517.102
iteration 0800: loss: 1516.996
iteration 0900: loss: 1515.352
====> Epoch: 028 Train loss: 1515.5211  took : 8.434037208557129
====> Test loss: 1518.9041
iteration 0000: loss: 1516.208
iteration 0100: loss: 1515.945
iteration 0200: loss: 1514.773
iteration 0300: loss: 1514.999
iteration 0400: loss: 1516.790
iteration 0500: loss: 1515.798
iteration 0600: loss: 1513.486
iteration 0700: loss: 1515.646
iteration 0800: loss: 1516.194
iteration 0900: loss: 1513.692
====> Epoch: 029 Train loss: 1515.4570  took : 8.425727605819702
====> Test loss: 1518.8092
iteration 0000: loss: 1518.230
iteration 0100: loss: 1515.565
iteration 0200: loss: 1516.433
iteration 0300: loss: 1515.439
iteration 0400: loss: 1517.612
iteration 0500: loss: 1516.063
iteration 0600: loss: 1518.010
iteration 0700: loss: 1514.720
iteration 0800: loss: 1515.512
iteration 0900: loss: 1517.203
====> Epoch: 030 Train loss: 1515.3897  took : 8.446827173233032
====> Test loss: 1518.7975
iteration 0000: loss: 1517.469
iteration 0100: loss: 1516.084
iteration 0200: loss: 1514.884
iteration 0300: loss: 1518.914
iteration 0400: loss: 1517.504
iteration 0500: loss: 1515.409
iteration 0600: loss: 1516.398
iteration 0700: loss: 1515.515
iteration 0800: loss: 1517.490
iteration 0900: loss: 1512.093
====> Epoch: 031 Train loss: 1515.2930  took : 8.507429361343384
====> Test loss: 1518.6814
iteration 0000: loss: 1513.507
iteration 0100: loss: 1514.133
iteration 0200: loss: 1516.017
iteration 0300: loss: 1516.885
iteration 0400: loss: 1517.252
iteration 0500: loss: 1515.560
iteration 0600: loss: 1516.163
iteration 0700: loss: 1515.109
iteration 0800: loss: 1512.956
iteration 0900: loss: 1515.680
====> Epoch: 032 Train loss: 1515.2336  took : 8.389508724212646
====> Test loss: 1518.9607
iteration 0000: loss: 1517.222
iteration 0100: loss: 1515.054
iteration 0200: loss: 1516.460
iteration 0300: loss: 1515.281
iteration 0400: loss: 1515.484
iteration 0500: loss: 1514.492
iteration 0600: loss: 1514.303
iteration 0700: loss: 1515.613
iteration 0800: loss: 1517.879
iteration 0900: loss: 1517.665
====> Epoch: 033 Train loss: 1515.1716  took : 8.518383979797363
====> Test loss: 1518.7532
iteration 0000: loss: 1515.453
iteration 0100: loss: 1515.929
iteration 0200: loss: 1515.967
iteration 0300: loss: 1513.932
iteration 0400: loss: 1515.948
iteration 0500: loss: 1515.773
iteration 0600: loss: 1519.247
iteration 0700: loss: 1515.004
iteration 0800: loss: 1514.298
iteration 0900: loss: 1516.528
====> Epoch: 034 Train loss: 1515.0906  took : 8.397065877914429
====> Test loss: 1518.6219
iteration 0000: loss: 1514.664
iteration 0100: loss: 1513.271
iteration 0200: loss: 1514.897
iteration 0300: loss: 1516.442
iteration 0400: loss: 1511.983
iteration 0500: loss: 1515.897
iteration 0600: loss: 1513.926
iteration 0700: loss: 1515.435
iteration 0800: loss: 1516.443
iteration 0900: loss: 1514.281
====> Epoch: 035 Train loss: 1515.0561  took : 8.438526630401611
====> Test loss: 1518.5749
iteration 0000: loss: 1516.645
iteration 0100: loss: 1514.681
iteration 0200: loss: 1513.518
iteration 0300: loss: 1512.067
iteration 0400: loss: 1515.053
iteration 0500: loss: 1516.241
iteration 0600: loss: 1516.039
iteration 0700: loss: 1516.837
iteration 0800: loss: 1516.043
iteration 0900: loss: 1512.655
====> Epoch: 036 Train loss: 1514.9639  took : 8.407652378082275
====> Test loss: 1518.5759
iteration 0000: loss: 1514.761
iteration 0100: loss: 1513.208
iteration 0200: loss: 1511.317
iteration 0300: loss: 1516.282
iteration 0400: loss: 1514.120
iteration 0500: loss: 1515.726
iteration 0600: loss: 1512.403
iteration 0700: loss: 1513.241
iteration 0800: loss: 1513.776
iteration 0900: loss: 1513.421
====> Epoch: 037 Train loss: 1514.9408  took : 8.41733431816101
====> Test loss: 1518.5577
iteration 0000: loss: 1514.244
iteration 0100: loss: 1516.554
iteration 0200: loss: 1516.602
iteration 0300: loss: 1515.924
iteration 0400: loss: 1513.915
iteration 0500: loss: 1515.039
iteration 0600: loss: 1515.140
iteration 0700: loss: 1513.876
iteration 0800: loss: 1514.644
iteration 0900: loss: 1517.507
====> Epoch: 038 Train loss: 1514.9083  took : 8.445785522460938
====> Test loss: 1518.5988
iteration 0000: loss: 1514.381
iteration 0100: loss: 1514.073
iteration 0200: loss: 1513.758
iteration 0300: loss: 1513.317
iteration 0400: loss: 1516.598
iteration 0500: loss: 1514.458
iteration 0600: loss: 1513.353
iteration 0700: loss: 1515.132
iteration 0800: loss: 1513.999
iteration 0900: loss: 1512.756
====> Epoch: 039 Train loss: 1514.8677  took : 8.488531351089478
====> Test loss: 1518.5443
iteration 0000: loss: 1513.745
iteration 0100: loss: 1512.805
iteration 0200: loss: 1512.247
iteration 0300: loss: 1513.404
iteration 0400: loss: 1514.102
iteration 0500: loss: 1513.838
iteration 0600: loss: 1516.699
iteration 0700: loss: 1517.065
iteration 0800: loss: 1512.965
iteration 0900: loss: 1517.126
====> Epoch: 040 Train loss: 1514.7646  took : 8.485241651535034
====> Test loss: 1518.4223
iteration 0000: loss: 1512.864
iteration 0100: loss: 1510.927
iteration 0200: loss: 1513.382
iteration 0300: loss: 1515.163
iteration 0400: loss: 1514.693
iteration 0500: loss: 1514.168
iteration 0600: loss: 1514.544
iteration 0700: loss: 1516.447
iteration 0800: loss: 1513.651
iteration 0900: loss: 1515.533
====> Epoch: 041 Train loss: 1514.7219  took : 8.47059965133667
====> Test loss: 1518.3885
iteration 0000: loss: 1516.394
iteration 0100: loss: 1512.444
iteration 0200: loss: 1515.756
iteration 0300: loss: 1515.502
iteration 0400: loss: 1514.810
iteration 0500: loss: 1513.945
iteration 0600: loss: 1517.194
iteration 0700: loss: 1514.315
iteration 0800: loss: 1513.607
iteration 0900: loss: 1516.151
====> Epoch: 042 Train loss: 1514.6476  took : 8.515058517456055
====> Test loss: 1518.3650
iteration 0000: loss: 1513.811
iteration 0100: loss: 1512.277
iteration 0200: loss: 1514.251
iteration 0300: loss: 1514.888
iteration 0400: loss: 1517.100
iteration 0500: loss: 1514.724
iteration 0600: loss: 1513.984
iteration 0700: loss: 1516.479
iteration 0800: loss: 1516.376
iteration 0900: loss: 1516.559
====> Epoch: 043 Train loss: 1514.6088  took : 8.543413400650024
====> Test loss: 1518.2975
iteration 0000: loss: 1515.115
iteration 0100: loss: 1516.572
iteration 0200: loss: 1513.952
iteration 0300: loss: 1515.117
iteration 0400: loss: 1511.925
iteration 0500: loss: 1516.345
iteration 0600: loss: 1514.225
iteration 0700: loss: 1515.801
iteration 0800: loss: 1516.477
iteration 0900: loss: 1516.108
====> Epoch: 044 Train loss: 1514.5533  took : 8.453059673309326
====> Test loss: 1518.4243
iteration 0000: loss: 1513.188
iteration 0100: loss: 1513.102
iteration 0200: loss: 1512.718
iteration 0300: loss: 1512.369
iteration 0400: loss: 1514.270
iteration 0500: loss: 1514.294
iteration 0600: loss: 1517.256
iteration 0700: loss: 1515.895
iteration 0800: loss: 1514.573
iteration 0900: loss: 1515.957
====> Epoch: 045 Train loss: 1514.5174  took : 8.511231660842896
====> Test loss: 1518.3502
iteration 0000: loss: 1514.170
iteration 0100: loss: 1514.794
iteration 0200: loss: 1514.663
iteration 0300: loss: 1515.797
iteration 0400: loss: 1516.986
iteration 0500: loss: 1512.964
iteration 0600: loss: 1514.892
iteration 0700: loss: 1517.250
iteration 0800: loss: 1512.442
iteration 0900: loss: 1515.053
====> Epoch: 046 Train loss: 1514.4660  took : 8.488730669021606
====> Test loss: 1518.2480
iteration 0000: loss: 1514.852
iteration 0100: loss: 1516.170
iteration 0200: loss: 1516.174
iteration 0300: loss: 1513.109
iteration 0400: loss: 1516.222
iteration 0500: loss: 1514.063
iteration 0600: loss: 1513.128
iteration 0700: loss: 1515.624
iteration 0800: loss: 1514.040
iteration 0900: loss: 1513.169
====> Epoch: 047 Train loss: 1514.4274  took : 8.40582537651062
====> Test loss: 1518.3668
iteration 0000: loss: 1515.178
iteration 0100: loss: 1514.830
iteration 0200: loss: 1513.714
iteration 0300: loss: 1515.112
iteration 0400: loss: 1513.178
iteration 0500: loss: 1512.456
iteration 0600: loss: 1514.772
iteration 0700: loss: 1511.906
iteration 0800: loss: 1511.523
iteration 0900: loss: 1514.730
====> Epoch: 048 Train loss: 1514.3745  took : 8.426769018173218
====> Test loss: 1518.2731
iteration 0000: loss: 1516.047
iteration 0100: loss: 1515.346
iteration 0200: loss: 1516.589
iteration 0300: loss: 1514.178
iteration 0400: loss: 1512.669
iteration 0500: loss: 1514.295
iteration 0600: loss: 1513.452
iteration 0700: loss: 1514.336
iteration 0800: loss: 1515.838
iteration 0900: loss: 1515.099
====> Epoch: 049 Train loss: 1514.3336  took : 8.39174199104309
====> Test loss: 1518.1478
iteration 0000: loss: 1514.296
iteration 0100: loss: 1514.736
iteration 0200: loss: 1515.926
iteration 0300: loss: 1512.475
iteration 0400: loss: 1515.194
iteration 0500: loss: 1515.458
iteration 0600: loss: 1513.509
iteration 0700: loss: 1515.049
iteration 0800: loss: 1513.908
iteration 0900: loss: 1517.009
====> Epoch: 050 Train loss: 1514.3272  took : 8.512819528579712
====> Test loss: 1518.1720
====> [MM-VAE] Time: 505.951s or 00:08:25
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  11
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_11
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_11
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.934
iteration 0100: loss: 2095.029
iteration 0200: loss: 2031.196
iteration 0300: loss: 2024.230
iteration 0400: loss: 2001.236
iteration 0500: loss: 1999.131
iteration 0600: loss: 1998.568
iteration 0700: loss: 1996.182
iteration 0800: loss: 1991.345
iteration 0900: loss: 1994.514
====> Epoch: 001 Train loss: 2022.0656  took : 12.953293561935425
====> Test loss: 1995.3372
iteration 0000: loss: 1989.359
iteration 0100: loss: 1982.468
iteration 0200: loss: 1990.663
iteration 0300: loss: 1984.125
iteration 0400: loss: 1978.876
iteration 0500: loss: 1984.011
iteration 0600: loss: 1988.783
iteration 0700: loss: 1983.750
iteration 0800: loss: 1981.170
iteration 0900: loss: 1978.366
====> Epoch: 002 Train loss: 1985.0223  took : 12.132834196090698
====> Test loss: 1980.5027
iteration 0000: loss: 1975.382
iteration 0100: loss: 1974.972
iteration 0200: loss: 1974.413
iteration 0300: loss: 1970.602
iteration 0400: loss: 1974.805
iteration 0500: loss: 1972.910
iteration 0600: loss: 1967.805
iteration 0700: loss: 1968.977
iteration 0800: loss: 1966.440
iteration 0900: loss: 1969.234
====> Epoch: 003 Train loss: 1971.3563  took : 12.12833285331726
====> Test loss: 1968.4721
iteration 0000: loss: 1963.987
iteration 0100: loss: 1962.182
iteration 0200: loss: 1963.846
iteration 0300: loss: 1971.462
iteration 0400: loss: 1966.042
iteration 0500: loss: 1966.359
iteration 0600: loss: 1968.194
iteration 0700: loss: 1963.212
iteration 0800: loss: 1960.208
iteration 0900: loss: 1960.973
====> Epoch: 004 Train loss: 1963.6476  took : 12.616341352462769
====> Test loss: 1963.4443
iteration 0000: loss: 1960.494
iteration 0100: loss: 1963.547
iteration 0200: loss: 1960.549
iteration 0300: loss: 1960.460
iteration 0400: loss: 1963.745
iteration 0500: loss: 1958.752
iteration 0600: loss: 1957.515
iteration 0700: loss: 1957.475
iteration 0800: loss: 1955.967
iteration 0900: loss: 1959.189
====> Epoch: 005 Train loss: 1959.2790  took : 12.34873628616333
====> Test loss: 1960.6931
iteration 0000: loss: 1959.764
iteration 0100: loss: 1956.997
iteration 0200: loss: 1955.757
iteration 0300: loss: 1959.801
iteration 0400: loss: 1955.833
iteration 0500: loss: 1956.268
iteration 0600: loss: 1954.876
iteration 0700: loss: 1956.752
iteration 0800: loss: 1953.700
iteration 0900: loss: 1955.702
====> Epoch: 006 Train loss: 1956.4717  took : 12.548601865768433
====> Test loss: 1957.2344
iteration 0000: loss: 1957.157
iteration 0100: loss: 1957.244
iteration 0200: loss: 1954.672
iteration 0300: loss: 1953.588
iteration 0400: loss: 1952.064
iteration 0500: loss: 1951.816
iteration 0600: loss: 1954.269
iteration 0700: loss: 1954.817
iteration 0800: loss: 1953.459
iteration 0900: loss: 1952.593
====> Epoch: 007 Train loss: 1954.2372  took : 12.098509788513184
====> Test loss: 1956.3702
iteration 0000: loss: 1955.545
iteration 0100: loss: 1955.894
iteration 0200: loss: 1953.327
iteration 0300: loss: 1954.551
iteration 0400: loss: 1953.312
iteration 0500: loss: 1953.795
iteration 0600: loss: 1951.291
iteration 0700: loss: 1953.527
iteration 0800: loss: 1950.986
iteration 0900: loss: 1951.335
====> Epoch: 008 Train loss: 1952.8935  took : 13.514809131622314
====> Test loss: 1954.4358
iteration 0000: loss: 1951.123
iteration 0100: loss: 1952.862
iteration 0200: loss: 1954.267
iteration 0300: loss: 1951.778
iteration 0400: loss: 1951.220
iteration 0500: loss: 1949.656
iteration 0600: loss: 1953.025
iteration 0700: loss: 1952.110
iteration 0800: loss: 1951.571
iteration 0900: loss: 1951.090
====> Epoch: 009 Train loss: 1951.7603  took : 11.729757308959961
====> Test loss: 1953.3490
iteration 0000: loss: 1950.510
iteration 0100: loss: 1951.955
iteration 0200: loss: 1951.430
iteration 0300: loss: 1954.085
iteration 0400: loss: 1951.278
iteration 0500: loss: 1951.544
iteration 0600: loss: 1950.582
iteration 0700: loss: 1950.425
iteration 0800: loss: 1950.318
iteration 0900: loss: 1950.547
====> Epoch: 010 Train loss: 1951.1110  took : 13.311765670776367
====> Test loss: 1952.7286
iteration 0000: loss: 1952.390
iteration 0100: loss: 1949.920
iteration 0200: loss: 1949.484
iteration 0300: loss: 1952.471
iteration 0400: loss: 1950.963
iteration 0500: loss: 1951.550
iteration 0600: loss: 1951.204
iteration 0700: loss: 1950.598
iteration 0800: loss: 1950.837
iteration 0900: loss: 1950.432
====> Epoch: 011 Train loss: 1950.7263  took : 13.319749593734741
====> Test loss: 1953.1777
iteration 0000: loss: 1949.452
iteration 0100: loss: 1948.577
iteration 0200: loss: 1950.849
iteration 0300: loss: 1951.694
iteration 0400: loss: 1950.340
iteration 0500: loss: 1951.169
iteration 0600: loss: 1949.812
iteration 0700: loss: 1950.051
iteration 0800: loss: 1950.426
iteration 0900: loss: 1949.627
====> Epoch: 012 Train loss: 1950.3874  took : 13.175450086593628
====> Test loss: 1951.9911
iteration 0000: loss: 1951.885
iteration 0100: loss: 1949.691
iteration 0200: loss: 1949.400
iteration 0300: loss: 1949.448
iteration 0400: loss: 1950.554
iteration 0500: loss: 1948.934
iteration 0600: loss: 1949.608
iteration 0700: loss: 1948.171
iteration 0800: loss: 1949.881
iteration 0900: loss: 1951.996
====> Epoch: 013 Train loss: 1950.0449  took : 12.99411678314209
====> Test loss: 1952.9023
iteration 0000: loss: 1950.837
iteration 0100: loss: 1950.098
iteration 0200: loss: 1950.164
iteration 0300: loss: 1949.422
iteration 0400: loss: 1950.138
iteration 0500: loss: 1951.224
iteration 0600: loss: 1949.972
iteration 0700: loss: 1948.709
iteration 0800: loss: 1951.010
iteration 0900: loss: 1951.394
====> Epoch: 014 Train loss: 1949.7680  took : 13.128270149230957
====> Test loss: 1952.9754
iteration 0000: loss: 1951.149
iteration 0100: loss: 1949.739
iteration 0200: loss: 1951.164
iteration 0300: loss: 1949.552
iteration 0400: loss: 1949.122
iteration 0500: loss: 1948.886
iteration 0600: loss: 1948.420
iteration 0700: loss: 1949.086
iteration 0800: loss: 1949.078
iteration 0900: loss: 1949.187
====> Epoch: 015 Train loss: 1949.5121  took : 12.270609617233276
====> Test loss: 1951.4197
iteration 0000: loss: 1949.238
iteration 0100: loss: 1948.798
iteration 0200: loss: 1949.453
iteration 0300: loss: 1949.440
iteration 0400: loss: 1948.582
iteration 0500: loss: 1948.850
iteration 0600: loss: 1948.820
iteration 0700: loss: 1949.364
iteration 0800: loss: 1949.388
iteration 0900: loss: 1949.598
====> Epoch: 016 Train loss: 1949.1608  took : 12.452187538146973
====> Test loss: 1951.1910
iteration 0000: loss: 1948.416
iteration 0100: loss: 1949.818
iteration 0200: loss: 1948.527
iteration 0300: loss: 1949.007
iteration 0400: loss: 1948.951
iteration 0500: loss: 1949.501
iteration 0600: loss: 1949.084
iteration 0700: loss: 1950.158
iteration 0800: loss: 1949.485
iteration 0900: loss: 1949.193
====> Epoch: 017 Train loss: 1949.3717  took : 11.723557472229004
====> Test loss: 1951.5524
iteration 0000: loss: 1950.213
iteration 0100: loss: 1949.285
iteration 0200: loss: 1949.283
iteration 0300: loss: 1949.203
iteration 0400: loss: 1949.461
iteration 0500: loss: 1948.555
iteration 0600: loss: 1949.936
iteration 0700: loss: 1948.102
iteration 0800: loss: 1948.786
iteration 0900: loss: 1950.466
====> Epoch: 018 Train loss: 1949.1050  took : 12.442610740661621
====> Test loss: 1951.5789
iteration 0000: loss: 1949.919
iteration 0100: loss: 1950.470
iteration 0200: loss: 1948.136
iteration 0300: loss: 1948.761
iteration 0400: loss: 1948.905
iteration 0500: loss: 1947.840
iteration 0600: loss: 1948.251
iteration 0700: loss: 1948.317
iteration 0800: loss: 1947.469
iteration 0900: loss: 1948.078
====> Epoch: 019 Train loss: 1948.9646  took : 12.252850532531738
====> Test loss: 1951.2684
iteration 0000: loss: 1947.683
iteration 0100: loss: 1948.928
iteration 0200: loss: 1948.543
iteration 0300: loss: 1950.906
iteration 0400: loss: 1948.292
iteration 0500: loss: 1948.466
iteration 0600: loss: 1948.147
iteration 0700: loss: 1948.340
iteration 0800: loss: 1948.707
iteration 0900: loss: 1948.890
====> Epoch: 020 Train loss: 1949.1162  took : 13.339369297027588
====> Test loss: 1950.8725
iteration 0000: loss: 1948.844
iteration 0100: loss: 1948.442
iteration 0200: loss: 1948.141
iteration 0300: loss: 1949.142
iteration 0400: loss: 1949.085
iteration 0500: loss: 1948.668
iteration 0600: loss: 1947.868
iteration 0700: loss: 1948.905
iteration 0800: loss: 1949.262
iteration 0900: loss: 1948.462
====> Epoch: 021 Train loss: 1948.9610  took : 12.860740661621094
====> Test loss: 1951.2032
iteration 0000: loss: 1948.855
iteration 0100: loss: 1948.991
iteration 0200: loss: 1947.359
iteration 0300: loss: 1949.618
iteration 0400: loss: 1948.185
iteration 0500: loss: 1948.072
iteration 0600: loss: 1947.941
iteration 0700: loss: 1948.004
iteration 0800: loss: 1950.030
iteration 0900: loss: 1950.125
====> Epoch: 022 Train loss: 1948.8388  took : 13.09976315498352
====> Test loss: 1951.1082
iteration 0000: loss: 1947.483
iteration 0100: loss: 1949.423
iteration 0200: loss: 1949.619
iteration 0300: loss: 1949.323
iteration 0400: loss: 1948.464
iteration 0500: loss: 1950.365
iteration 0600: loss: 1948.989
iteration 0700: loss: 1948.683
iteration 0800: loss: 1947.667
iteration 0900: loss: 1948.119
====> Epoch: 023 Train loss: 1948.8578  took : 13.34259819984436
====> Test loss: 1951.1930
iteration 0000: loss: 1948.370
iteration 0100: loss: 1948.665
iteration 0200: loss: 1949.893
iteration 0300: loss: 1948.143
iteration 0400: loss: 1948.445
iteration 0500: loss: 1949.549
iteration 0600: loss: 1949.208
iteration 0700: loss: 1949.533
iteration 0800: loss: 1948.685
iteration 0900: loss: 1950.319
====> Epoch: 024 Train loss: 1948.7590  took : 12.620146989822388
====> Test loss: 1950.4892
iteration 0000: loss: 1948.028
iteration 0100: loss: 1948.077
iteration 0200: loss: 1949.771
iteration 0300: loss: 1947.809
iteration 0400: loss: 1949.010
iteration 0500: loss: 1947.657
iteration 0600: loss: 1949.816
iteration 0700: loss: 1949.263
iteration 0800: loss: 1948.956
iteration 0900: loss: 1947.909
====> Epoch: 025 Train loss: 1948.7705  took : 12.121627569198608
====> Test loss: 1950.4786
iteration 0000: loss: 1947.545
iteration 0100: loss: 1948.184
iteration 0200: loss: 1949.452
iteration 0300: loss: 1948.761
iteration 0400: loss: 1947.513
iteration 0500: loss: 1947.683
iteration 0600: loss: 1948.917
iteration 0700: loss: 1949.468
iteration 0800: loss: 1948.468
iteration 0900: loss: 1949.745
====> Epoch: 026 Train loss: 1948.6046  took : 12.927676677703857
====> Test loss: 1951.3379
iteration 0000: loss: 1948.242
iteration 0100: loss: 1950.015
iteration 0200: loss: 1948.560
iteration 0300: loss: 1947.919
iteration 0400: loss: 1949.143
iteration 0500: loss: 1948.433
iteration 0600: loss: 1948.508
iteration 0700: loss: 1948.392
iteration 0800: loss: 1949.928
iteration 0900: loss: 1948.174
====> Epoch: 027 Train loss: 1948.5694  took : 12.647194862365723
====> Test loss: 1950.6241
iteration 0000: loss: 1947.380
iteration 0100: loss: 1948.784
iteration 0200: loss: 1947.774
iteration 0300: loss: 1948.012
iteration 0400: loss: 1948.403
iteration 0500: loss: 1948.349
iteration 0600: loss: 1947.960
iteration 0700: loss: 1948.144
iteration 0800: loss: 1949.219
iteration 0900: loss: 1947.476
====> Epoch: 028 Train loss: 1948.5343  took : 12.28020167350769
====> Test loss: 1950.5131
iteration 0000: loss: 1949.035
iteration 0100: loss: 1948.513
iteration 0200: loss: 1948.580
iteration 0300: loss: 1948.338
iteration 0400: loss: 1947.907
iteration 0500: loss: 1947.459
iteration 0600: loss: 1948.524
iteration 0700: loss: 1947.870
iteration 0800: loss: 1947.865
iteration 0900: loss: 1947.728
====> Epoch: 029 Train loss: 1948.4558  took : 13.458518266677856
====> Test loss: 1951.0164
iteration 0000: loss: 1947.950
iteration 0100: loss: 1950.013
iteration 0200: loss: 1947.699
iteration 0300: loss: 1947.881
iteration 0400: loss: 1948.613
iteration 0500: loss: 1949.037
iteration 0600: loss: 1947.671
iteration 0700: loss: 1947.772
iteration 0800: loss: 1947.369
iteration 0900: loss: 1947.699
====> Epoch: 030 Train loss: 1948.4464  took : 12.54831075668335
====> Test loss: 1949.9896
iteration 0000: loss: 1948.013
iteration 0100: loss: 1947.337
iteration 0200: loss: 1949.922
iteration 0300: loss: 1947.913
iteration 0400: loss: 1949.976
iteration 0500: loss: 1948.524
iteration 0600: loss: 1948.244
iteration 0700: loss: 1947.896
iteration 0800: loss: 1949.762
iteration 0900: loss: 1948.466
====> Epoch: 031 Train loss: 1948.2977  took : 12.826257705688477
====> Test loss: 1950.0482
iteration 0000: loss: 1947.543
iteration 0100: loss: 1947.378
iteration 0200: loss: 1947.999
iteration 0300: loss: 1948.723
iteration 0400: loss: 1947.732
iteration 0500: loss: 1948.064
iteration 0600: loss: 1949.018
iteration 0700: loss: 1949.059
iteration 0800: loss: 1948.212
iteration 0900: loss: 1947.908
====> Epoch: 032 Train loss: 1948.0663  took : 13.164989948272705
====> Test loss: 1949.9992
iteration 0000: loss: 1949.250
iteration 0100: loss: 1946.985
iteration 0200: loss: 1948.011
iteration 0300: loss: 1949.383
iteration 0400: loss: 1947.943
iteration 0500: loss: 1948.555
iteration 0600: loss: 1947.495
iteration 0700: loss: 1948.281
iteration 0800: loss: 1948.139
iteration 0900: loss: 1948.832
====> Epoch: 033 Train loss: 1948.1803  took : 12.427531003952026
====> Test loss: 1950.1904
iteration 0000: loss: 1948.950
iteration 0100: loss: 1949.490
iteration 0200: loss: 1948.497
iteration 0300: loss: 1947.818
iteration 0400: loss: 1947.964
iteration 0500: loss: 1946.945
iteration 0600: loss: 1947.598
iteration 0700: loss: 1947.863
iteration 0800: loss: 1948.422
iteration 0900: loss: 1948.108
====> Epoch: 034 Train loss: 1948.0553  took : 12.40070652961731
====> Test loss: 1950.5537
iteration 0000: loss: 1947.976
iteration 0100: loss: 1947.221
iteration 0200: loss: 1949.599
iteration 0300: loss: 1949.944
iteration 0400: loss: 1947.695
iteration 0500: loss: 1948.395
iteration 0600: loss: 1947.556
iteration 0700: loss: 1947.437
iteration 0800: loss: 1948.620
iteration 0900: loss: 1947.880
====> Epoch: 035 Train loss: 1948.2645  took : 13.661186933517456
====> Test loss: 1950.0437
iteration 0000: loss: 1947.271
iteration 0100: loss: 1949.465
iteration 0200: loss: 1947.445
iteration 0300: loss: 1947.704
iteration 0400: loss: 1946.918
iteration 0500: loss: 1948.014
iteration 0600: loss: 1947.750
iteration 0700: loss: 1947.552
iteration 0800: loss: 1948.581
iteration 0900: loss: 1948.646
====> Epoch: 036 Train loss: 1947.9984  took : 12.4676992893219
====> Test loss: 1950.1130
iteration 0000: loss: 1947.674
iteration 0100: loss: 1947.468
iteration 0200: loss: 1947.510
iteration 0300: loss: 1948.604
iteration 0400: loss: 1947.791
iteration 0500: loss: 1948.011
iteration 0600: loss: 1947.732
iteration 0700: loss: 1948.635
iteration 0800: loss: 1948.057
iteration 0900: loss: 1947.505
====> Epoch: 037 Train loss: 1948.1088  took : 11.88990306854248
====> Test loss: 1949.8189
iteration 0000: loss: 1947.380
iteration 0100: loss: 1948.208
iteration 0200: loss: 1948.488
iteration 0300: loss: 1948.485
iteration 0400: loss: 1948.412
iteration 0500: loss: 1947.868
iteration 0600: loss: 1948.371
iteration 0700: loss: 1948.034
iteration 0800: loss: 1948.542
iteration 0900: loss: 1946.907
====> Epoch: 038 Train loss: 1948.1489  took : 12.002387285232544
====> Test loss: 1950.2596
iteration 0000: loss: 1948.345
iteration 0100: loss: 1947.555
iteration 0200: loss: 1949.686
iteration 0300: loss: 1948.117
iteration 0400: loss: 1947.872
iteration 0500: loss: 1948.011
iteration 0600: loss: 1948.138
iteration 0700: loss: 1947.925
iteration 0800: loss: 1947.239
iteration 0900: loss: 1947.504
====> Epoch: 039 Train loss: 1947.9698  took : 12.045165777206421
====> Test loss: 1949.9001
iteration 0000: loss: 1948.818
iteration 0100: loss: 1948.733
iteration 0200: loss: 1947.525
iteration 0300: loss: 1949.297
iteration 0400: loss: 1948.366
iteration 0500: loss: 1949.581
iteration 0600: loss: 1947.401
iteration 0700: loss: 1947.380
iteration 0800: loss: 1947.812
iteration 0900: loss: 1948.890
====> Epoch: 040 Train loss: 1948.0334  took : 11.625823974609375
====> Test loss: 1949.8651
iteration 0000: loss: 1947.311
iteration 0100: loss: 1949.261
iteration 0200: loss: 1947.454
iteration 0300: loss: 1947.331
iteration 0400: loss: 1948.389
iteration 0500: loss: 1947.337
iteration 0600: loss: 1947.474
iteration 0700: loss: 1947.549
iteration 0800: loss: 1948.359
iteration 0900: loss: 1948.324
====> Epoch: 041 Train loss: 1948.0958  took : 12.408441305160522
====> Test loss: 1950.2709
iteration 0000: loss: 1947.316
iteration 0100: loss: 1948.579
iteration 0200: loss: 1947.245
iteration 0300: loss: 1947.377
iteration 0400: loss: 1948.596
iteration 0500: loss: 1948.196
iteration 0600: loss: 1948.092
iteration 0700: loss: 1947.486
iteration 0800: loss: 1948.679
iteration 0900: loss: 1947.924
====> Epoch: 042 Train loss: 1948.0192  took : 12.195072412490845
====> Test loss: 1949.8578
iteration 0000: loss: 1947.353
iteration 0100: loss: 1947.946
iteration 0200: loss: 1948.245
iteration 0300: loss: 1948.670
iteration 0400: loss: 1947.442
iteration 0500: loss: 1948.032
iteration 0600: loss: 1947.849
iteration 0700: loss: 1948.069
iteration 0800: loss: 1948.080
iteration 0900: loss: 1946.906
====> Epoch: 043 Train loss: 1947.8318  took : 12.039356231689453
====> Test loss: 1949.5676
iteration 0000: loss: 1947.451
iteration 0100: loss: 1947.122
iteration 0200: loss: 1947.904
iteration 0300: loss: 1947.853
iteration 0400: loss: 1947.226
iteration 0500: loss: 1947.404
iteration 0600: loss: 1947.859
iteration 0700: loss: 1950.040
iteration 0800: loss: 1947.372
iteration 0900: loss: 1947.194
====> Epoch: 044 Train loss: 1947.8551  took : 12.358342409133911
====> Test loss: 1949.6010
iteration 0000: loss: 1947.079
iteration 0100: loss: 1947.231
iteration 0200: loss: 1947.437
iteration 0300: loss: 1947.131
iteration 0400: loss: 1947.391
iteration 0500: loss: 1947.782
iteration 0600: loss: 1948.512
iteration 0700: loss: 1948.522
iteration 0800: loss: 1947.053
iteration 0900: loss: 1947.819
====> Epoch: 045 Train loss: 1947.7549  took : 12.122925996780396
====> Test loss: 1950.2934
iteration 0000: loss: 1947.306
iteration 0100: loss: 1948.366
iteration 0200: loss: 1947.728
iteration 0300: loss: 1947.608
iteration 0400: loss: 1947.535
iteration 0500: loss: 1947.182
iteration 0600: loss: 1947.172
iteration 0700: loss: 1947.926
iteration 0800: loss: 1947.422
iteration 0900: loss: 1948.044
====> Epoch: 046 Train loss: 1947.7485  took : 13.291414499282837
====> Test loss: 1949.6728
iteration 0000: loss: 1947.541
iteration 0100: loss: 1948.067
iteration 0200: loss: 1947.658
iteration 0300: loss: 1947.630
iteration 0400: loss: 1947.934
iteration 0500: loss: 1947.265
iteration 0600: loss: 1947.922
iteration 0700: loss: 1948.026
iteration 0800: loss: 1947.660
iteration 0900: loss: 1948.405
====> Epoch: 047 Train loss: 1947.7550  took : 13.063101291656494
====> Test loss: 1950.0162
iteration 0000: loss: 1947.713
iteration 0100: loss: 1947.171
iteration 0200: loss: 1947.498
iteration 0300: loss: 1946.921
iteration 0400: loss: 1948.093
iteration 0500: loss: 1947.979
iteration 0600: loss: 1947.353
iteration 0700: loss: 1947.748
iteration 0800: loss: 1948.512
iteration 0900: loss: 1948.355
====> Epoch: 048 Train loss: 1947.9104  took : 12.287314414978027
====> Test loss: 1949.6999
iteration 0000: loss: 1947.432
iteration 0100: loss: 1947.528
iteration 0200: loss: 1947.361
iteration 0300: loss: 1948.088
iteration 0400: loss: 1947.771
iteration 0500: loss: 1948.129
iteration 0600: loss: 1948.171
iteration 0700: loss: 1947.405
iteration 0800: loss: 1947.248
iteration 0900: loss: 1947.160
====> Epoch: 049 Train loss: 1947.6537  took : 13.072378396987915
====> Test loss: 1949.3349
iteration 0000: loss: 1947.409
iteration 0100: loss: 1948.080
iteration 0200: loss: 1947.388
iteration 0300: loss: 1947.973
iteration 0400: loss: 1947.282
iteration 0500: loss: 1947.907
iteration 0600: loss: 1947.678
iteration 0700: loss: 1947.576
iteration 0800: loss: 1947.325
iteration 0900: loss: 1947.233
====> Epoch: 050 Train loss: 1947.6433  took : 12.458812475204468
====> Test loss: 1949.3853
====> [MM-VAE] Time: 701.675s or 00:11:41
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  11
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_11
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_11
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5220.568
iteration 0100: loss: 4149.913
iteration 0200: loss: 4089.553
iteration 0300: loss: 4056.990
iteration 0400: loss: 4028.862
iteration 0500: loss: 4022.031
iteration 0600: loss: 4016.689
iteration 0700: loss: 4023.659
iteration 0800: loss: 4009.321
iteration 0900: loss: 4023.018
iteration 1000: loss: 3992.241
iteration 1100: loss: 3996.986
iteration 1200: loss: 3999.690
iteration 1300: loss: 4000.642
iteration 1400: loss: 3990.228
iteration 1500: loss: 3989.254
iteration 1600: loss: 4000.753
iteration 1700: loss: 4009.194
iteration 1800: loss: 3992.325
====> Epoch: 001 Train loss: 4028.0554  took : 53.569031715393066
====> Test loss: 3991.5133
iteration 0000: loss: 3986.032
iteration 0100: loss: 3982.331
iteration 0200: loss: 3995.471
iteration 0300: loss: 3973.842
iteration 0400: loss: 3972.987
iteration 0500: loss: 3979.495
iteration 0600: loss: 3975.192
iteration 0700: loss: 3976.069
iteration 0800: loss: 3969.864
iteration 0900: loss: 3971.490
iteration 1000: loss: 3966.293
iteration 1100: loss: 3972.108
iteration 1200: loss: 3951.791
iteration 1300: loss: 3972.478
iteration 1400: loss: 3977.567
iteration 1500: loss: 3965.763
iteration 1600: loss: 3970.113
iteration 1700: loss: 3963.438
iteration 1800: loss: 3963.344
====> Epoch: 002 Train loss: 3975.7392  took : 53.22236680984497
====> Test loss: 3968.7625
iteration 0000: loss: 3966.192
iteration 0100: loss: 3972.021
iteration 0200: loss: 3964.048
iteration 0300: loss: 3960.414
iteration 0400: loss: 3958.457
iteration 0500: loss: 3969.261
iteration 0600: loss: 3968.577
iteration 0700: loss: 3956.680
iteration 0800: loss: 3956.214
iteration 0900: loss: 3967.469
iteration 1000: loss: 3967.844
iteration 1100: loss: 3957.702
iteration 1200: loss: 3949.834
iteration 1300: loss: 3958.430
iteration 1400: loss: 3956.841
iteration 1500: loss: 3961.467
iteration 1600: loss: 3960.432
iteration 1700: loss: 3954.769
iteration 1800: loss: 3953.071
====> Epoch: 003 Train loss: 3960.3461  took : 53.31356143951416
====> Test loss: 3958.9732
iteration 0000: loss: 3958.894
iteration 0100: loss: 3958.374
iteration 0200: loss: 3956.326
iteration 0300: loss: 3953.511
iteration 0400: loss: 3953.856
iteration 0500: loss: 3963.653
iteration 0600: loss: 3950.931
iteration 0700: loss: 3949.606
iteration 0800: loss: 3954.164
iteration 0900: loss: 3954.549
iteration 1000: loss: 3945.211
iteration 1100: loss: 3944.774
iteration 1200: loss: 3957.019
iteration 1300: loss: 3951.465
iteration 1400: loss: 3952.997
iteration 1500: loss: 3946.678
iteration 1600: loss: 3944.858
iteration 1700: loss: 3947.490
iteration 1800: loss: 3950.354
====> Epoch: 004 Train loss: 3952.4177  took : 53.265554904937744
====> Test loss: 3952.4444
iteration 0000: loss: 3946.532
iteration 0100: loss: 3946.555
iteration 0200: loss: 3954.164
iteration 0300: loss: 3946.585
iteration 0400: loss: 3942.252
iteration 0500: loss: 3949.992
iteration 0600: loss: 3953.410
iteration 0700: loss: 3948.687
iteration 0800: loss: 3947.170
iteration 0900: loss: 3950.473
iteration 1000: loss: 3953.011
iteration 1100: loss: 3951.026
iteration 1200: loss: 3943.391
iteration 1300: loss: 3948.562
iteration 1400: loss: 3944.879
iteration 1500: loss: 3942.024
iteration 1600: loss: 3948.035
iteration 1700: loss: 3947.419
iteration 1800: loss: 3937.268
====> Epoch: 005 Train loss: 3947.3569  took : 53.25779914855957
====> Test loss: 3949.7056
iteration 0000: loss: 3947.114
iteration 0100: loss: 3949.874
iteration 0200: loss: 3946.153
iteration 0300: loss: 3950.088
iteration 0400: loss: 3944.805
iteration 0500: loss: 3941.561
iteration 0600: loss: 3951.940
iteration 0700: loss: 3943.973
iteration 0800: loss: 3939.331
iteration 0900: loss: 3944.323
iteration 1000: loss: 3942.919
iteration 1100: loss: 3946.953
iteration 1200: loss: 3939.185
iteration 1300: loss: 3946.946
iteration 1400: loss: 3948.520
iteration 1500: loss: 3954.082
iteration 1600: loss: 3945.200
iteration 1700: loss: 3946.971
iteration 1800: loss: 3944.387
====> Epoch: 006 Train loss: 3945.1261  took : 53.181352615356445
====> Test loss: 3947.3400
iteration 0000: loss: 3940.875
iteration 0100: loss: 3944.212
iteration 0200: loss: 3943.937
iteration 0300: loss: 3941.352
iteration 0400: loss: 3950.773
iteration 0500: loss: 3944.207
iteration 0600: loss: 3945.934
iteration 0700: loss: 3941.388
iteration 0800: loss: 3948.363
iteration 0900: loss: 3953.074
iteration 1000: loss: 3937.814
iteration 1100: loss: 3941.908
iteration 1200: loss: 3943.380
iteration 1300: loss: 3939.957
iteration 1400: loss: 3947.998
iteration 1500: loss: 3942.742
iteration 1600: loss: 3939.743
iteration 1700: loss: 3942.078
iteration 1800: loss: 3941.738
====> Epoch: 007 Train loss: 3943.7418  took : 52.76331400871277
====> Test loss: 3947.0035
iteration 0000: loss: 3944.401
iteration 0100: loss: 3943.151
iteration 0200: loss: 3939.021
iteration 0300: loss: 3942.488
iteration 0400: loss: 3938.516
iteration 0500: loss: 3947.118
iteration 0600: loss: 3942.708
iteration 0700: loss: 3943.072
iteration 0800: loss: 3941.175
iteration 0900: loss: 3945.270
iteration 1000: loss: 3938.793
iteration 1100: loss: 3938.620
iteration 1200: loss: 3945.510
iteration 1300: loss: 3943.049
iteration 1400: loss: 3942.964
iteration 1500: loss: 3952.090
iteration 1600: loss: 3936.918
iteration 1700: loss: 3944.106
iteration 1800: loss: 3943.129
====> Epoch: 008 Train loss: 3943.0539  took : 52.95981478691101
====> Test loss: 3946.2140
iteration 0000: loss: 3939.376
iteration 0100: loss: 3942.560
iteration 0200: loss: 3942.078
iteration 0300: loss: 3945.735
iteration 0400: loss: 3945.217
iteration 0500: loss: 3948.446
iteration 0600: loss: 3945.359
iteration 0700: loss: 3945.013
iteration 0800: loss: 3952.397
iteration 0900: loss: 3936.087
iteration 1000: loss: 3941.250
iteration 1100: loss: 3938.573
iteration 1200: loss: 3940.818
iteration 1300: loss: 3937.990
iteration 1400: loss: 3946.412
iteration 1500: loss: 3938.124
iteration 1600: loss: 3936.587
iteration 1700: loss: 3941.797
iteration 1800: loss: 3942.158
====> Epoch: 009 Train loss: 3942.7762  took : 53.12310290336609
====> Test loss: 3945.5342
iteration 0000: loss: 3941.141
iteration 0100: loss: 3942.823
iteration 0200: loss: 3944.393
iteration 0300: loss: 3941.146
iteration 0400: loss: 3938.532
iteration 0500: loss: 3939.365
iteration 0600: loss: 3940.993
iteration 0700: loss: 3944.368
iteration 0800: loss: 3942.492
iteration 0900: loss: 3937.211
iteration 1000: loss: 3945.690
iteration 1100: loss: 3942.078
iteration 1200: loss: 3947.557
iteration 1300: loss: 3938.079
iteration 1400: loss: 3940.873
iteration 1500: loss: 3941.278
iteration 1600: loss: 3935.791
iteration 1700: loss: 3935.298
iteration 1800: loss: 3943.875
====> Epoch: 010 Train loss: 3942.2664  took : 53.23254942893982
====> Test loss: 3945.6702
iteration 0000: loss: 3940.808
iteration 0100: loss: 3942.754
iteration 0200: loss: 3941.726
iteration 0300: loss: 3938.768
iteration 0400: loss: 3946.677
iteration 0500: loss: 3945.646
iteration 0600: loss: 3942.194
iteration 0700: loss: 3944.863
iteration 0800: loss: 3941.744
iteration 0900: loss: 3942.534
iteration 1000: loss: 3941.631
iteration 1100: loss: 3939.741
iteration 1200: loss: 3938.083
iteration 1300: loss: 3943.425
iteration 1400: loss: 3937.890
iteration 1500: loss: 3943.386
iteration 1600: loss: 3945.998
iteration 1700: loss: 3941.444
iteration 1800: loss: 3942.568
====> Epoch: 011 Train loss: 3942.0288  took : 52.84740948677063
====> Test loss: 3945.3901
iteration 0000: loss: 3941.829
iteration 0100: loss: 3941.981
iteration 0200: loss: 3936.034
iteration 0300: loss: 3946.664
iteration 0400: loss: 3945.996
iteration 0500: loss: 3937.787
iteration 0600: loss: 3937.058
iteration 0700: loss: 3940.943
iteration 0800: loss: 3935.187
iteration 0900: loss: 3939.958
iteration 1000: loss: 3936.604
iteration 1100: loss: 3935.197
iteration 1200: loss: 3940.487
iteration 1300: loss: 3944.845
iteration 1400: loss: 3942.684
iteration 1500: loss: 3943.371
iteration 1600: loss: 3943.426
iteration 1700: loss: 3941.535
iteration 1800: loss: 3936.219
====> Epoch: 012 Train loss: 3941.8573  took : 53.02363657951355
====> Test loss: 3945.1678
iteration 0000: loss: 3939.931
iteration 0100: loss: 3940.344
iteration 0200: loss: 3948.112
iteration 0300: loss: 3934.529
iteration 0400: loss: 3946.795
iteration 0500: loss: 3937.691
iteration 0600: loss: 3947.842
iteration 0700: loss: 3940.482
iteration 0800: loss: 3939.952
iteration 0900: loss: 3938.951
iteration 1000: loss: 3943.851
iteration 1100: loss: 3940.915
iteration 1200: loss: 3941.977
iteration 1300: loss: 3941.823
iteration 1400: loss: 3939.583
iteration 1500: loss: 3937.356
iteration 1600: loss: 3935.874
iteration 1700: loss: 3938.394
iteration 1800: loss: 3943.383
====> Epoch: 013 Train loss: 3941.6406  took : 53.17709040641785
====> Test loss: 3945.0162
iteration 0000: loss: 3939.262
iteration 0100: loss: 3940.268
iteration 0200: loss: 3937.145
iteration 0300: loss: 3940.169
iteration 0400: loss: 3936.302
iteration 0500: loss: 3940.853
iteration 0600: loss: 3946.930
iteration 0700: loss: 3936.025
iteration 0800: loss: 3941.396
iteration 0900: loss: 3936.671
iteration 1000: loss: 3944.359
iteration 1100: loss: 3943.663
iteration 1200: loss: 3944.286
iteration 1300: loss: 3942.710
iteration 1400: loss: 3941.452
iteration 1500: loss: 3939.383
iteration 1600: loss: 3946.994
iteration 1700: loss: 3944.785
iteration 1800: loss: 3940.169
====> Epoch: 014 Train loss: 3941.4168  took : 52.884313344955444
====> Test loss: 3945.4999
iteration 0000: loss: 3942.522
iteration 0100: loss: 3945.338
iteration 0200: loss: 3940.671
iteration 0300: loss: 3944.416
iteration 0400: loss: 3936.672
iteration 0500: loss: 3937.938
iteration 0600: loss: 3944.195
iteration 0700: loss: 3935.274
iteration 0800: loss: 3933.668
iteration 0900: loss: 3942.212
iteration 1000: loss: 3950.435
iteration 1100: loss: 3939.199
iteration 1200: loss: 3943.901
iteration 1300: loss: 3941.427
iteration 1400: loss: 3942.258
iteration 1500: loss: 3943.274
iteration 1600: loss: 3932.444
iteration 1700: loss: 3943.990
iteration 1800: loss: 3944.362
====> Epoch: 015 Train loss: 3941.1369  took : 52.92691397666931
====> Test loss: 3944.4325
iteration 0000: loss: 3945.231
iteration 0100: loss: 3944.114
iteration 0200: loss: 3940.997
iteration 0300: loss: 3936.529
iteration 0400: loss: 3944.765
iteration 0500: loss: 3935.649
iteration 0600: loss: 3944.141
iteration 0700: loss: 3943.240
iteration 0800: loss: 3942.053
iteration 0900: loss: 3939.997
iteration 1000: loss: 3937.461
iteration 1100: loss: 3937.426
iteration 1200: loss: 3944.590
iteration 1300: loss: 3941.103
iteration 1400: loss: 3940.921
iteration 1500: loss: 3945.294
iteration 1600: loss: 3935.463
iteration 1700: loss: 3937.404
iteration 1800: loss: 3942.984
====> Epoch: 016 Train loss: 3941.0426  took : 53.19080185890198
====> Test loss: 3944.4572
iteration 0000: loss: 3937.803
iteration 0100: loss: 3943.149
iteration 0200: loss: 3936.826
iteration 0300: loss: 3937.031
iteration 0400: loss: 3939.528
iteration 0500: loss: 3940.872
iteration 0600: loss: 3939.417
iteration 0700: loss: 3942.409
iteration 0800: loss: 3942.621
iteration 0900: loss: 3939.974
iteration 1000: loss: 3942.811
iteration 1100: loss: 3938.364
iteration 1200: loss: 3940.170
iteration 1300: loss: 3940.811
iteration 1400: loss: 3943.426
iteration 1500: loss: 3944.719
iteration 1600: loss: 3945.400
iteration 1700: loss: 3939.837
iteration 1800: loss: 3941.895
====> Epoch: 017 Train loss: 3940.5999  took : 52.81126809120178
====> Test loss: 3944.1524
iteration 0000: loss: 3939.239
iteration 0100: loss: 3936.369
iteration 0200: loss: 3936.666
iteration 0300: loss: 3938.017
iteration 0400: loss: 3944.100
iteration 0500: loss: 3939.937
iteration 0600: loss: 3943.134
iteration 0700: loss: 3940.423
iteration 0800: loss: 3939.753
iteration 0900: loss: 3938.967
iteration 1000: loss: 3939.892
iteration 1100: loss: 3941.798
iteration 1200: loss: 3943.497
iteration 1300: loss: 3943.647
iteration 1400: loss: 3940.067
iteration 1500: loss: 3941.065
iteration 1600: loss: 3943.948
iteration 1700: loss: 3950.398
iteration 1800: loss: 3942.637
====> Epoch: 018 Train loss: 3940.5706  took : 53.078861236572266
====> Test loss: 3944.3789
iteration 0000: loss: 3939.873
iteration 0100: loss: 3946.298
iteration 0200: loss: 3943.114
iteration 0300: loss: 3939.605
iteration 0400: loss: 3941.758
iteration 0500: loss: 3936.452
iteration 0600: loss: 3935.785
iteration 0700: loss: 3939.252
iteration 0800: loss: 3943.221
iteration 0900: loss: 3942.734
iteration 1000: loss: 3941.270
iteration 1100: loss: 3936.684
iteration 1200: loss: 3940.417
iteration 1300: loss: 3945.869
iteration 1400: loss: 3944.798
iteration 1500: loss: 3938.896
iteration 1600: loss: 3944.038
iteration 1700: loss: 3940.711
iteration 1800: loss: 3942.935
====> Epoch: 019 Train loss: 3940.5031  took : 53.10678696632385
====> Test loss: 3943.8389
iteration 0000: loss: 3935.249
iteration 0100: loss: 3945.213
iteration 0200: loss: 3937.505
iteration 0300: loss: 3940.887
iteration 0400: loss: 3940.944
iteration 0500: loss: 3946.453
iteration 0600: loss: 3943.995
iteration 0700: loss: 3942.361
iteration 0800: loss: 3939.899
iteration 0900: loss: 3936.803
iteration 1000: loss: 3942.663
iteration 1100: loss: 3937.624
iteration 1200: loss: 3942.838
iteration 1300: loss: 3940.473
iteration 1400: loss: 3938.852
iteration 1500: loss: 3935.868
iteration 1600: loss: 3946.184
iteration 1700: loss: 3941.017
iteration 1800: loss: 3937.960
====> Epoch: 020 Train loss: 3940.3620  took : 53.15858435630798
====> Test loss: 3943.5557
iteration 0000: loss: 3931.893
iteration 0100: loss: 3941.951
iteration 0200: loss: 3941.171
iteration 0300: loss: 3937.456
iteration 0400: loss: 3941.839
iteration 0500: loss: 3944.071
iteration 0600: loss: 3939.840
iteration 0700: loss: 3940.631
iteration 0800: loss: 3936.386
iteration 0900: loss: 3946.903
iteration 1000: loss: 3941.462
iteration 1100: loss: 3935.034
iteration 1200: loss: 3938.530
iteration 1300: loss: 3932.770
iteration 1400: loss: 3937.914
iteration 1500: loss: 3934.943
iteration 1600: loss: 3936.698
iteration 1700: loss: 3944.469
iteration 1800: loss: 3939.732
====> Epoch: 021 Train loss: 3940.1719  took : 53.00333023071289
====> Test loss: 3944.3103
iteration 0000: loss: 3933.364
iteration 0100: loss: 3945.818
iteration 0200: loss: 3936.670
iteration 0300: loss: 3939.686
iteration 0400: loss: 3942.642
iteration 0500: loss: 3941.914
iteration 0600: loss: 3935.878
iteration 0700: loss: 3935.873
iteration 0800: loss: 3933.448
iteration 0900: loss: 3937.046
iteration 1000: loss: 3936.162
iteration 1100: loss: 3936.029
iteration 1200: loss: 3944.159
iteration 1300: loss: 3930.486
iteration 1400: loss: 3940.304
iteration 1500: loss: 3937.720
iteration 1600: loss: 3940.756
iteration 1700: loss: 3937.326
iteration 1800: loss: 3940.550
====> Epoch: 022 Train loss: 3940.2392  took : 52.81078100204468
====> Test loss: 3943.5885
iteration 0000: loss: 3939.806
iteration 0100: loss: 3936.694
iteration 0200: loss: 3935.684
iteration 0300: loss: 3935.824
iteration 0400: loss: 3937.071
iteration 0500: loss: 3938.720
iteration 0600: loss: 3939.946
iteration 0700: loss: 3942.275
iteration 0800: loss: 3942.048
iteration 0900: loss: 3936.297
iteration 1000: loss: 3939.397
iteration 1100: loss: 3937.200
iteration 1200: loss: 3938.749
iteration 1300: loss: 3938.891
iteration 1400: loss: 3937.052
iteration 1500: loss: 3937.786
iteration 1600: loss: 3934.637
iteration 1700: loss: 3942.184
iteration 1800: loss: 3939.536
====> Epoch: 023 Train loss: 3939.9534  took : 53.032248973846436
====> Test loss: 3943.7255
iteration 0000: loss: 3941.541
iteration 0100: loss: 3941.024
iteration 0200: loss: 3936.498
iteration 0300: loss: 3935.941
iteration 0400: loss: 3941.286
iteration 0500: loss: 3939.233
iteration 0600: loss: 3942.323
iteration 0700: loss: 3939.651
iteration 0800: loss: 3932.280
iteration 0900: loss: 3940.442
iteration 1000: loss: 3944.599
iteration 1100: loss: 3933.881
iteration 1200: loss: 3940.050
iteration 1300: loss: 3938.146
iteration 1400: loss: 3944.568
iteration 1500: loss: 3939.978
iteration 1600: loss: 3933.688
iteration 1700: loss: 3946.874
iteration 1800: loss: 3945.993
====> Epoch: 024 Train loss: 3939.8808  took : 53.06263518333435
====> Test loss: 3943.8344
iteration 0000: loss: 3941.191
iteration 0100: loss: 3943.092
iteration 0200: loss: 3935.148
iteration 0300: loss: 3947.344
iteration 0400: loss: 3942.438
iteration 0500: loss: 3937.717
iteration 0600: loss: 3939.958
iteration 0700: loss: 3938.505
iteration 0800: loss: 3943.185
iteration 0900: loss: 3942.550
iteration 1000: loss: 3935.398
iteration 1100: loss: 3940.238
iteration 1200: loss: 3938.700
iteration 1300: loss: 3941.657
iteration 1400: loss: 3938.917
iteration 1500: loss: 3942.915
iteration 1600: loss: 3936.102
iteration 1700: loss: 3943.685
iteration 1800: loss: 3939.984
====> Epoch: 025 Train loss: 3939.7043  took : 52.78103065490723
====> Test loss: 3942.9448
iteration 0000: loss: 3934.424
iteration 0100: loss: 3945.964
iteration 0200: loss: 3931.116
iteration 0300: loss: 3939.521
iteration 0400: loss: 3935.818
iteration 0500: loss: 3949.457
iteration 0600: loss: 3945.056
iteration 0700: loss: 3939.193
iteration 0800: loss: 3940.662
iteration 0900: loss: 3937.630
iteration 1000: loss: 3938.222
iteration 1100: loss: 3937.508
iteration 1200: loss: 3938.334
iteration 1300: loss: 3936.968
iteration 1400: loss: 3940.311
iteration 1500: loss: 3945.443
iteration 1600: loss: 3942.998
iteration 1700: loss: 3941.759
iteration 1800: loss: 3937.520
====> Epoch: 026 Train loss: 3939.7742  took : 53.09657144546509
====> Test loss: 3943.9601
iteration 0000: loss: 3941.534
iteration 0100: loss: 3938.441
iteration 0200: loss: 3946.648
iteration 0300: loss: 3937.309
iteration 0400: loss: 3938.756
iteration 0500: loss: 3937.930
iteration 0600: loss: 3934.655
iteration 0700: loss: 3943.432
iteration 0800: loss: 3945.520
iteration 0900: loss: 3939.736
iteration 1000: loss: 3937.163
iteration 1100: loss: 3940.507
iteration 1200: loss: 3944.400
iteration 1300: loss: 3940.100
iteration 1400: loss: 3940.333
iteration 1500: loss: 3935.098
iteration 1600: loss: 3932.881
iteration 1700: loss: 3935.904
iteration 1800: loss: 3942.794
====> Epoch: 027 Train loss: 3939.9410  took : 52.83617377281189
====> Test loss: 3943.8433
iteration 0000: loss: 3936.665
iteration 0100: loss: 3943.207
iteration 0200: loss: 3945.439
iteration 0300: loss: 3937.219
iteration 0400: loss: 3936.954
iteration 0500: loss: 3941.406
iteration 0600: loss: 3936.124
iteration 0700: loss: 3933.216
iteration 0800: loss: 3943.896
iteration 0900: loss: 3935.901
iteration 1000: loss: 3936.943
iteration 1100: loss: 3935.383
iteration 1200: loss: 3933.245
iteration 1300: loss: 3930.350
iteration 1400: loss: 3942.687
iteration 1500: loss: 3932.860
iteration 1600: loss: 3950.726
iteration 1700: loss: 3946.789
iteration 1800: loss: 3937.483
====> Epoch: 028 Train loss: 3939.3623  took : 52.87923741340637
====> Test loss: 3943.0107
iteration 0000: loss: 3946.816
iteration 0100: loss: 3939.809
iteration 0200: loss: 3943.930
iteration 0300: loss: 3940.139
iteration 0400: loss: 3940.136
iteration 0500: loss: 3937.454
iteration 0600: loss: 3940.067
iteration 0700: loss: 3934.921
iteration 0800: loss: 3935.146
iteration 0900: loss: 3940.705
iteration 1000: loss: 3939.041
iteration 1100: loss: 3936.643
iteration 1200: loss: 3940.136
iteration 1300: loss: 3944.209
iteration 1400: loss: 3937.939
iteration 1500: loss: 3935.882
iteration 1600: loss: 3938.361
iteration 1700: loss: 3936.199
iteration 1800: loss: 3933.500
====> Epoch: 029 Train loss: 3939.2214  took : 52.943591356277466
====> Test loss: 3943.0268
iteration 0000: loss: 3942.037
iteration 0100: loss: 3942.547
iteration 0200: loss: 3939.250
iteration 0300: loss: 3939.575
iteration 0400: loss: 3946.290
iteration 0500: loss: 3937.905
iteration 0600: loss: 3948.436
iteration 0700: loss: 3952.013
iteration 0800: loss: 3937.143
iteration 0900: loss: 3941.459
iteration 1000: loss: 3937.708
iteration 1100: loss: 3946.740
iteration 1200: loss: 3939.305
iteration 1300: loss: 3940.418
iteration 1400: loss: 3944.896
iteration 1500: loss: 3932.354
iteration 1600: loss: 3938.092
iteration 1700: loss: 3939.270
iteration 1800: loss: 3942.695
====> Epoch: 030 Train loss: 3939.2296  took : 52.92188906669617
====> Test loss: 3943.1956
iteration 0000: loss: 3937.964
iteration 0100: loss: 3935.487
iteration 0200: loss: 3938.419
iteration 0300: loss: 3935.242
iteration 0400: loss: 3938.793
iteration 0500: loss: 3937.839
iteration 0600: loss: 3940.676
iteration 0700: loss: 3938.158
iteration 0800: loss: 3940.722
iteration 0900: loss: 3944.022
iteration 1000: loss: 3940.590
iteration 1100: loss: 3942.916
iteration 1200: loss: 3941.427
iteration 1300: loss: 3940.709
iteration 1400: loss: 3940.873
iteration 1500: loss: 3935.392
iteration 1600: loss: 3938.295
iteration 1700: loss: 3937.857
iteration 1800: loss: 3935.446
====> Epoch: 031 Train loss: 3939.1666  took : 53.17259764671326
====> Test loss: 3942.7055
iteration 0000: loss: 3941.754
iteration 0100: loss: 3941.499
iteration 0200: loss: 3939.585
iteration 0300: loss: 3939.121
iteration 0400: loss: 3934.133
iteration 0500: loss: 3936.163
iteration 0600: loss: 3943.289
iteration 0700: loss: 3940.016
iteration 0800: loss: 3934.741
iteration 0900: loss: 3937.854
iteration 1000: loss: 3944.103
iteration 1100: loss: 3941.854
iteration 1200: loss: 3937.782
iteration 1300: loss: 3937.677
iteration 1400: loss: 3943.145
iteration 1500: loss: 3928.647
iteration 1600: loss: 3937.732
iteration 1700: loss: 3936.147
iteration 1800: loss: 3938.134
====> Epoch: 032 Train loss: 3938.9412  took : 53.21966552734375
====> Test loss: 3943.1039
iteration 0000: loss: 3940.603
iteration 0100: loss: 3943.600
iteration 0200: loss: 3947.506
iteration 0300: loss: 3940.055
iteration 0400: loss: 3938.989
iteration 0500: loss: 3934.466
iteration 0600: loss: 3939.665
iteration 0700: loss: 3940.377
iteration 0800: loss: 3944.485
iteration 0900: loss: 3936.794
iteration 1000: loss: 3938.207
iteration 1100: loss: 3938.383
iteration 1200: loss: 3938.865
iteration 1300: loss: 3942.167
iteration 1400: loss: 3946.292
iteration 1500: loss: 3938.178
iteration 1600: loss: 3939.969
iteration 1700: loss: 3939.360
iteration 1800: loss: 3939.179
====> Epoch: 033 Train loss: 3938.8458  took : 53.128517866134644
====> Test loss: 3942.5857
iteration 0000: loss: 3941.528
iteration 0100: loss: 3933.406
iteration 0200: loss: 3935.839
iteration 0300: loss: 3938.480
iteration 0400: loss: 3937.292
iteration 0500: loss: 3938.724
iteration 0600: loss: 3941.768
iteration 0700: loss: 3945.957
iteration 0800: loss: 3944.476
iteration 0900: loss: 3945.311
iteration 1000: loss: 3939.690
iteration 1100: loss: 3939.758
iteration 1200: loss: 3939.951
iteration 1300: loss: 3936.761
iteration 1400: loss: 3943.434
iteration 1500: loss: 3938.627
iteration 1600: loss: 3937.446
iteration 1700: loss: 3937.381
iteration 1800: loss: 3938.641
====> Epoch: 034 Train loss: 3938.9455  took : 53.00365996360779
====> Test loss: 3942.8757
iteration 0000: loss: 3938.333
iteration 0100: loss: 3936.119
iteration 0200: loss: 3937.176
iteration 0300: loss: 3938.788
iteration 0400: loss: 3936.121
iteration 0500: loss: 3937.575
iteration 0600: loss: 3940.490
iteration 0700: loss: 3940.506
iteration 0800: loss: 3942.267
iteration 0900: loss: 3937.364
iteration 1000: loss: 3944.057
iteration 1100: loss: 3939.287
iteration 1200: loss: 3933.716
iteration 1300: loss: 3939.715
iteration 1400: loss: 3940.942
iteration 1500: loss: 3933.089
iteration 1600: loss: 3938.149
iteration 1700: loss: 3938.579
iteration 1800: loss: 3938.924
====> Epoch: 035 Train loss: 3938.8211  took : 53.03537607192993
====> Test loss: 3942.5918
iteration 0000: loss: 3934.288
iteration 0100: loss: 3940.765
iteration 0200: loss: 3939.821
iteration 0300: loss: 3939.717
iteration 0400: loss: 3946.761
iteration 0500: loss: 3942.036
iteration 0600: loss: 3944.570
iteration 0700: loss: 3945.279
iteration 0800: loss: 3940.967
iteration 0900: loss: 3941.831
iteration 1000: loss: 3939.456
iteration 1100: loss: 3937.123
iteration 1200: loss: 3933.488
iteration 1300: loss: 3939.945
iteration 1400: loss: 3938.244
iteration 1500: loss: 3939.106
iteration 1600: loss: 3934.293
iteration 1700: loss: 3937.117
iteration 1800: loss: 3936.032
====> Epoch: 036 Train loss: 3938.7159  took : 53.32938575744629
====> Test loss: 3942.5759
iteration 0000: loss: 3939.672
iteration 0100: loss: 3936.786
iteration 0200: loss: 3941.299
iteration 0300: loss: 3941.394
iteration 0400: loss: 3941.912
iteration 0500: loss: 3942.855
iteration 0600: loss: 3942.414
iteration 0700: loss: 3938.912
iteration 0800: loss: 3937.705
iteration 0900: loss: 3930.111
iteration 1000: loss: 3935.915
iteration 1100: loss: 3941.607
iteration 1200: loss: 3941.122
iteration 1300: loss: 3933.675
iteration 1400: loss: 3936.293
iteration 1500: loss: 3931.802
iteration 1600: loss: 3939.521
iteration 1700: loss: 3934.022
iteration 1800: loss: 3941.027
====> Epoch: 037 Train loss: 3938.5709  took : 52.95552730560303
====> Test loss: 3942.3882
iteration 0000: loss: 3937.791
iteration 0100: loss: 3937.025
iteration 0200: loss: 3940.061
iteration 0300: loss: 3935.763
iteration 0400: loss: 3934.722
iteration 0500: loss: 3944.999
iteration 0600: loss: 3941.937
iteration 0700: loss: 3945.194
iteration 0800: loss: 3937.948
iteration 0900: loss: 3937.660
iteration 1000: loss: 3941.333
iteration 1100: loss: 3939.477
iteration 1200: loss: 3943.526
iteration 1300: loss: 3936.228
iteration 1400: loss: 3938.921
iteration 1500: loss: 3937.325
iteration 1600: loss: 3937.911
iteration 1700: loss: 3942.482
iteration 1800: loss: 3939.277
====> Epoch: 038 Train loss: 3938.6099  took : 53.16052961349487
====> Test loss: 3942.6533
iteration 0000: loss: 3940.806
iteration 0100: loss: 3943.462
iteration 0200: loss: 3934.089
iteration 0300: loss: 3936.340
iteration 0400: loss: 3935.634
iteration 0500: loss: 3934.191
iteration 0600: loss: 3939.975
iteration 0700: loss: 3939.833
iteration 0800: loss: 3936.646
iteration 0900: loss: 3941.669
iteration 1000: loss: 3939.493
iteration 1100: loss: 3941.197
iteration 1200: loss: 3944.955
iteration 1300: loss: 3939.574
iteration 1400: loss: 3937.283
iteration 1500: loss: 3939.739
iteration 1600: loss: 3934.974
iteration 1700: loss: 3935.833
iteration 1800: loss: 3941.143
====> Epoch: 039 Train loss: 3938.6368  took : 53.06343197822571
====> Test loss: 3942.3338
iteration 0000: loss: 3938.060
iteration 0100: loss: 3942.146
iteration 0200: loss: 3933.254
iteration 0300: loss: 3945.048
iteration 0400: loss: 3934.391
iteration 0500: loss: 3935.405
iteration 0600: loss: 3936.280
iteration 0700: loss: 3939.939
iteration 0800: loss: 3935.198
iteration 0900: loss: 3937.295
iteration 1000: loss: 3941.896
iteration 1100: loss: 3937.525
iteration 1200: loss: 3936.632
iteration 1300: loss: 3935.183
iteration 1400: loss: 3930.457
iteration 1500: loss: 3935.371
iteration 1600: loss: 3933.895
iteration 1700: loss: 3942.003
iteration 1800: loss: 3936.299
====> Epoch: 040 Train loss: 3938.6424  took : 53.022300481796265
====> Test loss: 3942.7204
iteration 0000: loss: 3937.154
iteration 0100: loss: 3939.396
iteration 0200: loss: 3940.993
iteration 0300: loss: 3936.969
iteration 0400: loss: 3936.234
iteration 0500: loss: 3935.669
iteration 0600: loss: 3937.463
iteration 0700: loss: 3938.143
iteration 0800: loss: 3935.497
iteration 0900: loss: 3939.058
iteration 1000: loss: 3942.714
iteration 1100: loss: 3938.719
iteration 1200: loss: 3945.763
iteration 1300: loss: 3943.438
iteration 1400: loss: 3938.960
iteration 1500: loss: 3935.518
iteration 1600: loss: 3932.182
iteration 1700: loss: 3935.111
iteration 1800: loss: 3938.964
====> Epoch: 041 Train loss: 3938.4888  took : 53.10413932800293
====> Test loss: 3942.2683
iteration 0000: loss: 3933.524
iteration 0100: loss: 3938.215
iteration 0200: loss: 3937.532
iteration 0300: loss: 3941.326
iteration 0400: loss: 3938.639
iteration 0500: loss: 3938.295
iteration 0600: loss: 3942.396
iteration 0700: loss: 3944.236
iteration 0800: loss: 3941.689
iteration 0900: loss: 3943.086
iteration 1000: loss: 3942.129
iteration 1100: loss: 3933.440
iteration 1200: loss: 3933.493
iteration 1300: loss: 3936.178
iteration 1400: loss: 3932.972
iteration 1500: loss: 3937.775
iteration 1600: loss: 3937.301
iteration 1700: loss: 3936.749
iteration 1800: loss: 3940.750
====> Epoch: 042 Train loss: 3938.3661  took : 52.91994380950928
====> Test loss: 3942.4121
iteration 0000: loss: 3939.807
iteration 0100: loss: 3935.859
iteration 0200: loss: 3933.777
iteration 0300: loss: 3935.420
iteration 0400: loss: 3936.918
iteration 0500: loss: 3936.753
iteration 0600: loss: 3937.239
iteration 0700: loss: 3941.522
iteration 0800: loss: 3934.875
iteration 0900: loss: 3938.549
iteration 1000: loss: 3941.089
iteration 1100: loss: 3945.505
iteration 1200: loss: 3935.471
iteration 1300: loss: 3943.260
iteration 1400: loss: 3943.154
iteration 1500: loss: 3942.833
iteration 1600: loss: 3940.074
iteration 1700: loss: 3937.084
iteration 1800: loss: 3940.276
====> Epoch: 043 Train loss: 3938.5049  took : 53.15132212638855
====> Test loss: 3942.2515
iteration 0000: loss: 3930.158
iteration 0100: loss: 3943.502
iteration 0200: loss: 3939.307
iteration 0300: loss: 3943.448
iteration 0400: loss: 3945.991
iteration 0500: loss: 3940.542
iteration 0600: loss: 3937.860
iteration 0700: loss: 3947.508
iteration 0800: loss: 3934.528
iteration 0900: loss: 3934.910
iteration 1000: loss: 3938.961
iteration 1100: loss: 3940.718
iteration 1200: loss: 3935.608
iteration 1300: loss: 3935.200
iteration 1400: loss: 3936.094
iteration 1500: loss: 3935.476
iteration 1600: loss: 3932.619
iteration 1700: loss: 3935.387
iteration 1800: loss: 3935.943
====> Epoch: 044 Train loss: 3938.3596  took : 53.29480504989624
====> Test loss: 3942.1991
iteration 0000: loss: 3942.261
iteration 0100: loss: 3935.571
iteration 0200: loss: 3938.994
iteration 0300: loss: 3940.329
iteration 0400: loss: 3937.138
iteration 0500: loss: 3939.154
iteration 0600: loss: 3940.998
iteration 0700: loss: 3934.426
iteration 0800: loss: 3936.760
iteration 0900: loss: 3942.610
iteration 1000: loss: 3937.216
iteration 1100: loss: 3934.572
iteration 1200: loss: 3935.604
iteration 1300: loss: 3933.912
iteration 1400: loss: 3938.036
iteration 1500: loss: 3940.304
iteration 1600: loss: 3936.540
iteration 1700: loss: 3935.480
iteration 1800: loss: 3939.892
====> Epoch: 045 Train loss: 3938.4537  took : 53.05477428436279
====> Test loss: 3942.6102
iteration 0000: loss: 3941.461
iteration 0100: loss: 3940.450
iteration 0200: loss: 3945.556
iteration 0300: loss: 3933.021
iteration 0400: loss: 3931.968
iteration 0500: loss: 3933.625
iteration 0600: loss: 3933.028
iteration 0700: loss: 3937.380
iteration 0800: loss: 3931.525
iteration 0900: loss: 3932.591
iteration 1000: loss: 3942.153
iteration 1100: loss: 3937.780
iteration 1200: loss: 3941.783
iteration 1300: loss: 3937.957
iteration 1400: loss: 3937.399
iteration 1500: loss: 3935.771
iteration 1600: loss: 3940.725
iteration 1700: loss: 3937.075
iteration 1800: loss: 3945.005
====> Epoch: 046 Train loss: 3938.2477  took : 53.086716413497925
====> Test loss: 3942.4930
iteration 0000: loss: 3929.885
iteration 0100: loss: 3936.836
iteration 0200: loss: 3940.660
iteration 0300: loss: 3934.979
iteration 0400: loss: 3941.582
iteration 0500: loss: 3933.183
iteration 0600: loss: 3944.188
iteration 0700: loss: 3935.276
iteration 0800: loss: 3935.272
iteration 0900: loss: 3943.892
iteration 1000: loss: 3937.000
iteration 1100: loss: 3939.942
iteration 1200: loss: 3943.681
iteration 1300: loss: 3944.959
iteration 1400: loss: 3941.779
iteration 1500: loss: 3932.796
iteration 1600: loss: 3936.285
iteration 1700: loss: 3935.549
iteration 1800: loss: 3936.622
====> Epoch: 047 Train loss: 3938.4001  took : 53.06934404373169
====> Test loss: 3942.3046
iteration 0000: loss: 3943.624
iteration 0100: loss: 3949.933
iteration 0200: loss: 3935.335
iteration 0300: loss: 3938.677
iteration 0400: loss: 3944.360
iteration 0500: loss: 3935.198
iteration 0600: loss: 3936.033
iteration 0700: loss: 3937.697
iteration 0800: loss: 3938.776
iteration 0900: loss: 3935.139
iteration 1000: loss: 3946.605
iteration 1100: loss: 3938.300
iteration 1200: loss: 3938.600
iteration 1300: loss: 3941.078
iteration 1400: loss: 3937.946
iteration 1500: loss: 3942.954
iteration 1600: loss: 3934.737
iteration 1700: loss: 3943.431
iteration 1800: loss: 3944.020
====> Epoch: 048 Train loss: 3938.2942  took : 53.06884980201721
====> Test loss: 3942.2344
iteration 0000: loss: 3938.076
iteration 0100: loss: 3930.470
iteration 0200: loss: 3933.689
iteration 0300: loss: 3941.363
iteration 0400: loss: 3933.140
iteration 0500: loss: 3936.025
iteration 0600: loss: 3940.547
iteration 0700: loss: 3937.164
iteration 0800: loss: 3940.043
iteration 0900: loss: 3939.718
iteration 1000: loss: 3937.699
iteration 1100: loss: 3940.306
iteration 1200: loss: 3935.176
iteration 1300: loss: 3941.645
iteration 1400: loss: 3937.384
iteration 1500: loss: 3932.311
iteration 1600: loss: 3942.438
iteration 1700: loss: 3933.200
iteration 1800: loss: 3941.147
====> Epoch: 049 Train loss: 3938.2393  took : 53.525492906570435
====> Test loss: 3942.5827
iteration 0000: loss: 3942.436
iteration 0100: loss: 3934.563
iteration 0200: loss: 3938.542
iteration 0300: loss: 3938.629
iteration 0400: loss: 3934.441
iteration 0500: loss: 3938.197
iteration 0600: loss: 3940.246
iteration 0700: loss: 3939.028
iteration 0800: loss: 3945.905
iteration 0900: loss: 3937.631
iteration 1000: loss: 3940.758
iteration 1100: loss: 3933.393
iteration 1200: loss: 3934.269
iteration 1300: loss: 3933.930
iteration 1400: loss: 3939.406
iteration 1500: loss: 3936.866
iteration 1600: loss: 3941.185
iteration 1700: loss: 3935.922
iteration 1800: loss: 3940.155
====> Epoch: 050 Train loss: 3938.1489  took : 53.277416706085205
====> Test loss: 3942.0853
====> [MM-VAE] Time: 3163.977s or 00:52:43
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  12
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_12
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_12
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1988.154
iteration 0100: loss: 1567.000
iteration 0200: loss: 1561.462
iteration 0300: loss: 1568.919
iteration 0400: loss: 1551.549
iteration 0500: loss: 1544.351
iteration 0600: loss: 1547.188
iteration 0700: loss: 1534.810
iteration 0800: loss: 1535.933
iteration 0900: loss: 1532.244
====> Epoch: 001 Train loss: 1555.5683  took : 8.397828102111816
====> Test loss: 1536.4056
iteration 0000: loss: 1541.724
iteration 0100: loss: 1527.798
iteration 0200: loss: 1534.790
iteration 0300: loss: 1531.841
iteration 0400: loss: 1529.041
iteration 0500: loss: 1530.759
iteration 0600: loss: 1531.068
iteration 0700: loss: 1533.922
iteration 0800: loss: 1525.678
iteration 0900: loss: 1528.202
====> Epoch: 002 Train loss: 1530.6971  took : 8.4272301197052
====> Test loss: 1529.0083
iteration 0000: loss: 1525.837
iteration 0100: loss: 1526.194
iteration 0200: loss: 1526.970
iteration 0300: loss: 1529.281
iteration 0400: loss: 1525.397
iteration 0500: loss: 1523.896
iteration 0600: loss: 1525.712
iteration 0700: loss: 1523.492
iteration 0800: loss: 1521.266
iteration 0900: loss: 1524.226
====> Epoch: 003 Train loss: 1525.6210  took : 8.427322626113892
====> Test loss: 1526.2285
iteration 0000: loss: 1525.527
iteration 0100: loss: 1523.890
iteration 0200: loss: 1525.140
iteration 0300: loss: 1522.579
iteration 0400: loss: 1525.046
iteration 0500: loss: 1521.703
iteration 0600: loss: 1521.412
iteration 0700: loss: 1520.615
iteration 0800: loss: 1522.388
iteration 0900: loss: 1525.476
====> Epoch: 004 Train loss: 1523.3259  took : 8.497609376907349
====> Test loss: 1524.8467
iteration 0000: loss: 1521.914
iteration 0100: loss: 1519.232
iteration 0200: loss: 1520.513
iteration 0300: loss: 1521.641
iteration 0400: loss: 1521.767
iteration 0500: loss: 1520.224
iteration 0600: loss: 1523.384
iteration 0700: loss: 1522.517
iteration 0800: loss: 1519.698
iteration 0900: loss: 1523.683
====> Epoch: 005 Train loss: 1521.8050  took : 8.396008253097534
====> Test loss: 1523.4944
iteration 0000: loss: 1521.843
iteration 0100: loss: 1521.105
iteration 0200: loss: 1522.515
iteration 0300: loss: 1521.340
iteration 0400: loss: 1521.306
iteration 0500: loss: 1520.059
iteration 0600: loss: 1518.565
iteration 0700: loss: 1520.896
iteration 0800: loss: 1523.235
iteration 0900: loss: 1522.326
====> Epoch: 006 Train loss: 1520.6745  took : 8.448982000350952
====> Test loss: 1522.5912
iteration 0000: loss: 1518.745
iteration 0100: loss: 1517.753
iteration 0200: loss: 1519.251
iteration 0300: loss: 1518.763
iteration 0400: loss: 1519.010
iteration 0500: loss: 1519.943
iteration 0600: loss: 1517.914
iteration 0700: loss: 1525.241
iteration 0800: loss: 1518.140
iteration 0900: loss: 1519.740
====> Epoch: 007 Train loss: 1519.8110  took : 8.402276992797852
====> Test loss: 1521.9133
iteration 0000: loss: 1516.575
iteration 0100: loss: 1521.291
iteration 0200: loss: 1518.261
iteration 0300: loss: 1518.818
iteration 0400: loss: 1518.589
iteration 0500: loss: 1517.230
iteration 0600: loss: 1522.708
iteration 0700: loss: 1517.009
iteration 0800: loss: 1518.098
iteration 0900: loss: 1518.456
====> Epoch: 008 Train loss: 1519.1234  took : 8.525415182113647
====> Test loss: 1521.4007
iteration 0000: loss: 1518.178
iteration 0100: loss: 1517.672
iteration 0200: loss: 1517.539
iteration 0300: loss: 1516.582
iteration 0400: loss: 1521.285
iteration 0500: loss: 1518.281
iteration 0600: loss: 1518.526
iteration 0700: loss: 1516.305
iteration 0800: loss: 1521.513
iteration 0900: loss: 1522.674
====> Epoch: 009 Train loss: 1518.6090  took : 8.43344259262085
====> Test loss: 1520.8482
iteration 0000: loss: 1517.839
iteration 0100: loss: 1519.011
iteration 0200: loss: 1516.524
iteration 0300: loss: 1520.259
iteration 0400: loss: 1518.984
iteration 0500: loss: 1521.513
iteration 0600: loss: 1519.740
iteration 0700: loss: 1520.890
iteration 0800: loss: 1514.181
iteration 0900: loss: 1517.847
====> Epoch: 010 Train loss: 1518.1872  took : 8.453389644622803
====> Test loss: 1520.5749
iteration 0000: loss: 1520.564
iteration 0100: loss: 1517.103
iteration 0200: loss: 1518.587
iteration 0300: loss: 1518.271
iteration 0400: loss: 1517.837
iteration 0500: loss: 1516.689
iteration 0600: loss: 1518.871
iteration 0700: loss: 1520.125
iteration 0800: loss: 1516.763
iteration 0900: loss: 1517.157
====> Epoch: 011 Train loss: 1517.8481  took : 8.47148084640503
====> Test loss: 1520.3980
iteration 0000: loss: 1518.479
iteration 0100: loss: 1518.581
iteration 0200: loss: 1517.674
iteration 0300: loss: 1517.618
iteration 0400: loss: 1517.646
iteration 0500: loss: 1518.527
iteration 0600: loss: 1517.269
iteration 0700: loss: 1518.714
iteration 0800: loss: 1516.402
iteration 0900: loss: 1517.130
====> Epoch: 012 Train loss: 1517.5735  took : 8.531830549240112
====> Test loss: 1520.1291
iteration 0000: loss: 1520.433
iteration 0100: loss: 1517.342
iteration 0200: loss: 1517.782
iteration 0300: loss: 1516.269
iteration 0400: loss: 1518.714
iteration 0500: loss: 1516.577
iteration 0600: loss: 1516.058
iteration 0700: loss: 1516.737
iteration 0800: loss: 1521.318
iteration 0900: loss: 1515.417
====> Epoch: 013 Train loss: 1517.3078  took : 8.536992311477661
====> Test loss: 1520.0945
iteration 0000: loss: 1518.215
iteration 0100: loss: 1515.288
iteration 0200: loss: 1518.140
iteration 0300: loss: 1519.843
iteration 0400: loss: 1519.871
iteration 0500: loss: 1515.424
iteration 0600: loss: 1518.816
iteration 0700: loss: 1516.934
iteration 0800: loss: 1519.838
iteration 0900: loss: 1519.053
====> Epoch: 014 Train loss: 1517.1177  took : 8.413487195968628
====> Test loss: 1519.8890
iteration 0000: loss: 1519.656
iteration 0100: loss: 1515.937
iteration 0200: loss: 1515.755
iteration 0300: loss: 1515.936
iteration 0400: loss: 1515.964
iteration 0500: loss: 1517.661
iteration 0600: loss: 1514.331
iteration 0700: loss: 1516.476
iteration 0800: loss: 1519.102
iteration 0900: loss: 1512.834
====> Epoch: 015 Train loss: 1516.9228  took : 8.415964126586914
====> Test loss: 1519.5650
iteration 0000: loss: 1517.249
iteration 0100: loss: 1516.875
iteration 0200: loss: 1515.082
iteration 0300: loss: 1518.282
iteration 0400: loss: 1516.152
iteration 0500: loss: 1517.849
iteration 0600: loss: 1515.761
iteration 0700: loss: 1515.403
iteration 0800: loss: 1514.526
iteration 0900: loss: 1515.367
====> Epoch: 016 Train loss: 1516.7592  took : 8.507481336593628
====> Test loss: 1519.5714
iteration 0000: loss: 1514.359
iteration 0100: loss: 1517.954
iteration 0200: loss: 1518.465
iteration 0300: loss: 1515.087
iteration 0400: loss: 1518.684
iteration 0500: loss: 1518.985
iteration 0600: loss: 1514.928
iteration 0700: loss: 1518.147
iteration 0800: loss: 1518.354
iteration 0900: loss: 1515.019
====> Epoch: 017 Train loss: 1516.5576  took : 8.533765077590942
====> Test loss: 1519.5089
iteration 0000: loss: 1518.773
iteration 0100: loss: 1516.778
iteration 0200: loss: 1516.560
iteration 0300: loss: 1515.456
iteration 0400: loss: 1519.230
iteration 0500: loss: 1519.033
iteration 0600: loss: 1515.272
iteration 0700: loss: 1517.381
iteration 0800: loss: 1517.595
iteration 0900: loss: 1517.110
====> Epoch: 018 Train loss: 1516.4084  took : 8.477645874023438
====> Test loss: 1519.3751
iteration 0000: loss: 1515.597
iteration 0100: loss: 1516.006
iteration 0200: loss: 1518.113
iteration 0300: loss: 1513.742
iteration 0400: loss: 1516.389
iteration 0500: loss: 1515.043
iteration 0600: loss: 1515.006
iteration 0700: loss: 1518.850
iteration 0800: loss: 1520.671
iteration 0900: loss: 1518.509
====> Epoch: 019 Train loss: 1516.2486  took : 8.455871820449829
====> Test loss: 1519.1905
iteration 0000: loss: 1514.333
iteration 0100: loss: 1515.857
iteration 0200: loss: 1512.486
iteration 0300: loss: 1519.278
iteration 0400: loss: 1518.128
iteration 0500: loss: 1515.523
iteration 0600: loss: 1516.335
iteration 0700: loss: 1515.284
iteration 0800: loss: 1520.326
iteration 0900: loss: 1512.938
====> Epoch: 020 Train loss: 1516.1550  took : 8.510709285736084
====> Test loss: 1519.1410
iteration 0000: loss: 1515.475
iteration 0100: loss: 1515.911
iteration 0200: loss: 1517.556
iteration 0300: loss: 1518.906
iteration 0400: loss: 1514.610
iteration 0500: loss: 1516.806
iteration 0600: loss: 1515.547
iteration 0700: loss: 1515.591
iteration 0800: loss: 1513.570
iteration 0900: loss: 1516.211
====> Epoch: 021 Train loss: 1516.0151  took : 8.419084548950195
====> Test loss: 1519.1606
iteration 0000: loss: 1514.472
iteration 0100: loss: 1514.822
iteration 0200: loss: 1516.892
iteration 0300: loss: 1518.211
iteration 0400: loss: 1515.624
iteration 0500: loss: 1517.577
iteration 0600: loss: 1513.958
iteration 0700: loss: 1519.615
iteration 0800: loss: 1518.228
iteration 0900: loss: 1515.815
====> Epoch: 022 Train loss: 1515.8797  took : 8.451802015304565
====> Test loss: 1518.8997
iteration 0000: loss: 1513.456
iteration 0100: loss: 1518.411
iteration 0200: loss: 1514.695
iteration 0300: loss: 1516.391
iteration 0400: loss: 1515.841
iteration 0500: loss: 1514.106
iteration 0600: loss: 1516.693
iteration 0700: loss: 1514.445
iteration 0800: loss: 1515.474
iteration 0900: loss: 1517.872
====> Epoch: 023 Train loss: 1515.7595  took : 8.519686698913574
====> Test loss: 1518.8406
iteration 0000: loss: 1514.631
iteration 0100: loss: 1515.559
iteration 0200: loss: 1514.821
iteration 0300: loss: 1517.707
iteration 0400: loss: 1515.047
iteration 0500: loss: 1518.713
iteration 0600: loss: 1515.187
iteration 0700: loss: 1516.534
iteration 0800: loss: 1513.989
iteration 0900: loss: 1515.941
====> Epoch: 024 Train loss: 1515.6792  took : 8.51526165008545
====> Test loss: 1518.8222
iteration 0000: loss: 1516.292
iteration 0100: loss: 1513.593
iteration 0200: loss: 1513.985
iteration 0300: loss: 1514.732
iteration 0400: loss: 1516.645
iteration 0500: loss: 1515.863
iteration 0600: loss: 1516.088
iteration 0700: loss: 1515.036
iteration 0800: loss: 1514.570
iteration 0900: loss: 1515.338
====> Epoch: 025 Train loss: 1515.5723  took : 8.415347337722778
====> Test loss: 1518.7128
iteration 0000: loss: 1515.561
iteration 0100: loss: 1517.424
iteration 0200: loss: 1515.629
iteration 0300: loss: 1514.166
iteration 0400: loss: 1514.099
iteration 0500: loss: 1512.389
iteration 0600: loss: 1518.040
iteration 0700: loss: 1516.392
iteration 0800: loss: 1515.280
iteration 0900: loss: 1516.992
====> Epoch: 026 Train loss: 1515.4106  took : 8.514400720596313
====> Test loss: 1518.6616
iteration 0000: loss: 1515.667
iteration 0100: loss: 1516.571
iteration 0200: loss: 1514.135
iteration 0300: loss: 1514.341
iteration 0400: loss: 1513.186
iteration 0500: loss: 1515.794
iteration 0600: loss: 1516.302
iteration 0700: loss: 1515.107
iteration 0800: loss: 1517.767
iteration 0900: loss: 1513.102
====> Epoch: 027 Train loss: 1515.3783  took : 8.4838228225708
====> Test loss: 1518.5708
iteration 0000: loss: 1513.102
iteration 0100: loss: 1516.222
iteration 0200: loss: 1514.703
iteration 0300: loss: 1513.951
iteration 0400: loss: 1514.321
iteration 0500: loss: 1515.045
iteration 0600: loss: 1516.419
iteration 0700: loss: 1515.003
iteration 0800: loss: 1514.404
iteration 0900: loss: 1514.400
====> Epoch: 028 Train loss: 1515.2347  took : 8.520761251449585
====> Test loss: 1518.4407
iteration 0000: loss: 1512.403
iteration 0100: loss: 1516.084
iteration 0200: loss: 1514.945
iteration 0300: loss: 1514.902
iteration 0400: loss: 1514.286
iteration 0500: loss: 1517.536
iteration 0600: loss: 1514.889
iteration 0700: loss: 1513.551
iteration 0800: loss: 1513.933
iteration 0900: loss: 1517.095
====> Epoch: 029 Train loss: 1515.1476  took : 8.51690125465393
====> Test loss: 1518.2741
iteration 0000: loss: 1512.799
iteration 0100: loss: 1515.826
iteration 0200: loss: 1518.292
iteration 0300: loss: 1513.757
iteration 0400: loss: 1515.289
iteration 0500: loss: 1517.660
iteration 0600: loss: 1515.620
iteration 0700: loss: 1514.424
iteration 0800: loss: 1516.011
iteration 0900: loss: 1514.176
====> Epoch: 030 Train loss: 1515.0617  took : 8.453025102615356
====> Test loss: 1518.3766
iteration 0000: loss: 1514.756
iteration 0100: loss: 1514.647
iteration 0200: loss: 1515.934
iteration 0300: loss: 1515.906
iteration 0400: loss: 1516.392
iteration 0500: loss: 1517.194
iteration 0600: loss: 1516.727
iteration 0700: loss: 1514.392
iteration 0800: loss: 1513.348
iteration 0900: loss: 1516.088
====> Epoch: 031 Train loss: 1514.9880  took : 8.45602822303772
====> Test loss: 1518.3454
iteration 0000: loss: 1515.067
iteration 0100: loss: 1513.633
iteration 0200: loss: 1517.026
iteration 0300: loss: 1514.022
iteration 0400: loss: 1514.654
iteration 0500: loss: 1513.684
iteration 0600: loss: 1516.303
iteration 0700: loss: 1515.125
iteration 0800: loss: 1514.224
iteration 0900: loss: 1514.513
====> Epoch: 032 Train loss: 1514.9030  took : 8.475597620010376
====> Test loss: 1518.1211
iteration 0000: loss: 1513.185
iteration 0100: loss: 1513.848
iteration 0200: loss: 1517.974
iteration 0300: loss: 1516.076
iteration 0400: loss: 1515.162
iteration 0500: loss: 1515.395
iteration 0600: loss: 1513.469
iteration 0700: loss: 1515.664
iteration 0800: loss: 1511.696
iteration 0900: loss: 1515.081
====> Epoch: 033 Train loss: 1514.8438  took : 8.451707363128662
====> Test loss: 1518.2484
iteration 0000: loss: 1518.535
iteration 0100: loss: 1514.994
iteration 0200: loss: 1513.502
iteration 0300: loss: 1513.120
iteration 0400: loss: 1513.539
iteration 0500: loss: 1514.041
iteration 0600: loss: 1511.384
iteration 0700: loss: 1515.668
iteration 0800: loss: 1517.282
iteration 0900: loss: 1515.922
====> Epoch: 034 Train loss: 1514.7585  took : 8.490371227264404
====> Test loss: 1518.1082
iteration 0000: loss: 1516.318
iteration 0100: loss: 1514.389
iteration 0200: loss: 1515.901
iteration 0300: loss: 1513.726
iteration 0400: loss: 1515.792
iteration 0500: loss: 1517.785
iteration 0600: loss: 1515.938
iteration 0700: loss: 1515.258
iteration 0800: loss: 1514.403
iteration 0900: loss: 1514.828
====> Epoch: 035 Train loss: 1514.6759  took : 8.506271123886108
====> Test loss: 1518.2095
iteration 0000: loss: 1513.654
iteration 0100: loss: 1515.570
iteration 0200: loss: 1513.997
iteration 0300: loss: 1515.053
iteration 0400: loss: 1515.328
iteration 0500: loss: 1515.290
iteration 0600: loss: 1514.779
iteration 0700: loss: 1516.797
iteration 0800: loss: 1516.456
iteration 0900: loss: 1514.834
====> Epoch: 036 Train loss: 1514.6517  took : 8.501651763916016
====> Test loss: 1518.2082
iteration 0000: loss: 1514.719
iteration 0100: loss: 1513.803
iteration 0200: loss: 1516.188
iteration 0300: loss: 1515.394
iteration 0400: loss: 1515.740
iteration 0500: loss: 1513.133
iteration 0600: loss: 1514.301
iteration 0700: loss: 1514.807
iteration 0800: loss: 1516.965
iteration 0900: loss: 1515.500
====> Epoch: 037 Train loss: 1514.5583  took : 8.447240352630615
====> Test loss: 1517.9358
iteration 0000: loss: 1513.861
iteration 0100: loss: 1515.029
iteration 0200: loss: 1515.075
iteration 0300: loss: 1515.428
iteration 0400: loss: 1513.680
iteration 0500: loss: 1515.466
iteration 0600: loss: 1515.238
iteration 0700: loss: 1513.244
iteration 0800: loss: 1514.362
iteration 0900: loss: 1512.109
====> Epoch: 038 Train loss: 1514.4999  took : 8.51599931716919
====> Test loss: 1517.9345
iteration 0000: loss: 1515.243
iteration 0100: loss: 1516.217
iteration 0200: loss: 1516.339
iteration 0300: loss: 1515.358
iteration 0400: loss: 1515.085
iteration 0500: loss: 1514.084
iteration 0600: loss: 1515.052
iteration 0700: loss: 1515.658
iteration 0800: loss: 1516.592
iteration 0900: loss: 1514.616
====> Epoch: 039 Train loss: 1514.4546  took : 8.529809951782227
====> Test loss: 1517.8611
iteration 0000: loss: 1513.849
iteration 0100: loss: 1515.893
iteration 0200: loss: 1514.477
iteration 0300: loss: 1516.229
iteration 0400: loss: 1515.912
iteration 0500: loss: 1514.275
iteration 0600: loss: 1513.311
iteration 0700: loss: 1514.547
iteration 0800: loss: 1513.792
iteration 0900: loss: 1514.519
====> Epoch: 040 Train loss: 1514.3888  took : 8.51331901550293
====> Test loss: 1518.0643
iteration 0000: loss: 1514.125
iteration 0100: loss: 1517.311
iteration 0200: loss: 1515.105
iteration 0300: loss: 1513.257
iteration 0400: loss: 1514.940
iteration 0500: loss: 1514.556
iteration 0600: loss: 1515.250
iteration 0700: loss: 1513.891
iteration 0800: loss: 1514.108
iteration 0900: loss: 1514.510
====> Epoch: 041 Train loss: 1514.3105  took : 8.49959421157837
====> Test loss: 1517.7969
iteration 0000: loss: 1513.866
iteration 0100: loss: 1516.374
iteration 0200: loss: 1514.604
iteration 0300: loss: 1514.888
iteration 0400: loss: 1512.154
iteration 0500: loss: 1513.842
iteration 0600: loss: 1514.647
iteration 0700: loss: 1513.846
iteration 0800: loss: 1515.269
iteration 0900: loss: 1511.628
====> Epoch: 042 Train loss: 1514.2245  took : 8.53827953338623
====> Test loss: 1517.7685
iteration 0000: loss: 1514.995
iteration 0100: loss: 1514.444
iteration 0200: loss: 1512.760
iteration 0300: loss: 1513.527
iteration 0400: loss: 1513.822
iteration 0500: loss: 1515.973
iteration 0600: loss: 1512.117
iteration 0700: loss: 1514.603
iteration 0800: loss: 1516.053
iteration 0900: loss: 1513.736
====> Epoch: 043 Train loss: 1514.1967  took : 8.513969421386719
====> Test loss: 1517.7407
iteration 0000: loss: 1514.571
iteration 0100: loss: 1515.520
iteration 0200: loss: 1512.744
iteration 0300: loss: 1511.561
iteration 0400: loss: 1512.021
iteration 0500: loss: 1515.263
iteration 0600: loss: 1515.554
iteration 0700: loss: 1515.156
iteration 0800: loss: 1515.397
iteration 0900: loss: 1513.484
====> Epoch: 044 Train loss: 1514.0918  took : 8.5364408493042
====> Test loss: 1517.6087
iteration 0000: loss: 1514.435
iteration 0100: loss: 1515.088
iteration 0200: loss: 1514.218
iteration 0300: loss: 1514.527
iteration 0400: loss: 1511.700
iteration 0500: loss: 1512.157
iteration 0600: loss: 1518.069
iteration 0700: loss: 1513.411
iteration 0800: loss: 1512.819
iteration 0900: loss: 1515.060
====> Epoch: 045 Train loss: 1514.0714  took : 8.517881870269775
====> Test loss: 1517.7647
iteration 0000: loss: 1513.528
iteration 0100: loss: 1515.595
iteration 0200: loss: 1510.656
iteration 0300: loss: 1515.373
iteration 0400: loss: 1514.838
iteration 0500: loss: 1513.616
iteration 0600: loss: 1513.220
iteration 0700: loss: 1514.602
iteration 0800: loss: 1514.458
iteration 0900: loss: 1510.023
====> Epoch: 046 Train loss: 1514.0040  took : 8.40937614440918
====> Test loss: 1517.7793
iteration 0000: loss: 1513.900
iteration 0100: loss: 1512.920
iteration 0200: loss: 1514.027
iteration 0300: loss: 1513.494
iteration 0400: loss: 1514.422
iteration 0500: loss: 1514.234
iteration 0600: loss: 1514.773
iteration 0700: loss: 1513.994
iteration 0800: loss: 1513.997
iteration 0900: loss: 1514.215
====> Epoch: 047 Train loss: 1513.9597  took : 8.504838466644287
====> Test loss: 1517.7658
iteration 0000: loss: 1513.998
iteration 0100: loss: 1512.368
iteration 0200: loss: 1514.813
iteration 0300: loss: 1514.676
iteration 0400: loss: 1512.474
iteration 0500: loss: 1517.122
iteration 0600: loss: 1512.684
iteration 0700: loss: 1516.591
iteration 0800: loss: 1515.297
iteration 0900: loss: 1515.302
====> Epoch: 048 Train loss: 1513.9304  took : 8.393476486206055
====> Test loss: 1517.5224
iteration 0000: loss: 1511.057
iteration 0100: loss: 1513.814
iteration 0200: loss: 1512.152
iteration 0300: loss: 1513.135
iteration 0400: loss: 1513.210
iteration 0500: loss: 1513.562
iteration 0600: loss: 1513.152
iteration 0700: loss: 1516.386
iteration 0800: loss: 1513.325
iteration 0900: loss: 1514.750
====> Epoch: 049 Train loss: 1513.8651  took : 8.493277311325073
====> Test loss: 1517.4841
iteration 0000: loss: 1513.842
iteration 0100: loss: 1515.639
iteration 0200: loss: 1514.151
iteration 0300: loss: 1514.156
iteration 0400: loss: 1515.250
iteration 0500: loss: 1512.406
iteration 0600: loss: 1512.756
iteration 0700: loss: 1515.409
iteration 0800: loss: 1512.842
iteration 0900: loss: 1513.062
====> Epoch: 050 Train loss: 1513.8169  took : 8.39834475517273
====> Test loss: 1517.6869
====> [MM-VAE] Time: 506.232s or 00:08:26
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  12
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_12
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_12
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.426
iteration 0100: loss: 2112.333
iteration 0200: loss: 2042.850
iteration 0300: loss: 2012.442
iteration 0400: loss: 2001.966
iteration 0500: loss: 1988.414
iteration 0600: loss: 1993.696
iteration 0700: loss: 1998.927
iteration 0800: loss: 2003.995
iteration 0900: loss: 1994.375
====> Epoch: 001 Train loss: 2021.8003  took : 12.187516450881958
====> Test loss: 1996.1926
iteration 0000: loss: 1995.381
iteration 0100: loss: 1989.877
iteration 0200: loss: 1990.869
iteration 0300: loss: 1986.075
iteration 0400: loss: 1990.225
iteration 0500: loss: 1984.198
iteration 0600: loss: 1984.992
iteration 0700: loss: 1982.462
iteration 0800: loss: 1981.516
iteration 0900: loss: 1982.258
====> Epoch: 002 Train loss: 1985.4883  took : 12.248448610305786
====> Test loss: 1980.8397
iteration 0000: loss: 1977.035
iteration 0100: loss: 1981.021
iteration 0200: loss: 1965.841
iteration 0300: loss: 1970.586
iteration 0400: loss: 1973.986
iteration 0500: loss: 1977.682
iteration 0600: loss: 1969.080
iteration 0700: loss: 1973.515
iteration 0800: loss: 1972.307
iteration 0900: loss: 1974.107
====> Epoch: 003 Train loss: 1973.4024  took : 11.812615633010864
====> Test loss: 1973.1937
iteration 0000: loss: 1968.706
iteration 0100: loss: 1964.893
iteration 0200: loss: 1968.338
iteration 0300: loss: 1968.778
iteration 0400: loss: 1965.854
iteration 0500: loss: 1970.861
iteration 0600: loss: 1967.185
iteration 0700: loss: 1966.864
iteration 0800: loss: 1964.858
iteration 0900: loss: 1963.527
====> Epoch: 004 Train loss: 1968.0141  took : 11.534511089324951
====> Test loss: 1968.1787
iteration 0000: loss: 1966.114
iteration 0100: loss: 1962.948
iteration 0200: loss: 1964.320
iteration 0300: loss: 1966.549
iteration 0400: loss: 1969.984
iteration 0500: loss: 1960.903
iteration 0600: loss: 1967.144
iteration 0700: loss: 1963.435
iteration 0800: loss: 1965.497
iteration 0900: loss: 1962.427
====> Epoch: 005 Train loss: 1964.1933  took : 13.109612226486206
====> Test loss: 1965.7565
iteration 0000: loss: 1961.640
iteration 0100: loss: 1963.980
iteration 0200: loss: 1961.151
iteration 0300: loss: 1960.932
iteration 0400: loss: 1957.841
iteration 0500: loss: 1961.341
iteration 0600: loss: 1958.222
iteration 0700: loss: 1962.573
iteration 0800: loss: 1961.057
iteration 0900: loss: 1959.414
====> Epoch: 006 Train loss: 1961.7893  took : 11.537360906600952
====> Test loss: 1963.0885
iteration 0000: loss: 1959.913
iteration 0100: loss: 1959.630
iteration 0200: loss: 1960.391
iteration 0300: loss: 1960.095
iteration 0400: loss: 1961.448
iteration 0500: loss: 1965.117
iteration 0600: loss: 1957.955
iteration 0700: loss: 1958.228
iteration 0800: loss: 1958.773
iteration 0900: loss: 1957.360
====> Epoch: 007 Train loss: 1959.9540  took : 12.970184564590454
====> Test loss: 1961.6403
iteration 0000: loss: 1958.949
iteration 0100: loss: 1960.268
iteration 0200: loss: 1960.109
iteration 0300: loss: 1959.999
iteration 0400: loss: 1960.850
iteration 0500: loss: 1956.005
iteration 0600: loss: 1957.061
iteration 0700: loss: 1960.074
iteration 0800: loss: 1961.142
iteration 0900: loss: 1955.696
====> Epoch: 008 Train loss: 1958.0210  took : 12.317384958267212
====> Test loss: 1959.7505
iteration 0000: loss: 1957.364
iteration 0100: loss: 1959.324
iteration 0200: loss: 1955.108
iteration 0300: loss: 1956.415
iteration 0400: loss: 1955.069
iteration 0500: loss: 1956.319
iteration 0600: loss: 1957.688
iteration 0700: loss: 1955.682
iteration 0800: loss: 1957.299
iteration 0900: loss: 1953.483
====> Epoch: 009 Train loss: 1956.3264  took : 13.15425181388855
====> Test loss: 1957.8364
iteration 0000: loss: 1954.176
iteration 0100: loss: 1959.715
iteration 0200: loss: 1957.621
iteration 0300: loss: 1954.838
iteration 0400: loss: 1954.296
iteration 0500: loss: 1957.440
iteration 0600: loss: 1955.284
iteration 0700: loss: 1955.235
iteration 0800: loss: 1953.795
iteration 0900: loss: 1952.656
====> Epoch: 010 Train loss: 1955.5163  took : 13.010695457458496
====> Test loss: 1957.1025
iteration 0000: loss: 1952.376
iteration 0100: loss: 1957.329
iteration 0200: loss: 1953.321
iteration 0300: loss: 1954.889
iteration 0400: loss: 1954.119
iteration 0500: loss: 1955.732
iteration 0600: loss: 1954.113
iteration 0700: loss: 1956.185
iteration 0800: loss: 1954.992
iteration 0900: loss: 1955.671
====> Epoch: 011 Train loss: 1954.6470  took : 12.351049423217773
====> Test loss: 1956.4624
iteration 0000: loss: 1955.719
iteration 0100: loss: 1956.155
iteration 0200: loss: 1956.474
iteration 0300: loss: 1953.165
iteration 0400: loss: 1953.755
iteration 0500: loss: 1952.040
iteration 0600: loss: 1953.647
iteration 0700: loss: 1952.647
iteration 0800: loss: 1955.126
iteration 0900: loss: 1951.722
====> Epoch: 012 Train loss: 1953.9492  took : 12.900733232498169
====> Test loss: 1955.8706
iteration 0000: loss: 1952.944
iteration 0100: loss: 1951.530
iteration 0200: loss: 1952.371
iteration 0300: loss: 1953.507
iteration 0400: loss: 1953.050
iteration 0500: loss: 1952.495
iteration 0600: loss: 1954.625
iteration 0700: loss: 1952.477
iteration 0800: loss: 1953.107
iteration 0900: loss: 1954.448
====> Epoch: 013 Train loss: 1953.3175  took : 13.860732078552246
====> Test loss: 1955.0720
iteration 0000: loss: 1954.815
iteration 0100: loss: 1953.382
iteration 0200: loss: 1952.970
iteration 0300: loss: 1952.192
iteration 0400: loss: 1952.533
iteration 0500: loss: 1957.624
iteration 0600: loss: 1954.806
iteration 0700: loss: 1955.037
iteration 0800: loss: 1954.091
iteration 0900: loss: 1952.519
====> Epoch: 014 Train loss: 1953.0921  took : 11.73384141921997
====> Test loss: 1955.0125
iteration 0000: loss: 1951.614
iteration 0100: loss: 1952.882
iteration 0200: loss: 1953.876
iteration 0300: loss: 1952.826
iteration 0400: loss: 1952.158
iteration 0500: loss: 1953.884
iteration 0600: loss: 1952.945
iteration 0700: loss: 1951.227
iteration 0800: loss: 1952.974
iteration 0900: loss: 1952.138
====> Epoch: 015 Train loss: 1952.5711  took : 12.16357707977295
====> Test loss: 1954.1607
iteration 0000: loss: 1953.732
iteration 0100: loss: 1950.779
iteration 0200: loss: 1951.547
iteration 0300: loss: 1952.246
iteration 0400: loss: 1952.443
iteration 0500: loss: 1953.034
iteration 0600: loss: 1955.156
iteration 0700: loss: 1951.620
iteration 0800: loss: 1951.627
iteration 0900: loss: 1954.080
====> Epoch: 016 Train loss: 1952.3937  took : 13.279900789260864
====> Test loss: 1954.0990
iteration 0000: loss: 1953.072
iteration 0100: loss: 1952.183
iteration 0200: loss: 1952.634
iteration 0300: loss: 1952.314
iteration 0400: loss: 1950.921
iteration 0500: loss: 1951.711
iteration 0600: loss: 1951.818
iteration 0700: loss: 1952.258
iteration 0800: loss: 1950.991
iteration 0900: loss: 1951.307
====> Epoch: 017 Train loss: 1952.0263  took : 11.764326810836792
====> Test loss: 1954.2208
iteration 0000: loss: 1950.899
iteration 0100: loss: 1951.817
iteration 0200: loss: 1950.878
iteration 0300: loss: 1952.691
iteration 0400: loss: 1952.760
iteration 0500: loss: 1950.363
iteration 0600: loss: 1952.322
iteration 0700: loss: 1951.110
iteration 0800: loss: 1950.949
iteration 0900: loss: 1951.341
====> Epoch: 018 Train loss: 1951.8570  took : 13.137853860855103
====> Test loss: 1953.9783
iteration 0000: loss: 1952.190
iteration 0100: loss: 1951.833
iteration 0200: loss: 1951.439
iteration 0300: loss: 1951.997
iteration 0400: loss: 1950.161
iteration 0500: loss: 1950.378
iteration 0600: loss: 1951.154
iteration 0700: loss: 1951.869
iteration 0800: loss: 1953.634
iteration 0900: loss: 1951.789
====> Epoch: 019 Train loss: 1951.6832  took : 12.808369636535645
====> Test loss: 1953.3695
iteration 0000: loss: 1949.844
iteration 0100: loss: 1951.955
iteration 0200: loss: 1951.319
iteration 0300: loss: 1951.314
iteration 0400: loss: 1952.983
iteration 0500: loss: 1949.773
iteration 0600: loss: 1950.841
iteration 0700: loss: 1952.469
iteration 0800: loss: 1950.846
iteration 0900: loss: 1950.499
====> Epoch: 020 Train loss: 1951.5452  took : 13.080537557601929
====> Test loss: 1953.5109
iteration 0000: loss: 1950.897
iteration 0100: loss: 1950.098
iteration 0200: loss: 1951.089
iteration 0300: loss: 1951.446
iteration 0400: loss: 1952.877
iteration 0500: loss: 1951.548
iteration 0600: loss: 1952.441
iteration 0700: loss: 1952.739
iteration 0800: loss: 1951.323
iteration 0900: loss: 1951.127
====> Epoch: 021 Train loss: 1951.5066  took : 12.496540546417236
====> Test loss: 1953.3280
iteration 0000: loss: 1951.660
iteration 0100: loss: 1951.455
iteration 0200: loss: 1951.848
iteration 0300: loss: 1949.952
iteration 0400: loss: 1953.390
iteration 0500: loss: 1950.030
iteration 0600: loss: 1950.488
iteration 0700: loss: 1952.046
iteration 0800: loss: 1952.633
iteration 0900: loss: 1950.987
====> Epoch: 022 Train loss: 1951.5928  took : 12.050367832183838
====> Test loss: 1953.4980
iteration 0000: loss: 1950.810
iteration 0100: loss: 1950.390
iteration 0200: loss: 1952.392
iteration 0300: loss: 1952.829
iteration 0400: loss: 1951.436
iteration 0500: loss: 1950.381
iteration 0600: loss: 1951.647
iteration 0700: loss: 1952.775
iteration 0800: loss: 1951.815
iteration 0900: loss: 1950.900
====> Epoch: 023 Train loss: 1951.4653  took : 12.448165655136108
====> Test loss: 1953.4179
iteration 0000: loss: 1951.765
iteration 0100: loss: 1950.955
iteration 0200: loss: 1953.695
iteration 0300: loss: 1952.128
iteration 0400: loss: 1953.099
iteration 0500: loss: 1952.812
iteration 0600: loss: 1950.813
iteration 0700: loss: 1951.440
iteration 0800: loss: 1950.770
iteration 0900: loss: 1950.992
====> Epoch: 024 Train loss: 1951.3670  took : 12.252397537231445
====> Test loss: 1952.8471
iteration 0000: loss: 1950.045
iteration 0100: loss: 1950.418
iteration 0200: loss: 1951.254
iteration 0300: loss: 1951.850
iteration 0400: loss: 1951.212
iteration 0500: loss: 1952.611
iteration 0600: loss: 1952.470
iteration 0700: loss: 1951.871
iteration 0800: loss: 1950.381
iteration 0900: loss: 1950.919
====> Epoch: 025 Train loss: 1951.0738  took : 12.562259435653687
====> Test loss: 1952.7276
iteration 0000: loss: 1951.395
iteration 0100: loss: 1951.026
iteration 0200: loss: 1950.759
iteration 0300: loss: 1951.897
iteration 0400: loss: 1950.449
iteration 0500: loss: 1951.392
iteration 0600: loss: 1950.048
iteration 0700: loss: 1951.495
iteration 0800: loss: 1951.073
iteration 0900: loss: 1950.670
====> Epoch: 026 Train loss: 1951.0522  took : 13.40682578086853
====> Test loss: 1952.9385
iteration 0000: loss: 1951.158
iteration 0100: loss: 1950.273
iteration 0200: loss: 1950.425
iteration 0300: loss: 1950.720
iteration 0400: loss: 1951.101
iteration 0500: loss: 1950.316
iteration 0600: loss: 1950.488
iteration 0700: loss: 1951.195
iteration 0800: loss: 1950.695
iteration 0900: loss: 1951.572
====> Epoch: 027 Train loss: 1950.9437  took : 12.766094207763672
====> Test loss: 1952.5078
iteration 0000: loss: 1950.698
iteration 0100: loss: 1950.994
iteration 0200: loss: 1950.299
iteration 0300: loss: 1950.751
iteration 0400: loss: 1951.173
iteration 0500: loss: 1952.898
iteration 0600: loss: 1951.816
iteration 0700: loss: 1951.213
iteration 0800: loss: 1950.483
iteration 0900: loss: 1951.545
====> Epoch: 028 Train loss: 1950.9595  took : 11.701427698135376
====> Test loss: 1952.7505
iteration 0000: loss: 1950.735
iteration 0100: loss: 1951.061
iteration 0200: loss: 1951.113
iteration 0300: loss: 1950.511
iteration 0400: loss: 1949.566
iteration 0500: loss: 1950.190
iteration 0600: loss: 1951.466
iteration 0700: loss: 1950.852
iteration 0800: loss: 1952.812
iteration 0900: loss: 1950.944
====> Epoch: 029 Train loss: 1950.8381  took : 12.97911548614502
====> Test loss: 1952.7773
iteration 0000: loss: 1950.529
iteration 0100: loss: 1950.661
iteration 0200: loss: 1952.394
iteration 0300: loss: 1950.909
iteration 0400: loss: 1950.053
iteration 0500: loss: 1951.254
iteration 0600: loss: 1951.381
iteration 0700: loss: 1951.096
iteration 0800: loss: 1950.319
iteration 0900: loss: 1950.690
====> Epoch: 030 Train loss: 1951.1231  took : 13.263664722442627
====> Test loss: 1953.1006
iteration 0000: loss: 1950.765
iteration 0100: loss: 1950.894
iteration 0200: loss: 1950.267
iteration 0300: loss: 1950.565
iteration 0400: loss: 1950.241
iteration 0500: loss: 1950.846
iteration 0600: loss: 1952.445
iteration 0700: loss: 1950.703
iteration 0800: loss: 1950.708
iteration 0900: loss: 1950.556
====> Epoch: 031 Train loss: 1951.0524  took : 11.750868558883667
====> Test loss: 1952.9268
iteration 0000: loss: 1950.707
iteration 0100: loss: 1951.978
iteration 0200: loss: 1952.043
iteration 0300: loss: 1950.647
iteration 0400: loss: 1950.048
iteration 0500: loss: 1952.438
iteration 0600: loss: 1949.901
iteration 0700: loss: 1951.319
iteration 0800: loss: 1950.501
iteration 0900: loss: 1952.212
====> Epoch: 032 Train loss: 1951.0309  took : 12.233963251113892
====> Test loss: 1952.7669
iteration 0000: loss: 1951.054
iteration 0100: loss: 1950.723
iteration 0200: loss: 1951.607
iteration 0300: loss: 1951.640
iteration 0400: loss: 1950.095
iteration 0500: loss: 1950.443
iteration 0600: loss: 1951.196
iteration 0700: loss: 1951.596
iteration 0800: loss: 1950.910
iteration 0900: loss: 1951.055
====> Epoch: 033 Train loss: 1950.8088  took : 12.255103588104248
====> Test loss: 1952.5950
iteration 0000: loss: 1950.384
iteration 0100: loss: 1950.610
iteration 0200: loss: 1949.754
iteration 0300: loss: 1951.828
iteration 0400: loss: 1951.482
iteration 0500: loss: 1950.616
iteration 0600: loss: 1950.973
iteration 0700: loss: 1950.652
iteration 0800: loss: 1950.084
iteration 0900: loss: 1950.981
====> Epoch: 034 Train loss: 1950.7390  took : 12.58500075340271
====> Test loss: 1952.5213
iteration 0000: loss: 1950.474
iteration 0100: loss: 1950.297
iteration 0200: loss: 1950.320
iteration 0300: loss: 1951.089
iteration 0400: loss: 1950.477
iteration 0500: loss: 1951.891
iteration 0600: loss: 1949.225
iteration 0700: loss: 1950.959
iteration 0800: loss: 1951.291
iteration 0900: loss: 1950.575
====> Epoch: 035 Train loss: 1950.7624  took : 12.023908138275146
====> Test loss: 1952.3847
iteration 0000: loss: 1950.829
iteration 0100: loss: 1949.556
iteration 0200: loss: 1951.094
iteration 0300: loss: 1950.629
iteration 0400: loss: 1951.406
iteration 0500: loss: 1951.826
iteration 0600: loss: 1951.114
iteration 0700: loss: 1950.679
iteration 0800: loss: 1951.472
iteration 0900: loss: 1950.620
====> Epoch: 036 Train loss: 1950.6637  took : 12.78158974647522
====> Test loss: 1952.4268
iteration 0000: loss: 1950.141
iteration 0100: loss: 1950.548
iteration 0200: loss: 1950.797
iteration 0300: loss: 1950.405
iteration 0400: loss: 1951.053
iteration 0500: loss: 1949.120
iteration 0600: loss: 1951.826
iteration 0700: loss: 1950.750
iteration 0800: loss: 1951.180
iteration 0900: loss: 1950.451
====> Epoch: 037 Train loss: 1950.6532  took : 11.809391260147095
====> Test loss: 1952.4201
iteration 0000: loss: 1951.687
iteration 0100: loss: 1950.469
iteration 0200: loss: 1950.590
iteration 0300: loss: 1950.807
iteration 0400: loss: 1950.519
iteration 0500: loss: 1950.182
iteration 0600: loss: 1951.088
iteration 0700: loss: 1950.573
iteration 0800: loss: 1949.857
iteration 0900: loss: 1950.468
====> Epoch: 038 Train loss: 1950.6872  took : 11.780037879943848
====> Test loss: 1953.0916
iteration 0000: loss: 1951.385
iteration 0100: loss: 1951.476
iteration 0200: loss: 1950.635
iteration 0300: loss: 1950.818
iteration 0400: loss: 1949.442
iteration 0500: loss: 1950.086
iteration 0600: loss: 1950.696
iteration 0700: loss: 1950.414
iteration 0800: loss: 1950.113
iteration 0900: loss: 1951.586
====> Epoch: 039 Train loss: 1950.7643  took : 12.343197345733643
====> Test loss: 1952.4239
iteration 0000: loss: 1949.949
iteration 0100: loss: 1949.567
iteration 0200: loss: 1951.345
iteration 0300: loss: 1950.571
iteration 0400: loss: 1950.131
iteration 0500: loss: 1949.290
iteration 0600: loss: 1950.446
iteration 0700: loss: 1951.355
iteration 0800: loss: 1949.787
iteration 0900: loss: 1950.102
====> Epoch: 040 Train loss: 1950.8011  took : 12.020950317382812
====> Test loss: 1952.4567
iteration 0000: loss: 1950.632
iteration 0100: loss: 1950.068
iteration 0200: loss: 1950.053
iteration 0300: loss: 1951.135
iteration 0400: loss: 1950.805
iteration 0500: loss: 1950.212
iteration 0600: loss: 1950.712
iteration 0700: loss: 1950.385
iteration 0800: loss: 1951.812
iteration 0900: loss: 1950.999
====> Epoch: 041 Train loss: 1950.7587  took : 11.976839780807495
====> Test loss: 1952.4047
iteration 0000: loss: 1951.285
iteration 0100: loss: 1950.939
iteration 0200: loss: 1950.245
iteration 0300: loss: 1951.078
iteration 0400: loss: 1951.340
iteration 0500: loss: 1950.264
iteration 0600: loss: 1951.275
iteration 0700: loss: 1950.522
iteration 0800: loss: 1951.058
iteration 0900: loss: 1950.975
====> Epoch: 042 Train loss: 1950.6492  took : 12.35789680480957
====> Test loss: 1952.3207
iteration 0000: loss: 1950.620
iteration 0100: loss: 1950.863
iteration 0200: loss: 1950.459
iteration 0300: loss: 1951.080
iteration 0400: loss: 1949.842
iteration 0500: loss: 1949.871
iteration 0600: loss: 1949.418
iteration 0700: loss: 1950.104
iteration 0800: loss: 1950.933
iteration 0900: loss: 1950.527
====> Epoch: 043 Train loss: 1950.7621  took : 12.383788108825684
====> Test loss: 1952.3219
iteration 0000: loss: 1950.895
iteration 0100: loss: 1949.429
iteration 0200: loss: 1950.803
iteration 0300: loss: 1950.866
iteration 0400: loss: 1949.472
iteration 0500: loss: 1951.012
iteration 0600: loss: 1951.194
iteration 0700: loss: 1950.281
iteration 0800: loss: 1952.038
iteration 0900: loss: 1950.800
====> Epoch: 044 Train loss: 1950.5898  took : 13.015552997589111
====> Test loss: 1952.6666
iteration 0000: loss: 1949.995
iteration 0100: loss: 1952.017
iteration 0200: loss: 1950.147
iteration 0300: loss: 1951.318
iteration 0400: loss: 1951.358
iteration 0500: loss: 1952.563
iteration 0600: loss: 1951.030
iteration 0700: loss: 1950.390
iteration 0800: loss: 1950.908
iteration 0900: loss: 1950.441
====> Epoch: 045 Train loss: 1950.8007  took : 12.735964298248291
====> Test loss: 1952.8200
iteration 0000: loss: 1951.714
iteration 0100: loss: 1951.464
iteration 0200: loss: 1951.023
iteration 0300: loss: 1952.071
iteration 0400: loss: 1952.267
iteration 0500: loss: 1950.365
iteration 0600: loss: 1951.036
iteration 0700: loss: 1949.873
iteration 0800: loss: 1949.117
iteration 0900: loss: 1949.872
====> Epoch: 046 Train loss: 1950.6904  took : 11.86551833152771
====> Test loss: 1952.5761
iteration 0000: loss: 1951.712
iteration 0100: loss: 1950.298
iteration 0200: loss: 1950.299
iteration 0300: loss: 1950.119
iteration 0400: loss: 1950.590
iteration 0500: loss: 1950.578
iteration 0600: loss: 1951.553
iteration 0700: loss: 1950.473
iteration 0800: loss: 1950.531
iteration 0900: loss: 1950.633
====> Epoch: 047 Train loss: 1950.7118  took : 12.931209087371826
====> Test loss: 1952.3385
iteration 0000: loss: 1951.410
iteration 0100: loss: 1949.951
iteration 0200: loss: 1951.608
iteration 0300: loss: 1950.750
iteration 0400: loss: 1950.251
iteration 0500: loss: 1950.068
iteration 0600: loss: 1951.272
iteration 0700: loss: 1951.964
iteration 0800: loss: 1951.148
iteration 0900: loss: 1951.418
====> Epoch: 048 Train loss: 1950.6815  took : 12.491560459136963
====> Test loss: 1952.3249
iteration 0000: loss: 1950.628
iteration 0100: loss: 1949.693
iteration 0200: loss: 1951.042
iteration 0300: loss: 1951.344
iteration 0400: loss: 1950.255
iteration 0500: loss: 1950.161
iteration 0600: loss: 1951.773
iteration 0700: loss: 1950.008
iteration 0800: loss: 1950.783
iteration 0900: loss: 1950.717
====> Epoch: 049 Train loss: 1950.6612  took : 12.848903179168701
====> Test loss: 1952.2619
iteration 0000: loss: 1949.914
iteration 0100: loss: 1951.016
iteration 0200: loss: 1950.051
iteration 0300: loss: 1950.631
iteration 0400: loss: 1950.811
iteration 0500: loss: 1951.232
iteration 0600: loss: 1950.514
iteration 0700: loss: 1949.767
iteration 0800: loss: 1949.877
iteration 0900: loss: 1949.653
====> Epoch: 050 Train loss: 1950.6525  took : 12.760503053665161
====> Test loss: 1952.3177
====> [MM-VAE] Time: 695.212s or 00:11:35
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  12
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_12
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_12
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5220.492
iteration 0100: loss: 4127.149
iteration 0200: loss: 4080.453
iteration 0300: loss: 4068.656
iteration 0400: loss: 4021.164
iteration 0500: loss: 4029.523
iteration 0600: loss: 4015.510
iteration 0700: loss: 4001.148
iteration 0800: loss: 4014.341
iteration 0900: loss: 4010.524
iteration 1000: loss: 3998.205
iteration 1100: loss: 4008.152
iteration 1200: loss: 4006.093
iteration 1300: loss: 4005.931
iteration 1400: loss: 3991.955
iteration 1500: loss: 4001.859
iteration 1600: loss: 3987.475
iteration 1700: loss: 3984.448
iteration 1800: loss: 3994.506
====> Epoch: 001 Train loss: 4026.0872  took : 52.95800471305847
====> Test loss: 3995.2318
iteration 0000: loss: 3991.487
iteration 0100: loss: 3997.429
iteration 0200: loss: 3993.074
iteration 0300: loss: 4008.674
iteration 0400: loss: 3982.793
iteration 0500: loss: 3995.920
iteration 0600: loss: 3976.918
iteration 0700: loss: 3986.863
iteration 0800: loss: 3990.790
iteration 0900: loss: 3978.651
iteration 1000: loss: 3989.576
iteration 1100: loss: 3985.556
iteration 1200: loss: 3981.239
iteration 1300: loss: 3987.950
iteration 1400: loss: 3981.765
iteration 1500: loss: 3980.803
iteration 1600: loss: 3980.809
iteration 1700: loss: 3982.638
iteration 1800: loss: 3976.958
====> Epoch: 002 Train loss: 3984.4504  took : 52.946130990982056
====> Test loss: 3983.7443
iteration 0000: loss: 3978.053
iteration 0100: loss: 3971.948
iteration 0200: loss: 3975.411
iteration 0300: loss: 3972.473
iteration 0400: loss: 3965.284
iteration 0500: loss: 3985.710
iteration 0600: loss: 3978.443
iteration 0700: loss: 3974.646
iteration 0800: loss: 3974.182
iteration 0900: loss: 3981.483
iteration 1000: loss: 3973.876
iteration 1100: loss: 3973.884
iteration 1200: loss: 3977.722
iteration 1300: loss: 3973.294
iteration 1400: loss: 3974.133
iteration 1500: loss: 3980.234
iteration 1600: loss: 3971.240
iteration 1700: loss: 3977.971
iteration 1800: loss: 3970.645
====> Epoch: 003 Train loss: 3976.4491  took : 53.09904384613037
====> Test loss: 3979.3150
iteration 0000: loss: 3978.834
iteration 0100: loss: 3971.509
iteration 0200: loss: 3974.277
iteration 0300: loss: 3972.421
iteration 0400: loss: 3981.355
iteration 0500: loss: 3976.676
iteration 0600: loss: 3974.757
iteration 0700: loss: 3967.343
iteration 0800: loss: 3973.118
iteration 0900: loss: 3973.311
iteration 1000: loss: 3962.640
iteration 1100: loss: 3982.936
iteration 1200: loss: 3972.837
iteration 1300: loss: 3971.918
iteration 1400: loss: 3977.066
iteration 1500: loss: 3976.663
iteration 1600: loss: 3972.791
iteration 1700: loss: 3972.885
iteration 1800: loss: 3973.558
====> Epoch: 004 Train loss: 3973.3711  took : 52.84842920303345
====> Test loss: 3977.6059
iteration 0000: loss: 3966.113
iteration 0100: loss: 3973.484
iteration 0200: loss: 3971.710
iteration 0300: loss: 3961.640
iteration 0400: loss: 3968.966
iteration 0500: loss: 3964.566
iteration 0600: loss: 3967.953
iteration 0700: loss: 3960.062
iteration 0800: loss: 3972.569
iteration 0900: loss: 3974.392
iteration 1000: loss: 3965.413
iteration 1100: loss: 3972.936
iteration 1200: loss: 3976.644
iteration 1300: loss: 3966.549
iteration 1400: loss: 3966.459
iteration 1500: loss: 3970.924
iteration 1600: loss: 3976.694
iteration 1700: loss: 3970.340
iteration 1800: loss: 3972.249
====> Epoch: 005 Train loss: 3971.1289  took : 53.091959714889526
====> Test loss: 3976.8506
iteration 0000: loss: 3968.551
iteration 0100: loss: 3960.894
iteration 0200: loss: 3959.580
iteration 0300: loss: 3977.272
iteration 0400: loss: 3980.335
iteration 0500: loss: 3959.977
iteration 0600: loss: 3966.903
iteration 0700: loss: 3967.144
iteration 0800: loss: 3972.741
iteration 0900: loss: 3966.744
iteration 1000: loss: 3965.435
iteration 1100: loss: 3969.970
iteration 1200: loss: 3973.156
iteration 1300: loss: 3966.135
iteration 1400: loss: 3969.797
iteration 1500: loss: 3963.170
iteration 1600: loss: 3970.319
iteration 1700: loss: 3980.053
iteration 1800: loss: 3963.310
====> Epoch: 006 Train loss: 3969.7394  took : 53.16969156265259
====> Test loss: 3975.3278
iteration 0000: loss: 3974.534
iteration 0100: loss: 3964.583
iteration 0200: loss: 3964.054
iteration 0300: loss: 3964.323
iteration 0400: loss: 3971.958
iteration 0500: loss: 3966.376
iteration 0600: loss: 3970.048
iteration 0700: loss: 3973.413
iteration 0800: loss: 3972.314
iteration 0900: loss: 3970.206
iteration 1000: loss: 3969.276
iteration 1100: loss: 3973.789
iteration 1200: loss: 3976.058
iteration 1300: loss: 3970.386
iteration 1400: loss: 3966.576
iteration 1500: loss: 3967.043
iteration 1600: loss: 3967.930
iteration 1700: loss: 3974.713
iteration 1800: loss: 3973.645
====> Epoch: 007 Train loss: 3968.5001  took : 53.25929832458496
====> Test loss: 3973.2701
iteration 0000: loss: 3966.037
iteration 0100: loss: 3966.768
iteration 0200: loss: 3975.445
iteration 0300: loss: 3974.509
iteration 0400: loss: 3964.861
iteration 0500: loss: 3977.243
iteration 0600: loss: 3969.464
iteration 0700: loss: 3962.378
iteration 0800: loss: 3973.362
iteration 0900: loss: 3975.101
iteration 1000: loss: 3965.815
iteration 1100: loss: 3974.828
iteration 1200: loss: 3971.161
iteration 1300: loss: 3971.119
iteration 1400: loss: 3969.390
iteration 1500: loss: 3962.598
iteration 1600: loss: 3957.890
iteration 1700: loss: 3970.560
iteration 1800: loss: 3952.457
====> Epoch: 008 Train loss: 3967.6001  took : 52.931259870529175
====> Test loss: 3972.9863
iteration 0000: loss: 3970.833
iteration 0100: loss: 3965.991
iteration 0200: loss: 3969.045
iteration 0300: loss: 3968.099
iteration 0400: loss: 3974.871
iteration 0500: loss: 3972.706
iteration 0600: loss: 3971.660
iteration 0700: loss: 3969.141
iteration 0800: loss: 3959.221
iteration 0900: loss: 3957.380
iteration 1000: loss: 3970.017
iteration 1100: loss: 3974.680
iteration 1200: loss: 3968.996
iteration 1300: loss: 3966.128
iteration 1400: loss: 3965.788
iteration 1500: loss: 3971.037
iteration 1600: loss: 3962.792
iteration 1700: loss: 3958.823
iteration 1800: loss: 3967.704
====> Epoch: 009 Train loss: 3966.9442  took : 53.172828912734985
====> Test loss: 3972.3827
iteration 0000: loss: 3967.979
iteration 0100: loss: 3965.359
iteration 0200: loss: 3971.154
iteration 0300: loss: 3959.177
iteration 0400: loss: 3966.555
iteration 0500: loss: 3967.954
iteration 0600: loss: 3975.627
iteration 0700: loss: 3964.511
iteration 0800: loss: 3967.024
iteration 0900: loss: 3959.342
iteration 1000: loss: 3970.107
iteration 1100: loss: 3972.365
iteration 1200: loss: 3962.197
iteration 1300: loss: 3964.254
iteration 1400: loss: 3967.834
iteration 1500: loss: 3960.518
iteration 1600: loss: 3977.558
iteration 1700: loss: 3967.852
iteration 1800: loss: 3977.898
====> Epoch: 010 Train loss: 3966.7685  took : 52.95100498199463
====> Test loss: 3972.5948
iteration 0000: loss: 3955.154
iteration 0100: loss: 3963.457
iteration 0200: loss: 3959.801
iteration 0300: loss: 3957.438
iteration 0400: loss: 3974.190
iteration 0500: loss: 3969.039
iteration 0600: loss: 3967.644
iteration 0700: loss: 3959.314
iteration 0800: loss: 3964.342
iteration 0900: loss: 3973.906
iteration 1000: loss: 3970.797
iteration 1100: loss: 3955.576
iteration 1200: loss: 3965.631
iteration 1300: loss: 3963.882
iteration 1400: loss: 3969.399
iteration 1500: loss: 3970.160
iteration 1600: loss: 3962.833
iteration 1700: loss: 3971.579
iteration 1800: loss: 3960.443
====> Epoch: 011 Train loss: 3966.3615  took : 52.95542502403259
====> Test loss: 3972.5498
iteration 0000: loss: 3961.727
iteration 0100: loss: 3969.469
iteration 0200: loss: 3960.490
iteration 0300: loss: 3957.902
iteration 0400: loss: 3962.570
iteration 0500: loss: 3967.300
iteration 0600: loss: 3959.159
iteration 0700: loss: 3962.346
iteration 0800: loss: 3971.830
iteration 0900: loss: 3975.156
iteration 1000: loss: 3956.001
iteration 1100: loss: 3973.169
iteration 1200: loss: 3962.561
iteration 1300: loss: 3981.498
iteration 1400: loss: 3965.562
iteration 1500: loss: 3958.561
iteration 1600: loss: 3960.656
iteration 1700: loss: 3953.480
iteration 1800: loss: 3977.489
====> Epoch: 012 Train loss: 3967.1812  took : 53.09780669212341
====> Test loss: 3972.0674
iteration 0000: loss: 3968.406
iteration 0100: loss: 3963.777
iteration 0200: loss: 3971.356
iteration 0300: loss: 3980.692
iteration 0400: loss: 3962.185
iteration 0500: loss: 3961.491
iteration 0600: loss: 3962.177
iteration 0700: loss: 3967.924
iteration 0800: loss: 3970.436
iteration 0900: loss: 3973.533
iteration 1000: loss: 3967.971
iteration 1100: loss: 3962.416
iteration 1200: loss: 3956.150
iteration 1300: loss: 3962.778
iteration 1400: loss: 3967.159
iteration 1500: loss: 3965.080
iteration 1600: loss: 3975.772
iteration 1700: loss: 3968.360
iteration 1800: loss: 3959.226
====> Epoch: 013 Train loss: 3966.1830  took : 53.090320348739624
====> Test loss: 3972.0657
iteration 0000: loss: 3955.224
iteration 0100: loss: 3976.159
iteration 0200: loss: 3965.972
iteration 0300: loss: 3949.864
iteration 0400: loss: 3961.215
iteration 0500: loss: 3963.853
iteration 0600: loss: 3963.257
iteration 0700: loss: 3964.692
iteration 0800: loss: 3963.582
iteration 0900: loss: 3975.563
iteration 1000: loss: 3972.987
iteration 1100: loss: 3959.852
iteration 1200: loss: 3977.815
iteration 1300: loss: 3972.415
iteration 1400: loss: 3968.515
iteration 1500: loss: 3962.239
iteration 1600: loss: 3975.397
iteration 1700: loss: 3963.491
iteration 1800: loss: 3973.135
====> Epoch: 014 Train loss: 3965.9182  took : 53.10199952125549
====> Test loss: 3972.6949
iteration 0000: loss: 3964.038
iteration 0100: loss: 3962.503
iteration 0200: loss: 3954.543
iteration 0300: loss: 3968.559
iteration 0400: loss: 3970.537
iteration 0500: loss: 3967.672
iteration 0600: loss: 3967.399
iteration 0700: loss: 3969.913
iteration 0800: loss: 3961.187
iteration 0900: loss: 3960.897
iteration 1000: loss: 3961.152
iteration 1100: loss: 3966.960
iteration 1200: loss: 3956.439
iteration 1300: loss: 3978.699
iteration 1400: loss: 3968.791
iteration 1500: loss: 3975.742
iteration 1600: loss: 3970.528
iteration 1700: loss: 3968.962
iteration 1800: loss: 3969.982
====> Epoch: 015 Train loss: 3965.1368  took : 53.15539216995239
====> Test loss: 3971.3272
iteration 0000: loss: 3961.727
iteration 0100: loss: 3965.738
iteration 0200: loss: 3965.926
iteration 0300: loss: 3962.937
iteration 0400: loss: 3970.952
iteration 0500: loss: 3971.634
iteration 0600: loss: 3960.312
iteration 0700: loss: 3959.574
iteration 0800: loss: 3961.732
iteration 0900: loss: 3962.946
iteration 1000: loss: 3962.982
iteration 1100: loss: 3957.198
iteration 1200: loss: 3972.904
iteration 1300: loss: 3975.721
iteration 1400: loss: 3961.028
iteration 1500: loss: 3972.039
iteration 1600: loss: 3967.930
iteration 1700: loss: 3966.827
iteration 1800: loss: 3957.132
====> Epoch: 016 Train loss: 3964.8609  took : 53.09840273857117
====> Test loss: 3970.8467
iteration 0000: loss: 3958.258
iteration 0100: loss: 3965.752
iteration 0200: loss: 3968.063
iteration 0300: loss: 3966.952
iteration 0400: loss: 3963.621
iteration 0500: loss: 3963.012
iteration 0600: loss: 3962.888
iteration 0700: loss: 3962.316
iteration 0800: loss: 3958.384
iteration 0900: loss: 3964.140
iteration 1000: loss: 3962.078
iteration 1100: loss: 3959.238
iteration 1200: loss: 3961.408
iteration 1300: loss: 3963.267
iteration 1400: loss: 3957.701
iteration 1500: loss: 3963.919
iteration 1600: loss: 3957.526
iteration 1700: loss: 3953.466
iteration 1800: loss: 3963.709
====> Epoch: 017 Train loss: 3964.3486  took : 53.19843363761902
====> Test loss: 3970.4337
iteration 0000: loss: 3962.219
iteration 0100: loss: 3968.927
iteration 0200: loss: 3962.950
iteration 0300: loss: 3964.790
iteration 0400: loss: 3959.987
iteration 0500: loss: 3964.233
iteration 0600: loss: 3970.082
iteration 0700: loss: 3965.333
iteration 0800: loss: 3970.453
iteration 0900: loss: 3960.401
iteration 1000: loss: 3964.916
iteration 1100: loss: 3959.756
iteration 1200: loss: 3961.960
iteration 1300: loss: 3969.464
iteration 1400: loss: 3969.597
iteration 1500: loss: 3960.572
iteration 1600: loss: 3973.055
iteration 1700: loss: 3963.035
iteration 1800: loss: 3973.282
====> Epoch: 018 Train loss: 3964.3000  took : 53.25738596916199
====> Test loss: 3970.5648
iteration 0000: loss: 3959.280
iteration 0100: loss: 3960.542
iteration 0200: loss: 3968.907
iteration 0300: loss: 3967.254
iteration 0400: loss: 3963.651
iteration 0500: loss: 3964.347
iteration 0600: loss: 3957.069
iteration 0700: loss: 3965.115
iteration 0800: loss: 3959.682
iteration 0900: loss: 3960.558
iteration 1000: loss: 3955.670
iteration 1100: loss: 3962.065
iteration 1200: loss: 3965.001
iteration 1300: loss: 3955.310
iteration 1400: loss: 3967.979
iteration 1500: loss: 3962.464
iteration 1600: loss: 3962.009
iteration 1700: loss: 3962.911
iteration 1800: loss: 3963.249
====> Epoch: 019 Train loss: 3963.4900  took : 53.057231426239014
====> Test loss: 3970.6398
iteration 0000: loss: 3968.179
iteration 0100: loss: 3961.557
iteration 0200: loss: 3960.207
iteration 0300: loss: 3963.361
iteration 0400: loss: 3965.625
iteration 0500: loss: 3974.979
iteration 0600: loss: 3959.606
iteration 0700: loss: 3960.210
iteration 0800: loss: 3967.490
iteration 0900: loss: 3955.917
iteration 1000: loss: 3972.264
iteration 1100: loss: 3971.024
iteration 1200: loss: 3962.262
iteration 1300: loss: 3961.474
iteration 1400: loss: 3957.955
iteration 1500: loss: 3965.238
iteration 1600: loss: 3967.837
iteration 1700: loss: 3959.754
iteration 1800: loss: 3962.169
====> Epoch: 020 Train loss: 3963.3744  took : 53.22046518325806
====> Test loss: 3970.2687
iteration 0000: loss: 3962.523
iteration 0100: loss: 3959.041
iteration 0200: loss: 3962.379
iteration 0300: loss: 3957.550
iteration 0400: loss: 3959.889
iteration 0500: loss: 3958.453
iteration 0600: loss: 3958.187
iteration 0700: loss: 3966.969
iteration 0800: loss: 3962.869
iteration 0900: loss: 3960.143
iteration 1000: loss: 3963.140
iteration 1100: loss: 3958.478
iteration 1200: loss: 3969.536
iteration 1300: loss: 3967.840
iteration 1400: loss: 3959.324
iteration 1500: loss: 3965.553
iteration 1600: loss: 3968.359
iteration 1700: loss: 3969.714
iteration 1800: loss: 3965.233
====> Epoch: 021 Train loss: 3963.3104  took : 53.07849049568176
====> Test loss: 3970.4803
iteration 0000: loss: 3967.067
iteration 0100: loss: 3964.706
iteration 0200: loss: 3958.771
iteration 0300: loss: 3965.612
iteration 0400: loss: 3962.749
iteration 0500: loss: 3965.164
iteration 0600: loss: 3972.277
iteration 0700: loss: 3958.706
iteration 0800: loss: 3962.912
iteration 0900: loss: 3972.594
iteration 1000: loss: 3978.955
iteration 1100: loss: 3964.422
iteration 1200: loss: 3968.281
iteration 1300: loss: 3958.230
iteration 1400: loss: 3958.396
iteration 1500: loss: 3956.097
iteration 1600: loss: 3959.819
iteration 1700: loss: 3960.789
iteration 1800: loss: 3964.494
====> Epoch: 022 Train loss: 3962.9989  took : 53.09477734565735
====> Test loss: 3971.0329
iteration 0000: loss: 3976.935
iteration 0100: loss: 3966.450
iteration 0200: loss: 3963.781
iteration 0300: loss: 3955.583
iteration 0400: loss: 3963.551
iteration 0500: loss: 3961.843
iteration 0600: loss: 3964.631
iteration 0700: loss: 3970.879
iteration 0800: loss: 3962.701
iteration 0900: loss: 3961.168
iteration 1000: loss: 3959.886
iteration 1100: loss: 3971.313
iteration 1200: loss: 3966.122
iteration 1300: loss: 3958.388
iteration 1400: loss: 3960.354
iteration 1500: loss: 3960.017
iteration 1600: loss: 3959.375
iteration 1700: loss: 3963.810
iteration 1800: loss: 3963.771
====> Epoch: 023 Train loss: 3963.2532  took : 52.88361692428589
====> Test loss: 3970.0275
iteration 0000: loss: 3960.206
iteration 0100: loss: 3977.015
iteration 0200: loss: 3965.291
iteration 0300: loss: 3964.933
iteration 0400: loss: 3960.908
iteration 0500: loss: 3967.280
iteration 0600: loss: 3962.746
iteration 0700: loss: 3957.053
iteration 0800: loss: 3954.233
iteration 0900: loss: 3960.506
iteration 1000: loss: 3957.003
iteration 1100: loss: 3961.411
iteration 1200: loss: 3958.713
iteration 1300: loss: 3960.234
iteration 1400: loss: 3964.179
iteration 1500: loss: 3963.303
iteration 1600: loss: 3953.227
iteration 1700: loss: 3961.318
iteration 1800: loss: 3959.771
====> Epoch: 024 Train loss: 3964.0049  took : 53.027984857559204
====> Test loss: 3971.1576
iteration 0000: loss: 3953.510
iteration 0100: loss: 3971.191
iteration 0200: loss: 3960.590
iteration 0300: loss: 3956.027
iteration 0400: loss: 3958.788
iteration 0500: loss: 3958.857
iteration 0600: loss: 3968.114
iteration 0700: loss: 3950.259
iteration 0800: loss: 3964.456
iteration 0900: loss: 3964.727
iteration 1000: loss: 3959.827
iteration 1100: loss: 3960.530
iteration 1200: loss: 3955.480
iteration 1300: loss: 3958.631
iteration 1400: loss: 3954.310
iteration 1500: loss: 3963.462
iteration 1600: loss: 3958.637
iteration 1700: loss: 3961.018
iteration 1800: loss: 3969.375
====> Epoch: 025 Train loss: 3963.3314  took : 53.35050964355469
====> Test loss: 3969.6800
iteration 0000: loss: 3961.266
iteration 0100: loss: 3967.355
iteration 0200: loss: 3969.695
iteration 0300: loss: 3969.780
iteration 0400: loss: 3966.295
iteration 0500: loss: 3955.645
iteration 0600: loss: 3963.992
iteration 0700: loss: 3957.776
iteration 0800: loss: 3955.456
iteration 0900: loss: 3967.285
iteration 1000: loss: 3960.388
iteration 1100: loss: 3964.771
iteration 1200: loss: 3965.285
iteration 1300: loss: 3953.079
iteration 1400: loss: 3958.026
iteration 1500: loss: 3961.645
iteration 1600: loss: 3953.855
iteration 1700: loss: 3961.198
iteration 1800: loss: 3966.146
====> Epoch: 026 Train loss: 3962.7995  took : 53.133907318115234
====> Test loss: 3970.2083
iteration 0000: loss: 3957.119
iteration 0100: loss: 3952.731
iteration 0200: loss: 3960.610
iteration 0300: loss: 3959.090
iteration 0400: loss: 3963.358
iteration 0500: loss: 3962.969
iteration 0600: loss: 3959.322
iteration 0700: loss: 3963.917
iteration 0800: loss: 3970.363
iteration 0900: loss: 3967.456
iteration 1000: loss: 3977.628
iteration 1100: loss: 3951.061
iteration 1200: loss: 3958.212
iteration 1300: loss: 3959.129
iteration 1400: loss: 3949.433
iteration 1500: loss: 3969.209
iteration 1600: loss: 3959.834
iteration 1700: loss: 3957.569
iteration 1800: loss: 3965.713
====> Epoch: 027 Train loss: 3962.3289  took : 52.98293471336365
====> Test loss: 3969.6911
iteration 0000: loss: 3951.670
iteration 0100: loss: 3962.152
iteration 0200: loss: 3965.485
iteration 0300: loss: 3963.548
iteration 0400: loss: 3962.227
iteration 0500: loss: 3955.043
iteration 0600: loss: 3958.352
iteration 0700: loss: 3963.093
iteration 0800: loss: 3960.971
iteration 0900: loss: 3960.220
iteration 1000: loss: 3965.217
iteration 1100: loss: 3958.604
iteration 1200: loss: 3953.603
iteration 1300: loss: 3954.448
iteration 1400: loss: 3968.633
iteration 1500: loss: 3957.206
iteration 1600: loss: 3959.821
iteration 1700: loss: 3968.318
iteration 1800: loss: 3963.112
====> Epoch: 028 Train loss: 3962.1651  took : 52.982916593551636
====> Test loss: 3969.4416
iteration 0000: loss: 3972.909
iteration 0100: loss: 3956.738
iteration 0200: loss: 3964.842
iteration 0300: loss: 3961.737
iteration 0400: loss: 3963.994
iteration 0500: loss: 3953.357
iteration 0600: loss: 3959.030
iteration 0700: loss: 3964.621
iteration 0800: loss: 3961.333
iteration 0900: loss: 3955.523
iteration 1000: loss: 3962.222
iteration 1100: loss: 3958.564
iteration 1200: loss: 3962.559
iteration 1300: loss: 3967.320
iteration 1400: loss: 3962.037
iteration 1500: loss: 3953.215
iteration 1600: loss: 3955.935
iteration 1700: loss: 3966.751
iteration 1800: loss: 3961.966
====> Epoch: 029 Train loss: 3962.1229  took : 53.0071165561676
====> Test loss: 3969.7237
iteration 0000: loss: 3956.647
iteration 0100: loss: 3954.525
iteration 0200: loss: 3953.160
iteration 0300: loss: 3951.203
iteration 0400: loss: 3959.699
iteration 0500: loss: 3966.202
iteration 0600: loss: 3958.592
iteration 0700: loss: 3954.193
iteration 0800: loss: 3955.387
iteration 0900: loss: 3971.023
iteration 1000: loss: 3967.198
iteration 1100: loss: 3952.933
iteration 1200: loss: 3961.594
iteration 1300: loss: 3971.501
iteration 1400: loss: 3970.733
iteration 1500: loss: 3958.071
iteration 1600: loss: 3971.881
iteration 1700: loss: 3955.369
iteration 1800: loss: 3969.848
====> Epoch: 030 Train loss: 3961.7458  took : 53.056559324264526
====> Test loss: 3969.4360
iteration 0000: loss: 3955.094
iteration 0100: loss: 3958.231
iteration 0200: loss: 3949.223
iteration 0300: loss: 3949.196
iteration 0400: loss: 3958.964
iteration 0500: loss: 3960.074
iteration 0600: loss: 3954.091
iteration 0700: loss: 3963.010
iteration 0800: loss: 3963.932
iteration 0900: loss: 3957.579
iteration 1000: loss: 3956.273
iteration 1100: loss: 3950.826
iteration 1200: loss: 3959.281
iteration 1300: loss: 3963.810
iteration 1400: loss: 3963.134
iteration 1500: loss: 3956.986
iteration 1600: loss: 3962.565
iteration 1700: loss: 3960.633
iteration 1800: loss: 3963.532
====> Epoch: 031 Train loss: 3961.6798  took : 52.77617692947388
====> Test loss: 3969.0856
iteration 0000: loss: 3963.088
iteration 0100: loss: 3956.003
iteration 0200: loss: 3967.182
iteration 0300: loss: 3963.238
iteration 0400: loss: 3949.737
iteration 0500: loss: 3961.574
iteration 0600: loss: 3959.790
iteration 0700: loss: 3962.557
iteration 0800: loss: 3966.602
iteration 0900: loss: 3964.267
iteration 1000: loss: 3959.927
iteration 1100: loss: 3958.596
iteration 1200: loss: 3959.366
iteration 1300: loss: 3959.620
iteration 1400: loss: 3956.302
iteration 1500: loss: 3957.365
iteration 1600: loss: 3961.319
iteration 1700: loss: 3961.749
iteration 1800: loss: 3966.168
====> Epoch: 032 Train loss: 3961.6588  took : 52.969523191452026
====> Test loss: 3969.9073
iteration 0000: loss: 3963.899
iteration 0100: loss: 3965.167
iteration 0200: loss: 3971.841
iteration 0300: loss: 3966.542
iteration 0400: loss: 3954.743
iteration 0500: loss: 3957.087
iteration 0600: loss: 3961.754
iteration 0700: loss: 3966.005
iteration 0800: loss: 3966.115
iteration 0900: loss: 3953.558
iteration 1000: loss: 3958.840
iteration 1100: loss: 3959.259
iteration 1200: loss: 3961.462
iteration 1300: loss: 3960.247
iteration 1400: loss: 3958.646
iteration 1500: loss: 3963.260
iteration 1600: loss: 3958.075
iteration 1700: loss: 3951.952
iteration 1800: loss: 3963.563
====> Epoch: 033 Train loss: 3961.3212  took : 53.225059509277344
====> Test loss: 3969.1397
iteration 0000: loss: 3963.614
iteration 0100: loss: 3963.517
iteration 0200: loss: 3969.208
iteration 0300: loss: 3948.703
iteration 0400: loss: 3963.268
iteration 0500: loss: 3971.060
iteration 0600: loss: 3954.214
iteration 0700: loss: 3954.496
iteration 0800: loss: 3972.161
iteration 0900: loss: 3957.465
iteration 1000: loss: 3950.493
iteration 1100: loss: 3964.498
iteration 1200: loss: 3955.393
iteration 1300: loss: 3971.932
iteration 1400: loss: 3964.280
iteration 1500: loss: 3967.192
iteration 1600: loss: 3952.101
iteration 1700: loss: 3961.896
iteration 1800: loss: 3966.625
====> Epoch: 034 Train loss: 3961.3521  took : 52.914772033691406
====> Test loss: 3968.9908
iteration 0000: loss: 3958.199
iteration 0100: loss: 3957.675
iteration 0200: loss: 3965.384
iteration 0300: loss: 3951.518
iteration 0400: loss: 3951.531
iteration 0500: loss: 3965.592
iteration 0600: loss: 3954.276
iteration 0700: loss: 3967.998
iteration 0800: loss: 3973.554
iteration 0900: loss: 3959.734
iteration 1000: loss: 3951.927
iteration 1100: loss: 3964.493
iteration 1200: loss: 3947.643
iteration 1300: loss: 3958.510
iteration 1400: loss: 3963.069
iteration 1500: loss: 3957.563
iteration 1600: loss: 3965.581
iteration 1700: loss: 3971.214
iteration 1800: loss: 3962.529
====> Epoch: 035 Train loss: 3961.1868  took : 52.78820013999939
====> Test loss: 3969.2969
iteration 0000: loss: 3963.869
iteration 0100: loss: 3958.127
iteration 0200: loss: 3966.294
iteration 0300: loss: 3961.122
iteration 0400: loss: 3963.133
iteration 0500: loss: 3955.715
iteration 0600: loss: 3962.419
iteration 0700: loss: 3955.785
iteration 0800: loss: 3961.580
iteration 0900: loss: 3963.461
iteration 1000: loss: 3962.046
iteration 1100: loss: 3952.272
iteration 1200: loss: 3956.719
iteration 1300: loss: 3962.958
iteration 1400: loss: 3961.817
iteration 1500: loss: 3959.229
iteration 1600: loss: 3954.852
iteration 1700: loss: 3962.653
iteration 1800: loss: 3950.115
====> Epoch: 036 Train loss: 3961.1649  took : 53.03833079338074
====> Test loss: 3969.3182
iteration 0000: loss: 3952.677
iteration 0100: loss: 3959.303
iteration 0200: loss: 3958.919
iteration 0300: loss: 3967.939
iteration 0400: loss: 3956.663
iteration 0500: loss: 3962.723
iteration 0600: loss: 3954.577
iteration 0700: loss: 3967.027
iteration 0800: loss: 3962.124
iteration 0900: loss: 3953.889
iteration 1000: loss: 3963.640
iteration 1100: loss: 3958.658
iteration 1200: loss: 3958.236
iteration 1300: loss: 3964.217
iteration 1400: loss: 3953.561
iteration 1500: loss: 3957.531
iteration 1600: loss: 3959.208
iteration 1700: loss: 3962.179
iteration 1800: loss: 3952.001
====> Epoch: 037 Train loss: 3961.1026  took : 53.139934062957764
====> Test loss: 3969.8084
iteration 0000: loss: 3956.101
iteration 0100: loss: 3958.569
iteration 0200: loss: 3953.654
iteration 0300: loss: 3957.812
iteration 0400: loss: 3968.267
iteration 0500: loss: 3970.549
iteration 0600: loss: 3967.435
iteration 0700: loss: 3964.324
iteration 0800: loss: 3956.366
iteration 0900: loss: 3962.650
iteration 1000: loss: 3970.969
iteration 1100: loss: 3961.687
iteration 1200: loss: 3960.484
iteration 1300: loss: 3959.875
iteration 1400: loss: 3958.961
iteration 1500: loss: 3956.274
iteration 1600: loss: 3951.614
iteration 1700: loss: 3966.386
iteration 1800: loss: 3964.975
====> Epoch: 038 Train loss: 3961.2519  took : 52.6333794593811
====> Test loss: 3969.2096
iteration 0000: loss: 3966.585
iteration 0100: loss: 3961.814
iteration 0200: loss: 3963.258
iteration 0300: loss: 3960.486
iteration 0400: loss: 3962.057
iteration 0500: loss: 3968.437
iteration 0600: loss: 3958.864
iteration 0700: loss: 3968.349
iteration 0800: loss: 3955.567
iteration 0900: loss: 3952.513
iteration 1000: loss: 3960.071
iteration 1100: loss: 3965.377
iteration 1200: loss: 3963.443
iteration 1300: loss: 3955.048
iteration 1400: loss: 3956.497
iteration 1500: loss: 3966.607
iteration 1600: loss: 3967.634
iteration 1700: loss: 3967.824
iteration 1800: loss: 3959.795
====> Epoch: 039 Train loss: 3960.8834  took : 53.11825656890869
====> Test loss: 3969.4445
iteration 0000: loss: 3953.525
iteration 0100: loss: 3963.706
iteration 0200: loss: 3954.089
iteration 0300: loss: 3967.660
iteration 0400: loss: 3959.455
iteration 0500: loss: 3956.750
iteration 0600: loss: 3955.105
iteration 0700: loss: 3951.615
iteration 0800: loss: 3955.663
iteration 0900: loss: 3956.306
iteration 1000: loss: 3953.906
iteration 1100: loss: 3961.989
iteration 1200: loss: 3962.431
iteration 1300: loss: 3972.037
iteration 1400: loss: 3960.320
iteration 1500: loss: 3968.385
iteration 1600: loss: 3971.823
iteration 1700: loss: 3960.219
iteration 1800: loss: 3965.617
====> Epoch: 040 Train loss: 3961.6886  took : 53.07366394996643
====> Test loss: 3969.9761
iteration 0000: loss: 3961.155
iteration 0100: loss: 3950.207
iteration 0200: loss: 3961.447
iteration 0300: loss: 3958.481
iteration 0400: loss: 3959.710
iteration 0500: loss: 3954.392
iteration 0600: loss: 3956.496
iteration 0700: loss: 3969.747
iteration 0800: loss: 3948.861
iteration 0900: loss: 3963.945
iteration 1000: loss: 3962.543
iteration 1100: loss: 3961.248
iteration 1200: loss: 3955.915
iteration 1300: loss: 3959.846
iteration 1400: loss: 3956.215
iteration 1500: loss: 3976.290
iteration 1600: loss: 3963.890
iteration 1700: loss: 3954.224
iteration 1800: loss: 3957.342
====> Epoch: 041 Train loss: 3961.2029  took : 53.00317454338074
====> Test loss: 3969.0066
iteration 0000: loss: 3971.980
iteration 0100: loss: 3962.442
iteration 0200: loss: 3961.744
iteration 0300: loss: 3954.716
iteration 0400: loss: 3965.686
iteration 0500: loss: 3948.613
iteration 0600: loss: 3968.066
iteration 0700: loss: 3963.619
iteration 0800: loss: 3956.597
iteration 0900: loss: 3960.645
iteration 1000: loss: 3960.235
iteration 1100: loss: 3957.847
iteration 1200: loss: 3954.528
iteration 1300: loss: 3962.654
iteration 1400: loss: 3967.586
iteration 1500: loss: 3964.563
iteration 1600: loss: 3962.426
iteration 1700: loss: 3949.520
iteration 1800: loss: 3962.062
====> Epoch: 042 Train loss: 3960.7369  took : 52.8723258972168
====> Test loss: 3969.1284
iteration 0000: loss: 3955.757
iteration 0100: loss: 3964.823
iteration 0200: loss: 3952.556
iteration 0300: loss: 3963.076
iteration 0400: loss: 3960.542
iteration 0500: loss: 3955.601
iteration 0600: loss: 3962.805
iteration 0700: loss: 3952.333
iteration 0800: loss: 3959.901
iteration 0900: loss: 3959.760
iteration 1000: loss: 3968.380
iteration 1100: loss: 3958.068
iteration 1200: loss: 3968.112
iteration 1300: loss: 3959.874
iteration 1400: loss: 3952.789
iteration 1500: loss: 3963.132
iteration 1600: loss: 3968.631
iteration 1700: loss: 3968.811
iteration 1800: loss: 3969.530
====> Epoch: 043 Train loss: 3960.9873  took : 53.1167778968811
====> Test loss: 3971.0806
iteration 0000: loss: 3955.158
iteration 0100: loss: 3957.073
iteration 0200: loss: 3957.182
iteration 0300: loss: 3962.086
iteration 0400: loss: 3972.188
iteration 0500: loss: 3954.614
iteration 0600: loss: 3966.940
iteration 0700: loss: 3959.173
iteration 0800: loss: 3966.662
iteration 0900: loss: 3968.375
iteration 1000: loss: 3962.604
iteration 1100: loss: 3961.666
iteration 1200: loss: 3962.036
iteration 1300: loss: 3963.651
iteration 1400: loss: 3952.438
iteration 1500: loss: 3959.358
iteration 1600: loss: 3961.048
iteration 1700: loss: 3953.193
iteration 1800: loss: 3960.441
====> Epoch: 044 Train loss: 3961.0455  took : 53.213316440582275
====> Test loss: 3968.8508
iteration 0000: loss: 3961.289
iteration 0100: loss: 3971.743
iteration 0200: loss: 3953.938
iteration 0300: loss: 3951.699
iteration 0400: loss: 3960.181
iteration 0500: loss: 3961.754
iteration 0600: loss: 3958.539
iteration 0700: loss: 3952.780
iteration 0800: loss: 3968.602
iteration 0900: loss: 3956.806
iteration 1000: loss: 3965.670
iteration 1100: loss: 3965.803
iteration 1200: loss: 3957.378
iteration 1300: loss: 3956.414
iteration 1400: loss: 3963.843
iteration 1500: loss: 3981.332
iteration 1600: loss: 3964.779
iteration 1700: loss: 3959.154
iteration 1800: loss: 3964.406
====> Epoch: 045 Train loss: 3960.5271  took : 53.210341453552246
====> Test loss: 3969.9015
iteration 0000: loss: 3956.926
iteration 0100: loss: 3960.643
iteration 0200: loss: 3963.253
iteration 0300: loss: 3965.740
iteration 0400: loss: 3954.638
iteration 0500: loss: 3967.551
iteration 0600: loss: 3970.044
iteration 0700: loss: 3953.158
iteration 0800: loss: 3952.234
iteration 0900: loss: 3958.065
iteration 1000: loss: 3957.894
iteration 1100: loss: 3961.103
iteration 1200: loss: 3964.173
iteration 1300: loss: 3959.805
iteration 1400: loss: 3955.958
iteration 1500: loss: 3964.985
iteration 1600: loss: 3968.523
iteration 1700: loss: 3963.364
iteration 1800: loss: 3963.465
====> Epoch: 046 Train loss: 3960.7020  took : 53.149362087249756
====> Test loss: 3969.3049
iteration 0000: loss: 3968.282
iteration 0100: loss: 3967.070
iteration 0200: loss: 3971.453
iteration 0300: loss: 3955.801
iteration 0400: loss: 3966.672
iteration 0500: loss: 3958.994
iteration 0600: loss: 3961.784
iteration 0700: loss: 3957.352
iteration 0800: loss: 3958.289
iteration 0900: loss: 3956.956
iteration 1000: loss: 3958.169
iteration 1100: loss: 3964.271
iteration 1200: loss: 3962.618
iteration 1300: loss: 3962.058
iteration 1400: loss: 3955.398
iteration 1500: loss: 3965.234
iteration 1600: loss: 3963.006
iteration 1700: loss: 3961.054
iteration 1800: loss: 3958.119
====> Epoch: 047 Train loss: 3960.7609  took : 53.15090370178223
====> Test loss: 3970.1068
iteration 0000: loss: 3961.654
iteration 0100: loss: 3967.316
iteration 0200: loss: 3960.963
iteration 0300: loss: 3964.515
iteration 0400: loss: 3967.345
iteration 0500: loss: 3956.851
iteration 0600: loss: 3963.854
iteration 0700: loss: 3960.472
iteration 0800: loss: 3956.779
iteration 0900: loss: 3966.889
iteration 1000: loss: 3957.654
iteration 1100: loss: 3957.832
iteration 1200: loss: 3957.569
iteration 1300: loss: 3960.311
iteration 1400: loss: 3968.209
iteration 1500: loss: 3962.567
iteration 1600: loss: 3972.309
iteration 1700: loss: 3969.526
iteration 1800: loss: 3962.253
====> Epoch: 048 Train loss: 3960.8246  took : 53.05098271369934
====> Test loss: 3969.1076
iteration 0000: loss: 3958.771
iteration 0100: loss: 3961.559
iteration 0200: loss: 3960.694
iteration 0300: loss: 3950.915
iteration 0400: loss: 3959.420
iteration 0500: loss: 3964.565
iteration 0600: loss: 3953.111
iteration 0700: loss: 3960.499
iteration 0800: loss: 3963.837
iteration 0900: loss: 3963.605
iteration 1000: loss: 3960.730
iteration 1100: loss: 3958.884
iteration 1200: loss: 3964.511
iteration 1300: loss: 3963.868
iteration 1400: loss: 3956.230
iteration 1500: loss: 3959.005
iteration 1600: loss: 3967.868
iteration 1700: loss: 3958.944
iteration 1800: loss: 3960.157
====> Epoch: 049 Train loss: 3960.7662  took : 52.93041777610779
====> Test loss: 3969.5273
iteration 0000: loss: 3963.379
iteration 0100: loss: 3956.277
iteration 0200: loss: 3962.862
iteration 0300: loss: 3966.407
iteration 0400: loss: 3972.128
iteration 0500: loss: 3969.473
iteration 0600: loss: 3965.405
iteration 0700: loss: 3960.579
iteration 0800: loss: 3961.108
iteration 0900: loss: 3963.282
iteration 1000: loss: 3956.421
iteration 1100: loss: 3960.038
iteration 1200: loss: 3956.738
iteration 1300: loss: 3966.138
iteration 1400: loss: 3960.093
iteration 1500: loss: 3956.106
iteration 1600: loss: 3967.841
iteration 1700: loss: 3967.143
iteration 1800: loss: 3956.992
====> Epoch: 050 Train loss: 3961.3623  took : 52.83869409561157
====> Test loss: 3969.9005
====> [MM-VAE] Time: 3168.349s or 00:52:48
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  13
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_13
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_13
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1987.679
iteration 0100: loss: 1576.729
iteration 0200: loss: 1570.091
iteration 0300: loss: 1557.737
iteration 0400: loss: 1550.637
iteration 0500: loss: 1542.690
iteration 0600: loss: 1542.912
iteration 0700: loss: 1540.128
iteration 0800: loss: 1539.944
iteration 0900: loss: 1534.168
====> Epoch: 001 Train loss: 1555.0035  took : 8.399953126907349
====> Test loss: 1538.3718
iteration 0000: loss: 1538.542
iteration 0100: loss: 1534.464
iteration 0200: loss: 1535.430
iteration 0300: loss: 1534.418
iteration 0400: loss: 1533.671
iteration 0500: loss: 1531.575
iteration 0600: loss: 1530.160
iteration 0700: loss: 1531.735
iteration 0800: loss: 1526.304
iteration 0900: loss: 1522.953
====> Epoch: 002 Train loss: 1532.2569  took : 8.428367614746094
====> Test loss: 1529.4441
iteration 0000: loss: 1529.245
iteration 0100: loss: 1523.017
iteration 0200: loss: 1526.438
iteration 0300: loss: 1526.339
iteration 0400: loss: 1523.708
iteration 0500: loss: 1526.139
iteration 0600: loss: 1522.729
iteration 0700: loss: 1521.282
iteration 0800: loss: 1525.884
iteration 0900: loss: 1524.772
====> Epoch: 003 Train loss: 1525.6062  took : 8.499164342880249
====> Test loss: 1526.2394
iteration 0000: loss: 1524.191
iteration 0100: loss: 1525.215
iteration 0200: loss: 1522.341
iteration 0300: loss: 1518.436
iteration 0400: loss: 1521.000
iteration 0500: loss: 1520.350
iteration 0600: loss: 1524.866
iteration 0700: loss: 1522.077
iteration 0800: loss: 1519.095
iteration 0900: loss: 1518.367
====> Epoch: 004 Train loss: 1522.8725  took : 8.432043552398682
====> Test loss: 1524.2053
iteration 0000: loss: 1519.225
iteration 0100: loss: 1521.654
iteration 0200: loss: 1522.739
iteration 0300: loss: 1523.097
iteration 0400: loss: 1520.711
iteration 0500: loss: 1519.122
iteration 0600: loss: 1522.562
iteration 0700: loss: 1524.357
iteration 0800: loss: 1521.993
iteration 0900: loss: 1522.652
====> Epoch: 005 Train loss: 1521.3410  took : 8.544089078903198
====> Test loss: 1523.4324
iteration 0000: loss: 1520.305
iteration 0100: loss: 1520.564
iteration 0200: loss: 1519.086
iteration 0300: loss: 1519.942
iteration 0400: loss: 1522.825
iteration 0500: loss: 1522.665
iteration 0600: loss: 1517.341
iteration 0700: loss: 1517.042
iteration 0800: loss: 1520.231
iteration 0900: loss: 1520.777
====> Epoch: 006 Train loss: 1520.2466  took : 8.524854898452759
====> Test loss: 1522.0650
iteration 0000: loss: 1515.231
iteration 0100: loss: 1517.440
iteration 0200: loss: 1517.924
iteration 0300: loss: 1521.956
iteration 0400: loss: 1516.865
iteration 0500: loss: 1519.147
iteration 0600: loss: 1522.777
iteration 0700: loss: 1517.028
iteration 0800: loss: 1519.309
iteration 0900: loss: 1519.570
====> Epoch: 007 Train loss: 1519.4166  took : 8.434353113174438
====> Test loss: 1521.8724
iteration 0000: loss: 1521.651
iteration 0100: loss: 1516.927
iteration 0200: loss: 1519.044
iteration 0300: loss: 1520.740
iteration 0400: loss: 1515.934
iteration 0500: loss: 1520.092
iteration 0600: loss: 1519.872
iteration 0700: loss: 1518.080
iteration 0800: loss: 1518.027
iteration 0900: loss: 1517.251
====> Epoch: 008 Train loss: 1518.8475  took : 8.49341607093811
====> Test loss: 1521.2587
iteration 0000: loss: 1518.508
iteration 0100: loss: 1518.529
iteration 0200: loss: 1522.033
iteration 0300: loss: 1518.931
iteration 0400: loss: 1518.282
iteration 0500: loss: 1517.440
iteration 0600: loss: 1516.392
iteration 0700: loss: 1519.524
iteration 0800: loss: 1519.479
iteration 0900: loss: 1518.671
====> Epoch: 009 Train loss: 1518.4055  took : 8.486544370651245
====> Test loss: 1520.7637
iteration 0000: loss: 1517.852
iteration 0100: loss: 1517.057
iteration 0200: loss: 1517.263
iteration 0300: loss: 1514.421
iteration 0400: loss: 1518.401
iteration 0500: loss: 1520.256
iteration 0600: loss: 1519.995
iteration 0700: loss: 1516.646
iteration 0800: loss: 1522.382
iteration 0900: loss: 1516.372
====> Epoch: 010 Train loss: 1518.0599  took : 8.51875638961792
====> Test loss: 1520.4111
iteration 0000: loss: 1518.699
iteration 0100: loss: 1517.998
iteration 0200: loss: 1518.932
iteration 0300: loss: 1520.836
iteration 0400: loss: 1521.825
iteration 0500: loss: 1521.010
iteration 0600: loss: 1515.020
iteration 0700: loss: 1521.567
iteration 0800: loss: 1519.502
iteration 0900: loss: 1516.995
====> Epoch: 011 Train loss: 1517.7214  took : 8.520437479019165
====> Test loss: 1520.2288
iteration 0000: loss: 1516.411
iteration 0100: loss: 1517.277
iteration 0200: loss: 1517.357
iteration 0300: loss: 1520.662
iteration 0400: loss: 1515.621
iteration 0500: loss: 1514.841
iteration 0600: loss: 1515.519
iteration 0700: loss: 1515.981
iteration 0800: loss: 1518.122
iteration 0900: loss: 1514.574
====> Epoch: 012 Train loss: 1517.4489  took : 8.415597915649414
====> Test loss: 1519.8585
iteration 0000: loss: 1517.645
iteration 0100: loss: 1515.196
iteration 0200: loss: 1515.176
iteration 0300: loss: 1516.228
iteration 0400: loss: 1515.749
iteration 0500: loss: 1515.097
iteration 0600: loss: 1517.173
iteration 0700: loss: 1520.108
iteration 0800: loss: 1514.989
iteration 0900: loss: 1515.133
====> Epoch: 013 Train loss: 1517.2119  took : 8.518534421920776
====> Test loss: 1519.9194
iteration 0000: loss: 1520.276
iteration 0100: loss: 1519.385
iteration 0200: loss: 1515.031
iteration 0300: loss: 1518.127
iteration 0400: loss: 1519.706
iteration 0500: loss: 1516.601
iteration 0600: loss: 1516.498
iteration 0700: loss: 1518.875
iteration 0800: loss: 1519.529
iteration 0900: loss: 1515.793
====> Epoch: 014 Train loss: 1516.9793  took : 8.512824296951294
====> Test loss: 1519.8022
iteration 0000: loss: 1517.751
iteration 0100: loss: 1517.110
iteration 0200: loss: 1519.937
iteration 0300: loss: 1515.566
iteration 0400: loss: 1512.743
iteration 0500: loss: 1514.603
iteration 0600: loss: 1518.719
iteration 0700: loss: 1517.769
iteration 0800: loss: 1516.349
iteration 0900: loss: 1516.691
====> Epoch: 015 Train loss: 1516.7918  took : 8.511043071746826
====> Test loss: 1519.5831
iteration 0000: loss: 1516.130
iteration 0100: loss: 1517.344
iteration 0200: loss: 1513.341
iteration 0300: loss: 1516.432
iteration 0400: loss: 1516.884
iteration 0500: loss: 1516.502
iteration 0600: loss: 1519.087
iteration 0700: loss: 1514.602
iteration 0800: loss: 1518.870
iteration 0900: loss: 1515.883
====> Epoch: 016 Train loss: 1516.6619  took : 8.502392530441284
====> Test loss: 1519.5137
iteration 0000: loss: 1516.155
iteration 0100: loss: 1518.082
iteration 0200: loss: 1514.443
iteration 0300: loss: 1513.255
iteration 0400: loss: 1514.194
iteration 0500: loss: 1520.707
iteration 0600: loss: 1517.210
iteration 0700: loss: 1517.346
iteration 0800: loss: 1516.635
iteration 0900: loss: 1516.663
====> Epoch: 017 Train loss: 1516.5464  took : 8.427738666534424
====> Test loss: 1519.3620
iteration 0000: loss: 1514.991
iteration 0100: loss: 1517.066
iteration 0200: loss: 1516.018
iteration 0300: loss: 1520.231
iteration 0400: loss: 1520.492
iteration 0500: loss: 1515.524
iteration 0600: loss: 1515.240
iteration 0700: loss: 1517.410
iteration 0800: loss: 1514.677
iteration 0900: loss: 1515.770
====> Epoch: 018 Train loss: 1516.3546  took : 8.485833406448364
====> Test loss: 1519.2697
iteration 0000: loss: 1513.630
iteration 0100: loss: 1516.827
iteration 0200: loss: 1514.082
iteration 0300: loss: 1516.698
iteration 0400: loss: 1519.281
iteration 0500: loss: 1514.082
iteration 0600: loss: 1517.582
iteration 0700: loss: 1517.991
iteration 0800: loss: 1516.809
iteration 0900: loss: 1517.916
====> Epoch: 019 Train loss: 1516.2431  took : 8.418910026550293
====> Test loss: 1519.2357
iteration 0000: loss: 1520.183
iteration 0100: loss: 1516.000
iteration 0200: loss: 1519.628
iteration 0300: loss: 1513.975
iteration 0400: loss: 1515.866
iteration 0500: loss: 1516.004
iteration 0600: loss: 1516.148
iteration 0700: loss: 1516.897
iteration 0800: loss: 1516.224
iteration 0900: loss: 1514.588
====> Epoch: 020 Train loss: 1516.1413  took : 8.44668436050415
====> Test loss: 1519.1057
iteration 0000: loss: 1513.880
iteration 0100: loss: 1516.375
iteration 0200: loss: 1518.645
iteration 0300: loss: 1515.638
iteration 0400: loss: 1517.052
iteration 0500: loss: 1515.566
iteration 0600: loss: 1516.722
iteration 0700: loss: 1517.734
iteration 0800: loss: 1514.506
iteration 0900: loss: 1515.656
====> Epoch: 021 Train loss: 1515.9795  took : 8.418167352676392
====> Test loss: 1518.9829
iteration 0000: loss: 1513.682
iteration 0100: loss: 1516.378
iteration 0200: loss: 1515.431
iteration 0300: loss: 1513.304
iteration 0400: loss: 1514.089
iteration 0500: loss: 1516.381
iteration 0600: loss: 1516.004
iteration 0700: loss: 1515.204
iteration 0800: loss: 1515.752
iteration 0900: loss: 1515.466
====> Epoch: 022 Train loss: 1515.8855  took : 8.542349576950073
====> Test loss: 1518.8958
iteration 0000: loss: 1514.750
iteration 0100: loss: 1517.207
iteration 0200: loss: 1513.511
iteration 0300: loss: 1515.091
iteration 0400: loss: 1518.492
iteration 0500: loss: 1514.955
iteration 0600: loss: 1516.359
iteration 0700: loss: 1515.644
iteration 0800: loss: 1512.876
iteration 0900: loss: 1513.895
====> Epoch: 023 Train loss: 1515.7930  took : 8.491822242736816
====> Test loss: 1519.0182
iteration 0000: loss: 1520.869
iteration 0100: loss: 1515.922
iteration 0200: loss: 1519.387
iteration 0300: loss: 1514.641
iteration 0400: loss: 1516.072
iteration 0500: loss: 1513.893
iteration 0600: loss: 1514.286
iteration 0700: loss: 1515.996
iteration 0800: loss: 1517.840
iteration 0900: loss: 1517.778
====> Epoch: 024 Train loss: 1515.6994  took : 8.555826187133789
====> Test loss: 1518.7553
iteration 0000: loss: 1512.472
iteration 0100: loss: 1514.669
iteration 0200: loss: 1519.652
iteration 0300: loss: 1516.770
iteration 0400: loss: 1517.419
iteration 0500: loss: 1518.035
iteration 0600: loss: 1515.555
iteration 0700: loss: 1512.822
iteration 0800: loss: 1515.248
iteration 0900: loss: 1513.442
====> Epoch: 025 Train loss: 1515.6002  took : 8.521820306777954
====> Test loss: 1518.7926
iteration 0000: loss: 1515.584
iteration 0100: loss: 1515.431
iteration 0200: loss: 1512.937
iteration 0300: loss: 1514.613
iteration 0400: loss: 1516.070
iteration 0500: loss: 1515.485
iteration 0600: loss: 1513.500
iteration 0700: loss: 1515.688
iteration 0800: loss: 1515.824
iteration 0900: loss: 1514.374
====> Epoch: 026 Train loss: 1515.5613  took : 8.446334838867188
====> Test loss: 1518.7564
iteration 0000: loss: 1515.413
iteration 0100: loss: 1516.717
iteration 0200: loss: 1515.065
iteration 0300: loss: 1516.632
iteration 0400: loss: 1515.160
iteration 0500: loss: 1513.590
iteration 0600: loss: 1516.194
iteration 0700: loss: 1513.435
iteration 0800: loss: 1514.189
iteration 0900: loss: 1512.145
====> Epoch: 027 Train loss: 1515.4686  took : 8.527498722076416
====> Test loss: 1518.6231
iteration 0000: loss: 1516.822
iteration 0100: loss: 1514.089
iteration 0200: loss: 1515.099
iteration 0300: loss: 1515.073
iteration 0400: loss: 1515.836
iteration 0500: loss: 1515.613
iteration 0600: loss: 1517.574
iteration 0700: loss: 1520.282
iteration 0800: loss: 1514.327
iteration 0900: loss: 1514.893
====> Epoch: 028 Train loss: 1515.3867  took : 8.539937257766724
====> Test loss: 1518.5511
iteration 0000: loss: 1516.104
iteration 0100: loss: 1515.105
iteration 0200: loss: 1512.597
iteration 0300: loss: 1516.563
iteration 0400: loss: 1512.754
iteration 0500: loss: 1513.732
iteration 0600: loss: 1515.072
iteration 0700: loss: 1518.000
iteration 0800: loss: 1514.248
iteration 0900: loss: 1516.531
====> Epoch: 029 Train loss: 1515.3828  took : 8.453434467315674
====> Test loss: 1518.8484
iteration 0000: loss: 1515.220
iteration 0100: loss: 1515.494
iteration 0200: loss: 1516.636
iteration 0300: loss: 1514.357
iteration 0400: loss: 1515.882
iteration 0500: loss: 1517.697
iteration 0600: loss: 1514.338
iteration 0700: loss: 1514.029
iteration 0800: loss: 1514.083
iteration 0900: loss: 1516.693
====> Epoch: 030 Train loss: 1515.2749  took : 8.516942501068115
====> Test loss: 1518.5181
iteration 0000: loss: 1512.731
iteration 0100: loss: 1514.538
iteration 0200: loss: 1515.316
iteration 0300: loss: 1519.327
iteration 0400: loss: 1514.762
iteration 0500: loss: 1515.267
iteration 0600: loss: 1515.982
iteration 0700: loss: 1514.507
iteration 0800: loss: 1514.500
iteration 0900: loss: 1515.917
====> Epoch: 031 Train loss: 1515.1967  took : 8.486315250396729
====> Test loss: 1518.6139
iteration 0000: loss: 1514.846
iteration 0100: loss: 1517.461
iteration 0200: loss: 1516.437
iteration 0300: loss: 1514.159
iteration 0400: loss: 1515.461
iteration 0500: loss: 1514.086
iteration 0600: loss: 1515.396
iteration 0700: loss: 1515.326
iteration 0800: loss: 1515.156
iteration 0900: loss: 1516.316
====> Epoch: 032 Train loss: 1515.0937  took : 8.510182619094849
====> Test loss: 1518.5257
iteration 0000: loss: 1515.706
iteration 0100: loss: 1514.285
iteration 0200: loss: 1513.170
iteration 0300: loss: 1514.312
iteration 0400: loss: 1515.782
iteration 0500: loss: 1515.527
iteration 0600: loss: 1515.227
iteration 0700: loss: 1514.529
iteration 0800: loss: 1517.368
iteration 0900: loss: 1516.173
====> Epoch: 033 Train loss: 1515.1074  took : 8.468995571136475
====> Test loss: 1518.6495
iteration 0000: loss: 1519.799
iteration 0100: loss: 1515.537
iteration 0200: loss: 1514.393
iteration 0300: loss: 1513.152
iteration 0400: loss: 1515.808
iteration 0500: loss: 1516.624
iteration 0600: loss: 1516.376
iteration 0700: loss: 1513.299
iteration 0800: loss: 1514.503
iteration 0900: loss: 1514.800
====> Epoch: 034 Train loss: 1515.0430  took : 8.453639507293701
====> Test loss: 1518.4347
iteration 0000: loss: 1513.526
iteration 0100: loss: 1515.193
iteration 0200: loss: 1512.104
iteration 0300: loss: 1515.796
iteration 0400: loss: 1514.144
iteration 0500: loss: 1514.316
iteration 0600: loss: 1517.586
iteration 0700: loss: 1513.312
iteration 0800: loss: 1514.585
iteration 0900: loss: 1515.711
====> Epoch: 035 Train loss: 1514.9889  took : 8.527991771697998
====> Test loss: 1518.4480
iteration 0000: loss: 1514.076
iteration 0100: loss: 1513.776
iteration 0200: loss: 1514.771
iteration 0300: loss: 1514.349
iteration 0400: loss: 1514.448
iteration 0500: loss: 1515.546
iteration 0600: loss: 1515.303
iteration 0700: loss: 1513.415
iteration 0800: loss: 1516.154
iteration 0900: loss: 1515.714
====> Epoch: 036 Train loss: 1514.9130  took : 8.506788492202759
====> Test loss: 1518.3700
iteration 0000: loss: 1512.422
iteration 0100: loss: 1513.622
iteration 0200: loss: 1513.913
iteration 0300: loss: 1515.630
iteration 0400: loss: 1515.444
iteration 0500: loss: 1515.864
iteration 0600: loss: 1515.977
iteration 0700: loss: 1514.281
iteration 0800: loss: 1514.628
iteration 0900: loss: 1513.220
====> Epoch: 037 Train loss: 1514.8774  took : 8.519638299942017
====> Test loss: 1518.5845
iteration 0000: loss: 1514.289
iteration 0100: loss: 1513.924
iteration 0200: loss: 1516.146
iteration 0300: loss: 1514.761
iteration 0400: loss: 1514.673
iteration 0500: loss: 1516.292
iteration 0600: loss: 1514.210
iteration 0700: loss: 1514.905
iteration 0800: loss: 1513.659
iteration 0900: loss: 1516.208
====> Epoch: 038 Train loss: 1514.8047  took : 8.494926929473877
====> Test loss: 1518.2857
iteration 0000: loss: 1514.676
iteration 0100: loss: 1512.666
iteration 0200: loss: 1514.058
iteration 0300: loss: 1516.012
iteration 0400: loss: 1514.050
iteration 0500: loss: 1513.685
iteration 0600: loss: 1514.058
iteration 0700: loss: 1517.633
iteration 0800: loss: 1514.135
iteration 0900: loss: 1515.711
====> Epoch: 039 Train loss: 1514.7276  took : 8.539743900299072
====> Test loss: 1518.2502
iteration 0000: loss: 1515.427
iteration 0100: loss: 1517.135
iteration 0200: loss: 1515.471
iteration 0300: loss: 1512.656
iteration 0400: loss: 1513.698
iteration 0500: loss: 1513.180
iteration 0600: loss: 1514.885
iteration 0700: loss: 1514.175
iteration 0800: loss: 1511.996
iteration 0900: loss: 1516.173
====> Epoch: 040 Train loss: 1514.6664  took : 8.51784372329712
====> Test loss: 1518.3609
iteration 0000: loss: 1517.569
iteration 0100: loss: 1514.361
iteration 0200: loss: 1515.545
iteration 0300: loss: 1514.696
iteration 0400: loss: 1512.639
iteration 0500: loss: 1513.757
iteration 0600: loss: 1512.759
iteration 0700: loss: 1516.116
iteration 0800: loss: 1513.578
iteration 0900: loss: 1514.274
====> Epoch: 041 Train loss: 1514.6856  took : 8.544648170471191
====> Test loss: 1518.7759
iteration 0000: loss: 1514.058
iteration 0100: loss: 1515.458
iteration 0200: loss: 1516.322
iteration 0300: loss: 1512.733
iteration 0400: loss: 1513.056
iteration 0500: loss: 1516.511
iteration 0600: loss: 1513.557
iteration 0700: loss: 1514.005
iteration 0800: loss: 1515.042
iteration 0900: loss: 1516.499
====> Epoch: 042 Train loss: 1514.7010  took : 8.447827100753784
====> Test loss: 1518.1186
iteration 0000: loss: 1516.088
iteration 0100: loss: 1516.136
iteration 0200: loss: 1513.519
iteration 0300: loss: 1514.368
iteration 0400: loss: 1515.526
iteration 0500: loss: 1514.660
iteration 0600: loss: 1516.127
iteration 0700: loss: 1518.812
iteration 0800: loss: 1513.278
iteration 0900: loss: 1515.392
====> Epoch: 043 Train loss: 1514.6285  took : 8.441513299942017
====> Test loss: 1518.2562
iteration 0000: loss: 1517.081
iteration 0100: loss: 1512.211
iteration 0200: loss: 1512.745
iteration 0300: loss: 1513.645
iteration 0400: loss: 1514.380
iteration 0500: loss: 1515.029
iteration 0600: loss: 1515.488
iteration 0700: loss: 1515.067
iteration 0800: loss: 1515.230
iteration 0900: loss: 1514.535
====> Epoch: 044 Train loss: 1514.5083  took : 8.437117338180542
====> Test loss: 1518.1143
iteration 0000: loss: 1512.959
iteration 0100: loss: 1513.042
iteration 0200: loss: 1513.232
iteration 0300: loss: 1515.594
iteration 0400: loss: 1514.731
iteration 0500: loss: 1514.944
iteration 0600: loss: 1515.402
iteration 0700: loss: 1515.010
iteration 0800: loss: 1514.385
iteration 0900: loss: 1514.882
====> Epoch: 045 Train loss: 1514.4845  took : 8.39887523651123
====> Test loss: 1518.0658
iteration 0000: loss: 1515.178
iteration 0100: loss: 1513.432
iteration 0200: loss: 1513.105
iteration 0300: loss: 1514.359
iteration 0400: loss: 1512.223
iteration 0500: loss: 1515.816
iteration 0600: loss: 1512.818
iteration 0700: loss: 1514.645
iteration 0800: loss: 1515.284
iteration 0900: loss: 1512.562
====> Epoch: 046 Train loss: 1514.4253  took : 8.526747226715088
====> Test loss: 1518.0990
iteration 0000: loss: 1511.760
iteration 0100: loss: 1513.123
iteration 0200: loss: 1513.990
iteration 0300: loss: 1515.887
iteration 0400: loss: 1513.239
iteration 0500: loss: 1515.264
iteration 0600: loss: 1513.344
iteration 0700: loss: 1513.008
iteration 0800: loss: 1517.099
iteration 0900: loss: 1512.070
====> Epoch: 047 Train loss: 1514.3990  took : 8.408689498901367
====> Test loss: 1518.1926
iteration 0000: loss: 1513.477
iteration 0100: loss: 1515.172
iteration 0200: loss: 1515.511
iteration 0300: loss: 1515.722
iteration 0400: loss: 1515.159
iteration 0500: loss: 1513.269
iteration 0600: loss: 1515.676
iteration 0700: loss: 1514.844
iteration 0800: loss: 1514.767
iteration 0900: loss: 1516.041
====> Epoch: 048 Train loss: 1514.3341  took : 8.49802541732788
====> Test loss: 1517.9426
iteration 0000: loss: 1514.392
iteration 0100: loss: 1514.828
iteration 0200: loss: 1513.904
iteration 0300: loss: 1514.539
iteration 0400: loss: 1515.382
iteration 0500: loss: 1513.000
iteration 0600: loss: 1515.016
iteration 0700: loss: 1512.107
iteration 0800: loss: 1516.311
iteration 0900: loss: 1513.984
====> Epoch: 049 Train loss: 1514.2597  took : 8.529929161071777
====> Test loss: 1518.1571
iteration 0000: loss: 1516.800
iteration 0100: loss: 1512.154
iteration 0200: loss: 1514.079
iteration 0300: loss: 1516.744
iteration 0400: loss: 1513.732
iteration 0500: loss: 1515.082
iteration 0600: loss: 1515.297
iteration 0700: loss: 1513.339
iteration 0800: loss: 1515.959
iteration 0900: loss: 1514.296
====> Epoch: 050 Train loss: 1514.2797  took : 8.46390151977539
====> Test loss: 1518.0216
====> [MM-VAE] Time: 506.157s or 00:08:26
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  13
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_13
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_13
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.600
iteration 0100: loss: 2088.417
iteration 0200: loss: 2043.417
iteration 0300: loss: 2023.863
iteration 0400: loss: 2002.852
iteration 0500: loss: 1995.420
iteration 0600: loss: 1995.959
iteration 0700: loss: 1992.657
iteration 0800: loss: 1992.991
iteration 0900: loss: 1990.703
====> Epoch: 001 Train loss: 2020.2448  took : 13.297003984451294
====> Test loss: 1991.0748
iteration 0000: loss: 1988.084
iteration 0100: loss: 1983.857
iteration 0200: loss: 1979.359
iteration 0300: loss: 1983.654
iteration 0400: loss: 1980.642
iteration 0500: loss: 1975.130
iteration 0600: loss: 1979.054
iteration 0700: loss: 1973.564
iteration 0800: loss: 1972.508
iteration 0900: loss: 1972.418
====> Epoch: 002 Train loss: 1979.6786  took : 11.719454765319824
====> Test loss: 1976.1141
iteration 0000: loss: 1976.541
iteration 0100: loss: 1968.666
iteration 0200: loss: 1970.633
iteration 0300: loss: 1968.478
iteration 0400: loss: 1969.559
iteration 0500: loss: 1963.584
iteration 0600: loss: 1967.287
iteration 0700: loss: 1962.377
iteration 0800: loss: 1961.605
iteration 0900: loss: 1963.477
====> Epoch: 003 Train loss: 1967.1983  took : 11.901489019393921
====> Test loss: 1964.9020
iteration 0000: loss: 1961.821
iteration 0100: loss: 1965.049
iteration 0200: loss: 1960.386
iteration 0300: loss: 1959.376
iteration 0400: loss: 1959.935
iteration 0500: loss: 1958.723
iteration 0600: loss: 1961.108
iteration 0700: loss: 1958.634
iteration 0800: loss: 1958.714
iteration 0900: loss: 1956.924
====> Epoch: 004 Train loss: 1960.2859  took : 12.778488874435425
====> Test loss: 1960.8735
iteration 0000: loss: 1961.586
iteration 0100: loss: 1961.235
iteration 0200: loss: 1957.980
iteration 0300: loss: 1957.187
iteration 0400: loss: 1956.765
iteration 0500: loss: 1958.949
iteration 0600: loss: 1961.444
iteration 0700: loss: 1956.926
iteration 0800: loss: 1956.452
iteration 0900: loss: 1956.081
====> Epoch: 005 Train loss: 1957.3903  took : 13.021752119064331
====> Test loss: 1958.8214
iteration 0000: loss: 1955.761
iteration 0100: loss: 1955.168
iteration 0200: loss: 1956.241
iteration 0300: loss: 1954.196
iteration 0400: loss: 1954.861
iteration 0500: loss: 1959.659
iteration 0600: loss: 1953.860
iteration 0700: loss: 1953.577
iteration 0800: loss: 1955.526
iteration 0900: loss: 1953.958
====> Epoch: 006 Train loss: 1955.4515  took : 12.96905779838562
====> Test loss: 1957.7783
iteration 0000: loss: 1954.785
iteration 0100: loss: 1952.381
iteration 0200: loss: 1954.773
iteration 0300: loss: 1954.282
iteration 0400: loss: 1954.044
iteration 0500: loss: 1953.568
iteration 0600: loss: 1952.839
iteration 0700: loss: 1953.740
iteration 0800: loss: 1953.649
iteration 0900: loss: 1953.698
====> Epoch: 007 Train loss: 1954.1939  took : 12.813093662261963
====> Test loss: 1956.9156
iteration 0000: loss: 1956.514
iteration 0100: loss: 1952.015
iteration 0200: loss: 1954.935
iteration 0300: loss: 1953.542
iteration 0400: loss: 1953.066
iteration 0500: loss: 1954.246
iteration 0600: loss: 1952.238
iteration 0700: loss: 1951.776
iteration 0800: loss: 1955.431
iteration 0900: loss: 1952.824
====> Epoch: 008 Train loss: 1953.0572  took : 12.039149761199951
====> Test loss: 1955.0090
iteration 0000: loss: 1952.520
iteration 0100: loss: 1952.500
iteration 0200: loss: 1953.464
iteration 0300: loss: 1952.015
iteration 0400: loss: 1951.563
iteration 0500: loss: 1950.578
iteration 0600: loss: 1952.944
iteration 0700: loss: 1952.409
iteration 0800: loss: 1951.612
iteration 0900: loss: 1950.782
====> Epoch: 009 Train loss: 1951.9138  took : 13.449413299560547
====> Test loss: 1953.7642
iteration 0000: loss: 1951.526
iteration 0100: loss: 1950.385
iteration 0200: loss: 1950.345
iteration 0300: loss: 1952.145
iteration 0400: loss: 1951.725
iteration 0500: loss: 1951.097
iteration 0600: loss: 1952.501
iteration 0700: loss: 1949.199
iteration 0800: loss: 1949.892
iteration 0900: loss: 1951.157
====> Epoch: 010 Train loss: 1951.1330  took : 12.475362062454224
====> Test loss: 1953.2278
iteration 0000: loss: 1950.843
iteration 0100: loss: 1950.683
iteration 0200: loss: 1951.074
iteration 0300: loss: 1950.541
iteration 0400: loss: 1951.111
iteration 0500: loss: 1950.067
iteration 0600: loss: 1951.719
iteration 0700: loss: 1949.665
iteration 0800: loss: 1950.858
iteration 0900: loss: 1949.632
====> Epoch: 011 Train loss: 1950.5797  took : 12.667248725891113
====> Test loss: 1953.0668
iteration 0000: loss: 1951.418
iteration 0100: loss: 1949.629
iteration 0200: loss: 1949.692
iteration 0300: loss: 1952.705
iteration 0400: loss: 1950.708
iteration 0500: loss: 1950.349
iteration 0600: loss: 1951.171
iteration 0700: loss: 1949.980
iteration 0800: loss: 1948.162
iteration 0900: loss: 1950.179
====> Epoch: 012 Train loss: 1950.2297  took : 12.762693166732788
====> Test loss: 1952.3234
iteration 0000: loss: 1949.732
iteration 0100: loss: 1948.912
iteration 0200: loss: 1949.115
iteration 0300: loss: 1950.158
iteration 0400: loss: 1949.572
iteration 0500: loss: 1949.199
iteration 0600: loss: 1950.987
iteration 0700: loss: 1950.355
iteration 0800: loss: 1948.514
iteration 0900: loss: 1948.445
====> Epoch: 013 Train loss: 1949.7060  took : 12.706871271133423
====> Test loss: 1951.7446
iteration 0000: loss: 1948.589
iteration 0100: loss: 1949.490
iteration 0200: loss: 1948.034
iteration 0300: loss: 1948.896
iteration 0400: loss: 1949.188
iteration 0500: loss: 1950.193
iteration 0600: loss: 1949.170
iteration 0700: loss: 1948.717
iteration 0800: loss: 1948.658
iteration 0900: loss: 1948.639
====> Epoch: 014 Train loss: 1949.5672  took : 12.062183380126953
====> Test loss: 1951.5888
iteration 0000: loss: 1949.485
iteration 0100: loss: 1949.332
iteration 0200: loss: 1950.515
iteration 0300: loss: 1951.706
iteration 0400: loss: 1948.428
iteration 0500: loss: 1948.509
iteration 0600: loss: 1948.820
iteration 0700: loss: 1950.394
iteration 0800: loss: 1949.128
iteration 0900: loss: 1950.074
====> Epoch: 015 Train loss: 1949.3968  took : 12.059845685958862
====> Test loss: 1951.2537
iteration 0000: loss: 1948.856
iteration 0100: loss: 1948.498
iteration 0200: loss: 1947.542
iteration 0300: loss: 1950.238
iteration 0400: loss: 1949.846
iteration 0500: loss: 1949.229
iteration 0600: loss: 1948.850
iteration 0700: loss: 1949.947
iteration 0800: loss: 1949.162
iteration 0900: loss: 1948.693
====> Epoch: 016 Train loss: 1949.1600  took : 12.258938074111938
====> Test loss: 1951.2080
iteration 0000: loss: 1949.296
iteration 0100: loss: 1948.517
iteration 0200: loss: 1949.463
iteration 0300: loss: 1949.510
iteration 0400: loss: 1949.138
iteration 0500: loss: 1948.803
iteration 0600: loss: 1949.676
iteration 0700: loss: 1949.630
iteration 0800: loss: 1949.212
iteration 0900: loss: 1948.333
====> Epoch: 017 Train loss: 1948.9381  took : 12.161697387695312
====> Test loss: 1951.1193
iteration 0000: loss: 1949.542
iteration 0100: loss: 1949.211
iteration 0200: loss: 1948.933
iteration 0300: loss: 1947.582
iteration 0400: loss: 1948.676
iteration 0500: loss: 1948.827
iteration 0600: loss: 1948.170
iteration 0700: loss: 1949.956
iteration 0800: loss: 1948.571
iteration 0900: loss: 1948.553
====> Epoch: 018 Train loss: 1948.7102  took : 12.741312265396118
====> Test loss: 1950.9546
iteration 0000: loss: 1948.243
iteration 0100: loss: 1948.438
iteration 0200: loss: 1948.131
iteration 0300: loss: 1948.437
iteration 0400: loss: 1948.930
iteration 0500: loss: 1948.110
iteration 0600: loss: 1948.654
iteration 0700: loss: 1948.231
iteration 0800: loss: 1948.006
iteration 0900: loss: 1947.771
====> Epoch: 019 Train loss: 1948.5804  took : 13.135951280593872
====> Test loss: 1950.5932
iteration 0000: loss: 1949.516
iteration 0100: loss: 1948.221
iteration 0200: loss: 1947.579
iteration 0300: loss: 1949.325
iteration 0400: loss: 1948.432
iteration 0500: loss: 1948.176
iteration 0600: loss: 1949.004
iteration 0700: loss: 1949.886
iteration 0800: loss: 1947.461
iteration 0900: loss: 1947.918
====> Epoch: 020 Train loss: 1948.4602  took : 11.751173734664917
====> Test loss: 1950.4600
iteration 0000: loss: 1948.126
iteration 0100: loss: 1948.714
iteration 0200: loss: 1948.095
iteration 0300: loss: 1947.832
iteration 0400: loss: 1947.627
iteration 0500: loss: 1949.036
iteration 0600: loss: 1948.857
iteration 0700: loss: 1949.539
iteration 0800: loss: 1948.089
iteration 0900: loss: 1948.752
====> Epoch: 021 Train loss: 1948.3447  took : 12.737557411193848
====> Test loss: 1950.4438
iteration 0000: loss: 1949.260
iteration 0100: loss: 1947.077
iteration 0200: loss: 1947.726
iteration 0300: loss: 1947.370
iteration 0400: loss: 1948.712
iteration 0500: loss: 1948.327
iteration 0600: loss: 1947.850
iteration 0700: loss: 1949.217
iteration 0800: loss: 1947.124
iteration 0900: loss: 1947.927
====> Epoch: 022 Train loss: 1948.1544  took : 12.072159767150879
====> Test loss: 1950.3104
iteration 0000: loss: 1949.318
iteration 0100: loss: 1948.520
iteration 0200: loss: 1948.503
iteration 0300: loss: 1948.832
iteration 0400: loss: 1948.257
iteration 0500: loss: 1947.932
iteration 0600: loss: 1947.683
iteration 0700: loss: 1947.632
iteration 0800: loss: 1947.667
iteration 0900: loss: 1947.326
====> Epoch: 023 Train loss: 1948.1719  took : 12.453109741210938
====> Test loss: 1950.0872
iteration 0000: loss: 1947.715
iteration 0100: loss: 1948.619
iteration 0200: loss: 1948.063
iteration 0300: loss: 1948.199
iteration 0400: loss: 1947.482
iteration 0500: loss: 1947.212
iteration 0600: loss: 1948.456
iteration 0700: loss: 1949.441
iteration 0800: loss: 1948.232
iteration 0900: loss: 1947.892
====> Epoch: 024 Train loss: 1948.1996  took : 13.131303787231445
====> Test loss: 1950.2194
iteration 0000: loss: 1948.437
iteration 0100: loss: 1948.391
iteration 0200: loss: 1948.972
iteration 0300: loss: 1947.260
iteration 0400: loss: 1948.619
iteration 0500: loss: 1948.211
iteration 0600: loss: 1948.732
iteration 0700: loss: 1948.193
iteration 0800: loss: 1947.669
iteration 0900: loss: 1948.290
====> Epoch: 025 Train loss: 1948.1570  took : 12.686556339263916
====> Test loss: 1950.6980
iteration 0000: loss: 1948.699
iteration 0100: loss: 1948.073
iteration 0200: loss: 1947.741
iteration 0300: loss: 1948.272
iteration 0400: loss: 1947.521
iteration 0500: loss: 1947.008
iteration 0600: loss: 1947.977
iteration 0700: loss: 1947.251
iteration 0800: loss: 1947.645
iteration 0900: loss: 1947.539
====> Epoch: 026 Train loss: 1947.9431  took : 13.199757099151611
====> Test loss: 1949.9808
iteration 0000: loss: 1947.781
iteration 0100: loss: 1947.930
iteration 0200: loss: 1947.708
iteration 0300: loss: 1948.105
iteration 0400: loss: 1947.047
iteration 0500: loss: 1948.600
iteration 0600: loss: 1947.917
iteration 0700: loss: 1948.058
iteration 0800: loss: 1948.032
iteration 0900: loss: 1948.532
====> Epoch: 027 Train loss: 1948.0792  took : 12.828397512435913
====> Test loss: 1950.5856
iteration 0000: loss: 1948.036
iteration 0100: loss: 1948.290
iteration 0200: loss: 1947.339
iteration 0300: loss: 1947.813
iteration 0400: loss: 1948.084
iteration 0500: loss: 1947.537
iteration 0600: loss: 1947.707
iteration 0700: loss: 1947.400
iteration 0800: loss: 1947.313
iteration 0900: loss: 1947.344
====> Epoch: 028 Train loss: 1947.8941  took : 12.302167177200317
====> Test loss: 1949.5574
iteration 0000: loss: 1947.886
iteration 0100: loss: 1947.255
iteration 0200: loss: 1947.783
iteration 0300: loss: 1948.374
iteration 0400: loss: 1948.227
iteration 0500: loss: 1947.606
iteration 0600: loss: 1947.874
iteration 0700: loss: 1947.534
iteration 0800: loss: 1947.584
iteration 0900: loss: 1947.357
====> Epoch: 029 Train loss: 1947.7479  took : 13.157680988311768
====> Test loss: 1949.8160
iteration 0000: loss: 1947.703
iteration 0100: loss: 1947.239
iteration 0200: loss: 1947.051
iteration 0300: loss: 1949.329
iteration 0400: loss: 1947.709
iteration 0500: loss: 1948.701
iteration 0600: loss: 1947.831
iteration 0700: loss: 1947.815
iteration 0800: loss: 1947.575
iteration 0900: loss: 1947.892
====> Epoch: 030 Train loss: 1947.8867  took : 13.287127017974854
====> Test loss: 1949.9378
iteration 0000: loss: 1949.442
iteration 0100: loss: 1947.757
iteration 0200: loss: 1946.993
iteration 0300: loss: 1947.311
iteration 0400: loss: 1947.114
iteration 0500: loss: 1947.577
iteration 0600: loss: 1947.824
iteration 0700: loss: 1947.520
iteration 0800: loss: 1948.347
iteration 0900: loss: 1947.842
====> Epoch: 031 Train loss: 1947.8126  took : 12.780019998550415
====> Test loss: 1949.7479
iteration 0000: loss: 1947.191
iteration 0100: loss: 1948.018
iteration 0200: loss: 1947.944
iteration 0300: loss: 1947.220
iteration 0400: loss: 1949.279
iteration 0500: loss: 1947.617
iteration 0600: loss: 1948.626
iteration 0700: loss: 1948.125
iteration 0800: loss: 1947.613
iteration 0900: loss: 1947.637
====> Epoch: 032 Train loss: 1947.6571  took : 11.658382415771484
====> Test loss: 1949.5646
iteration 0000: loss: 1947.530
iteration 0100: loss: 1947.542
iteration 0200: loss: 1947.330
iteration 0300: loss: 1947.146
iteration 0400: loss: 1947.058
iteration 0500: loss: 1947.139
iteration 0600: loss: 1947.649
iteration 0700: loss: 1947.270
iteration 0800: loss: 1948.572
iteration 0900: loss: 1947.467
====> Epoch: 033 Train loss: 1947.6600  took : 12.825767993927002
====> Test loss: 1949.6453
iteration 0000: loss: 1947.860
iteration 0100: loss: 1947.334
iteration 0200: loss: 1947.114
iteration 0300: loss: 1949.163
iteration 0400: loss: 1947.439
iteration 0500: loss: 1948.067
iteration 0600: loss: 1947.146
iteration 0700: loss: 1947.726
iteration 0800: loss: 1947.855
iteration 0900: loss: 1947.057
====> Epoch: 034 Train loss: 1947.7083  took : 12.376625061035156
====> Test loss: 1949.2041
iteration 0000: loss: 1947.402
iteration 0100: loss: 1948.301
iteration 0200: loss: 1949.145
iteration 0300: loss: 1947.746
iteration 0400: loss: 1947.361
iteration 0500: loss: 1947.299
iteration 0600: loss: 1947.338
iteration 0700: loss: 1947.983
iteration 0800: loss: 1947.277
iteration 0900: loss: 1947.441
====> Epoch: 035 Train loss: 1947.7194  took : 12.721136569976807
====> Test loss: 1949.3745
iteration 0000: loss: 1946.856
iteration 0100: loss: 1947.699
iteration 0200: loss: 1947.982
iteration 0300: loss: 1947.437
iteration 0400: loss: 1947.338
iteration 0500: loss: 1947.181
iteration 0600: loss: 1947.555
iteration 0700: loss: 1948.008
iteration 0800: loss: 1948.389
iteration 0900: loss: 1947.443
====> Epoch: 036 Train loss: 1947.6519  took : 12.624358415603638
====> Test loss: 1949.4192
iteration 0000: loss: 1947.413
iteration 0100: loss: 1946.982
iteration 0200: loss: 1947.911
iteration 0300: loss: 1947.705
iteration 0400: loss: 1947.550
iteration 0500: loss: 1947.123
iteration 0600: loss: 1947.671
iteration 0700: loss: 1947.103
iteration 0800: loss: 1947.304
iteration 0900: loss: 1948.165
====> Epoch: 037 Train loss: 1947.5248  took : 12.017553567886353
====> Test loss: 1949.2928
iteration 0000: loss: 1946.983
iteration 0100: loss: 1947.808
iteration 0200: loss: 1946.976
iteration 0300: loss: 1947.169
iteration 0400: loss: 1947.011
iteration 0500: loss: 1947.218
iteration 0600: loss: 1947.925
iteration 0700: loss: 1947.485
iteration 0800: loss: 1947.164
iteration 0900: loss: 1948.552
====> Epoch: 038 Train loss: 1947.6698  took : 13.10879635810852
====> Test loss: 1949.8938
iteration 0000: loss: 1947.170
iteration 0100: loss: 1948.019
iteration 0200: loss: 1948.050
iteration 0300: loss: 1948.327
iteration 0400: loss: 1947.999
iteration 0500: loss: 1947.307
iteration 0600: loss: 1948.128
iteration 0700: loss: 1946.500
iteration 0800: loss: 1947.052
iteration 0900: loss: 1947.301
====> Epoch: 039 Train loss: 1947.5126  took : 12.746936559677124
====> Test loss: 1949.3916
iteration 0000: loss: 1947.336
iteration 0100: loss: 1949.212
iteration 0200: loss: 1948.166
iteration 0300: loss: 1947.622
iteration 0400: loss: 1947.123
iteration 0500: loss: 1947.333
iteration 0600: loss: 1948.686
iteration 0700: loss: 1948.050
iteration 0800: loss: 1947.577
iteration 0900: loss: 1948.653
====> Epoch: 040 Train loss: 1947.6144  took : 12.605676412582397
====> Test loss: 1951.5456
iteration 0000: loss: 1949.342
iteration 0100: loss: 1947.496
iteration 0200: loss: 1947.151
iteration 0300: loss: 1947.681
iteration 0400: loss: 1947.205
iteration 0500: loss: 1947.262
iteration 0600: loss: 1947.279
iteration 0700: loss: 1947.456
iteration 0800: loss: 1946.787
iteration 0900: loss: 1947.376
====> Epoch: 041 Train loss: 1947.3693  took : 11.953941345214844
====> Test loss: 1949.1266
iteration 0000: loss: 1947.503
iteration 0100: loss: 1947.227
iteration 0200: loss: 1947.775
iteration 0300: loss: 1947.100
iteration 0400: loss: 1946.920
iteration 0500: loss: 1947.271
iteration 0600: loss: 1946.858
iteration 0700: loss: 1948.288
iteration 0800: loss: 1949.338
iteration 0900: loss: 1946.886
====> Epoch: 042 Train loss: 1947.3387  took : 12.373402118682861
====> Test loss: 1949.2289
iteration 0000: loss: 1946.791
iteration 0100: loss: 1948.000
iteration 0200: loss: 1947.063
iteration 0300: loss: 1947.316
iteration 0400: loss: 1947.261
iteration 0500: loss: 1947.557
iteration 0600: loss: 1946.668
iteration 0700: loss: 1947.090
iteration 0800: loss: 1947.236
iteration 0900: loss: 1947.217
====> Epoch: 043 Train loss: 1947.4317  took : 12.712035894393921
====> Test loss: 1949.8224
iteration 0000: loss: 1947.506
iteration 0100: loss: 1947.732
iteration 0200: loss: 1946.581
iteration 0300: loss: 1947.156
iteration 0400: loss: 1946.695
iteration 0500: loss: 1947.322
iteration 0600: loss: 1947.910
iteration 0700: loss: 1947.212
iteration 0800: loss: 1948.090
iteration 0900: loss: 1947.513
====> Epoch: 044 Train loss: 1947.4538  took : 12.108680963516235
====> Test loss: 1950.0566
iteration 0000: loss: 1948.982
iteration 0100: loss: 1947.902
iteration 0200: loss: 1947.684
iteration 0300: loss: 1946.641
iteration 0400: loss: 1946.656
iteration 0500: loss: 1947.751
iteration 0600: loss: 1947.155
iteration 0700: loss: 1947.745
iteration 0800: loss: 1946.758
iteration 0900: loss: 1948.070
====> Epoch: 045 Train loss: 1947.4893  took : 12.367270946502686
====> Test loss: 1949.4865
iteration 0000: loss: 1947.436
iteration 0100: loss: 1946.771
iteration 0200: loss: 1947.580
iteration 0300: loss: 1947.789
iteration 0400: loss: 1948.697
iteration 0500: loss: 1948.007
iteration 0600: loss: 1946.809
iteration 0700: loss: 1948.036
iteration 0800: loss: 1947.664
iteration 0900: loss: 1947.339
====> Epoch: 046 Train loss: 1947.4113  took : 12.810375213623047
====> Test loss: 1949.0249
iteration 0000: loss: 1947.216
iteration 0100: loss: 1947.083
iteration 0200: loss: 1947.874
iteration 0300: loss: 1947.794
iteration 0400: loss: 1948.014
iteration 0500: loss: 1947.538
iteration 0600: loss: 1947.291
iteration 0700: loss: 1947.443
iteration 0800: loss: 1947.678
iteration 0900: loss: 1947.034
====> Epoch: 047 Train loss: 1947.4256  took : 12.18603253364563
====> Test loss: 1949.2814
iteration 0000: loss: 1947.076
iteration 0100: loss: 1947.878
iteration 0200: loss: 1947.022
iteration 0300: loss: 1947.861
iteration 0400: loss: 1948.111
iteration 0500: loss: 1948.712
iteration 0600: loss: 1947.073
iteration 0700: loss: 1946.688
iteration 0800: loss: 1947.208
iteration 0900: loss: 1947.571
====> Epoch: 048 Train loss: 1947.4257  took : 11.684549570083618
====> Test loss: 1949.3912
iteration 0000: loss: 1947.706
iteration 0100: loss: 1947.661
iteration 0200: loss: 1947.510
iteration 0300: loss: 1947.348
iteration 0400: loss: 1946.940
iteration 0500: loss: 1947.419
iteration 0600: loss: 1947.549
iteration 0700: loss: 1947.809
iteration 0800: loss: 1947.214
iteration 0900: loss: 1946.881
====> Epoch: 049 Train loss: 1947.4180  took : 12.660332441329956
====> Test loss: 1949.0187
iteration 0000: loss: 1947.204
iteration 0100: loss: 1948.832
iteration 0200: loss: 1947.810
iteration 0300: loss: 1947.086
iteration 0400: loss: 1947.755
iteration 0500: loss: 1947.473
iteration 0600: loss: 1947.481
iteration 0700: loss: 1947.194
iteration 0800: loss: 1947.272
iteration 0900: loss: 1947.240
====> Epoch: 050 Train loss: 1947.4604  took : 12.28681206703186
====> Test loss: 1949.1584
====> [MM-VAE] Time: 702.163s or 00:11:42
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  13
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_13
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_13
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5219.908
iteration 0100: loss: 4163.378
iteration 0200: loss: 4104.990
iteration 0300: loss: 4087.768
iteration 0400: loss: 4023.008
iteration 0500: loss: 4032.504
iteration 0600: loss: 4012.208
iteration 0700: loss: 4011.437
iteration 0800: loss: 4022.718
iteration 0900: loss: 4022.469
iteration 1000: loss: 4008.438
iteration 1100: loss: 3996.042
iteration 1200: loss: 4004.214
iteration 1300: loss: 3992.923
iteration 1400: loss: 4009.604
iteration 1500: loss: 3995.997
iteration 1600: loss: 3997.774
iteration 1700: loss: 4000.260
iteration 1800: loss: 3979.567
====> Epoch: 001 Train loss: 4031.5488  took : 53.289620876312256
====> Test loss: 3990.1760
iteration 0000: loss: 3991.667
iteration 0100: loss: 3979.255
iteration 0200: loss: 3976.625
iteration 0300: loss: 3984.059
iteration 0400: loss: 3982.303
iteration 0500: loss: 3987.372
iteration 0600: loss: 3984.889
iteration 0700: loss: 3985.130
iteration 0800: loss: 3978.792
iteration 0900: loss: 3983.148
iteration 1000: loss: 3985.622
iteration 1100: loss: 3973.595
iteration 1200: loss: 3974.952
iteration 1300: loss: 3971.890
iteration 1400: loss: 3966.411
iteration 1500: loss: 3966.889
iteration 1600: loss: 3960.883
iteration 1700: loss: 3963.190
iteration 1800: loss: 3976.778
====> Epoch: 002 Train loss: 3976.0621  took : 53.50695490837097
====> Test loss: 3966.0018
iteration 0000: loss: 3963.714
iteration 0100: loss: 3964.473
iteration 0200: loss: 3969.238
iteration 0300: loss: 3957.565
iteration 0400: loss: 3957.604
iteration 0500: loss: 3965.603
iteration 0600: loss: 3960.426
iteration 0700: loss: 3958.213
iteration 0800: loss: 3959.994
iteration 0900: loss: 3962.593
iteration 1000: loss: 3958.396
iteration 1100: loss: 3959.989
iteration 1200: loss: 3949.817
iteration 1300: loss: 3957.836
iteration 1400: loss: 3964.658
iteration 1500: loss: 3951.803
iteration 1600: loss: 3958.325
iteration 1700: loss: 3949.431
iteration 1800: loss: 3942.636
====> Epoch: 003 Train loss: 3955.2228  took : 53.196441650390625
====> Test loss: 3951.4223
iteration 0000: loss: 3944.800
iteration 0100: loss: 3944.037
iteration 0200: loss: 3946.156
iteration 0300: loss: 3943.304
iteration 0400: loss: 3943.373
iteration 0500: loss: 3952.968
iteration 0600: loss: 3946.679
iteration 0700: loss: 3940.856
iteration 0800: loss: 3942.123
iteration 0900: loss: 3939.774
iteration 1000: loss: 3944.344
iteration 1100: loss: 3942.068
iteration 1200: loss: 3942.777
iteration 1300: loss: 3941.226
iteration 1400: loss: 3942.572
iteration 1500: loss: 3950.835
iteration 1600: loss: 3951.710
iteration 1700: loss: 3953.601
iteration 1800: loss: 3947.366
====> Epoch: 004 Train loss: 3945.8889  took : 53.201186180114746
====> Test loss: 3947.8977
iteration 0000: loss: 3944.551
iteration 0100: loss: 3936.867
iteration 0200: loss: 3948.611
iteration 0300: loss: 3950.401
iteration 0400: loss: 3945.755
iteration 0500: loss: 3943.733
iteration 0600: loss: 3940.136
iteration 0700: loss: 3938.057
iteration 0800: loss: 3941.198
iteration 0900: loss: 3935.153
iteration 1000: loss: 3935.591
iteration 1100: loss: 3939.170
iteration 1200: loss: 3940.050
iteration 1300: loss: 3944.347
iteration 1400: loss: 3947.117
iteration 1500: loss: 3937.933
iteration 1600: loss: 3949.479
iteration 1700: loss: 3942.613
iteration 1800: loss: 3945.361
====> Epoch: 005 Train loss: 3943.5785  took : 53.19958209991455
====> Test loss: 3946.4930
iteration 0000: loss: 3947.702
iteration 0100: loss: 3944.469
iteration 0200: loss: 3947.079
iteration 0300: loss: 3938.154
iteration 0400: loss: 3939.730
iteration 0500: loss: 3944.324
iteration 0600: loss: 3939.755
iteration 0700: loss: 3940.120
iteration 0800: loss: 3936.065
iteration 0900: loss: 3944.738
iteration 1000: loss: 3935.966
iteration 1100: loss: 3939.485
iteration 1200: loss: 3940.911
iteration 1300: loss: 3940.074
iteration 1400: loss: 3941.931
iteration 1500: loss: 3942.694
iteration 1600: loss: 3941.397
iteration 1700: loss: 3945.976
iteration 1800: loss: 3941.380
====> Epoch: 006 Train loss: 3942.3435  took : 53.32401466369629
====> Test loss: 3945.9063
iteration 0000: loss: 3947.846
iteration 0100: loss: 3943.886
iteration 0200: loss: 3937.015
iteration 0300: loss: 3936.970
iteration 0400: loss: 3950.704
iteration 0500: loss: 3941.671
iteration 0600: loss: 3936.874
iteration 0700: loss: 3939.206
iteration 0800: loss: 3941.654
iteration 0900: loss: 3943.361
iteration 1000: loss: 3946.918
iteration 1100: loss: 3936.127
iteration 1200: loss: 3941.656
iteration 1300: loss: 3941.470
iteration 1400: loss: 3942.500
iteration 1500: loss: 3945.610
iteration 1600: loss: 3941.779
iteration 1700: loss: 3946.982
iteration 1800: loss: 3939.185
====> Epoch: 007 Train loss: 3941.9405  took : 53.11931324005127
====> Test loss: 3944.5906
iteration 0000: loss: 3937.326
iteration 0100: loss: 3942.120
iteration 0200: loss: 3952.756
iteration 0300: loss: 3943.076
iteration 0400: loss: 3943.162
iteration 0500: loss: 3945.882
iteration 0600: loss: 3941.206
iteration 0700: loss: 3935.100
iteration 0800: loss: 3945.375
iteration 0900: loss: 3939.994
iteration 1000: loss: 3946.584
iteration 1100: loss: 3944.016
iteration 1200: loss: 3947.362
iteration 1300: loss: 3936.004
iteration 1400: loss: 3939.055
iteration 1500: loss: 3942.104
iteration 1600: loss: 3942.592
iteration 1700: loss: 3936.348
iteration 1800: loss: 3938.078
====> Epoch: 008 Train loss: 3941.2720  took : 53.24302649497986
====> Test loss: 3945.0838
iteration 0000: loss: 3940.853
iteration 0100: loss: 3939.934
iteration 0200: loss: 3947.379
iteration 0300: loss: 3944.765
iteration 0400: loss: 3942.674
iteration 0500: loss: 3934.426
iteration 0600: loss: 3949.432
iteration 0700: loss: 3942.890
iteration 0800: loss: 3938.384
iteration 0900: loss: 3941.504
iteration 1000: loss: 3938.906
iteration 1100: loss: 3944.823
iteration 1200: loss: 3938.421
iteration 1300: loss: 3942.599
iteration 1400: loss: 3938.079
iteration 1500: loss: 3943.200
iteration 1600: loss: 3949.883
iteration 1700: loss: 3941.927
iteration 1800: loss: 3941.162
====> Epoch: 009 Train loss: 3940.9090  took : 52.874645471572876
====> Test loss: 3944.3026
iteration 0000: loss: 3940.593
iteration 0100: loss: 3941.354
iteration 0200: loss: 3944.591
iteration 0300: loss: 3939.536
iteration 0400: loss: 3945.091
iteration 0500: loss: 3937.559
iteration 0600: loss: 3932.641
iteration 0700: loss: 3947.733
iteration 0800: loss: 3938.886
iteration 0900: loss: 3936.541
iteration 1000: loss: 3944.053
iteration 1100: loss: 3944.202
iteration 1200: loss: 3940.350
iteration 1300: loss: 3933.663
iteration 1400: loss: 3942.102
iteration 1500: loss: 3939.431
iteration 1600: loss: 3944.250
iteration 1700: loss: 3934.923
iteration 1800: loss: 3943.892
====> Epoch: 010 Train loss: 3940.6959  took : 53.227317333221436
====> Test loss: 3944.5043
iteration 0000: loss: 3934.684
iteration 0100: loss: 3938.915
iteration 0200: loss: 3946.568
iteration 0300: loss: 3939.699
iteration 0400: loss: 3938.770
iteration 0500: loss: 3934.021
iteration 0600: loss: 3932.448
iteration 0700: loss: 3939.412
iteration 0800: loss: 3933.042
iteration 0900: loss: 3944.613
iteration 1000: loss: 3941.720
iteration 1100: loss: 3942.539
iteration 1200: loss: 3941.738
iteration 1300: loss: 3938.570
iteration 1400: loss: 3944.388
iteration 1500: loss: 3938.557
iteration 1600: loss: 3940.785
iteration 1700: loss: 3943.719
iteration 1800: loss: 3940.342
====> Epoch: 011 Train loss: 3940.2841  took : 53.11739373207092
====> Test loss: 3943.7988
iteration 0000: loss: 3932.111
iteration 0100: loss: 3938.506
iteration 0200: loss: 3936.190
iteration 0300: loss: 3945.408
iteration 0400: loss: 3932.423
iteration 0500: loss: 3945.873
iteration 0600: loss: 3938.490
iteration 0700: loss: 3939.275
iteration 0800: loss: 3940.598
iteration 0900: loss: 3940.513
iteration 1000: loss: 3944.659
iteration 1100: loss: 3941.749
iteration 1200: loss: 3939.875
iteration 1300: loss: 3934.130
iteration 1400: loss: 3939.245
iteration 1500: loss: 3943.222
iteration 1600: loss: 3938.572
iteration 1700: loss: 3940.432
iteration 1800: loss: 3934.933
====> Epoch: 012 Train loss: 3940.0599  took : 52.984275102615356
====> Test loss: 3943.6406
iteration 0000: loss: 3936.223
iteration 0100: loss: 3934.398
iteration 0200: loss: 3943.870
iteration 0300: loss: 3937.547
iteration 0400: loss: 3945.313
iteration 0500: loss: 3932.581
iteration 0600: loss: 3941.480
iteration 0700: loss: 3937.943
iteration 0800: loss: 3942.698
iteration 0900: loss: 3939.778
iteration 1000: loss: 3945.431
iteration 1100: loss: 3943.432
iteration 1200: loss: 3940.826
iteration 1300: loss: 3936.709
iteration 1400: loss: 3941.256
iteration 1500: loss: 3938.984
iteration 1600: loss: 3938.112
iteration 1700: loss: 3938.098
iteration 1800: loss: 3936.435
====> Epoch: 013 Train loss: 3939.9813  took : 53.17583990097046
====> Test loss: 3943.1320
iteration 0000: loss: 3940.680
iteration 0100: loss: 3936.442
iteration 0200: loss: 3938.219
iteration 0300: loss: 3944.162
iteration 0400: loss: 3936.918
iteration 0500: loss: 3941.249
iteration 0600: loss: 3937.513
iteration 0700: loss: 3936.691
iteration 0800: loss: 3940.530
iteration 0900: loss: 3939.076
iteration 1000: loss: 3935.518
iteration 1100: loss: 3940.071
iteration 1200: loss: 3940.116
iteration 1300: loss: 3934.159
iteration 1400: loss: 3942.938
iteration 1500: loss: 3941.455
iteration 1600: loss: 3936.994
iteration 1700: loss: 3939.585
iteration 1800: loss: 3938.318
====> Epoch: 014 Train loss: 3939.6270  took : 53.13607120513916
====> Test loss: 3942.9990
iteration 0000: loss: 3936.481
iteration 0100: loss: 3936.289
iteration 0200: loss: 3936.299
iteration 0300: loss: 3940.284
iteration 0400: loss: 3937.550
iteration 0500: loss: 3937.803
iteration 0600: loss: 3945.317
iteration 0700: loss: 3938.800
iteration 0800: loss: 3938.465
iteration 0900: loss: 3943.833
iteration 1000: loss: 3938.520
iteration 1100: loss: 3941.147
iteration 1200: loss: 3944.386
iteration 1300: loss: 3938.404
iteration 1400: loss: 3944.547
iteration 1500: loss: 3936.978
iteration 1600: loss: 3940.308
iteration 1700: loss: 3939.363
iteration 1800: loss: 3938.704
====> Epoch: 015 Train loss: 3939.5211  took : 53.03900122642517
====> Test loss: 3943.4801
iteration 0000: loss: 3935.755
iteration 0100: loss: 3937.853
iteration 0200: loss: 3937.733
iteration 0300: loss: 3932.806
iteration 0400: loss: 3944.504
iteration 0500: loss: 3934.020
iteration 0600: loss: 3933.044
iteration 0700: loss: 3940.451
iteration 0800: loss: 3943.651
iteration 0900: loss: 3937.146
iteration 1000: loss: 3938.519
iteration 1100: loss: 3936.136
iteration 1200: loss: 3935.208
iteration 1300: loss: 3938.677
iteration 1400: loss: 3939.554
iteration 1500: loss: 3941.283
iteration 1600: loss: 3945.497
iteration 1700: loss: 3939.775
iteration 1800: loss: 3938.010
====> Epoch: 016 Train loss: 3939.4743  took : 52.88491916656494
====> Test loss: 3944.4586
iteration 0000: loss: 3937.684
iteration 0100: loss: 3935.941
iteration 0200: loss: 3938.293
iteration 0300: loss: 3946.262
iteration 0400: loss: 3940.611
iteration 0500: loss: 3948.251
iteration 0600: loss: 3944.382
iteration 0700: loss: 3933.066
iteration 0800: loss: 3936.779
iteration 0900: loss: 3937.972
iteration 1000: loss: 3939.309
iteration 1100: loss: 3938.904
iteration 1200: loss: 3941.439
iteration 1300: loss: 3938.700
iteration 1400: loss: 3938.089
iteration 1500: loss: 3942.225
iteration 1600: loss: 3939.603
iteration 1700: loss: 3943.378
iteration 1800: loss: 3936.183
====> Epoch: 017 Train loss: 3939.2498  took : 53.02888774871826
====> Test loss: 3942.9328
iteration 0000: loss: 3938.237
iteration 0100: loss: 3941.675
iteration 0200: loss: 3937.761
iteration 0300: loss: 3938.702
iteration 0400: loss: 3939.627
iteration 0500: loss: 3934.207
iteration 0600: loss: 3937.219
iteration 0700: loss: 3942.004
iteration 0800: loss: 3938.061
iteration 0900: loss: 3936.021
iteration 1000: loss: 3939.308
iteration 1100: loss: 3934.067
iteration 1200: loss: 3943.139
iteration 1300: loss: 3947.400
iteration 1400: loss: 3935.569
iteration 1500: loss: 3939.912
iteration 1600: loss: 3945.099
iteration 1700: loss: 3934.535
iteration 1800: loss: 3935.566
====> Epoch: 018 Train loss: 3939.1543  took : 53.09265494346619
====> Test loss: 3944.6200
iteration 0000: loss: 3942.732
iteration 0100: loss: 3942.250
iteration 0200: loss: 3936.737
iteration 0300: loss: 3937.836
iteration 0400: loss: 3941.418
iteration 0500: loss: 3940.446
iteration 0600: loss: 3932.863
iteration 0700: loss: 3935.184
iteration 0800: loss: 3937.671
iteration 0900: loss: 3935.235
iteration 1000: loss: 3936.882
iteration 1100: loss: 3937.159
iteration 1200: loss: 3938.058
iteration 1300: loss: 3934.156
iteration 1400: loss: 3941.166
iteration 1500: loss: 3941.695
iteration 1600: loss: 3934.834
iteration 1700: loss: 3938.815
iteration 1800: loss: 3937.532
====> Epoch: 019 Train loss: 3939.2176  took : 52.89150404930115
====> Test loss: 3943.3322
iteration 0000: loss: 3937.437
iteration 0100: loss: 3937.070
iteration 0200: loss: 3934.291
iteration 0300: loss: 3934.378
iteration 0400: loss: 3938.432
iteration 0500: loss: 3935.272
iteration 0600: loss: 3938.204
iteration 0700: loss: 3936.148
iteration 0800: loss: 3935.857
iteration 0900: loss: 3937.581
iteration 1000: loss: 3936.502
iteration 1100: loss: 3949.765
iteration 1200: loss: 3935.868
iteration 1300: loss: 3942.044
iteration 1400: loss: 3942.602
iteration 1500: loss: 3944.118
iteration 1600: loss: 3942.213
iteration 1700: loss: 3940.362
iteration 1800: loss: 3938.881
====> Epoch: 020 Train loss: 3939.0912  took : 52.93962836265564
====> Test loss: 3942.7640
iteration 0000: loss: 3941.802
iteration 0100: loss: 3936.772
iteration 0200: loss: 3941.041
iteration 0300: loss: 3937.984
iteration 0400: loss: 3936.645
iteration 0500: loss: 3939.928
iteration 0600: loss: 3936.738
iteration 0700: loss: 3939.693
iteration 0800: loss: 3942.309
iteration 0900: loss: 3949.569
iteration 1000: loss: 3943.202
iteration 1100: loss: 3936.876
iteration 1200: loss: 3938.485
iteration 1300: loss: 3935.934
iteration 1400: loss: 3938.180
iteration 1500: loss: 3939.206
iteration 1600: loss: 3941.420
iteration 1700: loss: 3939.128
iteration 1800: loss: 3940.304
====> Epoch: 021 Train loss: 3938.9182  took : 52.95110869407654
====> Test loss: 3942.9991
iteration 0000: loss: 3930.876
iteration 0100: loss: 3938.461
iteration 0200: loss: 3939.187
iteration 0300: loss: 3939.695
iteration 0400: loss: 3935.983
iteration 0500: loss: 3939.887
iteration 0600: loss: 3930.340
iteration 0700: loss: 3938.180
iteration 0800: loss: 3938.925
iteration 0900: loss: 3938.515
iteration 1000: loss: 3943.222
iteration 1100: loss: 3933.841
iteration 1200: loss: 3937.617
iteration 1300: loss: 3936.806
iteration 1400: loss: 3933.510
iteration 1500: loss: 3936.481
iteration 1600: loss: 3927.824
iteration 1700: loss: 3938.099
iteration 1800: loss: 3938.993
====> Epoch: 022 Train loss: 3938.8027  took : 53.07842183113098
====> Test loss: 3942.8225
iteration 0000: loss: 3937.086
iteration 0100: loss: 3939.258
iteration 0200: loss: 3939.753
iteration 0300: loss: 3942.948
iteration 0400: loss: 3938.529
iteration 0500: loss: 3938.081
iteration 0600: loss: 3936.187
iteration 0700: loss: 3943.672
iteration 0800: loss: 3936.895
iteration 0900: loss: 3934.837
iteration 1000: loss: 3942.999
iteration 1100: loss: 3938.799
iteration 1200: loss: 3943.063
iteration 1300: loss: 3938.787
iteration 1400: loss: 3940.731
iteration 1500: loss: 3936.000
iteration 1600: loss: 3936.450
iteration 1700: loss: 3944.599
iteration 1800: loss: 3933.227
====> Epoch: 023 Train loss: 3938.8854  took : 53.16309571266174
====> Test loss: 3943.8341
iteration 0000: loss: 3934.194
iteration 0100: loss: 3935.177
iteration 0200: loss: 3945.142
iteration 0300: loss: 3941.394
iteration 0400: loss: 3937.673
iteration 0500: loss: 3942.857
iteration 0600: loss: 3935.109
iteration 0700: loss: 3940.379
iteration 0800: loss: 3938.220
iteration 0900: loss: 3937.400
iteration 1000: loss: 3938.925
iteration 1100: loss: 3942.734
iteration 1200: loss: 3933.543
iteration 1300: loss: 3938.964
iteration 1400: loss: 3941.995
iteration 1500: loss: 3936.336
iteration 1600: loss: 3943.352
iteration 1700: loss: 3938.458
iteration 1800: loss: 3939.614
====> Epoch: 024 Train loss: 3938.6681  took : 52.84927320480347
====> Test loss: 3942.4524
iteration 0000: loss: 3932.111
iteration 0100: loss: 3938.736
iteration 0200: loss: 3933.918
iteration 0300: loss: 3935.236
iteration 0400: loss: 3937.556
iteration 0500: loss: 3937.607
iteration 0600: loss: 3940.633
iteration 0700: loss: 3939.177
iteration 0800: loss: 3939.816
iteration 0900: loss: 3936.027
iteration 1000: loss: 3933.106
iteration 1100: loss: 3941.392
iteration 1200: loss: 3941.170
iteration 1300: loss: 3934.200
iteration 1400: loss: 3941.119
iteration 1500: loss: 3941.400
iteration 1600: loss: 3941.049
iteration 1700: loss: 3942.546
iteration 1800: loss: 3942.370
====> Epoch: 025 Train loss: 3938.5482  took : 53.150330543518066
====> Test loss: 3942.8352
iteration 0000: loss: 3935.965
iteration 0100: loss: 3936.144
iteration 0200: loss: 3939.071
iteration 0300: loss: 3937.872
iteration 0400: loss: 3933.318
iteration 0500: loss: 3935.816
iteration 0600: loss: 3937.438
iteration 0700: loss: 3938.619
iteration 0800: loss: 3938.501
iteration 0900: loss: 3940.387
iteration 1000: loss: 3940.071
iteration 1100: loss: 3941.370
iteration 1200: loss: 3939.283
iteration 1300: loss: 3938.104
iteration 1400: loss: 3936.378
iteration 1500: loss: 3940.490
iteration 1600: loss: 3934.568
iteration 1700: loss: 3935.785
iteration 1800: loss: 3936.046
====> Epoch: 026 Train loss: 3938.6192  took : 52.81487226486206
====> Test loss: 3943.8287
iteration 0000: loss: 3938.044
iteration 0100: loss: 3937.866
iteration 0200: loss: 3941.843
iteration 0300: loss: 3939.191
iteration 0400: loss: 3942.149
iteration 0500: loss: 3936.511
iteration 0600: loss: 3938.371
iteration 0700: loss: 3939.883
iteration 0800: loss: 3938.015
iteration 0900: loss: 3938.023
iteration 1000: loss: 3939.217
iteration 1100: loss: 3940.885
iteration 1200: loss: 3937.867
iteration 1300: loss: 3938.412
iteration 1400: loss: 3938.069
iteration 1500: loss: 3935.079
iteration 1600: loss: 3943.914
iteration 1700: loss: 3935.116
iteration 1800: loss: 3938.755
====> Epoch: 027 Train loss: 3938.5508  took : 52.921276807785034
====> Test loss: 3942.3997
iteration 0000: loss: 3933.030
iteration 0100: loss: 3937.664
iteration 0200: loss: 3938.610
iteration 0300: loss: 3931.068
iteration 0400: loss: 3938.120
iteration 0500: loss: 3937.971
iteration 0600: loss: 3944.604
iteration 0700: loss: 3934.750
iteration 0800: loss: 3934.315
iteration 0900: loss: 3937.744
iteration 1000: loss: 3929.862
iteration 1100: loss: 3940.647
iteration 1200: loss: 3940.835
iteration 1300: loss: 3939.004
iteration 1400: loss: 3940.562
iteration 1500: loss: 3935.237
iteration 1600: loss: 3947.569
iteration 1700: loss: 3939.964
iteration 1800: loss: 3941.306
====> Epoch: 028 Train loss: 3938.2231  took : 53.24701929092407
====> Test loss: 3942.1429
iteration 0000: loss: 3935.788
iteration 0100: loss: 3936.400
iteration 0200: loss: 3932.975
iteration 0300: loss: 3935.188
iteration 0400: loss: 3939.504
iteration 0500: loss: 3934.229
iteration 0600: loss: 3936.505
iteration 0700: loss: 3941.457
iteration 0800: loss: 3937.843
iteration 0900: loss: 3931.007
iteration 1000: loss: 3936.806
iteration 1100: loss: 3943.668
iteration 1200: loss: 3932.397
iteration 1300: loss: 3939.770
iteration 1400: loss: 3939.252
iteration 1500: loss: 3937.089
iteration 1600: loss: 3941.267
iteration 1700: loss: 3936.240
iteration 1800: loss: 3938.241
====> Epoch: 029 Train loss: 3938.3911  took : 52.97746992111206
====> Test loss: 3942.3529
iteration 0000: loss: 3938.242
iteration 0100: loss: 3934.839
iteration 0200: loss: 3938.543
iteration 0300: loss: 3939.803
iteration 0400: loss: 3936.138
iteration 0500: loss: 3934.459
iteration 0600: loss: 3936.535
iteration 0700: loss: 3942.825
iteration 0800: loss: 3944.778
iteration 0900: loss: 3931.716
iteration 1000: loss: 3938.116
iteration 1100: loss: 3944.797
iteration 1200: loss: 3939.585
iteration 1300: loss: 3937.436
iteration 1400: loss: 3935.193
iteration 1500: loss: 3941.932
iteration 1600: loss: 3933.042
iteration 1700: loss: 3938.039
iteration 1800: loss: 3941.507
====> Epoch: 030 Train loss: 3938.2035  took : 53.10892033576965
====> Test loss: 3942.8850
iteration 0000: loss: 3944.300
iteration 0100: loss: 3938.170
iteration 0200: loss: 3937.943
iteration 0300: loss: 3932.208
iteration 0400: loss: 3934.883
iteration 0500: loss: 3932.151
iteration 0600: loss: 3934.660
iteration 0700: loss: 3935.875
iteration 0800: loss: 3938.639
iteration 0900: loss: 3935.727
iteration 1000: loss: 3947.118
iteration 1100: loss: 3932.487
iteration 1200: loss: 3932.582
iteration 1300: loss: 3939.336
iteration 1400: loss: 3934.008
iteration 1500: loss: 3936.886
iteration 1600: loss: 3939.725
iteration 1700: loss: 3940.368
iteration 1800: loss: 3935.914
====> Epoch: 031 Train loss: 3938.1662  took : 53.14421844482422
====> Test loss: 3942.4565
iteration 0000: loss: 3936.062
iteration 0100: loss: 3938.886
iteration 0200: loss: 3942.519
iteration 0300: loss: 3939.565
iteration 0400: loss: 3939.026
iteration 0500: loss: 3940.384
iteration 0600: loss: 3942.163
iteration 0700: loss: 3937.259
iteration 0800: loss: 3935.624
iteration 0900: loss: 3940.639
iteration 1000: loss: 3936.412
iteration 1100: loss: 3938.244
iteration 1200: loss: 3936.126
iteration 1300: loss: 3947.520
iteration 1400: loss: 3935.936
iteration 1500: loss: 3940.439
iteration 1600: loss: 3934.927
iteration 1700: loss: 3933.226
iteration 1800: loss: 3931.398
====> Epoch: 032 Train loss: 3938.1310  took : 53.11782145500183
====> Test loss: 3941.9147
iteration 0000: loss: 3943.129
iteration 0100: loss: 3935.273
iteration 0200: loss: 3935.579
iteration 0300: loss: 3936.200
iteration 0400: loss: 3935.426
iteration 0500: loss: 3943.826
iteration 0600: loss: 3934.261
iteration 0700: loss: 3937.471
iteration 0800: loss: 3938.824
iteration 0900: loss: 3934.534
iteration 1000: loss: 3935.235
iteration 1100: loss: 3935.007
iteration 1200: loss: 3940.860
iteration 1300: loss: 3939.358
iteration 1400: loss: 3935.022
iteration 1500: loss: 3937.333
iteration 1600: loss: 3937.388
iteration 1700: loss: 3937.300
iteration 1800: loss: 3942.542
====> Epoch: 033 Train loss: 3938.0003  took : 53.15050768852234
====> Test loss: 3942.1223
iteration 0000: loss: 3934.683
iteration 0100: loss: 3938.627
iteration 0200: loss: 3931.823
iteration 0300: loss: 3944.557
iteration 0400: loss: 3939.446
iteration 0500: loss: 3938.025
iteration 0600: loss: 3943.465
iteration 0700: loss: 3937.583
iteration 0800: loss: 3945.880
iteration 0900: loss: 3941.283
iteration 1000: loss: 3937.906
iteration 1100: loss: 3941.478
iteration 1200: loss: 3937.957
iteration 1300: loss: 3939.943
iteration 1400: loss: 3936.457
iteration 1500: loss: 3941.583
iteration 1600: loss: 3939.358
iteration 1700: loss: 3936.529
iteration 1800: loss: 3942.223
====> Epoch: 034 Train loss: 3938.0300  took : 53.176791191101074
====> Test loss: 3941.8175
iteration 0000: loss: 3930.501
iteration 0100: loss: 3940.305
iteration 0200: loss: 3936.199
iteration 0300: loss: 3939.992
iteration 0400: loss: 3936.378
iteration 0500: loss: 3936.624
iteration 0600: loss: 3934.533
iteration 0700: loss: 3937.536
iteration 0800: loss: 3934.917
iteration 0900: loss: 3940.005
iteration 1000: loss: 3934.977
iteration 1100: loss: 3933.034
iteration 1200: loss: 3939.277
iteration 1300: loss: 3938.371
iteration 1400: loss: 3935.611
iteration 1500: loss: 3933.775
iteration 1600: loss: 3946.968
iteration 1700: loss: 3936.382
iteration 1800: loss: 3934.147
====> Epoch: 035 Train loss: 3937.9932  took : 52.93475556373596
====> Test loss: 3942.4132
iteration 0000: loss: 3936.632
iteration 0100: loss: 3934.691
iteration 0200: loss: 3941.934
iteration 0300: loss: 3932.888
iteration 0400: loss: 3936.844
iteration 0500: loss: 3937.133
iteration 0600: loss: 3941.636
iteration 0700: loss: 3941.728
iteration 0800: loss: 3940.868
iteration 0900: loss: 3935.743
iteration 1000: loss: 3940.540
iteration 1100: loss: 3937.836
iteration 1200: loss: 3938.671
iteration 1300: loss: 3931.979
iteration 1400: loss: 3940.128
iteration 1500: loss: 3934.380
iteration 1600: loss: 3933.175
iteration 1700: loss: 3943.216
iteration 1800: loss: 3938.454
====> Epoch: 036 Train loss: 3937.8283  took : 52.90874695777893
====> Test loss: 3942.3098
iteration 0000: loss: 3938.318
iteration 0100: loss: 3930.912
iteration 0200: loss: 3941.536
iteration 0300: loss: 3938.232
iteration 0400: loss: 3933.078
iteration 0500: loss: 3931.065
iteration 0600: loss: 3936.547
iteration 0700: loss: 3932.037
iteration 0800: loss: 3939.257
iteration 0900: loss: 3937.629
iteration 1000: loss: 3944.969
iteration 1100: loss: 3944.711
iteration 1200: loss: 3944.824
iteration 1300: loss: 3938.034
iteration 1400: loss: 3936.013
iteration 1500: loss: 3939.546
iteration 1600: loss: 3937.278
iteration 1700: loss: 3940.288
iteration 1800: loss: 3937.043
====> Epoch: 037 Train loss: 3937.8988  took : 53.15797185897827
====> Test loss: 3942.3357
iteration 0000: loss: 3933.987
iteration 0100: loss: 3934.691
iteration 0200: loss: 3939.895
iteration 0300: loss: 3933.604
iteration 0400: loss: 3937.662
iteration 0500: loss: 3939.423
iteration 0600: loss: 3938.029
iteration 0700: loss: 3931.872
iteration 0800: loss: 3947.226
iteration 0900: loss: 3941.306
iteration 1000: loss: 3935.917
iteration 1100: loss: 3945.784
iteration 1200: loss: 3943.085
iteration 1300: loss: 3949.187
iteration 1400: loss: 3932.955
iteration 1500: loss: 3937.590
iteration 1600: loss: 3937.507
iteration 1700: loss: 3937.779
iteration 1800: loss: 3942.110
====> Epoch: 038 Train loss: 3937.9989  took : 52.85734176635742
====> Test loss: 3942.0337
iteration 0000: loss: 3949.643
iteration 0100: loss: 3933.501
iteration 0200: loss: 3937.331
iteration 0300: loss: 3930.015
iteration 0400: loss: 3938.164
iteration 0500: loss: 3935.733
iteration 0600: loss: 3945.715
iteration 0700: loss: 3935.989
iteration 0800: loss: 3938.976
iteration 0900: loss: 3938.255
iteration 1000: loss: 3938.404
iteration 1100: loss: 3937.173
iteration 1200: loss: 3935.749
iteration 1300: loss: 3936.435
iteration 1400: loss: 3938.114
iteration 1500: loss: 3938.243
iteration 1600: loss: 3936.578
iteration 1700: loss: 3938.118
iteration 1800: loss: 3938.332
====> Epoch: 039 Train loss: 3937.8129  took : 52.902832984924316
====> Test loss: 3942.2660
iteration 0000: loss: 3938.433
iteration 0100: loss: 3939.850
iteration 0200: loss: 3940.375
iteration 0300: loss: 3936.758
iteration 0400: loss: 3936.615
iteration 0500: loss: 3935.068
iteration 0600: loss: 3940.856
iteration 0700: loss: 3938.026
iteration 0800: loss: 3936.769
iteration 0900: loss: 3937.574
iteration 1000: loss: 3936.626
iteration 1100: loss: 3941.573
iteration 1200: loss: 3935.954
iteration 1300: loss: 3933.051
iteration 1400: loss: 3942.058
iteration 1500: loss: 3936.987
iteration 1600: loss: 3940.643
iteration 1700: loss: 3938.835
iteration 1800: loss: 3936.708
====> Epoch: 040 Train loss: 3937.7073  took : 52.83058500289917
====> Test loss: 3941.8135
iteration 0000: loss: 3938.629
iteration 0100: loss: 3931.620
iteration 0200: loss: 3943.041
iteration 0300: loss: 3942.477
iteration 0400: loss: 3939.680
iteration 0500: loss: 3939.531
iteration 0600: loss: 3934.882
iteration 0700: loss: 3939.595
iteration 0800: loss: 3938.807
iteration 0900: loss: 3937.033
iteration 1000: loss: 3934.582
iteration 1100: loss: 3939.847
iteration 1200: loss: 3941.363
iteration 1300: loss: 3941.651
iteration 1400: loss: 3941.623
iteration 1500: loss: 3943.864
iteration 1600: loss: 3941.319
iteration 1700: loss: 3941.925
iteration 1800: loss: 3942.674
====> Epoch: 041 Train loss: 3937.7669  took : 53.26610326766968
====> Test loss: 3942.2376
iteration 0000: loss: 3940.703
iteration 0100: loss: 3932.287
iteration 0200: loss: 3934.751
iteration 0300: loss: 3937.626
iteration 0400: loss: 3935.540
iteration 0500: loss: 3934.991
iteration 0600: loss: 3941.688
iteration 0700: loss: 3939.032
iteration 0800: loss: 3939.212
iteration 0900: loss: 3935.852
iteration 1000: loss: 3939.128
iteration 1100: loss: 3935.244
iteration 1200: loss: 3940.490
iteration 1300: loss: 3939.956
iteration 1400: loss: 3936.472
iteration 1500: loss: 3936.209
iteration 1600: loss: 3939.805
iteration 1700: loss: 3937.531
iteration 1800: loss: 3945.941
====> Epoch: 042 Train loss: 3937.7210  took : 52.910231828689575
====> Test loss: 3941.7800
iteration 0000: loss: 3937.086
iteration 0100: loss: 3938.741
iteration 0200: loss: 3938.077
iteration 0300: loss: 3941.369
iteration 0400: loss: 3939.185
iteration 0500: loss: 3944.292
iteration 0600: loss: 3938.627
iteration 0700: loss: 3937.170
iteration 0800: loss: 3942.056
iteration 0900: loss: 3934.791
iteration 1000: loss: 3943.537
iteration 1100: loss: 3938.125
iteration 1200: loss: 3936.717
iteration 1300: loss: 3933.943
iteration 1400: loss: 3937.815
iteration 1500: loss: 3932.303
iteration 1600: loss: 3934.835
iteration 1700: loss: 3940.637
iteration 1800: loss: 3940.429
====> Epoch: 043 Train loss: 3937.6889  took : 52.890297174453735
====> Test loss: 3941.6226
iteration 0000: loss: 3940.206
iteration 0100: loss: 3938.963
iteration 0200: loss: 3939.241
iteration 0300: loss: 3933.774
iteration 0400: loss: 3943.555
iteration 0500: loss: 3941.956
iteration 0600: loss: 3937.637
iteration 0700: loss: 3938.127
iteration 0800: loss: 3939.662
iteration 0900: loss: 3933.668
iteration 1000: loss: 3940.334
iteration 1100: loss: 3945.131
iteration 1200: loss: 3938.936
iteration 1300: loss: 3936.252
iteration 1400: loss: 3939.331
iteration 1500: loss: 3939.954
iteration 1600: loss: 3939.423
iteration 1700: loss: 3944.088
iteration 1800: loss: 3938.384
====> Epoch: 044 Train loss: 3937.5774  took : 53.09217381477356
====> Test loss: 3941.9331
iteration 0000: loss: 3934.135
iteration 0100: loss: 3931.606
iteration 0200: loss: 3937.060
iteration 0300: loss: 3934.700
iteration 0400: loss: 3933.889
iteration 0500: loss: 3936.617
iteration 0600: loss: 3941.615
iteration 0700: loss: 3934.682
iteration 0800: loss: 3936.562
iteration 0900: loss: 3939.522
iteration 1000: loss: 3938.023
iteration 1100: loss: 3936.531
iteration 1200: loss: 3943.623
iteration 1300: loss: 3933.161
iteration 1400: loss: 3937.151
iteration 1500: loss: 3936.232
iteration 1600: loss: 3939.797
iteration 1700: loss: 3931.639
iteration 1800: loss: 3936.680
====> Epoch: 045 Train loss: 3937.5317  took : 53.16395354270935
====> Test loss: 3941.6040
iteration 0000: loss: 3934.981
iteration 0100: loss: 3937.738
iteration 0200: loss: 3940.347
iteration 0300: loss: 3938.499
iteration 0400: loss: 3940.734
iteration 0500: loss: 3942.000
iteration 0600: loss: 3941.863
iteration 0700: loss: 3931.570
iteration 0800: loss: 3938.417
iteration 0900: loss: 3945.435
iteration 1000: loss: 3932.336
iteration 1100: loss: 3937.009
iteration 1200: loss: 3943.358
iteration 1300: loss: 3933.851
iteration 1400: loss: 3944.594
iteration 1500: loss: 3931.175
iteration 1600: loss: 3937.764
iteration 1700: loss: 3933.855
iteration 1800: loss: 3937.410
====> Epoch: 046 Train loss: 3937.5211  took : 53.03591513633728
====> Test loss: 3941.3652
iteration 0000: loss: 3938.993
iteration 0100: loss: 3938.077
iteration 0200: loss: 3936.778
iteration 0300: loss: 3934.476
iteration 0400: loss: 3944.787
iteration 0500: loss: 3937.340
iteration 0600: loss: 3933.064
iteration 0700: loss: 3938.005
iteration 0800: loss: 3938.318
iteration 0900: loss: 3938.463
iteration 1000: loss: 3935.250
iteration 1100: loss: 3932.719
iteration 1200: loss: 3933.662
iteration 1300: loss: 3930.443
iteration 1400: loss: 3936.747
iteration 1500: loss: 3939.019
iteration 1600: loss: 3939.015
iteration 1700: loss: 3941.203
iteration 1800: loss: 3942.959
====> Epoch: 047 Train loss: 3937.4722  took : 52.8685564994812
====> Test loss: 3941.8943
iteration 0000: loss: 3932.917
iteration 0100: loss: 3932.977
iteration 0200: loss: 3937.836
iteration 0300: loss: 3936.329
iteration 0400: loss: 3940.951
iteration 0500: loss: 3936.119
iteration 0600: loss: 3938.304
iteration 0700: loss: 3937.011
iteration 0800: loss: 3937.990
iteration 0900: loss: 3938.428
iteration 1000: loss: 3946.320
iteration 1100: loss: 3936.354
iteration 1200: loss: 3934.508
iteration 1300: loss: 3935.398
iteration 1400: loss: 3937.051
iteration 1500: loss: 3938.838
iteration 1600: loss: 3938.296
iteration 1700: loss: 3938.251
iteration 1800: loss: 3936.508
====> Epoch: 048 Train loss: 3937.3267  took : 53.13337707519531
====> Test loss: 3941.8556
iteration 0000: loss: 3934.785
iteration 0100: loss: 3941.635
iteration 0200: loss: 3940.601
iteration 0300: loss: 3936.955
iteration 0400: loss: 3939.547
iteration 0500: loss: 3940.625
iteration 0600: loss: 3942.592
iteration 0700: loss: 3940.994
iteration 0800: loss: 3934.469
iteration 0900: loss: 3937.525
iteration 1000: loss: 3940.060
iteration 1100: loss: 3933.189
iteration 1200: loss: 3936.164
iteration 1300: loss: 3946.187
iteration 1400: loss: 3937.381
iteration 1500: loss: 3938.926
iteration 1600: loss: 3942.821
iteration 1700: loss: 3936.668
iteration 1800: loss: 3935.524
====> Epoch: 049 Train loss: 3937.3940  took : 53.07321214675903
====> Test loss: 3942.4813
iteration 0000: loss: 3934.869
iteration 0100: loss: 3942.788
iteration 0200: loss: 3940.269
iteration 0300: loss: 3939.938
iteration 0400: loss: 3941.286
iteration 0500: loss: 3941.999
iteration 0600: loss: 3937.122
iteration 0700: loss: 3928.789
iteration 0800: loss: 3938.864
iteration 0900: loss: 3936.352
iteration 1000: loss: 3937.175
iteration 1100: loss: 3934.689
iteration 1200: loss: 3936.170
iteration 1300: loss: 3936.272
iteration 1400: loss: 3941.946
iteration 1500: loss: 3932.586
iteration 1600: loss: 3935.445
iteration 1700: loss: 3940.735
iteration 1800: loss: 3937.044
====> Epoch: 050 Train loss: 3937.3568  took : 53.045416831970215
====> Test loss: 3942.7080
====> [MM-VAE] Time: 3167.973s or 00:52:47
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  14
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_14
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_14
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1988.131
iteration 0100: loss: 1566.455
iteration 0200: loss: 1566.222
iteration 0300: loss: 1560.656
iteration 0400: loss: 1552.312
iteration 0500: loss: 1550.939
iteration 0600: loss: 1549.153
iteration 0700: loss: 1538.635
iteration 0800: loss: 1537.192
iteration 0900: loss: 1541.589
====> Epoch: 001 Train loss: 1555.6449  took : 8.513992547988892
====> Test loss: 1538.7125
iteration 0000: loss: 1534.944
iteration 0100: loss: 1534.929
iteration 0200: loss: 1537.741
iteration 0300: loss: 1530.529
iteration 0400: loss: 1536.850
iteration 0500: loss: 1527.584
iteration 0600: loss: 1528.581
iteration 0700: loss: 1534.014
iteration 0800: loss: 1528.439
iteration 0900: loss: 1525.704
====> Epoch: 002 Train loss: 1531.7291  took : 8.489015817642212
====> Test loss: 1529.7131
iteration 0000: loss: 1526.544
iteration 0100: loss: 1525.195
iteration 0200: loss: 1523.429
iteration 0300: loss: 1523.604
iteration 0400: loss: 1523.602
iteration 0500: loss: 1527.886
iteration 0600: loss: 1521.367
iteration 0700: loss: 1527.201
iteration 0800: loss: 1525.066
iteration 0900: loss: 1524.175
====> Epoch: 003 Train loss: 1525.9517  took : 8.50520658493042
====> Test loss: 1526.7808
iteration 0000: loss: 1527.321
iteration 0100: loss: 1522.153
iteration 0200: loss: 1524.229
iteration 0300: loss: 1525.410
iteration 0400: loss: 1519.497
iteration 0500: loss: 1525.452
iteration 0600: loss: 1526.305
iteration 0700: loss: 1523.620
iteration 0800: loss: 1522.276
iteration 0900: loss: 1523.016
====> Epoch: 004 Train loss: 1523.4962  took : 8.426343202590942
====> Test loss: 1525.0070
iteration 0000: loss: 1520.160
iteration 0100: loss: 1520.875
iteration 0200: loss: 1518.545
iteration 0300: loss: 1523.074
iteration 0400: loss: 1520.955
iteration 0500: loss: 1522.496
iteration 0600: loss: 1524.187
iteration 0700: loss: 1518.951
iteration 0800: loss: 1518.851
iteration 0900: loss: 1520.529
====> Epoch: 005 Train loss: 1521.9501  took : 8.44546389579773
====> Test loss: 1523.8057
iteration 0000: loss: 1520.024
iteration 0100: loss: 1522.303
iteration 0200: loss: 1519.990
iteration 0300: loss: 1519.291
iteration 0400: loss: 1520.467
iteration 0500: loss: 1519.698
iteration 0600: loss: 1522.175
iteration 0700: loss: 1521.124
iteration 0800: loss: 1522.823
iteration 0900: loss: 1519.820
====> Epoch: 006 Train loss: 1520.8883  took : 8.445173025131226
====> Test loss: 1522.9455
iteration 0000: loss: 1521.330
iteration 0100: loss: 1522.644
iteration 0200: loss: 1524.219
iteration 0300: loss: 1520.135
iteration 0400: loss: 1521.644
iteration 0500: loss: 1520.150
iteration 0600: loss: 1520.044
iteration 0700: loss: 1522.095
iteration 0800: loss: 1517.123
iteration 0900: loss: 1520.479
====> Epoch: 007 Train loss: 1520.0178  took : 8.479583501815796
====> Test loss: 1522.3252
iteration 0000: loss: 1517.475
iteration 0100: loss: 1518.072
iteration 0200: loss: 1517.371
iteration 0300: loss: 1518.596
iteration 0400: loss: 1518.460
iteration 0500: loss: 1521.430
iteration 0600: loss: 1518.088
iteration 0700: loss: 1519.953
iteration 0800: loss: 1521.151
iteration 0900: loss: 1519.251
====> Epoch: 008 Train loss: 1519.3926  took : 8.428271532058716
====> Test loss: 1521.8414
iteration 0000: loss: 1521.117
iteration 0100: loss: 1518.698
iteration 0200: loss: 1517.708
iteration 0300: loss: 1519.008
iteration 0400: loss: 1516.564
iteration 0500: loss: 1520.101
iteration 0600: loss: 1522.308
iteration 0700: loss: 1519.673
iteration 0800: loss: 1518.145
iteration 0900: loss: 1519.026
====> Epoch: 009 Train loss: 1518.8474  took : 8.406766414642334
====> Test loss: 1521.2635
iteration 0000: loss: 1516.360
iteration 0100: loss: 1519.114
iteration 0200: loss: 1519.522
iteration 0300: loss: 1519.013
iteration 0400: loss: 1522.136
iteration 0500: loss: 1518.281
iteration 0600: loss: 1519.967
iteration 0700: loss: 1518.734
iteration 0800: loss: 1516.515
iteration 0900: loss: 1518.092
====> Epoch: 010 Train loss: 1518.4062  took : 8.513344764709473
====> Test loss: 1520.8388
iteration 0000: loss: 1514.707
iteration 0100: loss: 1517.107
iteration 0200: loss: 1521.201
iteration 0300: loss: 1516.515
iteration 0400: loss: 1517.503
iteration 0500: loss: 1517.217
iteration 0600: loss: 1518.770
iteration 0700: loss: 1517.744
iteration 0800: loss: 1515.131
iteration 0900: loss: 1518.992
====> Epoch: 011 Train loss: 1518.0330  took : 8.508697032928467
====> Test loss: 1520.6562
iteration 0000: loss: 1519.581
iteration 0100: loss: 1517.591
iteration 0200: loss: 1518.167
iteration 0300: loss: 1519.518
iteration 0400: loss: 1518.142
iteration 0500: loss: 1515.192
iteration 0600: loss: 1517.745
iteration 0700: loss: 1520.792
iteration 0800: loss: 1517.851
iteration 0900: loss: 1515.362
====> Epoch: 012 Train loss: 1517.7351  took : 8.480010509490967
====> Test loss: 1520.5613
iteration 0000: loss: 1516.189
iteration 0100: loss: 1519.803
iteration 0200: loss: 1515.450
iteration 0300: loss: 1515.058
iteration 0400: loss: 1518.294
iteration 0500: loss: 1517.869
iteration 0600: loss: 1517.475
iteration 0700: loss: 1518.644
iteration 0800: loss: 1516.636
iteration 0900: loss: 1515.429
====> Epoch: 013 Train loss: 1517.4811  took : 8.50343656539917
====> Test loss: 1520.2106
iteration 0000: loss: 1514.373
iteration 0100: loss: 1521.766
iteration 0200: loss: 1518.797
iteration 0300: loss: 1519.597
iteration 0400: loss: 1513.850
iteration 0500: loss: 1518.803
iteration 0600: loss: 1517.974
iteration 0700: loss: 1519.598
iteration 0800: loss: 1518.041
iteration 0900: loss: 1521.396
====> Epoch: 014 Train loss: 1517.2915  took : 8.536802291870117
====> Test loss: 1520.0172
iteration 0000: loss: 1516.221
iteration 0100: loss: 1519.414
iteration 0200: loss: 1516.250
iteration 0300: loss: 1519.607
iteration 0400: loss: 1515.814
iteration 0500: loss: 1518.518
iteration 0600: loss: 1515.728
iteration 0700: loss: 1516.856
iteration 0800: loss: 1517.194
iteration 0900: loss: 1515.045
====> Epoch: 015 Train loss: 1517.0840  took : 8.472991704940796
====> Test loss: 1520.0414
iteration 0000: loss: 1516.606
iteration 0100: loss: 1514.862
iteration 0200: loss: 1517.917
iteration 0300: loss: 1516.494
iteration 0400: loss: 1514.565
iteration 0500: loss: 1514.804
iteration 0600: loss: 1515.923
iteration 0700: loss: 1516.976
iteration 0800: loss: 1515.160
iteration 0900: loss: 1519.273
====> Epoch: 016 Train loss: 1516.8883  took : 8.48634648323059
====> Test loss: 1519.7056
iteration 0000: loss: 1515.586
iteration 0100: loss: 1518.388
iteration 0200: loss: 1515.617
iteration 0300: loss: 1517.025
iteration 0400: loss: 1517.797
iteration 0500: loss: 1516.644
iteration 0600: loss: 1516.151
iteration 0700: loss: 1519.114
iteration 0800: loss: 1516.835
iteration 0900: loss: 1514.517
====> Epoch: 017 Train loss: 1516.7495  took : 8.522643089294434
====> Test loss: 1519.7249
iteration 0000: loss: 1516.520
iteration 0100: loss: 1520.060
iteration 0200: loss: 1516.547
iteration 0300: loss: 1514.649
iteration 0400: loss: 1514.131
iteration 0500: loss: 1515.721
iteration 0600: loss: 1515.022
iteration 0700: loss: 1518.300
iteration 0800: loss: 1513.120
iteration 0900: loss: 1519.424
====> Epoch: 018 Train loss: 1516.6107  took : 8.396504640579224
====> Test loss: 1519.5063
iteration 0000: loss: 1519.696
iteration 0100: loss: 1517.590
iteration 0200: loss: 1512.561
iteration 0300: loss: 1515.157
iteration 0400: loss: 1516.056
iteration 0500: loss: 1515.809
iteration 0600: loss: 1514.853
iteration 0700: loss: 1518.698
iteration 0800: loss: 1517.689
iteration 0900: loss: 1517.453
====> Epoch: 019 Train loss: 1516.4843  took : 8.518275499343872
====> Test loss: 1519.3824
iteration 0000: loss: 1519.079
iteration 0100: loss: 1512.628
iteration 0200: loss: 1516.168
iteration 0300: loss: 1516.698
iteration 0400: loss: 1514.254
iteration 0500: loss: 1514.764
iteration 0600: loss: 1517.794
iteration 0700: loss: 1517.192
iteration 0800: loss: 1516.729
iteration 0900: loss: 1513.588
====> Epoch: 020 Train loss: 1516.3600  took : 8.41356635093689
====> Test loss: 1519.2848
iteration 0000: loss: 1516.804
iteration 0100: loss: 1516.780
iteration 0200: loss: 1517.359
iteration 0300: loss: 1514.649
iteration 0400: loss: 1516.341
iteration 0500: loss: 1516.169
iteration 0600: loss: 1512.769
iteration 0700: loss: 1518.172
iteration 0800: loss: 1515.007
iteration 0900: loss: 1515.690
====> Epoch: 021 Train loss: 1516.2388  took : 8.500176668167114
====> Test loss: 1519.3554
iteration 0000: loss: 1516.190
iteration 0100: loss: 1512.108
iteration 0200: loss: 1517.136
iteration 0300: loss: 1516.848
iteration 0400: loss: 1514.190
iteration 0500: loss: 1518.780
iteration 0600: loss: 1515.055
iteration 0700: loss: 1517.413
iteration 0800: loss: 1515.667
iteration 0900: loss: 1515.251
====> Epoch: 022 Train loss: 1516.1347  took : 8.381277799606323
====> Test loss: 1519.2428
iteration 0000: loss: 1517.954
iteration 0100: loss: 1515.846
iteration 0200: loss: 1515.661
iteration 0300: loss: 1515.434
iteration 0400: loss: 1517.676
iteration 0500: loss: 1516.183
iteration 0600: loss: 1518.102
iteration 0700: loss: 1516.541
iteration 0800: loss: 1515.030
iteration 0900: loss: 1514.471
====> Epoch: 023 Train loss: 1516.0428  took : 8.509544849395752
====> Test loss: 1519.2561
iteration 0000: loss: 1519.137
iteration 0100: loss: 1517.040
iteration 0200: loss: 1514.353
iteration 0300: loss: 1514.680
iteration 0400: loss: 1516.283
iteration 0500: loss: 1514.342
iteration 0600: loss: 1517.015
iteration 0700: loss: 1514.657
iteration 0800: loss: 1518.927
iteration 0900: loss: 1515.810
====> Epoch: 024 Train loss: 1515.9632  took : 8.526347637176514
====> Test loss: 1519.3214
iteration 0000: loss: 1518.452
iteration 0100: loss: 1517.394
iteration 0200: loss: 1517.007
iteration 0300: loss: 1517.082
iteration 0400: loss: 1516.195
iteration 0500: loss: 1515.155
iteration 0600: loss: 1517.524
iteration 0700: loss: 1515.789
iteration 0800: loss: 1517.095
iteration 0900: loss: 1517.622
====> Epoch: 025 Train loss: 1515.8906  took : 8.533638954162598
====> Test loss: 1519.0008
iteration 0000: loss: 1515.131
iteration 0100: loss: 1514.485
iteration 0200: loss: 1513.979
iteration 0300: loss: 1515.086
iteration 0400: loss: 1516.948
iteration 0500: loss: 1512.575
iteration 0600: loss: 1515.395
iteration 0700: loss: 1516.487
iteration 0800: loss: 1517.037
iteration 0900: loss: 1514.317
====> Epoch: 026 Train loss: 1515.7905  took : 8.538389444351196
====> Test loss: 1518.9837
iteration 0000: loss: 1514.956
iteration 0100: loss: 1514.014
iteration 0200: loss: 1514.496
iteration 0300: loss: 1514.304
iteration 0400: loss: 1516.601
iteration 0500: loss: 1515.486
iteration 0600: loss: 1516.199
iteration 0700: loss: 1516.130
iteration 0800: loss: 1514.060
iteration 0900: loss: 1516.793
====> Epoch: 027 Train loss: 1515.7391  took : 8.539131164550781
====> Test loss: 1519.0492
iteration 0000: loss: 1514.240
iteration 0100: loss: 1516.958
iteration 0200: loss: 1513.690
iteration 0300: loss: 1518.518
iteration 0400: loss: 1514.219
iteration 0500: loss: 1514.538
iteration 0600: loss: 1517.496
iteration 0700: loss: 1512.573
iteration 0800: loss: 1514.218
iteration 0900: loss: 1513.762
====> Epoch: 028 Train loss: 1515.6280  took : 8.41503643989563
====> Test loss: 1518.9164
iteration 0000: loss: 1516.855
iteration 0100: loss: 1513.712
iteration 0200: loss: 1514.350
iteration 0300: loss: 1517.387
iteration 0400: loss: 1515.140
iteration 0500: loss: 1516.984
iteration 0600: loss: 1517.135
iteration 0700: loss: 1516.444
iteration 0800: loss: 1516.016
iteration 0900: loss: 1517.695
====> Epoch: 029 Train loss: 1515.5576  took : 8.489577293395996
====> Test loss: 1518.8563
iteration 0000: loss: 1515.565
iteration 0100: loss: 1513.939
iteration 0200: loss: 1514.001
iteration 0300: loss: 1515.220
iteration 0400: loss: 1515.774
iteration 0500: loss: 1516.856
iteration 0600: loss: 1514.243
iteration 0700: loss: 1516.497
iteration 0800: loss: 1518.097
iteration 0900: loss: 1517.715
====> Epoch: 030 Train loss: 1515.5202  took : 8.421755075454712
====> Test loss: 1518.8482
iteration 0000: loss: 1516.766
iteration 0100: loss: 1516.468
iteration 0200: loss: 1516.605
iteration 0300: loss: 1514.885
iteration 0400: loss: 1516.174
iteration 0500: loss: 1515.930
iteration 0600: loss: 1513.185
iteration 0700: loss: 1514.393
iteration 0800: loss: 1512.547
iteration 0900: loss: 1514.759
====> Epoch: 031 Train loss: 1515.4553  took : 8.476932525634766
====> Test loss: 1518.8125
iteration 0000: loss: 1518.102
iteration 0100: loss: 1515.900
iteration 0200: loss: 1515.747
iteration 0300: loss: 1515.562
iteration 0400: loss: 1514.527
iteration 0500: loss: 1515.013
iteration 0600: loss: 1513.399
iteration 0700: loss: 1516.115
iteration 0800: loss: 1518.685
iteration 0900: loss: 1515.163
====> Epoch: 032 Train loss: 1515.3638  took : 8.504345893859863
====> Test loss: 1518.6187
iteration 0000: loss: 1513.490
iteration 0100: loss: 1517.613
iteration 0200: loss: 1515.306
iteration 0300: loss: 1517.189
iteration 0400: loss: 1513.994
iteration 0500: loss: 1513.702
iteration 0600: loss: 1516.002
iteration 0700: loss: 1515.780
iteration 0800: loss: 1515.914
iteration 0900: loss: 1515.390
====> Epoch: 033 Train loss: 1515.2959  took : 8.491330862045288
====> Test loss: 1518.7021
iteration 0000: loss: 1514.030
iteration 0100: loss: 1515.579
iteration 0200: loss: 1519.167
iteration 0300: loss: 1513.783
iteration 0400: loss: 1516.931
iteration 0500: loss: 1513.329
iteration 0600: loss: 1516.329
iteration 0700: loss: 1518.534
iteration 0800: loss: 1514.840
iteration 0900: loss: 1515.906
====> Epoch: 034 Train loss: 1515.3088  took : 8.510416984558105
====> Test loss: 1518.8411
iteration 0000: loss: 1513.326
iteration 0100: loss: 1515.052
iteration 0200: loss: 1515.813
iteration 0300: loss: 1515.177
iteration 0400: loss: 1515.195
iteration 0500: loss: 1515.831
iteration 0600: loss: 1514.578
iteration 0700: loss: 1514.369
iteration 0800: loss: 1512.943
iteration 0900: loss: 1513.941
====> Epoch: 035 Train loss: 1515.1883  took : 8.4188814163208
====> Test loss: 1518.6018
iteration 0000: loss: 1513.782
iteration 0100: loss: 1513.279
iteration 0200: loss: 1518.667
iteration 0300: loss: 1516.255
iteration 0400: loss: 1514.616
iteration 0500: loss: 1515.147
iteration 0600: loss: 1514.201
iteration 0700: loss: 1515.714
iteration 0800: loss: 1515.671
iteration 0900: loss: 1515.802
====> Epoch: 036 Train loss: 1515.1424  took : 8.422346353530884
====> Test loss: 1518.8109
iteration 0000: loss: 1516.402
iteration 0100: loss: 1513.438
iteration 0200: loss: 1514.340
iteration 0300: loss: 1517.615
iteration 0400: loss: 1515.331
iteration 0500: loss: 1513.396
iteration 0600: loss: 1512.651
iteration 0700: loss: 1518.783
iteration 0800: loss: 1514.740
iteration 0900: loss: 1513.772
====> Epoch: 037 Train loss: 1515.0850  took : 8.515769243240356
====> Test loss: 1518.6632
iteration 0000: loss: 1513.163
iteration 0100: loss: 1513.718
iteration 0200: loss: 1513.011
iteration 0300: loss: 1514.420
iteration 0400: loss: 1511.956
iteration 0500: loss: 1517.916
iteration 0600: loss: 1512.324
iteration 0700: loss: 1515.181
iteration 0800: loss: 1516.890
iteration 0900: loss: 1513.282
====> Epoch: 038 Train loss: 1515.0377  took : 8.548537492752075
====> Test loss: 1518.5424
iteration 0000: loss: 1514.319
iteration 0100: loss: 1514.046
iteration 0200: loss: 1513.289
iteration 0300: loss: 1515.771
iteration 0400: loss: 1514.643
iteration 0500: loss: 1513.470
iteration 0600: loss: 1515.580
iteration 0700: loss: 1513.360
iteration 0800: loss: 1513.187
iteration 0900: loss: 1516.217
====> Epoch: 039 Train loss: 1514.9719  took : 8.444544553756714
====> Test loss: 1518.6906
iteration 0000: loss: 1514.252
iteration 0100: loss: 1513.755
iteration 0200: loss: 1513.721
iteration 0300: loss: 1513.186
iteration 0400: loss: 1513.906
iteration 0500: loss: 1514.931
iteration 0600: loss: 1513.269
iteration 0700: loss: 1515.719
iteration 0800: loss: 1516.528
iteration 0900: loss: 1514.976
====> Epoch: 040 Train loss: 1514.9533  took : 8.532992124557495
====> Test loss: 1518.4910
iteration 0000: loss: 1515.165
iteration 0100: loss: 1513.275
iteration 0200: loss: 1514.970
iteration 0300: loss: 1515.100
iteration 0400: loss: 1515.131
iteration 0500: loss: 1515.519
iteration 0600: loss: 1515.440
iteration 0700: loss: 1515.571
iteration 0800: loss: 1512.719
iteration 0900: loss: 1514.696
====> Epoch: 041 Train loss: 1514.9099  took : 8.55239224433899
====> Test loss: 1518.6034
iteration 0000: loss: 1512.392
iteration 0100: loss: 1515.207
iteration 0200: loss: 1513.658
iteration 0300: loss: 1514.595
iteration 0400: loss: 1514.447
iteration 0500: loss: 1513.267
iteration 0600: loss: 1515.974
iteration 0700: loss: 1517.267
iteration 0800: loss: 1511.816
iteration 0900: loss: 1514.106
====> Epoch: 042 Train loss: 1514.8522  took : 8.75684380531311
====> Test loss: 1518.4000
iteration 0000: loss: 1514.318
iteration 0100: loss: 1512.189
iteration 0200: loss: 1514.536
iteration 0300: loss: 1515.247
iteration 0400: loss: 1514.809
iteration 0500: loss: 1517.648
iteration 0600: loss: 1516.766
iteration 0700: loss: 1515.355
iteration 0800: loss: 1516.361
iteration 0900: loss: 1516.572
====> Epoch: 043 Train loss: 1514.8193  took : 8.477742671966553
====> Test loss: 1518.4802
iteration 0000: loss: 1513.953
iteration 0100: loss: 1515.200
iteration 0200: loss: 1516.998
iteration 0300: loss: 1515.458
iteration 0400: loss: 1514.693
iteration 0500: loss: 1514.870
iteration 0600: loss: 1511.660
iteration 0700: loss: 1517.186
iteration 0800: loss: 1514.489
iteration 0900: loss: 1514.166
====> Epoch: 044 Train loss: 1514.7919  took : 8.502983570098877
====> Test loss: 1518.3568
iteration 0000: loss: 1513.700
iteration 0100: loss: 1516.636
iteration 0200: loss: 1514.946
iteration 0300: loss: 1515.486
iteration 0400: loss: 1514.398
iteration 0500: loss: 1515.751
iteration 0600: loss: 1514.831
iteration 0700: loss: 1518.971
iteration 0800: loss: 1516.119
iteration 0900: loss: 1514.215
====> Epoch: 045 Train loss: 1514.7345  took : 8.453758001327515
====> Test loss: 1518.4626
iteration 0000: loss: 1515.829
iteration 0100: loss: 1513.731
iteration 0200: loss: 1516.774
iteration 0300: loss: 1514.784
iteration 0400: loss: 1515.011
iteration 0500: loss: 1514.995
iteration 0600: loss: 1514.989
iteration 0700: loss: 1517.326
iteration 0800: loss: 1514.641
iteration 0900: loss: 1515.910
====> Epoch: 046 Train loss: 1514.6999  took : 8.48104190826416
====> Test loss: 1518.3541
iteration 0000: loss: 1513.045
iteration 0100: loss: 1514.449
iteration 0200: loss: 1513.867
iteration 0300: loss: 1513.574
iteration 0400: loss: 1514.603
iteration 0500: loss: 1514.698
iteration 0600: loss: 1514.208
iteration 0700: loss: 1515.104
iteration 0800: loss: 1514.064
iteration 0900: loss: 1514.252
====> Epoch: 047 Train loss: 1514.6532  took : 8.450461149215698
====> Test loss: 1518.3702
iteration 0000: loss: 1512.050
iteration 0100: loss: 1512.934
iteration 0200: loss: 1513.709
iteration 0300: loss: 1513.022
iteration 0400: loss: 1510.665
iteration 0500: loss: 1513.353
iteration 0600: loss: 1513.104
iteration 0700: loss: 1514.618
iteration 0800: loss: 1514.671
iteration 0900: loss: 1515.776
====> Epoch: 048 Train loss: 1514.6318  took : 8.480485439300537
====> Test loss: 1518.3335
iteration 0000: loss: 1516.077
iteration 0100: loss: 1514.687
iteration 0200: loss: 1514.825
iteration 0300: loss: 1513.576
iteration 0400: loss: 1514.656
iteration 0500: loss: 1516.056
iteration 0600: loss: 1513.793
iteration 0700: loss: 1515.995
iteration 0800: loss: 1512.484
iteration 0900: loss: 1512.855
====> Epoch: 049 Train loss: 1514.6009  took : 8.407631635665894
====> Test loss: 1518.3094
iteration 0000: loss: 1511.907
iteration 0100: loss: 1513.089
iteration 0200: loss: 1515.661
iteration 0300: loss: 1514.827
iteration 0400: loss: 1512.844
iteration 0500: loss: 1513.771
iteration 0600: loss: 1514.100
iteration 0700: loss: 1516.765
iteration 0800: loss: 1514.796
iteration 0900: loss: 1513.685
====> Epoch: 050 Train loss: 1514.5561  took : 8.4289071559906
====> Test loss: 1518.1596
====> [MM-VAE] Time: 508.082s or 00:08:28
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  14
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_14
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_14
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.663
iteration 0100: loss: 2098.463
iteration 0200: loss: 2046.601
iteration 0300: loss: 2018.222
iteration 0400: loss: 1999.066
iteration 0500: loss: 1998.114
iteration 0600: loss: 2001.283
iteration 0700: loss: 1990.128
iteration 0800: loss: 1992.058
iteration 0900: loss: 1990.575
====> Epoch: 001 Train loss: 2020.4708  took : 13.640060901641846
====> Test loss: 1991.7777
iteration 0000: loss: 1987.741
iteration 0100: loss: 1982.729
iteration 0200: loss: 1988.229
iteration 0300: loss: 1984.097
iteration 0400: loss: 1984.795
iteration 0500: loss: 1984.616
iteration 0600: loss: 1982.047
iteration 0700: loss: 1981.009
iteration 0800: loss: 1979.172
iteration 0900: loss: 1972.535
====> Epoch: 002 Train loss: 1981.7850  took : 13.413116693496704
====> Test loss: 1978.2371
iteration 0000: loss: 1975.505
iteration 0100: loss: 1976.224
iteration 0200: loss: 1975.356
iteration 0300: loss: 1971.803
iteration 0400: loss: 1971.121
iteration 0500: loss: 1971.154
iteration 0600: loss: 1975.155
iteration 0700: loss: 1969.338
iteration 0800: loss: 1968.413
iteration 0900: loss: 1974.268
====> Epoch: 003 Train loss: 1972.6765  took : 13.127925872802734
====> Test loss: 1972.1695
iteration 0000: loss: 1970.746
iteration 0100: loss: 1972.391
iteration 0200: loss: 1972.249
iteration 0300: loss: 1963.083
iteration 0400: loss: 1963.428
iteration 0500: loss: 1963.788
iteration 0600: loss: 1961.608
iteration 0700: loss: 1966.106
iteration 0800: loss: 1963.361
iteration 0900: loss: 1961.865
====> Epoch: 004 Train loss: 1965.0852  took : 11.97750449180603
====> Test loss: 1963.8015
iteration 0000: loss: 1959.587
iteration 0100: loss: 1959.663
iteration 0200: loss: 1960.587
iteration 0300: loss: 1957.778
iteration 0400: loss: 1958.578
iteration 0500: loss: 1960.271
iteration 0600: loss: 1956.185
iteration 0700: loss: 1956.627
iteration 0800: loss: 1958.895
iteration 0900: loss: 1957.149
====> Epoch: 005 Train loss: 1959.0075  took : 12.792004108428955
====> Test loss: 1960.4749
iteration 0000: loss: 1956.966
iteration 0100: loss: 1955.847
iteration 0200: loss: 1958.121
iteration 0300: loss: 1953.832
iteration 0400: loss: 1953.346
iteration 0500: loss: 1953.993
iteration 0600: loss: 1955.630
iteration 0700: loss: 1952.609
iteration 0800: loss: 1952.469
iteration 0900: loss: 1952.467
====> Epoch: 006 Train loss: 1955.6316  took : 12.94113826751709
====> Test loss: 1956.2718
iteration 0000: loss: 1952.959
iteration 0100: loss: 1952.426
iteration 0200: loss: 1954.553
iteration 0300: loss: 1953.335
iteration 0400: loss: 1954.852
iteration 0500: loss: 1953.798
iteration 0600: loss: 1953.638
iteration 0700: loss: 1954.932
iteration 0800: loss: 1954.901
iteration 0900: loss: 1953.357
====> Epoch: 007 Train loss: 1953.3632  took : 12.281894207000732
====> Test loss: 1955.1366
iteration 0000: loss: 1952.009
iteration 0100: loss: 1951.485
iteration 0200: loss: 1952.872
iteration 0300: loss: 1952.988
iteration 0400: loss: 1953.472
iteration 0500: loss: 1954.620
iteration 0600: loss: 1953.302
iteration 0700: loss: 1953.685
iteration 0800: loss: 1953.519
iteration 0900: loss: 1951.770
====> Epoch: 008 Train loss: 1952.5609  took : 13.349683046340942
====> Test loss: 1954.2816
iteration 0000: loss: 1950.226
iteration 0100: loss: 1950.623
iteration 0200: loss: 1952.876
iteration 0300: loss: 1951.023
iteration 0400: loss: 1951.955
iteration 0500: loss: 1952.434
iteration 0600: loss: 1952.186
iteration 0700: loss: 1950.860
iteration 0800: loss: 1951.517
iteration 0900: loss: 1951.700
====> Epoch: 009 Train loss: 1951.9473  took : 13.340089797973633
====> Test loss: 1953.9783
iteration 0000: loss: 1951.039
iteration 0100: loss: 1951.495
iteration 0200: loss: 1952.131
iteration 0300: loss: 1950.593
iteration 0400: loss: 1951.461
iteration 0500: loss: 1950.009
iteration 0600: loss: 1953.021
iteration 0700: loss: 1950.660
iteration 0800: loss: 1951.639
iteration 0900: loss: 1949.867
====> Epoch: 010 Train loss: 1951.5392  took : 13.200818538665771
====> Test loss: 1953.6177
iteration 0000: loss: 1952.168
iteration 0100: loss: 1951.337
iteration 0200: loss: 1951.720
iteration 0300: loss: 1950.412
iteration 0400: loss: 1949.962
iteration 0500: loss: 1951.561
iteration 0600: loss: 1950.564
iteration 0700: loss: 1951.802
iteration 0800: loss: 1951.381
iteration 0900: loss: 1951.714
====> Epoch: 011 Train loss: 1951.2847  took : 11.870150327682495
====> Test loss: 1953.0826
iteration 0000: loss: 1951.062
iteration 0100: loss: 1951.495
iteration 0200: loss: 1950.750
iteration 0300: loss: 1952.145
iteration 0400: loss: 1950.920
iteration 0500: loss: 1950.912
iteration 0600: loss: 1949.798
iteration 0700: loss: 1951.016
iteration 0800: loss: 1951.008
iteration 0900: loss: 1950.018
====> Epoch: 012 Train loss: 1950.8461  took : 12.108073949813843
====> Test loss: 1952.4321
iteration 0000: loss: 1950.023
iteration 0100: loss: 1950.351
iteration 0200: loss: 1950.512
iteration 0300: loss: 1950.857
iteration 0400: loss: 1951.791
iteration 0500: loss: 1949.882
iteration 0600: loss: 1950.579
iteration 0700: loss: 1949.981
iteration 0800: loss: 1951.140
iteration 0900: loss: 1949.297
====> Epoch: 013 Train loss: 1950.4462  took : 11.944459199905396
====> Test loss: 1952.3219
iteration 0000: loss: 1950.033
iteration 0100: loss: 1948.622
iteration 0200: loss: 1950.352
iteration 0300: loss: 1950.767
iteration 0400: loss: 1949.936
iteration 0500: loss: 1950.190
iteration 0600: loss: 1949.947
iteration 0700: loss: 1950.035
iteration 0800: loss: 1950.137
iteration 0900: loss: 1949.521
====> Epoch: 014 Train loss: 1950.1890  took : 11.687483549118042
====> Test loss: 1952.1319
iteration 0000: loss: 1948.827
iteration 0100: loss: 1950.616
iteration 0200: loss: 1949.215
iteration 0300: loss: 1949.987
iteration 0400: loss: 1949.093
iteration 0500: loss: 1949.664
iteration 0600: loss: 1950.092
iteration 0700: loss: 1948.842
iteration 0800: loss: 1949.690
iteration 0900: loss: 1949.386
====> Epoch: 015 Train loss: 1949.8574  took : 12.010608911514282
====> Test loss: 1951.7496
iteration 0000: loss: 1949.817
iteration 0100: loss: 1949.563
iteration 0200: loss: 1950.799
iteration 0300: loss: 1950.951
iteration 0400: loss: 1949.995
iteration 0500: loss: 1950.186
iteration 0600: loss: 1949.600
iteration 0700: loss: 1949.688
iteration 0800: loss: 1948.105
iteration 0900: loss: 1948.612
====> Epoch: 016 Train loss: 1949.8436  took : 13.234518766403198
====> Test loss: 1951.5178
iteration 0000: loss: 1948.997
iteration 0100: loss: 1949.702
iteration 0200: loss: 1949.667
iteration 0300: loss: 1950.652
iteration 0400: loss: 1950.657
iteration 0500: loss: 1951.816
iteration 0600: loss: 1949.763
iteration 0700: loss: 1949.105
iteration 0800: loss: 1948.671
iteration 0900: loss: 1953.323
====> Epoch: 017 Train loss: 1949.6500  took : 12.952484846115112
====> Test loss: 1952.1062
iteration 0000: loss: 1950.655
iteration 0100: loss: 1949.203
iteration 0200: loss: 1948.794
iteration 0300: loss: 1951.115
iteration 0400: loss: 1950.230
iteration 0500: loss: 1948.500
iteration 0600: loss: 1949.175
iteration 0700: loss: 1950.408
iteration 0800: loss: 1949.259
iteration 0900: loss: 1948.362
====> Epoch: 018 Train loss: 1949.4608  took : 12.18644380569458
====> Test loss: 1951.5551
iteration 0000: loss: 1949.142
iteration 0100: loss: 1948.672
iteration 0200: loss: 1951.267
iteration 0300: loss: 1950.287
iteration 0400: loss: 1950.850
iteration 0500: loss: 1948.179
iteration 0600: loss: 1949.052
iteration 0700: loss: 1949.167
iteration 0800: loss: 1948.223
iteration 0900: loss: 1949.451
====> Epoch: 019 Train loss: 1949.4522  took : 13.135492086410522
====> Test loss: 1951.2949
iteration 0000: loss: 1949.865
iteration 0100: loss: 1949.485
iteration 0200: loss: 1948.629
iteration 0300: loss: 1949.139
iteration 0400: loss: 1948.440
iteration 0500: loss: 1949.843
iteration 0600: loss: 1950.025
iteration 0700: loss: 1949.730
iteration 0800: loss: 1949.078
iteration 0900: loss: 1950.842
====> Epoch: 020 Train loss: 1949.3930  took : 13.225301504135132
====> Test loss: 1951.2727
iteration 0000: loss: 1948.371
iteration 0100: loss: 1948.640
iteration 0200: loss: 1948.971
iteration 0300: loss: 1950.115
iteration 0400: loss: 1950.627
iteration 0500: loss: 1949.380
iteration 0600: loss: 1949.556
iteration 0700: loss: 1949.729
iteration 0800: loss: 1948.066
iteration 0900: loss: 1950.147
====> Epoch: 021 Train loss: 1949.2550  took : 12.124348640441895
====> Test loss: 1951.1412
iteration 0000: loss: 1948.225
iteration 0100: loss: 1949.332
iteration 0200: loss: 1949.961
iteration 0300: loss: 1949.175
iteration 0400: loss: 1949.066
iteration 0500: loss: 1948.058
iteration 0600: loss: 1949.588
iteration 0700: loss: 1951.149
iteration 0800: loss: 1948.758
iteration 0900: loss: 1950.662
====> Epoch: 022 Train loss: 1949.2108  took : 12.154455661773682
====> Test loss: 1951.1068
iteration 0000: loss: 1949.380
iteration 0100: loss: 1950.040
iteration 0200: loss: 1948.211
iteration 0300: loss: 1949.010
iteration 0400: loss: 1949.538
iteration 0500: loss: 1950.874
iteration 0600: loss: 1948.254
iteration 0700: loss: 1949.875
iteration 0800: loss: 1948.551
iteration 0900: loss: 1948.475
====> Epoch: 023 Train loss: 1949.3437  took : 12.973409414291382
====> Test loss: 1951.3948
iteration 0000: loss: 1948.915
iteration 0100: loss: 1948.141
iteration 0200: loss: 1950.157
iteration 0300: loss: 1949.820
iteration 0400: loss: 1949.295
iteration 0500: loss: 1949.604
iteration 0600: loss: 1949.557
iteration 0700: loss: 1949.100
iteration 0800: loss: 1948.971
iteration 0900: loss: 1948.473
====> Epoch: 024 Train loss: 1949.2264  took : 12.69563913345337
====> Test loss: 1951.4669
iteration 0000: loss: 1948.769
iteration 0100: loss: 1948.183
iteration 0200: loss: 1949.384
iteration 0300: loss: 1949.829
iteration 0400: loss: 1950.393
iteration 0500: loss: 1948.269
iteration 0600: loss: 1949.068
iteration 0700: loss: 1949.706
iteration 0800: loss: 1949.624
iteration 0900: loss: 1949.104
====> Epoch: 025 Train loss: 1949.1424  took : 13.340479850769043
====> Test loss: 1951.1211
iteration 0000: loss: 1948.711
iteration 0100: loss: 1950.291
iteration 0200: loss: 1949.100
iteration 0300: loss: 1949.801
iteration 0400: loss: 1948.709
iteration 0500: loss: 1949.104
iteration 0600: loss: 1948.838
iteration 0700: loss: 1949.436
iteration 0800: loss: 1948.957
iteration 0900: loss: 1948.818
====> Epoch: 026 Train loss: 1949.0475  took : 13.033090591430664
====> Test loss: 1950.7018
iteration 0000: loss: 1947.858
iteration 0100: loss: 1948.719
iteration 0200: loss: 1947.860
iteration 0300: loss: 1948.312
iteration 0400: loss: 1948.214
iteration 0500: loss: 1947.966
iteration 0600: loss: 1950.583
iteration 0700: loss: 1948.605
iteration 0800: loss: 1949.054
iteration 0900: loss: 1949.423
====> Epoch: 027 Train loss: 1948.9943  took : 13.602578401565552
====> Test loss: 1951.0143
iteration 0000: loss: 1948.997
iteration 0100: loss: 1948.362
iteration 0200: loss: 1947.892
iteration 0300: loss: 1949.550
iteration 0400: loss: 1949.547
iteration 0500: loss: 1948.503
iteration 0600: loss: 1949.292
iteration 0700: loss: 1949.165
iteration 0800: loss: 1948.391
iteration 0900: loss: 1949.121
====> Epoch: 028 Train loss: 1949.0419  took : 13.669424772262573
====> Test loss: 1951.1197
iteration 0000: loss: 1948.456
iteration 0100: loss: 1948.651
iteration 0200: loss: 1949.437
iteration 0300: loss: 1948.334
iteration 0400: loss: 1949.291
iteration 0500: loss: 1948.365
iteration 0600: loss: 1948.114
iteration 0700: loss: 1948.563
iteration 0800: loss: 1948.488
iteration 0900: loss: 1949.179
====> Epoch: 029 Train loss: 1949.1509  took : 12.27143120765686
====> Test loss: 1950.7452
iteration 0000: loss: 1950.408
iteration 0100: loss: 1949.258
iteration 0200: loss: 1949.251
iteration 0300: loss: 1948.861
iteration 0400: loss: 1948.591
iteration 0500: loss: 1948.864
iteration 0600: loss: 1952.147
iteration 0700: loss: 1948.204
iteration 0800: loss: 1948.482
iteration 0900: loss: 1948.262
====> Epoch: 030 Train loss: 1948.7775  took : 12.943325996398926
====> Test loss: 1950.8955
iteration 0000: loss: 1949.433
iteration 0100: loss: 1948.984
iteration 0200: loss: 1948.089
iteration 0300: loss: 1948.391
iteration 0400: loss: 1949.301
iteration 0500: loss: 1947.750
iteration 0600: loss: 1948.471
iteration 0700: loss: 1948.339
iteration 0800: loss: 1948.432
iteration 0900: loss: 1948.426
====> Epoch: 031 Train loss: 1948.5672  took : 12.92571759223938
====> Test loss: 1950.3299
iteration 0000: loss: 1949.329
iteration 0100: loss: 1948.476
iteration 0200: loss: 1948.642
iteration 0300: loss: 1948.039
iteration 0400: loss: 1947.786
iteration 0500: loss: 1948.431
iteration 0600: loss: 1948.756
iteration 0700: loss: 1948.302
iteration 0800: loss: 1948.917
iteration 0900: loss: 1948.566
====> Epoch: 032 Train loss: 1948.3475  took : 12.526288270950317
====> Test loss: 1950.2507
iteration 0000: loss: 1947.988
iteration 0100: loss: 1948.753
iteration 0200: loss: 1947.984
iteration 0300: loss: 1948.367
iteration 0400: loss: 1949.182
iteration 0500: loss: 1948.351
iteration 0600: loss: 1948.431
iteration 0700: loss: 1948.807
iteration 0800: loss: 1948.771
iteration 0900: loss: 1948.620
====> Epoch: 033 Train loss: 1948.4885  took : 12.499595642089844
====> Test loss: 1950.2841
iteration 0000: loss: 1947.071
iteration 0100: loss: 1947.635
iteration 0200: loss: 1948.959
iteration 0300: loss: 1948.639
iteration 0400: loss: 1947.644
iteration 0500: loss: 1947.418
iteration 0600: loss: 1949.768
iteration 0700: loss: 1948.216
iteration 0800: loss: 1947.812
iteration 0900: loss: 1949.500
====> Epoch: 034 Train loss: 1948.3284  took : 12.043842792510986
====> Test loss: 1950.3302
iteration 0000: loss: 1948.154
iteration 0100: loss: 1947.684
iteration 0200: loss: 1948.656
iteration 0300: loss: 1947.272
iteration 0400: loss: 1948.512
iteration 0500: loss: 1948.037
iteration 0600: loss: 1948.632
iteration 0700: loss: 1948.102
iteration 0800: loss: 1947.800
iteration 0900: loss: 1948.509
====> Epoch: 035 Train loss: 1948.3744  took : 11.880980730056763
====> Test loss: 1950.1045
iteration 0000: loss: 1948.127
iteration 0100: loss: 1947.883
iteration 0200: loss: 1947.417
iteration 0300: loss: 1947.748
iteration 0400: loss: 1948.864
iteration 0500: loss: 1948.009
iteration 0600: loss: 1948.264
iteration 0700: loss: 1949.164
iteration 0800: loss: 1948.100
iteration 0900: loss: 1947.607
====> Epoch: 036 Train loss: 1948.4064  took : 12.928068161010742
====> Test loss: 1950.0575
iteration 0000: loss: 1948.365
iteration 0100: loss: 1947.901
iteration 0200: loss: 1948.075
iteration 0300: loss: 1947.920
iteration 0400: loss: 1948.662
iteration 0500: loss: 1948.804
iteration 0600: loss: 1947.606
iteration 0700: loss: 1948.435
iteration 0800: loss: 1947.962
iteration 0900: loss: 1948.122
====> Epoch: 037 Train loss: 1948.2711  took : 12.036369323730469
====> Test loss: 1950.7206
iteration 0000: loss: 1948.979
iteration 0100: loss: 1949.307
iteration 0200: loss: 1947.564
iteration 0300: loss: 1948.603
iteration 0400: loss: 1947.942
iteration 0500: loss: 1948.683
iteration 0600: loss: 1948.125
iteration 0700: loss: 1949.379
iteration 0800: loss: 1948.364
iteration 0900: loss: 1949.149
====> Epoch: 038 Train loss: 1948.4090  took : 12.742577075958252
====> Test loss: 1950.2341
iteration 0000: loss: 1949.080
iteration 0100: loss: 1947.898
iteration 0200: loss: 1948.601
iteration 0300: loss: 1948.377
iteration 0400: loss: 1948.463
iteration 0500: loss: 1949.003
iteration 0600: loss: 1948.758
iteration 0700: loss: 1948.431
iteration 0800: loss: 1948.498
iteration 0900: loss: 1948.131
====> Epoch: 039 Train loss: 1948.5050  took : 12.543911933898926
====> Test loss: 1950.3763
iteration 0000: loss: 1948.076
iteration 0100: loss: 1948.800
iteration 0200: loss: 1948.708
iteration 0300: loss: 1948.685
iteration 0400: loss: 1947.834
iteration 0500: loss: 1948.054
iteration 0600: loss: 1947.792
iteration 0700: loss: 1948.619
iteration 0800: loss: 1948.900
iteration 0900: loss: 1948.240
====> Epoch: 040 Train loss: 1948.4101  took : 13.052395582199097
====> Test loss: 1950.2283
iteration 0000: loss: 1948.066
iteration 0100: loss: 1948.326
iteration 0200: loss: 1948.692
iteration 0300: loss: 1948.265
iteration 0400: loss: 1947.849
iteration 0500: loss: 1947.951
iteration 0600: loss: 1947.924
iteration 0700: loss: 1949.333
iteration 0800: loss: 1948.280
iteration 0900: loss: 1948.187
====> Epoch: 041 Train loss: 1948.2921  took : 12.439112424850464
====> Test loss: 1950.3054
iteration 0000: loss: 1948.817
iteration 0100: loss: 1949.854
iteration 0200: loss: 1948.990
iteration 0300: loss: 1947.590
iteration 0400: loss: 1948.183
iteration 0500: loss: 1948.733
iteration 0600: loss: 1948.639
iteration 0700: loss: 1947.652
iteration 0800: loss: 1948.257
iteration 0900: loss: 1948.207
====> Epoch: 042 Train loss: 1948.3285  took : 12.68689751625061
====> Test loss: 1950.4254
iteration 0000: loss: 1949.833
iteration 0100: loss: 1948.824
iteration 0200: loss: 1947.934
iteration 0300: loss: 1948.296
iteration 0400: loss: 1949.751
iteration 0500: loss: 1949.450
iteration 0600: loss: 1948.858
iteration 0700: loss: 1947.901
iteration 0800: loss: 1948.062
iteration 0900: loss: 1948.454
====> Epoch: 043 Train loss: 1948.2979  took : 11.544551849365234
====> Test loss: 1949.9822
iteration 0000: loss: 1947.919
iteration 0100: loss: 1948.561
iteration 0200: loss: 1950.145
iteration 0300: loss: 1947.765
iteration 0400: loss: 1948.965
iteration 0500: loss: 1948.045
iteration 0600: loss: 1948.005
iteration 0700: loss: 1947.959
iteration 0800: loss: 1948.190
iteration 0900: loss: 1948.899
====> Epoch: 044 Train loss: 1948.3866  took : 12.183383703231812
====> Test loss: 1950.7214
iteration 0000: loss: 1948.706
iteration 0100: loss: 1950.110
iteration 0200: loss: 1949.728
iteration 0300: loss: 1948.123
iteration 0400: loss: 1947.606
iteration 0500: loss: 1948.474
iteration 0600: loss: 1948.102
iteration 0700: loss: 1948.421
iteration 0800: loss: 1948.245
iteration 0900: loss: 1948.199
====> Epoch: 045 Train loss: 1948.6102  took : 13.074454069137573
====> Test loss: 1950.4425
iteration 0000: loss: 1947.805
iteration 0100: loss: 1948.458
iteration 0200: loss: 1949.199
iteration 0300: loss: 1947.862
iteration 0400: loss: 1949.140
iteration 0500: loss: 1948.467
iteration 0600: loss: 1948.408
iteration 0700: loss: 1948.487
iteration 0800: loss: 1949.174
iteration 0900: loss: 1948.975
====> Epoch: 046 Train loss: 1948.4348  took : 12.263613224029541
====> Test loss: 1950.0514
iteration 0000: loss: 1947.777
iteration 0100: loss: 1947.814
iteration 0200: loss: 1947.404
iteration 0300: loss: 1948.061
iteration 0400: loss: 1948.122
iteration 0500: loss: 1947.930
iteration 0600: loss: 1947.880
iteration 0700: loss: 1947.805
iteration 0800: loss: 1947.891
iteration 0900: loss: 1948.304
====> Epoch: 047 Train loss: 1948.2516  took : 12.116737365722656
====> Test loss: 1949.9203
iteration 0000: loss: 1947.687
iteration 0100: loss: 1947.366
iteration 0200: loss: 1949.219
iteration 0300: loss: 1947.920
iteration 0400: loss: 1949.580
iteration 0500: loss: 1948.575
iteration 0600: loss: 1948.245
iteration 0700: loss: 1947.655
iteration 0800: loss: 1948.594
iteration 0900: loss: 1948.505
====> Epoch: 048 Train loss: 1948.2274  took : 11.772515773773193
====> Test loss: 1950.3150
iteration 0000: loss: 1948.399
iteration 0100: loss: 1948.593
iteration 0200: loss: 1949.788
iteration 0300: loss: 1948.996
iteration 0400: loss: 1949.214
iteration 0500: loss: 1948.077
iteration 0600: loss: 1948.260
iteration 0700: loss: 1948.075
iteration 0800: loss: 1948.554
iteration 0900: loss: 1948.493
====> Epoch: 049 Train loss: 1948.3069  took : 12.673667907714844
====> Test loss: 1950.1935
iteration 0000: loss: 1948.269
iteration 0100: loss: 1948.018
iteration 0200: loss: 1948.436
iteration 0300: loss: 1947.880
iteration 0400: loss: 1948.414
iteration 0500: loss: 1947.697
iteration 0600: loss: 1947.828
iteration 0700: loss: 1948.019
iteration 0800: loss: 1949.377
iteration 0900: loss: 1948.254
====> Epoch: 050 Train loss: 1948.2455  took : 12.386850118637085
====> Test loss: 1950.2787
====> [MM-VAE] Time: 705.852s or 00:11:45
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  14
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_14
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_14
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5220.052
iteration 0100: loss: 4158.925
iteration 0200: loss: 4085.500
iteration 0300: loss: 4044.337
iteration 0400: loss: 4024.847
iteration 0500: loss: 4022.408
iteration 0600: loss: 4017.056
iteration 0700: loss: 4009.723
iteration 0800: loss: 4017.095
iteration 0900: loss: 4007.473
iteration 1000: loss: 3997.042
iteration 1100: loss: 4004.226
iteration 1200: loss: 3989.229
iteration 1300: loss: 4002.296
iteration 1400: loss: 3993.909
iteration 1500: loss: 4008.164
iteration 1600: loss: 3998.916
iteration 1700: loss: 3995.795
iteration 1800: loss: 3989.114
====> Epoch: 001 Train loss: 4026.8946  took : 53.45346188545227
====> Test loss: 3993.0198
iteration 0000: loss: 3996.065
iteration 0100: loss: 3992.206
iteration 0200: loss: 3999.506
iteration 0300: loss: 3986.417
iteration 0400: loss: 3974.837
iteration 0500: loss: 3980.631
iteration 0600: loss: 3975.544
iteration 0700: loss: 3988.786
iteration 0800: loss: 4002.773
iteration 0900: loss: 3990.558
iteration 1000: loss: 3986.293
iteration 1100: loss: 3972.227
iteration 1200: loss: 3984.934
iteration 1300: loss: 3976.032
iteration 1400: loss: 3974.989
iteration 1500: loss: 3970.097
iteration 1600: loss: 3976.990
iteration 1700: loss: 3981.327
iteration 1800: loss: 3969.534
====> Epoch: 002 Train loss: 3981.9910  took : 53.371214389801025
====> Test loss: 3980.2999
iteration 0000: loss: 3967.623
iteration 0100: loss: 3966.600
iteration 0200: loss: 3963.250
iteration 0300: loss: 3975.848
iteration 0400: loss: 3977.624
iteration 0500: loss: 3979.788
iteration 0600: loss: 3964.474
iteration 0700: loss: 3963.006
iteration 0800: loss: 3980.556
iteration 0900: loss: 3980.283
iteration 1000: loss: 3973.582
iteration 1100: loss: 3976.332
iteration 1200: loss: 3973.199
iteration 1300: loss: 3975.618
iteration 1400: loss: 3979.178
iteration 1500: loss: 3977.703
iteration 1600: loss: 3965.139
iteration 1700: loss: 3976.581
iteration 1800: loss: 3973.573
====> Epoch: 003 Train loss: 3973.1483  took : 53.266396045684814
====> Test loss: 3975.2965
iteration 0000: loss: 3974.461
iteration 0100: loss: 3957.403
iteration 0200: loss: 3974.028
iteration 0300: loss: 3968.709
iteration 0400: loss: 3971.985
iteration 0500: loss: 3971.881
iteration 0600: loss: 3968.316
iteration 0700: loss: 3970.122
iteration 0800: loss: 3968.459
iteration 0900: loss: 3971.564
iteration 1000: loss: 3961.491
iteration 1100: loss: 3966.637
iteration 1200: loss: 3971.225
iteration 1300: loss: 3969.926
iteration 1400: loss: 3964.565
iteration 1500: loss: 3963.629
iteration 1600: loss: 3968.545
iteration 1700: loss: 3959.749
iteration 1800: loss: 3966.471
====> Epoch: 004 Train loss: 3967.3409  took : 53.62891745567322
====> Test loss: 3968.1945
iteration 0000: loss: 3966.079
iteration 0100: loss: 3955.378
iteration 0200: loss: 3968.483
iteration 0300: loss: 3962.490
iteration 0400: loss: 3959.618
iteration 0500: loss: 3963.672
iteration 0600: loss: 3956.672
iteration 0700: loss: 3957.687
iteration 0800: loss: 3955.108
iteration 0900: loss: 3964.860
iteration 1000: loss: 3956.451
iteration 1100: loss: 3956.551
iteration 1200: loss: 3967.355
iteration 1300: loss: 3957.735
iteration 1400: loss: 3953.838
iteration 1500: loss: 3963.000
iteration 1600: loss: 3946.068
iteration 1700: loss: 3953.130
iteration 1800: loss: 3946.725
====> Epoch: 005 Train loss: 3959.0671  took : 53.25137495994568
====> Test loss: 3957.3532
iteration 0000: loss: 3956.901
iteration 0100: loss: 3951.153
iteration 0200: loss: 3951.975
iteration 0300: loss: 3945.029
iteration 0400: loss: 3947.224
iteration 0500: loss: 3954.780
iteration 0600: loss: 3957.201
iteration 0700: loss: 3953.786
iteration 0800: loss: 3953.036
iteration 0900: loss: 3957.792
iteration 1000: loss: 3951.357
iteration 1100: loss: 3959.849
iteration 1200: loss: 3949.679
iteration 1300: loss: 3952.193
iteration 1400: loss: 3948.499
iteration 1500: loss: 3952.428
iteration 1600: loss: 3943.236
iteration 1700: loss: 3949.205
iteration 1800: loss: 3952.663
====> Epoch: 006 Train loss: 3951.2268  took : 53.223254680633545
====> Test loss: 3951.6475
iteration 0000: loss: 3943.241
iteration 0100: loss: 3950.574
iteration 0200: loss: 3945.329
iteration 0300: loss: 3946.293
iteration 0400: loss: 3953.578
iteration 0500: loss: 3948.656
iteration 0600: loss: 3953.756
iteration 0700: loss: 3947.870
iteration 0800: loss: 3949.321
iteration 0900: loss: 3946.902
iteration 1000: loss: 3949.413
iteration 1100: loss: 3947.743
iteration 1200: loss: 3948.072
iteration 1300: loss: 3941.702
iteration 1400: loss: 3950.561
iteration 1500: loss: 3946.621
iteration 1600: loss: 3943.798
iteration 1700: loss: 3943.881
iteration 1800: loss: 3940.666
====> Epoch: 007 Train loss: 3946.5641  took : 53.32261037826538
====> Test loss: 3948.5140
iteration 0000: loss: 3946.855
iteration 0100: loss: 3949.713
iteration 0200: loss: 3939.883
iteration 0300: loss: 3942.499
iteration 0400: loss: 3943.467
iteration 0500: loss: 3948.950
iteration 0600: loss: 3945.372
iteration 0700: loss: 3943.652
iteration 0800: loss: 3939.875
iteration 0900: loss: 3947.407
iteration 1000: loss: 3950.817
iteration 1100: loss: 3944.908
iteration 1200: loss: 3949.545
iteration 1300: loss: 3947.517
iteration 1400: loss: 3942.258
iteration 1500: loss: 3945.239
iteration 1600: loss: 3950.247
iteration 1700: loss: 3942.435
iteration 1800: loss: 3940.367
====> Epoch: 008 Train loss: 3944.5547  took : 53.286561489105225
====> Test loss: 3947.3605
iteration 0000: loss: 3939.933
iteration 0100: loss: 3945.586
iteration 0200: loss: 3936.559
iteration 0300: loss: 3945.281
iteration 0400: loss: 3943.057
iteration 0500: loss: 3944.048
iteration 0600: loss: 3943.191
iteration 0700: loss: 3944.959
iteration 0800: loss: 3946.510
iteration 0900: loss: 3947.856
iteration 1000: loss: 3938.412
iteration 1100: loss: 3944.159
iteration 1200: loss: 3937.294
iteration 1300: loss: 3940.504
iteration 1400: loss: 3941.311
iteration 1500: loss: 3943.395
iteration 1600: loss: 3943.045
iteration 1700: loss: 3937.371
iteration 1800: loss: 3943.800
====> Epoch: 009 Train loss: 3942.7956  took : 53.05110287666321
====> Test loss: 3946.1769
iteration 0000: loss: 3937.863
iteration 0100: loss: 3942.969
iteration 0200: loss: 3947.206
iteration 0300: loss: 3950.735
iteration 0400: loss: 3940.546
iteration 0500: loss: 3932.950
iteration 0600: loss: 3937.288
iteration 0700: loss: 3943.065
iteration 0800: loss: 3944.424
iteration 0900: loss: 3951.081
iteration 1000: loss: 3943.353
iteration 1100: loss: 3941.367
iteration 1200: loss: 3935.136
iteration 1300: loss: 3940.363
iteration 1400: loss: 3939.695
iteration 1500: loss: 3942.300
iteration 1600: loss: 3942.636
iteration 1700: loss: 3940.940
iteration 1800: loss: 3943.617
====> Epoch: 010 Train loss: 3941.8713  took : 53.061596393585205
====> Test loss: 3945.8040
iteration 0000: loss: 3948.490
iteration 0100: loss: 3938.387
iteration 0200: loss: 3943.697
iteration 0300: loss: 3935.525
iteration 0400: loss: 3945.854
iteration 0500: loss: 3944.998
iteration 0600: loss: 3936.895
iteration 0700: loss: 3938.057
iteration 0800: loss: 3937.783
iteration 0900: loss: 3935.198
iteration 1000: loss: 3940.650
iteration 1100: loss: 3940.285
iteration 1200: loss: 3946.715
iteration 1300: loss: 3936.631
iteration 1400: loss: 3942.616
iteration 1500: loss: 3938.658
iteration 1600: loss: 3940.484
iteration 1700: loss: 3938.219
iteration 1800: loss: 3939.457
====> Epoch: 011 Train loss: 3941.1896  took : 53.107417583465576
====> Test loss: 3944.9847
iteration 0000: loss: 3942.917
iteration 0100: loss: 3933.826
iteration 0200: loss: 3934.074
iteration 0300: loss: 3941.770
iteration 0400: loss: 3942.401
iteration 0500: loss: 3934.257
iteration 0600: loss: 3946.340
iteration 0700: loss: 3941.640
iteration 0800: loss: 3940.729
iteration 0900: loss: 3944.552
iteration 1000: loss: 3938.730
iteration 1100: loss: 3935.008
iteration 1200: loss: 3940.526
iteration 1300: loss: 3937.938
iteration 1400: loss: 3946.597
iteration 1500: loss: 3940.937
iteration 1600: loss: 3941.022
iteration 1700: loss: 3934.644
iteration 1800: loss: 3933.607
====> Epoch: 012 Train loss: 3940.5491  took : 53.06489181518555
====> Test loss: 3943.7906
iteration 0000: loss: 3934.721
iteration 0100: loss: 3938.591
iteration 0200: loss: 3939.522
iteration 0300: loss: 3946.167
iteration 0400: loss: 3938.446
iteration 0500: loss: 3940.173
iteration 0600: loss: 3938.138
iteration 0700: loss: 3938.362
iteration 0800: loss: 3941.287
iteration 0900: loss: 3941.574
iteration 1000: loss: 3939.564
iteration 1100: loss: 3934.896
iteration 1200: loss: 3943.860
iteration 1300: loss: 3938.532
iteration 1400: loss: 3942.579
iteration 1500: loss: 3942.270
iteration 1600: loss: 3946.945
iteration 1700: loss: 3938.825
iteration 1800: loss: 3939.896
====> Epoch: 013 Train loss: 3940.1530  took : 53.20795726776123
====> Test loss: 3944.0446
iteration 0000: loss: 3941.443
iteration 0100: loss: 3942.855
iteration 0200: loss: 3934.035
iteration 0300: loss: 3942.094
iteration 0400: loss: 3947.177
iteration 0500: loss: 3938.782
iteration 0600: loss: 3943.423
iteration 0700: loss: 3936.235
iteration 0800: loss: 3941.340
iteration 0900: loss: 3934.399
iteration 1000: loss: 3935.593
iteration 1100: loss: 3940.171
iteration 1200: loss: 3936.101
iteration 1300: loss: 3945.131
iteration 1400: loss: 3937.922
iteration 1500: loss: 3934.644
iteration 1600: loss: 3950.713
iteration 1700: loss: 3943.164
iteration 1800: loss: 3940.496
====> Epoch: 014 Train loss: 3940.1751  took : 53.17859172821045
====> Test loss: 3943.4324
iteration 0000: loss: 3936.635
iteration 0100: loss: 3945.631
iteration 0200: loss: 3942.814
iteration 0300: loss: 3937.381
iteration 0400: loss: 3940.789
iteration 0500: loss: 3941.478
iteration 0600: loss: 3935.459
iteration 0700: loss: 3938.637
iteration 0800: loss: 3942.525
iteration 0900: loss: 3938.808
iteration 1000: loss: 3944.177
iteration 1100: loss: 3939.478
iteration 1200: loss: 3936.850
iteration 1300: loss: 3945.936
iteration 1400: loss: 3946.631
iteration 1500: loss: 3937.314
iteration 1600: loss: 3933.751
iteration 1700: loss: 3937.504
iteration 1800: loss: 3937.130
====> Epoch: 015 Train loss: 3939.8179  took : 53.065099477767944
====> Test loss: 3944.2314
iteration 0000: loss: 3942.953
iteration 0100: loss: 3946.644
iteration 0200: loss: 3936.419
iteration 0300: loss: 3940.246
iteration 0400: loss: 3940.745
iteration 0500: loss: 3942.163
iteration 0600: loss: 3941.025
iteration 0700: loss: 3945.037
iteration 0800: loss: 3947.186
iteration 0900: loss: 3935.393
iteration 1000: loss: 3942.922
iteration 1100: loss: 3935.710
iteration 1200: loss: 3939.962
iteration 1300: loss: 3940.442
iteration 1400: loss: 3939.571
iteration 1500: loss: 3940.092
iteration 1600: loss: 3934.667
iteration 1700: loss: 3937.601
iteration 1800: loss: 3941.309
====> Epoch: 016 Train loss: 3939.3954  took : 53.39705562591553
====> Test loss: 3943.4496
iteration 0000: loss: 3941.757
iteration 0100: loss: 3941.119
iteration 0200: loss: 3937.646
iteration 0300: loss: 3939.561
iteration 0400: loss: 3933.373
iteration 0500: loss: 3940.146
iteration 0600: loss: 3941.283
iteration 0700: loss: 3934.791
iteration 0800: loss: 3940.955
iteration 0900: loss: 3938.183
iteration 1000: loss: 3934.940
iteration 1100: loss: 3944.904
iteration 1200: loss: 3933.792
iteration 1300: loss: 3936.705
iteration 1400: loss: 3936.113
iteration 1500: loss: 3935.982
iteration 1600: loss: 3943.753
iteration 1700: loss: 3936.285
iteration 1800: loss: 3941.680
====> Epoch: 017 Train loss: 3939.3500  took : 53.014198303222656
====> Test loss: 3943.2672
iteration 0000: loss: 3943.829
iteration 0100: loss: 3936.302
iteration 0200: loss: 3938.405
iteration 0300: loss: 3944.889
iteration 0400: loss: 3939.797
iteration 0500: loss: 3940.341
iteration 0600: loss: 3939.232
iteration 0700: loss: 3939.531
iteration 0800: loss: 3938.756
iteration 0900: loss: 3943.408
iteration 1000: loss: 3940.933
iteration 1100: loss: 3942.943
iteration 1200: loss: 3940.651
iteration 1300: loss: 3940.430
iteration 1400: loss: 3941.884
iteration 1500: loss: 3945.823
iteration 1600: loss: 3938.536
iteration 1700: loss: 3935.577
iteration 1800: loss: 3943.661
====> Epoch: 018 Train loss: 3939.2904  took : 53.11292600631714
====> Test loss: 3943.0734
iteration 0000: loss: 3936.104
iteration 0100: loss: 3943.059
iteration 0200: loss: 3941.264
iteration 0300: loss: 3936.854
iteration 0400: loss: 3943.058
iteration 0500: loss: 3935.589
iteration 0600: loss: 3936.742
iteration 0700: loss: 3943.630
iteration 0800: loss: 3942.528
iteration 0900: loss: 3936.809
iteration 1000: loss: 3937.789
iteration 1100: loss: 3938.763
iteration 1200: loss: 3942.845
iteration 1300: loss: 3945.442
iteration 1400: loss: 3934.227
iteration 1500: loss: 3939.217
iteration 1600: loss: 3941.049
iteration 1700: loss: 3944.092
iteration 1800: loss: 3938.556
====> Epoch: 019 Train loss: 3938.8119  took : 53.5073606967926
====> Test loss: 3942.8552
iteration 0000: loss: 3937.321
iteration 0100: loss: 3942.171
iteration 0200: loss: 3937.791
iteration 0300: loss: 3939.830
iteration 0400: loss: 3936.334
iteration 0500: loss: 3935.815
iteration 0600: loss: 3937.827
iteration 0700: loss: 3939.284
iteration 0800: loss: 3940.987
iteration 0900: loss: 3932.983
iteration 1000: loss: 3944.023
iteration 1100: loss: 3937.188
iteration 1200: loss: 3934.691
iteration 1300: loss: 3942.824
iteration 1400: loss: 3938.894
iteration 1500: loss: 3940.025
iteration 1600: loss: 3940.127
iteration 1700: loss: 3931.918
iteration 1800: loss: 3934.642
====> Epoch: 020 Train loss: 3938.7511  took : 53.199535608291626
====> Test loss: 3943.7632
iteration 0000: loss: 3938.347
iteration 0100: loss: 3936.255
iteration 0200: loss: 3936.105
iteration 0300: loss: 3940.108
iteration 0400: loss: 3940.554
iteration 0500: loss: 3940.928
iteration 0600: loss: 3938.262
iteration 0700: loss: 3935.436
iteration 0800: loss: 3945.879
iteration 0900: loss: 3940.020
iteration 1000: loss: 3939.127
iteration 1100: loss: 3935.132
iteration 1200: loss: 3944.306
iteration 1300: loss: 3951.397
iteration 1400: loss: 3935.969
iteration 1500: loss: 3933.732
iteration 1600: loss: 3936.890
iteration 1700: loss: 3939.555
iteration 1800: loss: 3930.922
====> Epoch: 021 Train loss: 3938.7646  took : 52.97392964363098
====> Test loss: 3942.2694
iteration 0000: loss: 3937.267
iteration 0100: loss: 3940.523
iteration 0200: loss: 3935.255
iteration 0300: loss: 3938.304
iteration 0400: loss: 3945.678
iteration 0500: loss: 3943.924
iteration 0600: loss: 3934.210
iteration 0700: loss: 3933.797
iteration 0800: loss: 3939.758
iteration 0900: loss: 3937.112
iteration 1000: loss: 3933.193
iteration 1100: loss: 3943.369
iteration 1200: loss: 3942.029
iteration 1300: loss: 3936.005
iteration 1400: loss: 3942.876
iteration 1500: loss: 3939.522
iteration 1600: loss: 3939.960
iteration 1700: loss: 3938.794
iteration 1800: loss: 3943.237
====> Epoch: 022 Train loss: 3938.2061  took : 53.01671862602234
====> Test loss: 3942.2866
iteration 0000: loss: 3938.485
iteration 0100: loss: 3941.967
iteration 0200: loss: 3940.229
iteration 0300: loss: 3936.045
iteration 0400: loss: 3934.818
iteration 0500: loss: 3940.150
iteration 0600: loss: 3936.670
iteration 0700: loss: 3936.838
iteration 0800: loss: 3937.535
iteration 0900: loss: 3938.112
iteration 1000: loss: 3941.325
iteration 1100: loss: 3939.907
iteration 1200: loss: 3935.883
iteration 1300: loss: 3938.803
iteration 1400: loss: 3934.812
iteration 1500: loss: 3939.075
iteration 1600: loss: 3934.186
iteration 1700: loss: 3930.737
iteration 1800: loss: 3940.731
====> Epoch: 023 Train loss: 3938.2541  took : 53.1107075214386
====> Test loss: 3942.4662
iteration 0000: loss: 3938.411
iteration 0100: loss: 3937.217
iteration 0200: loss: 3938.286
iteration 0300: loss: 3936.076
iteration 0400: loss: 3938.442
iteration 0500: loss: 3936.965
iteration 0600: loss: 3939.349
iteration 0700: loss: 3944.605
iteration 0800: loss: 3935.078
iteration 0900: loss: 3933.298
iteration 1000: loss: 3940.938
iteration 1100: loss: 3936.607
iteration 1200: loss: 3941.874
iteration 1300: loss: 3937.475
iteration 1400: loss: 3941.378
iteration 1500: loss: 3933.879
iteration 1600: loss: 3941.943
iteration 1700: loss: 3939.291
iteration 1800: loss: 3945.513
====> Epoch: 024 Train loss: 3938.2707  took : 52.97840476036072
====> Test loss: 3942.0309
iteration 0000: loss: 3938.877
iteration 0100: loss: 3938.843
iteration 0200: loss: 3943.769
iteration 0300: loss: 3937.935
iteration 0400: loss: 3934.804
iteration 0500: loss: 3935.333
iteration 0600: loss: 3936.566
iteration 0700: loss: 3935.757
iteration 0800: loss: 3939.571
iteration 0900: loss: 3939.849
iteration 1000: loss: 3934.170
iteration 1100: loss: 3942.113
iteration 1200: loss: 3933.714
iteration 1300: loss: 3945.622
iteration 1400: loss: 3936.501
iteration 1500: loss: 3940.360
iteration 1600: loss: 3934.860
iteration 1700: loss: 3937.764
iteration 1800: loss: 3942.094
====> Epoch: 025 Train loss: 3938.1802  took : 52.97930598258972
====> Test loss: 3943.0554
iteration 0000: loss: 3944.746
iteration 0100: loss: 3936.036
iteration 0200: loss: 3940.409
iteration 0300: loss: 3938.533
iteration 0400: loss: 3938.021
iteration 0500: loss: 3937.807
iteration 0600: loss: 3940.941
iteration 0700: loss: 3934.797
iteration 0800: loss: 3940.654
iteration 0900: loss: 3933.800
iteration 1000: loss: 3934.849
iteration 1100: loss: 3937.093
iteration 1200: loss: 3944.262
iteration 1300: loss: 3936.140
iteration 1400: loss: 3935.548
iteration 1500: loss: 3938.519
iteration 1600: loss: 3937.284
iteration 1700: loss: 3942.833
iteration 1800: loss: 3936.351
====> Epoch: 026 Train loss: 3938.2525  took : 53.408793210983276
====> Test loss: 3941.8080
iteration 0000: loss: 3933.686
iteration 0100: loss: 3937.498
iteration 0200: loss: 3931.657
iteration 0300: loss: 3936.254
iteration 0400: loss: 3940.477
iteration 0500: loss: 3936.621
iteration 0600: loss: 3935.851
iteration 0700: loss: 3936.525
iteration 0800: loss: 3941.689
iteration 0900: loss: 3936.186
iteration 1000: loss: 3938.999
iteration 1100: loss: 3939.428
iteration 1200: loss: 3943.123
iteration 1300: loss: 3935.105
iteration 1400: loss: 3934.791
iteration 1500: loss: 3936.653
iteration 1600: loss: 3936.722
iteration 1700: loss: 3938.193
iteration 1800: loss: 3940.002
====> Epoch: 027 Train loss: 3937.8858  took : 53.230753660202026
====> Test loss: 3941.7106
iteration 0000: loss: 3940.057
iteration 0100: loss: 3934.600
iteration 0200: loss: 3937.569
iteration 0300: loss: 3936.501
iteration 0400: loss: 3940.350
iteration 0500: loss: 3934.831
iteration 0600: loss: 3932.979
iteration 0700: loss: 3934.820
iteration 0800: loss: 3932.168
iteration 0900: loss: 3934.290
iteration 1000: loss: 3935.090
iteration 1100: loss: 3932.780
iteration 1200: loss: 3939.799
iteration 1300: loss: 3941.788
iteration 1400: loss: 3935.229
iteration 1500: loss: 3936.624
iteration 1600: loss: 3930.052
iteration 1700: loss: 3936.015
iteration 1800: loss: 3937.675
====> Epoch: 028 Train loss: 3937.6993  took : 53.00003457069397
====> Test loss: 3941.9587
iteration 0000: loss: 3941.142
iteration 0100: loss: 3935.295
iteration 0200: loss: 3936.310
iteration 0300: loss: 3937.978
iteration 0400: loss: 3934.225
iteration 0500: loss: 3941.745
iteration 0600: loss: 3930.626
iteration 0700: loss: 3942.195
iteration 0800: loss: 3939.897
iteration 0900: loss: 3938.664
iteration 1000: loss: 3931.965
iteration 1100: loss: 3938.190
iteration 1200: loss: 3938.669
iteration 1300: loss: 3937.842
iteration 1400: loss: 3936.889
iteration 1500: loss: 3937.692
iteration 1600: loss: 3929.597
iteration 1700: loss: 3938.860
iteration 1800: loss: 3933.817
====> Epoch: 029 Train loss: 3937.6186  took : 53.227333784103394
====> Test loss: 3942.1415
iteration 0000: loss: 3933.585
iteration 0100: loss: 3940.600
iteration 0200: loss: 3941.149
iteration 0300: loss: 3943.152
iteration 0400: loss: 3940.538
iteration 0500: loss: 3934.045
iteration 0600: loss: 3933.369
iteration 0700: loss: 3931.123
iteration 0800: loss: 3939.810
iteration 0900: loss: 3937.677
iteration 1000: loss: 3938.645
iteration 1100: loss: 3942.166
iteration 1200: loss: 3941.076
iteration 1300: loss: 3934.265
iteration 1400: loss: 3933.172
iteration 1500: loss: 3933.627
iteration 1600: loss: 3944.851
iteration 1700: loss: 3934.646
iteration 1800: loss: 3937.406
====> Epoch: 030 Train loss: 3937.6047  took : 53.029703855514526
====> Test loss: 3942.4927
iteration 0000: loss: 3938.759
iteration 0100: loss: 3934.745
iteration 0200: loss: 3936.354
iteration 0300: loss: 3939.253
iteration 0400: loss: 3937.339
iteration 0500: loss: 3940.307
iteration 0600: loss: 3932.913
iteration 0700: loss: 3942.038
iteration 0800: loss: 3940.581
iteration 0900: loss: 3937.874
iteration 1000: loss: 3940.294
iteration 1100: loss: 3935.355
iteration 1200: loss: 3933.539
iteration 1300: loss: 3936.895
iteration 1400: loss: 3938.288
iteration 1500: loss: 3938.082
iteration 1600: loss: 3929.027
iteration 1700: loss: 3942.914
iteration 1800: loss: 3937.929
====> Epoch: 031 Train loss: 3937.3142  took : 53.10954737663269
====> Test loss: 3941.8923
iteration 0000: loss: 3932.137
iteration 0100: loss: 3937.231
iteration 0200: loss: 3940.179
iteration 0300: loss: 3932.186
iteration 0400: loss: 3940.312
iteration 0500: loss: 3939.276
iteration 0600: loss: 3940.494
iteration 0700: loss: 3939.668
iteration 0800: loss: 3935.081
iteration 0900: loss: 3939.968
iteration 1000: loss: 3938.854
iteration 1100: loss: 3933.932
iteration 1200: loss: 3943.471
iteration 1300: loss: 3939.151
iteration 1400: loss: 3931.737
iteration 1500: loss: 3937.250
iteration 1600: loss: 3944.184
iteration 1700: loss: 3933.718
iteration 1800: loss: 3932.107
====> Epoch: 032 Train loss: 3937.2747  took : 53.1095654964447
====> Test loss: 3941.6244
iteration 0000: loss: 3938.454
iteration 0100: loss: 3941.986
iteration 0200: loss: 3942.901
iteration 0300: loss: 3946.129
iteration 0400: loss: 3936.065
iteration 0500: loss: 3935.229
iteration 0600: loss: 3935.125
iteration 0700: loss: 3935.245
iteration 0800: loss: 3939.090
iteration 0900: loss: 3936.513
iteration 1000: loss: 3936.874
iteration 1100: loss: 3940.658
iteration 1200: loss: 3935.647
iteration 1300: loss: 3935.918
iteration 1400: loss: 3934.562
iteration 1500: loss: 3933.286
iteration 1600: loss: 3937.707
iteration 1700: loss: 3941.240
iteration 1800: loss: 3938.091
====> Epoch: 033 Train loss: 3937.1099  took : 53.119797229766846
====> Test loss: 3941.2408
iteration 0000: loss: 3939.774
iteration 0100: loss: 3936.705
iteration 0200: loss: 3936.341
iteration 0300: loss: 3939.903
iteration 0400: loss: 3934.185
iteration 0500: loss: 3938.465
iteration 0600: loss: 3936.278
iteration 0700: loss: 3939.524
iteration 0800: loss: 3936.040
iteration 0900: loss: 3937.519
iteration 1000: loss: 3935.886
iteration 1100: loss: 3940.150
iteration 1200: loss: 3935.767
iteration 1300: loss: 3935.671
iteration 1400: loss: 3934.847
iteration 1500: loss: 3935.612
iteration 1600: loss: 3934.973
iteration 1700: loss: 3939.468
iteration 1800: loss: 3938.524
====> Epoch: 034 Train loss: 3937.2756  took : 53.12105131149292
====> Test loss: 3941.6513
iteration 0000: loss: 3933.908
iteration 0100: loss: 3939.486
iteration 0200: loss: 3931.587
iteration 0300: loss: 3934.352
iteration 0400: loss: 3934.441
iteration 0500: loss: 3941.447
iteration 0600: loss: 3937.627
iteration 0700: loss: 3939.028
iteration 0800: loss: 3934.854
iteration 0900: loss: 3936.989
iteration 1000: loss: 3940.278
iteration 1100: loss: 3936.178
iteration 1200: loss: 3933.570
iteration 1300: loss: 3933.361
iteration 1400: loss: 3941.568
iteration 1500: loss: 3935.804
iteration 1600: loss: 3936.810
iteration 1700: loss: 3938.479
iteration 1800: loss: 3936.410
====> Epoch: 035 Train loss: 3937.1135  took : 53.204920291900635
====> Test loss: 3941.5127
iteration 0000: loss: 3940.699
iteration 0100: loss: 3941.511
iteration 0200: loss: 3941.718
iteration 0300: loss: 3937.476
iteration 0400: loss: 3932.765
iteration 0500: loss: 3937.619
iteration 0600: loss: 3937.865
iteration 0700: loss: 3934.362
iteration 0800: loss: 3939.557
iteration 0900: loss: 3940.859
iteration 1000: loss: 3937.455
iteration 1100: loss: 3933.998
iteration 1200: loss: 3935.880
iteration 1300: loss: 3931.727
iteration 1400: loss: 3934.256
iteration 1500: loss: 3936.259
iteration 1600: loss: 3935.078
iteration 1700: loss: 3934.441
iteration 1800: loss: 3930.546
====> Epoch: 036 Train loss: 3937.1429  took : 53.201722860336304
====> Test loss: 3941.4228
iteration 0000: loss: 3934.594
iteration 0100: loss: 3937.039
iteration 0200: loss: 3939.106
iteration 0300: loss: 3936.744
iteration 0400: loss: 3939.137
iteration 0500: loss: 3936.471
iteration 0600: loss: 3935.294
iteration 0700: loss: 3933.547
iteration 0800: loss: 3936.627
iteration 0900: loss: 3940.429
iteration 1000: loss: 3933.780
iteration 1100: loss: 3939.739
iteration 1200: loss: 3933.011
iteration 1300: loss: 3935.620
iteration 1400: loss: 3934.909
iteration 1500: loss: 3931.027
iteration 1600: loss: 3939.425
iteration 1700: loss: 3939.005
iteration 1800: loss: 3935.204
====> Epoch: 037 Train loss: 3937.1164  took : 53.131964683532715
====> Test loss: 3941.5009
iteration 0000: loss: 3936.255
iteration 0100: loss: 3934.301
iteration 0200: loss: 3937.103
iteration 0300: loss: 3939.222
iteration 0400: loss: 3939.396
iteration 0500: loss: 3941.666
iteration 0600: loss: 3939.640
iteration 0700: loss: 3937.588
iteration 0800: loss: 3934.070
iteration 0900: loss: 3935.729
iteration 1000: loss: 3933.555
iteration 1100: loss: 3929.413
iteration 1200: loss: 3939.994
iteration 1300: loss: 3943.377
iteration 1400: loss: 3931.337
iteration 1500: loss: 3931.241
iteration 1600: loss: 3946.521
iteration 1700: loss: 3941.684
iteration 1800: loss: 3938.303
====> Epoch: 038 Train loss: 3937.1236  took : 53.114590883255005
====> Test loss: 3942.1085
iteration 0000: loss: 3937.650
iteration 0100: loss: 3934.107
iteration 0200: loss: 3933.019
iteration 0300: loss: 3933.276
iteration 0400: loss: 3941.472
iteration 0500: loss: 3937.257
iteration 0600: loss: 3936.591
iteration 0700: loss: 3936.132
iteration 0800: loss: 3938.697
iteration 0900: loss: 3940.152
iteration 1000: loss: 3933.590
iteration 1100: loss: 3936.852
iteration 1200: loss: 3938.556
iteration 1300: loss: 3939.100
iteration 1400: loss: 3932.533
iteration 1500: loss: 3934.007
iteration 1600: loss: 3934.632
iteration 1700: loss: 3940.273
iteration 1800: loss: 3938.843
====> Epoch: 039 Train loss: 3937.0794  took : 53.06882882118225
====> Test loss: 3941.0365
iteration 0000: loss: 3936.058
iteration 0100: loss: 3934.186
iteration 0200: loss: 3937.772
iteration 0300: loss: 3936.569
iteration 0400: loss: 3941.831
iteration 0500: loss: 3942.199
iteration 0600: loss: 3936.049
iteration 0700: loss: 3933.660
iteration 0800: loss: 3941.149
iteration 0900: loss: 3933.867
iteration 1000: loss: 3938.329
iteration 1100: loss: 3925.890
iteration 1200: loss: 3934.555
iteration 1300: loss: 3943.493
iteration 1400: loss: 3936.958
iteration 1500: loss: 3931.148
iteration 1600: loss: 3934.956
iteration 1700: loss: 3935.399
iteration 1800: loss: 3939.741
====> Epoch: 040 Train loss: 3936.8584  took : 53.02059626579285
====> Test loss: 3941.2938
iteration 0000: loss: 3936.016
iteration 0100: loss: 3937.739
iteration 0200: loss: 3927.961
iteration 0300: loss: 3931.465
iteration 0400: loss: 3940.039
iteration 0500: loss: 3939.933
iteration 0600: loss: 3941.716
iteration 0700: loss: 3932.943
iteration 0800: loss: 3931.728
iteration 0900: loss: 3936.138
iteration 1000: loss: 3936.252
iteration 1100: loss: 3932.888
iteration 1200: loss: 3934.625
iteration 1300: loss: 3939.426
iteration 1400: loss: 3938.073
iteration 1500: loss: 3935.752
iteration 1600: loss: 3935.503
iteration 1700: loss: 3935.115
iteration 1800: loss: 3939.398
====> Epoch: 041 Train loss: 3936.8169  took : 53.2518744468689
====> Test loss: 3941.2888
iteration 0000: loss: 3938.164
iteration 0100: loss: 3939.250
iteration 0200: loss: 3941.303
iteration 0300: loss: 3934.965
iteration 0400: loss: 3929.341
iteration 0500: loss: 3935.113
iteration 0600: loss: 3940.454
iteration 0700: loss: 3942.445
iteration 0800: loss: 3936.555
iteration 0900: loss: 3938.689
iteration 1000: loss: 3934.102
iteration 1100: loss: 3937.497
iteration 1200: loss: 3938.857
iteration 1300: loss: 3932.310
iteration 1400: loss: 3932.371
iteration 1500: loss: 3938.471
iteration 1600: loss: 3935.298
iteration 1700: loss: 3938.452
iteration 1800: loss: 3941.412
====> Epoch: 042 Train loss: 3936.7631  took : 52.981379985809326
====> Test loss: 3941.1597
iteration 0000: loss: 3942.222
iteration 0100: loss: 3935.483
iteration 0200: loss: 3932.538
iteration 0300: loss: 3933.745
iteration 0400: loss: 3935.674
iteration 0500: loss: 3938.398
iteration 0600: loss: 3941.281
iteration 0700: loss: 3939.182
iteration 0800: loss: 3942.310
iteration 0900: loss: 3936.091
iteration 1000: loss: 3935.545
iteration 1100: loss: 3944.173
iteration 1200: loss: 3939.345
iteration 1300: loss: 3934.154
iteration 1400: loss: 3928.205
iteration 1500: loss: 3935.788
iteration 1600: loss: 3939.676
iteration 1700: loss: 3942.576
iteration 1800: loss: 3930.747
====> Epoch: 043 Train loss: 3936.9631  took : 52.99719262123108
====> Test loss: 3941.3245
iteration 0000: loss: 3935.469
iteration 0100: loss: 3935.530
iteration 0200: loss: 3940.810
iteration 0300: loss: 3937.510
iteration 0400: loss: 3936.667
iteration 0500: loss: 3928.867
iteration 0600: loss: 3939.744
iteration 0700: loss: 3935.338
iteration 0800: loss: 3940.465
iteration 0900: loss: 3938.567
iteration 1000: loss: 3941.220
iteration 1100: loss: 3931.314
iteration 1200: loss: 3937.507
iteration 1300: loss: 3936.873
iteration 1400: loss: 3938.193
iteration 1500: loss: 3934.939
iteration 1600: loss: 3938.376
iteration 1700: loss: 3934.113
iteration 1800: loss: 3935.205
====> Epoch: 044 Train loss: 3936.7586  took : 52.890419483184814
====> Test loss: 3941.1332
iteration 0000: loss: 3938.121
iteration 0100: loss: 3934.824
iteration 0200: loss: 3942.175
iteration 0300: loss: 3937.604
iteration 0400: loss: 3929.558
iteration 0500: loss: 3932.967
iteration 0600: loss: 3933.761
iteration 0700: loss: 3943.348
iteration 0800: loss: 3940.333
iteration 0900: loss: 3942.181
iteration 1000: loss: 3940.755
iteration 1100: loss: 3935.591
iteration 1200: loss: 3936.441
iteration 1300: loss: 3936.870
iteration 1400: loss: 3935.823
iteration 1500: loss: 3936.545
iteration 1600: loss: 3935.494
iteration 1700: loss: 3936.357
iteration 1800: loss: 3937.331
====> Epoch: 045 Train loss: 3936.7895  took : 52.85261058807373
====> Test loss: 3941.1602
iteration 0000: loss: 3935.876
iteration 0100: loss: 3942.860
iteration 0200: loss: 3935.094
iteration 0300: loss: 3937.058
iteration 0400: loss: 3930.384
iteration 0500: loss: 3939.780
iteration 0600: loss: 3938.345
iteration 0700: loss: 3936.839
iteration 0800: loss: 3932.220
iteration 0900: loss: 3939.974
iteration 1000: loss: 3938.911
iteration 1100: loss: 3937.453
iteration 1200: loss: 3939.061
iteration 1300: loss: 3940.347
iteration 1400: loss: 3931.015
iteration 1500: loss: 3936.345
iteration 1600: loss: 3936.921
iteration 1700: loss: 3945.841
iteration 1800: loss: 3939.852
====> Epoch: 046 Train loss: 3936.7830  took : 53.01276135444641
====> Test loss: 3941.2890
iteration 0000: loss: 3939.403
iteration 0100: loss: 3932.920
iteration 0200: loss: 3932.089
iteration 0300: loss: 3937.876
iteration 0400: loss: 3934.447
iteration 0500: loss: 3932.314
iteration 0600: loss: 3939.358
iteration 0700: loss: 3932.135
iteration 0800: loss: 3935.746
iteration 0900: loss: 3943.443
iteration 1000: loss: 3931.126
iteration 1100: loss: 3933.628
iteration 1200: loss: 3934.654
iteration 1300: loss: 3937.186
iteration 1400: loss: 3935.377
iteration 1500: loss: 3937.335
iteration 1600: loss: 3935.315
iteration 1700: loss: 3932.832
iteration 1800: loss: 3941.007
====> Epoch: 047 Train loss: 3936.5916  took : 53.00875282287598
====> Test loss: 3941.1251
iteration 0000: loss: 3932.273
iteration 0100: loss: 3935.667
iteration 0200: loss: 3930.778
iteration 0300: loss: 3937.727
iteration 0400: loss: 3942.273
iteration 0500: loss: 3936.329
iteration 0600: loss: 3944.413
iteration 0700: loss: 3932.989
iteration 0800: loss: 3936.520
iteration 0900: loss: 3935.982
iteration 1000: loss: 3936.453
iteration 1100: loss: 3937.563
iteration 1200: loss: 3931.469
iteration 1300: loss: 3934.783
iteration 1400: loss: 3936.540
iteration 1500: loss: 3937.297
iteration 1600: loss: 3936.913
iteration 1700: loss: 3937.684
iteration 1800: loss: 3936.773
====> Epoch: 048 Train loss: 3936.4486  took : 53.17389798164368
====> Test loss: 3941.1880
iteration 0000: loss: 3934.254
iteration 0100: loss: 3939.283
iteration 0200: loss: 3935.136
iteration 0300: loss: 3935.911
iteration 0400: loss: 3940.951
iteration 0500: loss: 3933.766
iteration 0600: loss: 3936.198
iteration 0700: loss: 3940.776
iteration 0800: loss: 3935.736
iteration 0900: loss: 3935.284
iteration 1000: loss: 3933.934
iteration 1100: loss: 3933.531
iteration 1200: loss: 3935.282
iteration 1300: loss: 3933.326
iteration 1400: loss: 3933.819
iteration 1500: loss: 3933.085
iteration 1600: loss: 3937.822
iteration 1700: loss: 3937.203
iteration 1800: loss: 3936.720
====> Epoch: 049 Train loss: 3936.5449  took : 52.940470695495605
====> Test loss: 3940.7499
iteration 0000: loss: 3938.033
iteration 0100: loss: 3935.150
iteration 0200: loss: 3933.383
iteration 0300: loss: 3927.256
iteration 0400: loss: 3939.015
iteration 0500: loss: 3930.902
iteration 0600: loss: 3937.375
iteration 0700: loss: 3934.413
iteration 0800: loss: 3935.021
iteration 0900: loss: 3931.617
iteration 1000: loss: 3937.408
iteration 1100: loss: 3936.899
iteration 1200: loss: 3933.347
iteration 1300: loss: 3936.604
iteration 1400: loss: 3935.209
iteration 1500: loss: 3937.295
iteration 1600: loss: 3932.020
iteration 1700: loss: 3939.633
iteration 1800: loss: 3937.040
====> Epoch: 050 Train loss: 3936.2107  took : 53.05169486999512
====> Test loss: 3940.5626
====> [MM-VAE] Time: 3170.927s or 00:52:50
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  15
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_15
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_15
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1990.421
iteration 0100: loss: 1566.625
iteration 0200: loss: 1560.479
iteration 0300: loss: 1553.033
iteration 0400: loss: 1551.011
iteration 0500: loss: 1539.546
iteration 0600: loss: 1544.637
iteration 0700: loss: 1536.818
iteration 0800: loss: 1541.270
iteration 0900: loss: 1539.579
====> Epoch: 001 Train loss: 1553.9776  took : 8.400614738464355
====> Test loss: 1536.4594
iteration 0000: loss: 1533.120
iteration 0100: loss: 1531.527
iteration 0200: loss: 1529.293
iteration 0300: loss: 1534.171
iteration 0400: loss: 1531.089
iteration 0500: loss: 1530.476
iteration 0600: loss: 1529.114
iteration 0700: loss: 1527.860
iteration 0800: loss: 1528.116
iteration 0900: loss: 1523.841
====> Epoch: 002 Train loss: 1530.2556  took : 8.495523691177368
====> Test loss: 1528.0963
iteration 0000: loss: 1521.675
iteration 0100: loss: 1529.965
iteration 0200: loss: 1528.071
iteration 0300: loss: 1527.430
iteration 0400: loss: 1528.136
iteration 0500: loss: 1523.640
iteration 0600: loss: 1522.722
iteration 0700: loss: 1523.771
iteration 0800: loss: 1522.308
iteration 0900: loss: 1524.736
====> Epoch: 003 Train loss: 1525.0221  took : 8.43911075592041
====> Test loss: 1525.9201
iteration 0000: loss: 1525.294
iteration 0100: loss: 1518.324
iteration 0200: loss: 1520.535
iteration 0300: loss: 1525.619
iteration 0400: loss: 1526.540
iteration 0500: loss: 1521.529
iteration 0600: loss: 1526.297
iteration 0700: loss: 1524.795
iteration 0800: loss: 1522.130
iteration 0900: loss: 1522.271
====> Epoch: 004 Train loss: 1523.0567  took : 8.518734455108643
====> Test loss: 1524.7876
iteration 0000: loss: 1521.158
iteration 0100: loss: 1521.764
iteration 0200: loss: 1521.019
iteration 0300: loss: 1519.775
iteration 0400: loss: 1521.586
iteration 0500: loss: 1519.147
iteration 0600: loss: 1521.418
iteration 0700: loss: 1520.221
iteration 0800: loss: 1521.429
iteration 0900: loss: 1521.947
====> Epoch: 005 Train loss: 1521.8524  took : 8.434864044189453
====> Test loss: 1523.6142
iteration 0000: loss: 1523.358
iteration 0100: loss: 1522.946
iteration 0200: loss: 1520.010
iteration 0300: loss: 1518.474
iteration 0400: loss: 1521.126
iteration 0500: loss: 1519.747
iteration 0600: loss: 1521.791
iteration 0700: loss: 1523.952
iteration 0800: loss: 1521.832
iteration 0900: loss: 1520.952
====> Epoch: 006 Train loss: 1520.9128  took : 8.471179008483887
====> Test loss: 1523.1875
iteration 0000: loss: 1521.094
iteration 0100: loss: 1520.129
iteration 0200: loss: 1522.235
iteration 0300: loss: 1517.701
iteration 0400: loss: 1517.117
iteration 0500: loss: 1517.082
iteration 0600: loss: 1518.346
iteration 0700: loss: 1520.564
iteration 0800: loss: 1521.160
iteration 0900: loss: 1519.580
====> Epoch: 007 Train loss: 1520.0717  took : 8.533221244812012
====> Test loss: 1522.1651
iteration 0000: loss: 1518.643
iteration 0100: loss: 1520.160
iteration 0200: loss: 1523.025
iteration 0300: loss: 1518.745
iteration 0400: loss: 1522.668
iteration 0500: loss: 1519.415
iteration 0600: loss: 1519.062
iteration 0700: loss: 1517.859
iteration 0800: loss: 1519.792
iteration 0900: loss: 1519.412
====> Epoch: 008 Train loss: 1519.4232  took : 8.523448944091797
====> Test loss: 1521.7408
iteration 0000: loss: 1518.559
iteration 0100: loss: 1522.385
iteration 0200: loss: 1520.387
iteration 0300: loss: 1517.607
iteration 0400: loss: 1519.885
iteration 0500: loss: 1519.721
iteration 0600: loss: 1519.060
iteration 0700: loss: 1518.330
iteration 0800: loss: 1518.864
iteration 0900: loss: 1518.988
====> Epoch: 009 Train loss: 1518.8862  took : 8.406307220458984
====> Test loss: 1521.3697
iteration 0000: loss: 1518.886
iteration 0100: loss: 1519.199
iteration 0200: loss: 1517.637
iteration 0300: loss: 1519.772
iteration 0400: loss: 1516.223
iteration 0500: loss: 1517.916
iteration 0600: loss: 1521.068
iteration 0700: loss: 1515.622
iteration 0800: loss: 1517.559
iteration 0900: loss: 1515.130
====> Epoch: 010 Train loss: 1518.4769  took : 8.406840324401855
====> Test loss: 1521.0279
iteration 0000: loss: 1515.646
iteration 0100: loss: 1523.841
iteration 0200: loss: 1518.754
iteration 0300: loss: 1518.083
iteration 0400: loss: 1520.708
iteration 0500: loss: 1519.703
iteration 0600: loss: 1518.686
iteration 0700: loss: 1518.767
iteration 0800: loss: 1519.891
iteration 0900: loss: 1521.381
====> Epoch: 011 Train loss: 1518.1378  took : 8.423067808151245
====> Test loss: 1521.2094
iteration 0000: loss: 1518.761
iteration 0100: loss: 1518.809
iteration 0200: loss: 1516.722
iteration 0300: loss: 1515.831
iteration 0400: loss: 1517.779
iteration 0500: loss: 1518.391
iteration 0600: loss: 1518.413
iteration 0700: loss: 1520.998
iteration 0800: loss: 1517.274
iteration 0900: loss: 1519.552
====> Epoch: 012 Train loss: 1517.8374  took : 8.529287099838257
====> Test loss: 1520.4427
iteration 0000: loss: 1515.392
iteration 0100: loss: 1513.399
iteration 0200: loss: 1519.267
iteration 0300: loss: 1517.996
iteration 0400: loss: 1514.307
iteration 0500: loss: 1514.946
iteration 0600: loss: 1515.732
iteration 0700: loss: 1514.384
iteration 0800: loss: 1516.253
iteration 0900: loss: 1515.829
====> Epoch: 013 Train loss: 1517.4964  took : 8.394535064697266
====> Test loss: 1520.3019
iteration 0000: loss: 1514.748
iteration 0100: loss: 1520.336
iteration 0200: loss: 1515.461
iteration 0300: loss: 1518.240
iteration 0400: loss: 1514.941
iteration 0500: loss: 1518.768
iteration 0600: loss: 1517.071
iteration 0700: loss: 1521.608
iteration 0800: loss: 1518.314
iteration 0900: loss: 1515.470
====> Epoch: 014 Train loss: 1517.2869  took : 8.392399549484253
====> Test loss: 1520.0874
iteration 0000: loss: 1515.555
iteration 0100: loss: 1516.130
iteration 0200: loss: 1514.323
iteration 0300: loss: 1518.168
iteration 0400: loss: 1516.364
iteration 0500: loss: 1517.365
iteration 0600: loss: 1516.460
iteration 0700: loss: 1516.205
iteration 0800: loss: 1515.021
iteration 0900: loss: 1517.280
====> Epoch: 015 Train loss: 1517.0401  took : 8.528315782546997
====> Test loss: 1519.7299
iteration 0000: loss: 1517.315
iteration 0100: loss: 1514.564
iteration 0200: loss: 1514.641
iteration 0300: loss: 1516.449
iteration 0400: loss: 1518.521
iteration 0500: loss: 1518.417
iteration 0600: loss: 1517.720
iteration 0700: loss: 1518.984
iteration 0800: loss: 1520.464
iteration 0900: loss: 1515.864
====> Epoch: 016 Train loss: 1516.8532  took : 8.539386987686157
====> Test loss: 1519.7553
iteration 0000: loss: 1518.505
iteration 0100: loss: 1517.098
iteration 0200: loss: 1515.731
iteration 0300: loss: 1514.201
iteration 0400: loss: 1517.855
iteration 0500: loss: 1519.219
iteration 0600: loss: 1514.972
iteration 0700: loss: 1516.749
iteration 0800: loss: 1517.596
iteration 0900: loss: 1513.087
====> Epoch: 017 Train loss: 1516.7249  took : 8.506908416748047
====> Test loss: 1519.6966
iteration 0000: loss: 1517.026
iteration 0100: loss: 1513.874
iteration 0200: loss: 1515.107
iteration 0300: loss: 1516.713
iteration 0400: loss: 1516.188
iteration 0500: loss: 1514.036
iteration 0600: loss: 1517.581
iteration 0700: loss: 1516.785
iteration 0800: loss: 1516.273
iteration 0900: loss: 1515.663
====> Epoch: 018 Train loss: 1516.5402  took : 8.522887706756592
====> Test loss: 1519.4571
iteration 0000: loss: 1515.043
iteration 0100: loss: 1513.917
iteration 0200: loss: 1517.087
iteration 0300: loss: 1514.511
iteration 0400: loss: 1514.667
iteration 0500: loss: 1515.789
iteration 0600: loss: 1514.354
iteration 0700: loss: 1516.388
iteration 0800: loss: 1514.927
iteration 0900: loss: 1517.874
====> Epoch: 019 Train loss: 1516.4858  took : 8.45869493484497
====> Test loss: 1519.6019
iteration 0000: loss: 1517.349
iteration 0100: loss: 1513.962
iteration 0200: loss: 1517.312
iteration 0300: loss: 1516.026
iteration 0400: loss: 1515.690
iteration 0500: loss: 1513.677
iteration 0600: loss: 1515.761
iteration 0700: loss: 1513.743
iteration 0800: loss: 1518.827
iteration 0900: loss: 1518.375
====> Epoch: 020 Train loss: 1516.3282  took : 8.529877185821533
====> Test loss: 1519.3496
iteration 0000: loss: 1515.447
iteration 0100: loss: 1516.689
iteration 0200: loss: 1516.290
iteration 0300: loss: 1518.311
iteration 0400: loss: 1518.036
iteration 0500: loss: 1514.517
iteration 0600: loss: 1514.756
iteration 0700: loss: 1517.191
iteration 0800: loss: 1517.418
iteration 0900: loss: 1514.265
====> Epoch: 021 Train loss: 1516.1847  took : 8.508681535720825
====> Test loss: 1519.1302
iteration 0000: loss: 1514.517
iteration 0100: loss: 1513.358
iteration 0200: loss: 1514.815
iteration 0300: loss: 1517.166
iteration 0400: loss: 1515.262
iteration 0500: loss: 1515.086
iteration 0600: loss: 1514.337
iteration 0700: loss: 1516.158
iteration 0800: loss: 1514.931
iteration 0900: loss: 1520.643
====> Epoch: 022 Train loss: 1516.0546  took : 8.519874334335327
====> Test loss: 1519.2115
iteration 0000: loss: 1514.851
iteration 0100: loss: 1517.434
iteration 0200: loss: 1514.507
iteration 0300: loss: 1517.272
iteration 0400: loss: 1516.992
iteration 0500: loss: 1513.261
iteration 0600: loss: 1516.055
iteration 0700: loss: 1516.334
iteration 0800: loss: 1515.520
iteration 0900: loss: 1517.528
====> Epoch: 023 Train loss: 1515.9483  took : 8.52592420578003
====> Test loss: 1519.0747
iteration 0000: loss: 1514.025
iteration 0100: loss: 1513.572
iteration 0200: loss: 1515.596
iteration 0300: loss: 1516.090
iteration 0400: loss: 1515.774
iteration 0500: loss: 1513.664
iteration 0600: loss: 1514.843
iteration 0700: loss: 1517.127
iteration 0800: loss: 1517.406
iteration 0900: loss: 1511.864
====> Epoch: 024 Train loss: 1515.8851  took : 8.48622465133667
====> Test loss: 1519.0204
iteration 0000: loss: 1514.865
iteration 0100: loss: 1515.037
iteration 0200: loss: 1515.801
iteration 0300: loss: 1515.751
iteration 0400: loss: 1517.346
iteration 0500: loss: 1515.039
iteration 0600: loss: 1514.517
iteration 0700: loss: 1516.719
iteration 0800: loss: 1515.920
iteration 0900: loss: 1515.073
====> Epoch: 025 Train loss: 1515.8016  took : 8.543842315673828
====> Test loss: 1519.0116
iteration 0000: loss: 1518.358
iteration 0100: loss: 1513.515
iteration 0200: loss: 1511.388
iteration 0300: loss: 1514.903
iteration 0400: loss: 1515.668
iteration 0500: loss: 1519.403
iteration 0600: loss: 1514.473
iteration 0700: loss: 1517.438
iteration 0800: loss: 1517.091
iteration 0900: loss: 1514.755
====> Epoch: 026 Train loss: 1515.7644  took : 8.45282769203186
====> Test loss: 1518.8903
iteration 0000: loss: 1515.282
iteration 0100: loss: 1513.921
iteration 0200: loss: 1513.752
iteration 0300: loss: 1516.586
iteration 0400: loss: 1515.186
iteration 0500: loss: 1515.460
iteration 0600: loss: 1511.627
iteration 0700: loss: 1514.561
iteration 0800: loss: 1513.022
iteration 0900: loss: 1512.303
====> Epoch: 027 Train loss: 1515.6070  took : 8.447185039520264
====> Test loss: 1518.7784
iteration 0000: loss: 1515.109
iteration 0100: loss: 1516.444
iteration 0200: loss: 1515.605
iteration 0300: loss: 1514.444
iteration 0400: loss: 1516.239
iteration 0500: loss: 1517.155
iteration 0600: loss: 1515.422
iteration 0700: loss: 1512.344
iteration 0800: loss: 1514.450
iteration 0900: loss: 1518.536
====> Epoch: 028 Train loss: 1515.5695  took : 8.528648614883423
====> Test loss: 1519.0149
iteration 0000: loss: 1513.715
iteration 0100: loss: 1515.359
iteration 0200: loss: 1514.584
iteration 0300: loss: 1514.000
iteration 0400: loss: 1517.156
iteration 0500: loss: 1515.292
iteration 0600: loss: 1513.462
iteration 0700: loss: 1514.859
iteration 0800: loss: 1516.601
iteration 0900: loss: 1513.392
====> Epoch: 029 Train loss: 1515.4998  took : 8.47210693359375
====> Test loss: 1518.6898
iteration 0000: loss: 1514.252
iteration 0100: loss: 1517.034
iteration 0200: loss: 1514.079
iteration 0300: loss: 1516.161
iteration 0400: loss: 1513.588
iteration 0500: loss: 1517.745
iteration 0600: loss: 1516.100
iteration 0700: loss: 1514.266
iteration 0800: loss: 1517.932
iteration 0900: loss: 1513.958
====> Epoch: 030 Train loss: 1515.4456  took : 8.499098300933838
====> Test loss: 1518.7829
iteration 0000: loss: 1515.602
iteration 0100: loss: 1519.422
iteration 0200: loss: 1516.343
iteration 0300: loss: 1515.260
iteration 0400: loss: 1515.405
iteration 0500: loss: 1515.906
iteration 0600: loss: 1517.099
iteration 0700: loss: 1517.362
iteration 0800: loss: 1516.473
iteration 0900: loss: 1515.076
====> Epoch: 031 Train loss: 1515.3117  took : 8.536073207855225
====> Test loss: 1518.6722
iteration 0000: loss: 1514.544
iteration 0100: loss: 1517.390
iteration 0200: loss: 1516.970
iteration 0300: loss: 1514.066
iteration 0400: loss: 1515.266
iteration 0500: loss: 1515.170
iteration 0600: loss: 1515.750
iteration 0700: loss: 1512.914
iteration 0800: loss: 1515.404
iteration 0900: loss: 1515.341
====> Epoch: 032 Train loss: 1515.2333  took : 8.397960662841797
====> Test loss: 1518.7627
iteration 0000: loss: 1515.191
iteration 0100: loss: 1517.497
iteration 0200: loss: 1515.824
iteration 0300: loss: 1515.701
iteration 0400: loss: 1514.786
iteration 0500: loss: 1516.280
iteration 0600: loss: 1516.921
iteration 0700: loss: 1516.318
iteration 0800: loss: 1513.533
iteration 0900: loss: 1514.602
====> Epoch: 033 Train loss: 1515.1974  took : 8.495500564575195
====> Test loss: 1518.7342
iteration 0000: loss: 1514.002
iteration 0100: loss: 1512.007
iteration 0200: loss: 1520.097
iteration 0300: loss: 1513.883
iteration 0400: loss: 1513.332
iteration 0500: loss: 1514.071
iteration 0600: loss: 1513.281
iteration 0700: loss: 1513.664
iteration 0800: loss: 1515.452
iteration 0900: loss: 1517.420
====> Epoch: 034 Train loss: 1515.2066  took : 8.491233587265015
====> Test loss: 1518.5395
iteration 0000: loss: 1514.576
iteration 0100: loss: 1516.231
iteration 0200: loss: 1516.361
iteration 0300: loss: 1514.438
iteration 0400: loss: 1516.500
iteration 0500: loss: 1513.903
iteration 0600: loss: 1516.561
iteration 0700: loss: 1515.932
iteration 0800: loss: 1516.048
iteration 0900: loss: 1512.013
====> Epoch: 035 Train loss: 1515.1124  took : 8.504775285720825
====> Test loss: 1518.5944
iteration 0000: loss: 1513.651
iteration 0100: loss: 1515.119
iteration 0200: loss: 1514.928
iteration 0300: loss: 1518.025
iteration 0400: loss: 1512.858
iteration 0500: loss: 1515.744
iteration 0600: loss: 1513.874
iteration 0700: loss: 1516.943
iteration 0800: loss: 1512.820
iteration 0900: loss: 1515.088
====> Epoch: 036 Train loss: 1515.0267  took : 8.44060730934143
====> Test loss: 1518.4414
iteration 0000: loss: 1515.694
iteration 0100: loss: 1514.283
iteration 0200: loss: 1514.707
iteration 0300: loss: 1516.020
iteration 0400: loss: 1516.583
iteration 0500: loss: 1514.846
iteration 0600: loss: 1517.857
iteration 0700: loss: 1517.747
iteration 0800: loss: 1515.256
iteration 0900: loss: 1515.152
====> Epoch: 037 Train loss: 1514.9719  took : 8.435590028762817
====> Test loss: 1518.5129
iteration 0000: loss: 1514.807
iteration 0100: loss: 1515.742
iteration 0200: loss: 1514.985
iteration 0300: loss: 1514.077
iteration 0400: loss: 1516.353
iteration 0500: loss: 1515.825
iteration 0600: loss: 1514.198
iteration 0700: loss: 1515.911
iteration 0800: loss: 1515.705
iteration 0900: loss: 1514.131
====> Epoch: 038 Train loss: 1514.9306  took : 8.430157661437988
====> Test loss: 1518.4188
iteration 0000: loss: 1515.501
iteration 0100: loss: 1513.562
iteration 0200: loss: 1513.894
iteration 0300: loss: 1516.769
iteration 0400: loss: 1513.747
iteration 0500: loss: 1513.596
iteration 0600: loss: 1513.661
iteration 0700: loss: 1517.905
iteration 0800: loss: 1515.450
iteration 0900: loss: 1515.183
====> Epoch: 039 Train loss: 1514.8856  took : 8.486000537872314
====> Test loss: 1518.4530
iteration 0000: loss: 1515.802
iteration 0100: loss: 1515.854
iteration 0200: loss: 1513.082
iteration 0300: loss: 1514.046
iteration 0400: loss: 1513.854
iteration 0500: loss: 1517.515
iteration 0600: loss: 1514.836
iteration 0700: loss: 1516.936
iteration 0800: loss: 1517.244
iteration 0900: loss: 1513.370
====> Epoch: 040 Train loss: 1514.8103  took : 8.43216872215271
====> Test loss: 1518.3923
iteration 0000: loss: 1514.562
iteration 0100: loss: 1517.400
iteration 0200: loss: 1514.851
iteration 0300: loss: 1512.883
iteration 0400: loss: 1514.014
iteration 0500: loss: 1512.289
iteration 0600: loss: 1516.555
iteration 0700: loss: 1515.896
iteration 0800: loss: 1518.111
iteration 0900: loss: 1514.455
====> Epoch: 041 Train loss: 1514.7626  took : 8.424539804458618
====> Test loss: 1518.3686
iteration 0000: loss: 1513.624
iteration 0100: loss: 1513.094
iteration 0200: loss: 1515.045
iteration 0300: loss: 1511.915
iteration 0400: loss: 1514.210
iteration 0500: loss: 1515.972
iteration 0600: loss: 1514.506
iteration 0700: loss: 1514.491
iteration 0800: loss: 1512.952
iteration 0900: loss: 1513.565
====> Epoch: 042 Train loss: 1514.9687  took : 8.551267385482788
====> Test loss: 1518.6476
iteration 0000: loss: 1514.487
iteration 0100: loss: 1514.501
iteration 0200: loss: 1514.786
iteration 0300: loss: 1513.729
iteration 0400: loss: 1517.333
iteration 0500: loss: 1512.786
iteration 0600: loss: 1515.893
iteration 0700: loss: 1513.210
iteration 0800: loss: 1514.955
iteration 0900: loss: 1513.241
====> Epoch: 043 Train loss: 1514.7545  took : 8.462767839431763
====> Test loss: 1518.3678
iteration 0000: loss: 1515.867
iteration 0100: loss: 1513.331
iteration 0200: loss: 1513.164
iteration 0300: loss: 1513.717
iteration 0400: loss: 1513.458
iteration 0500: loss: 1513.193
iteration 0600: loss: 1514.634
iteration 0700: loss: 1514.547
iteration 0800: loss: 1515.686
iteration 0900: loss: 1512.516
====> Epoch: 044 Train loss: 1514.6914  took : 8.431655406951904
====> Test loss: 1518.3538
iteration 0000: loss: 1513.455
iteration 0100: loss: 1512.455
iteration 0200: loss: 1514.682
iteration 0300: loss: 1514.898
iteration 0400: loss: 1515.767
iteration 0500: loss: 1514.150
iteration 0600: loss: 1513.750
iteration 0700: loss: 1514.211
iteration 0800: loss: 1516.404
iteration 0900: loss: 1513.168
====> Epoch: 045 Train loss: 1514.6106  took : 8.393232583999634
====> Test loss: 1518.2750
iteration 0000: loss: 1513.621
iteration 0100: loss: 1513.428
iteration 0200: loss: 1516.630
iteration 0300: loss: 1513.034
iteration 0400: loss: 1514.893
iteration 0500: loss: 1515.012
iteration 0600: loss: 1515.858
iteration 0700: loss: 1514.229
iteration 0800: loss: 1513.005
iteration 0900: loss: 1512.814
====> Epoch: 046 Train loss: 1514.5958  took : 8.469888925552368
====> Test loss: 1518.2411
iteration 0000: loss: 1515.102
iteration 0100: loss: 1514.147
iteration 0200: loss: 1511.988
iteration 0300: loss: 1513.329
iteration 0400: loss: 1514.066
iteration 0500: loss: 1514.460
iteration 0600: loss: 1513.318
iteration 0700: loss: 1515.440
iteration 0800: loss: 1514.926
iteration 0900: loss: 1511.581
====> Epoch: 047 Train loss: 1514.4988  took : 8.58811330795288
====> Test loss: 1518.1884
iteration 0000: loss: 1515.156
iteration 0100: loss: 1513.715
iteration 0200: loss: 1515.840
iteration 0300: loss: 1515.907
iteration 0400: loss: 1515.960
iteration 0500: loss: 1515.704
iteration 0600: loss: 1514.141
iteration 0700: loss: 1515.182
iteration 0800: loss: 1514.660
iteration 0900: loss: 1516.536
====> Epoch: 048 Train loss: 1514.4141  took : 8.49398398399353
====> Test loss: 1518.1614
iteration 0000: loss: 1515.729
iteration 0100: loss: 1515.388
iteration 0200: loss: 1512.766
iteration 0300: loss: 1514.274
iteration 0400: loss: 1515.390
iteration 0500: loss: 1512.282
iteration 0600: loss: 1514.308
iteration 0700: loss: 1514.093
iteration 0800: loss: 1515.017
iteration 0900: loss: 1515.314
====> Epoch: 049 Train loss: 1514.3817  took : 8.447171211242676
====> Test loss: 1518.2294
iteration 0000: loss: 1513.650
iteration 0100: loss: 1513.666
iteration 0200: loss: 1515.783
iteration 0300: loss: 1514.063
iteration 0400: loss: 1515.272
iteration 0500: loss: 1511.685
iteration 0600: loss: 1517.224
iteration 0700: loss: 1514.039
iteration 0800: loss: 1514.302
iteration 0900: loss: 1513.858
====> Epoch: 050 Train loss: 1514.4242  took : 8.466142892837524
====> Test loss: 1518.2808
====> [MM-VAE] Time: 507.699s or 00:08:27
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  15
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_15
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_15
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.154
iteration 0100: loss: 2122.917
iteration 0200: loss: 2058.954
iteration 0300: loss: 2023.664
iteration 0400: loss: 2018.860
iteration 0500: loss: 2001.732
iteration 0600: loss: 1999.837
iteration 0700: loss: 1996.291
iteration 0800: loss: 1993.033
iteration 0900: loss: 1983.711
====> Epoch: 001 Train loss: 2026.2799  took : 13.331488609313965
====> Test loss: 1993.8724
iteration 0000: loss: 1996.797
iteration 0100: loss: 1988.425
iteration 0200: loss: 1983.184
iteration 0300: loss: 1983.356
iteration 0400: loss: 1980.974
iteration 0500: loss: 1983.434
iteration 0600: loss: 1976.888
iteration 0700: loss: 1973.824
iteration 0800: loss: 1980.721
iteration 0900: loss: 1978.284
====> Epoch: 002 Train loss: 1981.6883  took : 12.642159938812256
====> Test loss: 1977.2316
iteration 0000: loss: 1972.134
iteration 0100: loss: 1978.797
iteration 0200: loss: 1970.519
iteration 0300: loss: 1974.558
iteration 0400: loss: 1973.699
iteration 0500: loss: 1968.403
iteration 0600: loss: 1967.848
iteration 0700: loss: 1970.179
iteration 0800: loss: 1966.492
iteration 0900: loss: 1965.440
====> Epoch: 003 Train loss: 1970.2651  took : 11.969016790390015
====> Test loss: 1968.6039
iteration 0000: loss: 1963.162
iteration 0100: loss: 1967.446
iteration 0200: loss: 1963.358
iteration 0300: loss: 1964.823
iteration 0400: loss: 1965.470
iteration 0500: loss: 1961.218
iteration 0600: loss: 1964.591
iteration 0700: loss: 1959.259
iteration 0800: loss: 1964.549
iteration 0900: loss: 1960.645
====> Epoch: 004 Train loss: 1963.6979  took : 12.306634187698364
====> Test loss: 1963.6720
iteration 0000: loss: 1961.928
iteration 0100: loss: 1964.102
iteration 0200: loss: 1962.843
iteration 0300: loss: 1961.509
iteration 0400: loss: 1959.830
iteration 0500: loss: 1958.370
iteration 0600: loss: 1957.411
iteration 0700: loss: 1961.629
iteration 0800: loss: 1960.708
iteration 0900: loss: 1961.026
====> Epoch: 005 Train loss: 1960.1277  took : 11.716793775558472
====> Test loss: 1960.9003
iteration 0000: loss: 1961.864
iteration 0100: loss: 1958.033
iteration 0200: loss: 1961.237
iteration 0300: loss: 1956.996
iteration 0400: loss: 1954.346
iteration 0500: loss: 1952.855
iteration 0600: loss: 1958.110
iteration 0700: loss: 1957.957
iteration 0800: loss: 1958.616
iteration 0900: loss: 1956.229
====> Epoch: 006 Train loss: 1957.5551  took : 12.372568368911743
====> Test loss: 1959.5038
iteration 0000: loss: 1955.537
iteration 0100: loss: 1954.609
iteration 0200: loss: 1956.605
iteration 0300: loss: 1955.186
iteration 0400: loss: 1955.231
iteration 0500: loss: 1957.635
iteration 0600: loss: 1954.914
iteration 0700: loss: 1954.945
iteration 0800: loss: 1956.322
iteration 0900: loss: 1953.636
====> Epoch: 007 Train loss: 1956.0028  took : 12.385272979736328
====> Test loss: 1958.5204
iteration 0000: loss: 1955.203
iteration 0100: loss: 1954.366
iteration 0200: loss: 1954.378
iteration 0300: loss: 1954.907
iteration 0400: loss: 1955.781
iteration 0500: loss: 1955.703
iteration 0600: loss: 1954.619
iteration 0700: loss: 1956.178
iteration 0800: loss: 1954.232
iteration 0900: loss: 1953.594
====> Epoch: 008 Train loss: 1954.8836  took : 12.41735029220581
====> Test loss: 1956.1700
iteration 0000: loss: 1954.483
iteration 0100: loss: 1954.378
iteration 0200: loss: 1952.889
iteration 0300: loss: 1955.152
iteration 0400: loss: 1953.785
iteration 0500: loss: 1953.892
iteration 0600: loss: 1951.812
iteration 0700: loss: 1951.903
iteration 0800: loss: 1956.281
iteration 0900: loss: 1952.913
====> Epoch: 009 Train loss: 1953.6508  took : 11.974270820617676
====> Test loss: 1954.9189
iteration 0000: loss: 1951.098
iteration 0100: loss: 1952.204
iteration 0200: loss: 1951.900
iteration 0300: loss: 1954.929
iteration 0400: loss: 1953.557
iteration 0500: loss: 1953.648
iteration 0600: loss: 1953.254
iteration 0700: loss: 1952.541
iteration 0800: loss: 1957.357
iteration 0900: loss: 1951.487
====> Epoch: 010 Train loss: 1952.9000  took : 12.32302713394165
====> Test loss: 1954.6379
iteration 0000: loss: 1950.740
iteration 0100: loss: 1953.519
iteration 0200: loss: 1952.995
iteration 0300: loss: 1952.884
iteration 0400: loss: 1953.540
iteration 0500: loss: 1951.641
iteration 0600: loss: 1952.031
iteration 0700: loss: 1952.948
iteration 0800: loss: 1953.765
iteration 0900: loss: 1952.435
====> Epoch: 011 Train loss: 1952.3655  took : 12.378461837768555
====> Test loss: 1954.1832
iteration 0000: loss: 1951.611
iteration 0100: loss: 1951.112
iteration 0200: loss: 1952.552
iteration 0300: loss: 1951.326
iteration 0400: loss: 1955.627
iteration 0500: loss: 1951.075
iteration 0600: loss: 1952.785
iteration 0700: loss: 1951.871
iteration 0800: loss: 1952.333
iteration 0900: loss: 1952.103
====> Epoch: 012 Train loss: 1951.8991  took : 12.44980764389038
====> Test loss: 1953.7747
iteration 0000: loss: 1952.044
iteration 0100: loss: 1951.991
iteration 0200: loss: 1952.946
iteration 0300: loss: 1950.234
iteration 0400: loss: 1950.617
iteration 0500: loss: 1951.074
iteration 0600: loss: 1954.084
iteration 0700: loss: 1952.153
iteration 0800: loss: 1952.845
iteration 0900: loss: 1950.701
====> Epoch: 013 Train loss: 1951.3399  took : 12.067252159118652
====> Test loss: 1952.9106
iteration 0000: loss: 1950.764
iteration 0100: loss: 1950.419
iteration 0200: loss: 1951.576
iteration 0300: loss: 1951.568
iteration 0400: loss: 1951.038
iteration 0500: loss: 1950.315
iteration 0600: loss: 1950.635
iteration 0700: loss: 1949.723
iteration 0800: loss: 1950.281
iteration 0900: loss: 1949.588
====> Epoch: 014 Train loss: 1950.8392  took : 13.024408102035522
====> Test loss: 1953.0753
iteration 0000: loss: 1953.550
iteration 0100: loss: 1950.383
iteration 0200: loss: 1950.742
iteration 0300: loss: 1951.794
iteration 0400: loss: 1951.859
iteration 0500: loss: 1950.079
iteration 0600: loss: 1952.944
iteration 0700: loss: 1952.779
iteration 0800: loss: 1951.088
iteration 0900: loss: 1950.943
====> Epoch: 015 Train loss: 1950.8569  took : 11.891231298446655
====> Test loss: 1952.4826
iteration 0000: loss: 1952.359
iteration 0100: loss: 1950.103
iteration 0200: loss: 1951.440
iteration 0300: loss: 1949.228
iteration 0400: loss: 1951.317
iteration 0500: loss: 1949.456
iteration 0600: loss: 1949.973
iteration 0700: loss: 1949.700
iteration 0800: loss: 1949.682
iteration 0900: loss: 1951.240
====> Epoch: 016 Train loss: 1950.5950  took : 12.373720169067383
====> Test loss: 1952.8979
iteration 0000: loss: 1949.740
iteration 0100: loss: 1951.517
iteration 0200: loss: 1950.029
iteration 0300: loss: 1950.070
iteration 0400: loss: 1949.836
iteration 0500: loss: 1952.292
iteration 0600: loss: 1951.208
iteration 0700: loss: 1953.192
iteration 0800: loss: 1949.931
iteration 0900: loss: 1949.596
====> Epoch: 017 Train loss: 1950.6421  took : 12.62700891494751
====> Test loss: 1952.7000
iteration 0000: loss: 1952.086
iteration 0100: loss: 1951.340
iteration 0200: loss: 1949.291
iteration 0300: loss: 1949.769
iteration 0400: loss: 1949.523
iteration 0500: loss: 1951.375
iteration 0600: loss: 1949.373
iteration 0700: loss: 1950.415
iteration 0800: loss: 1950.017
iteration 0900: loss: 1949.729
====> Epoch: 018 Train loss: 1950.3929  took : 11.724522113800049
====> Test loss: 1952.0349
iteration 0000: loss: 1950.074
iteration 0100: loss: 1951.853
iteration 0200: loss: 1951.186
iteration 0300: loss: 1951.057
iteration 0400: loss: 1950.279
iteration 0500: loss: 1950.055
iteration 0600: loss: 1949.427
iteration 0700: loss: 1950.775
iteration 0800: loss: 1949.391
iteration 0900: loss: 1949.858
====> Epoch: 019 Train loss: 1950.2722  took : 12.891566276550293
====> Test loss: 1952.1522
iteration 0000: loss: 1950.182
iteration 0100: loss: 1950.235
iteration 0200: loss: 1948.884
iteration 0300: loss: 1949.130
iteration 0400: loss: 1949.884
iteration 0500: loss: 1949.990
iteration 0600: loss: 1949.933
iteration 0700: loss: 1948.950
iteration 0800: loss: 1951.484
iteration 0900: loss: 1949.688
====> Epoch: 020 Train loss: 1950.0642  took : 11.797211647033691
====> Test loss: 1951.8121
iteration 0000: loss: 1950.560
iteration 0100: loss: 1948.843
iteration 0200: loss: 1949.670
iteration 0300: loss: 1949.297
iteration 0400: loss: 1950.316
iteration 0500: loss: 1948.454
iteration 0600: loss: 1950.182
iteration 0700: loss: 1949.180
iteration 0800: loss: 1949.861
iteration 0900: loss: 1949.284
====> Epoch: 021 Train loss: 1949.9721  took : 12.875663757324219
====> Test loss: 1951.7923
iteration 0000: loss: 1949.010
iteration 0100: loss: 1948.918
iteration 0200: loss: 1949.891
iteration 0300: loss: 1950.300
iteration 0400: loss: 1949.876
iteration 0500: loss: 1950.297
iteration 0600: loss: 1949.540
iteration 0700: loss: 1949.957
iteration 0800: loss: 1949.746
iteration 0900: loss: 1949.817
====> Epoch: 022 Train loss: 1949.8892  took : 11.885155439376831
====> Test loss: 1951.4691
iteration 0000: loss: 1948.791
iteration 0100: loss: 1948.684
iteration 0200: loss: 1949.696
iteration 0300: loss: 1949.376
iteration 0400: loss: 1952.547
iteration 0500: loss: 1950.033
iteration 0600: loss: 1951.011
iteration 0700: loss: 1950.792
iteration 0800: loss: 1948.622
iteration 0900: loss: 1949.461
====> Epoch: 023 Train loss: 1950.0435  took : 12.530385732650757
====> Test loss: 1951.9391
iteration 0000: loss: 1950.791
iteration 0100: loss: 1949.346
iteration 0200: loss: 1950.109
iteration 0300: loss: 1949.740
iteration 0400: loss: 1949.041
iteration 0500: loss: 1949.448
iteration 0600: loss: 1951.766
iteration 0700: loss: 1949.275
iteration 0800: loss: 1950.171
iteration 0900: loss: 1951.608
====> Epoch: 024 Train loss: 1950.0805  took : 12.424790859222412
====> Test loss: 1951.9084
iteration 0000: loss: 1949.973
iteration 0100: loss: 1949.749
iteration 0200: loss: 1949.775
iteration 0300: loss: 1950.355
iteration 0400: loss: 1951.503
iteration 0500: loss: 1948.751
iteration 0600: loss: 1949.949
iteration 0700: loss: 1949.002
iteration 0800: loss: 1949.331
iteration 0900: loss: 1950.455
====> Epoch: 025 Train loss: 1949.9791  took : 12.414087295532227
====> Test loss: 1952.6212
iteration 0000: loss: 1950.108
iteration 0100: loss: 1950.036
iteration 0200: loss: 1950.668
iteration 0300: loss: 1950.240
iteration 0400: loss: 1950.054
iteration 0500: loss: 1948.338
iteration 0600: loss: 1948.895
iteration 0700: loss: 1951.972
iteration 0800: loss: 1950.531
iteration 0900: loss: 1949.970
====> Epoch: 026 Train loss: 1949.8905  took : 11.80558156967163
====> Test loss: 1951.4059
iteration 0000: loss: 1949.065
iteration 0100: loss: 1949.252
iteration 0200: loss: 1951.582
iteration 0300: loss: 1949.729
iteration 0400: loss: 1950.118
iteration 0500: loss: 1949.528
iteration 0600: loss: 1950.485
iteration 0700: loss: 1949.849
iteration 0800: loss: 1948.353
iteration 0900: loss: 1948.689
====> Epoch: 027 Train loss: 1949.8247  took : 12.624128580093384
====> Test loss: 1951.4701
iteration 0000: loss: 1949.398
iteration 0100: loss: 1949.684
iteration 0200: loss: 1950.451
iteration 0300: loss: 1950.899
iteration 0400: loss: 1949.866
iteration 0500: loss: 1950.626
iteration 0600: loss: 1950.961
iteration 0700: loss: 1949.795
iteration 0800: loss: 1949.216
iteration 0900: loss: 1949.007
====> Epoch: 028 Train loss: 1949.9511  took : 11.774085521697998
====> Test loss: 1951.8893
iteration 0000: loss: 1950.827
iteration 0100: loss: 1950.323
iteration 0200: loss: 1948.350
iteration 0300: loss: 1949.833
iteration 0400: loss: 1951.150
iteration 0500: loss: 1949.821
iteration 0600: loss: 1949.422
iteration 0700: loss: 1949.327
iteration 0800: loss: 1948.329
iteration 0900: loss: 1949.137
====> Epoch: 029 Train loss: 1949.8921  took : 13.39097809791565
====> Test loss: 1951.6933
iteration 0000: loss: 1948.269
iteration 0100: loss: 1948.786
iteration 0200: loss: 1950.974
iteration 0300: loss: 1948.943
iteration 0400: loss: 1950.104
iteration 0500: loss: 1949.359
iteration 0600: loss: 1949.047
iteration 0700: loss: 1950.173
iteration 0800: loss: 1950.447
iteration 0900: loss: 1950.596
====> Epoch: 030 Train loss: 1949.7232  took : 11.62751054763794
====> Test loss: 1951.3372
iteration 0000: loss: 1949.548
iteration 0100: loss: 1949.584
iteration 0200: loss: 1949.786
iteration 0300: loss: 1951.500
iteration 0400: loss: 1950.094
iteration 0500: loss: 1949.895
iteration 0600: loss: 1948.899
iteration 0700: loss: 1950.509
iteration 0800: loss: 1948.851
iteration 0900: loss: 1949.362
====> Epoch: 031 Train loss: 1949.5470  took : 13.159448146820068
====> Test loss: 1951.1661
iteration 0000: loss: 1948.932
iteration 0100: loss: 1949.848
iteration 0200: loss: 1950.663
iteration 0300: loss: 1950.082
iteration 0400: loss: 1948.561
iteration 0500: loss: 1950.062
iteration 0600: loss: 1948.624
iteration 0700: loss: 1950.261
iteration 0800: loss: 1949.095
iteration 0900: loss: 1949.915
====> Epoch: 032 Train loss: 1949.6289  took : 12.134108066558838
====> Test loss: 1951.4127
iteration 0000: loss: 1948.994
iteration 0100: loss: 1949.173
iteration 0200: loss: 1949.275
iteration 0300: loss: 1949.712
iteration 0400: loss: 1948.907
iteration 0500: loss: 1949.611
iteration 0600: loss: 1951.229
iteration 0700: loss: 1948.679
iteration 0800: loss: 1948.800
iteration 0900: loss: 1950.992
====> Epoch: 033 Train loss: 1949.4149  took : 13.049512386322021
====> Test loss: 1951.1422
iteration 0000: loss: 1949.233
iteration 0100: loss: 1948.797
iteration 0200: loss: 1949.704
iteration 0300: loss: 1948.281
iteration 0400: loss: 1950.690
iteration 0500: loss: 1949.342
iteration 0600: loss: 1950.986
iteration 0700: loss: 1948.273
iteration 0800: loss: 1949.060
iteration 0900: loss: 1948.809
====> Epoch: 034 Train loss: 1949.2650  took : 13.45681357383728
====> Test loss: 1950.8740
iteration 0000: loss: 1948.964
iteration 0100: loss: 1948.998
iteration 0200: loss: 1949.820
iteration 0300: loss: 1948.945
iteration 0400: loss: 1948.265
iteration 0500: loss: 1948.078
iteration 0600: loss: 1949.740
iteration 0700: loss: 1950.434
iteration 0800: loss: 1949.359
iteration 0900: loss: 1949.872
====> Epoch: 035 Train loss: 1949.2930  took : 12.162008047103882
====> Test loss: 1951.2722
iteration 0000: loss: 1950.852
iteration 0100: loss: 1949.187
iteration 0200: loss: 1950.296
iteration 0300: loss: 1951.138
iteration 0400: loss: 1948.824
iteration 0500: loss: 1948.722
iteration 0600: loss: 1950.286
iteration 0700: loss: 1948.963
iteration 0800: loss: 1949.432
iteration 0900: loss: 1949.164
====> Epoch: 036 Train loss: 1949.4055  took : 12.434633731842041
====> Test loss: 1951.5120
iteration 0000: loss: 1949.931
iteration 0100: loss: 1949.324
iteration 0200: loss: 1948.581
iteration 0300: loss: 1948.225
iteration 0400: loss: 1948.714
iteration 0500: loss: 1949.713
iteration 0600: loss: 1948.901
iteration 0700: loss: 1949.446
iteration 0800: loss: 1948.494
iteration 0900: loss: 1949.069
====> Epoch: 037 Train loss: 1949.1879  took : 12.403887748718262
====> Test loss: 1950.8107
iteration 0000: loss: 1948.661
iteration 0100: loss: 1949.249
iteration 0200: loss: 1950.127
iteration 0300: loss: 1948.827
iteration 0400: loss: 1949.123
iteration 0500: loss: 1948.860
iteration 0600: loss: 1949.149
iteration 0700: loss: 1949.502
iteration 0800: loss: 1948.270
iteration 0900: loss: 1949.375
====> Epoch: 038 Train loss: 1949.3059  took : 13.254037618637085
====> Test loss: 1950.9836
iteration 0000: loss: 1949.742
iteration 0100: loss: 1948.816
iteration 0200: loss: 1949.987
iteration 0300: loss: 1948.994
iteration 0400: loss: 1948.859
iteration 0500: loss: 1949.344
iteration 0600: loss: 1949.064
iteration 0700: loss: 1949.243
iteration 0800: loss: 1949.182
iteration 0900: loss: 1948.528
====> Epoch: 039 Train loss: 1949.2858  took : 12.159098625183105
====> Test loss: 1950.9338
iteration 0000: loss: 1948.659
iteration 0100: loss: 1949.220
iteration 0200: loss: 1949.502
iteration 0300: loss: 1949.982
iteration 0400: loss: 1947.977
iteration 0500: loss: 1949.446
iteration 0600: loss: 1951.262
iteration 0700: loss: 1949.839
iteration 0800: loss: 1949.921
iteration 0900: loss: 1948.630
====> Epoch: 040 Train loss: 1949.2823  took : 13.046018362045288
====> Test loss: 1951.1474
iteration 0000: loss: 1949.556
iteration 0100: loss: 1948.356
iteration 0200: loss: 1948.870
iteration 0300: loss: 1951.062
iteration 0400: loss: 1948.343
iteration 0500: loss: 1949.131
iteration 0600: loss: 1949.224
iteration 0700: loss: 1950.104
iteration 0800: loss: 1949.233
iteration 0900: loss: 1949.607
====> Epoch: 041 Train loss: 1949.2673  took : 12.107202053070068
====> Test loss: 1951.1912
iteration 0000: loss: 1949.758
iteration 0100: loss: 1948.628
iteration 0200: loss: 1948.277
iteration 0300: loss: 1949.277
iteration 0400: loss: 1948.649
iteration 0500: loss: 1949.545
iteration 0600: loss: 1948.930
iteration 0700: loss: 1948.965
iteration 0800: loss: 1948.927
iteration 0900: loss: 1950.302
====> Epoch: 042 Train loss: 1949.2005  took : 13.232477903366089
====> Test loss: 1951.7056
iteration 0000: loss: 1949.590
iteration 0100: loss: 1950.158
iteration 0200: loss: 1949.238
iteration 0300: loss: 1949.290
iteration 0400: loss: 1948.644
iteration 0500: loss: 1948.443
iteration 0600: loss: 1949.024
iteration 0700: loss: 1950.240
iteration 0800: loss: 1950.097
iteration 0900: loss: 1949.649
====> Epoch: 043 Train loss: 1949.2963  took : 12.131888389587402
====> Test loss: 1951.1118
iteration 0000: loss: 1949.574
iteration 0100: loss: 1948.060
iteration 0200: loss: 1949.532
iteration 0300: loss: 1949.478
iteration 0400: loss: 1948.764
iteration 0500: loss: 1948.844
iteration 0600: loss: 1949.007
iteration 0700: loss: 1950.068
iteration 0800: loss: 1950.439
iteration 0900: loss: 1948.872
====> Epoch: 044 Train loss: 1949.1484  took : 12.145107984542847
====> Test loss: 1950.9479
iteration 0000: loss: 1949.020
iteration 0100: loss: 1949.725
iteration 0200: loss: 1948.770
iteration 0300: loss: 1948.774
iteration 0400: loss: 1948.732
iteration 0500: loss: 1949.345
iteration 0600: loss: 1948.881
iteration 0700: loss: 1948.161
iteration 0800: loss: 1950.957
iteration 0900: loss: 1948.875
====> Epoch: 045 Train loss: 1949.1486  took : 13.244423151016235
====> Test loss: 1950.9593
iteration 0000: loss: 1949.047
iteration 0100: loss: 1948.695
iteration 0200: loss: 1949.289
iteration 0300: loss: 1948.348
iteration 0400: loss: 1948.664
iteration 0500: loss: 1949.304
iteration 0600: loss: 1949.941
iteration 0700: loss: 1949.359
iteration 0800: loss: 1948.534
iteration 0900: loss: 1949.190
====> Epoch: 046 Train loss: 1949.0694  took : 12.929419994354248
====> Test loss: 1950.8396
iteration 0000: loss: 1948.901
iteration 0100: loss: 1948.691
iteration 0200: loss: 1948.481
iteration 0300: loss: 1949.448
iteration 0400: loss: 1948.581
iteration 0500: loss: 1949.528
iteration 0600: loss: 1948.763
iteration 0700: loss: 1948.693
iteration 0800: loss: 1949.939
iteration 0900: loss: 1949.981
====> Epoch: 047 Train loss: 1949.1596  took : 12.597718954086304
====> Test loss: 1951.2510
iteration 0000: loss: 1949.907
iteration 0100: loss: 1948.177
iteration 0200: loss: 1948.744
iteration 0300: loss: 1951.259
iteration 0400: loss: 1950.865
iteration 0500: loss: 1948.895
iteration 0600: loss: 1948.078
iteration 0700: loss: 1949.190
iteration 0800: loss: 1949.350
iteration 0900: loss: 1949.910
====> Epoch: 048 Train loss: 1949.1266  took : 11.764411211013794
====> Test loss: 1950.8224
iteration 0000: loss: 1948.927
iteration 0100: loss: 1950.476
iteration 0200: loss: 1949.246
iteration 0300: loss: 1948.049
iteration 0400: loss: 1948.193
iteration 0500: loss: 1948.928
iteration 0600: loss: 1949.333
iteration 0700: loss: 1948.561
iteration 0800: loss: 1949.443
iteration 0900: loss: 1948.466
====> Epoch: 049 Train loss: 1949.0210  took : 13.414703369140625
====> Test loss: 1950.5904
iteration 0000: loss: 1948.905
iteration 0100: loss: 1948.085
iteration 0200: loss: 1948.599
iteration 0300: loss: 1949.590
iteration 0400: loss: 1948.658
iteration 0500: loss: 1949.179
iteration 0600: loss: 1949.403
iteration 0700: loss: 1949.164
iteration 0800: loss: 1948.553
iteration 0900: loss: 1949.358
====> Epoch: 050 Train loss: 1949.1046  took : 12.331410646438599
====> Test loss: 1951.0611
====> [MM-VAE] Time: 694.750s or 00:11:34
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  15
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_15
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_15
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5219.445
iteration 0100: loss: 4144.986
iteration 0200: loss: 4094.130
iteration 0300: loss: 4057.979
iteration 0400: loss: 4033.994
iteration 0500: loss: 4022.988
iteration 0600: loss: 4020.500
iteration 0700: loss: 4018.368
iteration 0800: loss: 4002.332
iteration 0900: loss: 4010.813
iteration 1000: loss: 3999.846
iteration 1100: loss: 3993.682
iteration 1200: loss: 3996.156
iteration 1300: loss: 4004.494
iteration 1400: loss: 4006.945
iteration 1500: loss: 3998.468
iteration 1600: loss: 3997.864
iteration 1700: loss: 3985.099
iteration 1800: loss: 3991.708
====> Epoch: 001 Train loss: 4027.5974  took : 53.586610317230225
====> Test loss: 3994.0940
iteration 0000: loss: 3989.306
iteration 0100: loss: 3985.294
iteration 0200: loss: 3993.611
iteration 0300: loss: 4001.562
iteration 0400: loss: 3985.894
iteration 0500: loss: 3987.243
iteration 0600: loss: 3978.961
iteration 0700: loss: 3975.939
iteration 0800: loss: 3976.311
iteration 0900: loss: 3988.182
iteration 1000: loss: 3970.337
iteration 1100: loss: 3977.844
iteration 1200: loss: 3981.247
iteration 1300: loss: 3971.677
iteration 1400: loss: 3973.612
iteration 1500: loss: 3972.573
iteration 1600: loss: 3977.165
iteration 1700: loss: 3963.347
iteration 1800: loss: 3976.636
====> Epoch: 002 Train loss: 3981.4407  took : 53.653669595718384
====> Test loss: 3976.5757
iteration 0000: loss: 3968.323
iteration 0100: loss: 3974.191
iteration 0200: loss: 3973.525
iteration 0300: loss: 3978.009
iteration 0400: loss: 3974.518
iteration 0500: loss: 3971.510
iteration 0600: loss: 3970.272
iteration 0700: loss: 3974.353
iteration 0800: loss: 3971.168
iteration 0900: loss: 3970.470
iteration 1000: loss: 3954.471
iteration 1100: loss: 3966.345
iteration 1200: loss: 3970.879
iteration 1300: loss: 3963.747
iteration 1400: loss: 3963.877
iteration 1500: loss: 3963.901
iteration 1600: loss: 3958.438
iteration 1700: loss: 3970.014
iteration 1800: loss: 3958.083
====> Epoch: 003 Train loss: 3966.4897  took : 53.48594856262207
====> Test loss: 3962.5032
iteration 0000: loss: 3961.650
iteration 0100: loss: 3966.269
iteration 0200: loss: 3965.658
iteration 0300: loss: 3951.055
iteration 0400: loss: 3957.899
iteration 0500: loss: 3956.992
iteration 0600: loss: 3954.273
iteration 0700: loss: 3956.682
iteration 0800: loss: 3952.634
iteration 0900: loss: 3949.159
iteration 1000: loss: 3952.631
iteration 1100: loss: 3954.276
iteration 1200: loss: 3965.265
iteration 1300: loss: 3950.081
iteration 1400: loss: 3949.884
iteration 1500: loss: 3950.046
iteration 1600: loss: 3945.762
iteration 1700: loss: 3946.920
iteration 1800: loss: 3952.397
====> Epoch: 004 Train loss: 3955.5049  took : 53.108033418655396
====> Test loss: 3953.5026
iteration 0000: loss: 3954.457
iteration 0100: loss: 3950.529
iteration 0200: loss: 3944.379
iteration 0300: loss: 3950.878
iteration 0400: loss: 3961.432
iteration 0500: loss: 3948.456
iteration 0600: loss: 3948.274
iteration 0700: loss: 3940.831
iteration 0800: loss: 3952.851
iteration 0900: loss: 3943.667
iteration 1000: loss: 3958.091
iteration 1100: loss: 3953.133
iteration 1200: loss: 3945.853
iteration 1300: loss: 3949.509
iteration 1400: loss: 3948.637
iteration 1500: loss: 3949.756
iteration 1600: loss: 3954.243
iteration 1700: loss: 3948.625
iteration 1800: loss: 3947.790
====> Epoch: 005 Train loss: 3948.2822  took : 53.02892780303955
====> Test loss: 3949.5768
iteration 0000: loss: 3945.642
iteration 0100: loss: 3944.307
iteration 0200: loss: 3947.844
iteration 0300: loss: 3946.388
iteration 0400: loss: 3956.927
iteration 0500: loss: 3950.200
iteration 0600: loss: 3946.199
iteration 0700: loss: 3950.260
iteration 0800: loss: 3943.770
iteration 0900: loss: 3953.858
iteration 1000: loss: 3947.779
iteration 1100: loss: 3939.136
iteration 1200: loss: 3939.909
iteration 1300: loss: 3950.607
iteration 1400: loss: 3942.075
iteration 1500: loss: 3940.153
iteration 1600: loss: 3947.707
iteration 1700: loss: 3946.445
iteration 1800: loss: 3948.194
====> Epoch: 006 Train loss: 3944.6070  took : 52.92252564430237
====> Test loss: 3946.7267
iteration 0000: loss: 3946.129
iteration 0100: loss: 3940.668
iteration 0200: loss: 3937.822
iteration 0300: loss: 3946.981
iteration 0400: loss: 3941.318
iteration 0500: loss: 3946.020
iteration 0600: loss: 3942.521
iteration 0700: loss: 3941.223
iteration 0800: loss: 3940.355
iteration 0900: loss: 3938.211
iteration 1000: loss: 3941.783
iteration 1100: loss: 3945.081
iteration 1200: loss: 3952.158
iteration 1300: loss: 3941.090
iteration 1400: loss: 3944.557
iteration 1500: loss: 3950.127
iteration 1600: loss: 3946.486
iteration 1700: loss: 3937.455
iteration 1800: loss: 3933.634
====> Epoch: 007 Train loss: 3943.1082  took : 53.55653929710388
====> Test loss: 3946.0625
iteration 0000: loss: 3945.417
iteration 0100: loss: 3941.820
iteration 0200: loss: 3944.508
iteration 0300: loss: 3936.913
iteration 0400: loss: 3943.233
iteration 0500: loss: 3938.462
iteration 0600: loss: 3941.974
iteration 0700: loss: 3938.378
iteration 0800: loss: 3937.313
iteration 0900: loss: 3940.892
iteration 1000: loss: 3935.777
iteration 1100: loss: 3940.565
iteration 1200: loss: 3943.743
iteration 1300: loss: 3939.458
iteration 1400: loss: 3942.557
iteration 1500: loss: 3942.006
iteration 1600: loss: 3940.990
iteration 1700: loss: 3944.997
iteration 1800: loss: 3942.887
====> Epoch: 008 Train loss: 3942.1174  took : 53.00396656990051
====> Test loss: 3945.8604
iteration 0000: loss: 3940.859
iteration 0100: loss: 3942.789
iteration 0200: loss: 3946.271
iteration 0300: loss: 3941.831
iteration 0400: loss: 3947.066
iteration 0500: loss: 3947.583
iteration 0600: loss: 3938.527
iteration 0700: loss: 3939.611
iteration 0800: loss: 3947.031
iteration 0900: loss: 3941.552
iteration 1000: loss: 3941.411
iteration 1100: loss: 3946.292
iteration 1200: loss: 3938.051
iteration 1300: loss: 3946.770
iteration 1400: loss: 3947.161
iteration 1500: loss: 3941.545
iteration 1600: loss: 3940.955
iteration 1700: loss: 3946.969
iteration 1800: loss: 3940.687
====> Epoch: 009 Train loss: 3941.7263  took : 52.845659017562866
====> Test loss: 3945.9399
iteration 0000: loss: 3939.194
iteration 0100: loss: 3944.472
iteration 0200: loss: 3944.744
iteration 0300: loss: 3939.679
iteration 0400: loss: 3941.253
iteration 0500: loss: 3937.624
iteration 0600: loss: 3940.699
iteration 0700: loss: 3940.339
iteration 0800: loss: 3939.857
iteration 0900: loss: 3941.333
iteration 1000: loss: 3939.766
iteration 1100: loss: 3944.745
iteration 1200: loss: 3938.514
iteration 1300: loss: 3938.426
iteration 1400: loss: 3945.130
iteration 1500: loss: 3935.814
iteration 1600: loss: 3937.221
iteration 1700: loss: 3944.638
iteration 1800: loss: 3944.041
====> Epoch: 010 Train loss: 3941.3154  took : 53.30241346359253
====> Test loss: 3945.0627
iteration 0000: loss: 3936.598
iteration 0100: loss: 3939.283
iteration 0200: loss: 3942.494
iteration 0300: loss: 3943.074
iteration 0400: loss: 3936.417
iteration 0500: loss: 3937.416
iteration 0600: loss: 3945.246
iteration 0700: loss: 3947.414
iteration 0800: loss: 3938.394
iteration 0900: loss: 3937.963
iteration 1000: loss: 3937.885
iteration 1100: loss: 3942.086
iteration 1200: loss: 3939.856
iteration 1300: loss: 3933.143
iteration 1400: loss: 3934.761
iteration 1500: loss: 3939.937
iteration 1600: loss: 3940.634
iteration 1700: loss: 3937.537
iteration 1800: loss: 3938.952
====> Epoch: 011 Train loss: 3940.9909  took : 53.13242030143738
====> Test loss: 3944.0775
iteration 0000: loss: 3944.789
iteration 0100: loss: 3942.503
iteration 0200: loss: 3944.195
iteration 0300: loss: 3937.103
iteration 0400: loss: 3939.349
iteration 0500: loss: 3933.869
iteration 0600: loss: 3941.019
iteration 0700: loss: 3946.702
iteration 0800: loss: 3941.204
iteration 0900: loss: 3945.926
iteration 1000: loss: 3944.124
iteration 1100: loss: 3934.415
iteration 1200: loss: 3938.768
iteration 1300: loss: 3941.965
iteration 1400: loss: 3940.712
iteration 1500: loss: 3937.940
iteration 1600: loss: 3948.661
iteration 1700: loss: 3942.586
iteration 1800: loss: 3938.226
====> Epoch: 012 Train loss: 3940.6523  took : 53.07538342475891
====> Test loss: 3944.2086
iteration 0000: loss: 3942.257
iteration 0100: loss: 3939.362
iteration 0200: loss: 3944.213
iteration 0300: loss: 3935.362
iteration 0400: loss: 3941.871
iteration 0500: loss: 3943.994
iteration 0600: loss: 3934.391
iteration 0700: loss: 3942.433
iteration 0800: loss: 3936.110
iteration 0900: loss: 3935.639
iteration 1000: loss: 3936.693
iteration 1100: loss: 3947.701
iteration 1200: loss: 3941.908
iteration 1300: loss: 3944.533
iteration 1400: loss: 3943.499
iteration 1500: loss: 3935.512
iteration 1600: loss: 3933.129
iteration 1700: loss: 3936.262
iteration 1800: loss: 3940.977
====> Epoch: 013 Train loss: 3940.2665  took : 53.1210732460022
====> Test loss: 3944.4562
iteration 0000: loss: 3938.020
iteration 0100: loss: 3939.570
iteration 0200: loss: 3940.789
iteration 0300: loss: 3934.963
iteration 0400: loss: 3937.284
iteration 0500: loss: 3947.494
iteration 0600: loss: 3938.378
iteration 0700: loss: 3941.981
iteration 0800: loss: 3940.357
iteration 0900: loss: 3938.946
iteration 1000: loss: 3941.700
iteration 1100: loss: 3945.483
iteration 1200: loss: 3951.659
iteration 1300: loss: 3946.157
iteration 1400: loss: 3939.326
iteration 1500: loss: 3941.723
iteration 1600: loss: 3938.587
iteration 1700: loss: 3939.803
iteration 1800: loss: 3941.630
====> Epoch: 014 Train loss: 3939.9785  took : 53.00182032585144
====> Test loss: 3943.7550
iteration 0000: loss: 3940.582
iteration 0100: loss: 3937.864
iteration 0200: loss: 3931.669
iteration 0300: loss: 3940.618
iteration 0400: loss: 3948.782
iteration 0500: loss: 3943.493
iteration 0600: loss: 3942.637
iteration 0700: loss: 3938.409
iteration 0800: loss: 3937.769
iteration 0900: loss: 3938.637
iteration 1000: loss: 3944.074
iteration 1100: loss: 3934.854
iteration 1200: loss: 3937.486
iteration 1300: loss: 3943.056
iteration 1400: loss: 3940.986
iteration 1500: loss: 3942.645
iteration 1600: loss: 3945.455
iteration 1700: loss: 3938.154
iteration 1800: loss: 3944.191
====> Epoch: 015 Train loss: 3939.9764  took : 53.06413269042969
====> Test loss: 3943.7781
iteration 0000: loss: 3942.126
iteration 0100: loss: 3942.207
iteration 0200: loss: 3940.698
iteration 0300: loss: 3936.619
iteration 0400: loss: 3942.663
iteration 0500: loss: 3940.484
iteration 0600: loss: 3945.185
iteration 0700: loss: 3936.096
iteration 0800: loss: 3942.882
iteration 0900: loss: 3932.148
iteration 1000: loss: 3939.873
iteration 1100: loss: 3940.924
iteration 1200: loss: 3941.397
iteration 1300: loss: 3939.725
iteration 1400: loss: 3936.352
iteration 1500: loss: 3937.771
iteration 1600: loss: 3940.075
iteration 1700: loss: 3937.073
iteration 1800: loss: 3943.128
====> Epoch: 016 Train loss: 3939.9023  took : 53.095765113830566
====> Test loss: 3943.7728
iteration 0000: loss: 3936.274
iteration 0100: loss: 3937.625
iteration 0200: loss: 3948.724
iteration 0300: loss: 3939.543
iteration 0400: loss: 3938.274
iteration 0500: loss: 3939.240
iteration 0600: loss: 3938.998
iteration 0700: loss: 3944.999
iteration 0800: loss: 3938.231
iteration 0900: loss: 3938.043
iteration 1000: loss: 3944.203
iteration 1100: loss: 3940.700
iteration 1200: loss: 3941.958
iteration 1300: loss: 3943.122
iteration 1400: loss: 3947.041
iteration 1500: loss: 3944.177
iteration 1600: loss: 3942.055
iteration 1700: loss: 3941.860
iteration 1800: loss: 3947.790
====> Epoch: 017 Train loss: 3939.6196  took : 53.20632982254028
====> Test loss: 3943.4246
iteration 0000: loss: 3944.123
iteration 0100: loss: 3941.549
iteration 0200: loss: 3937.950
iteration 0300: loss: 3942.379
iteration 0400: loss: 3941.368
iteration 0500: loss: 3943.073
iteration 0600: loss: 3937.439
iteration 0700: loss: 3934.635
iteration 0800: loss: 3939.289
iteration 0900: loss: 3933.021
iteration 1000: loss: 3941.388
iteration 1100: loss: 3939.358
iteration 1200: loss: 3938.390
iteration 1300: loss: 3939.923
iteration 1400: loss: 3934.604
iteration 1500: loss: 3939.099
iteration 1600: loss: 3937.034
iteration 1700: loss: 3936.427
iteration 1800: loss: 3941.969
====> Epoch: 018 Train loss: 3939.4533  took : 52.97444009780884
====> Test loss: 3942.9975
iteration 0000: loss: 3939.208
iteration 0100: loss: 3938.438
iteration 0200: loss: 3933.765
iteration 0300: loss: 3935.389
iteration 0400: loss: 3938.363
iteration 0500: loss: 3941.514
iteration 0600: loss: 3938.257
iteration 0700: loss: 3942.837
iteration 0800: loss: 3937.124
iteration 0900: loss: 3936.433
iteration 1000: loss: 3939.273
iteration 1100: loss: 3938.247
iteration 1200: loss: 3938.334
iteration 1300: loss: 3942.480
iteration 1400: loss: 3932.065
iteration 1500: loss: 3934.403
iteration 1600: loss: 3939.776
iteration 1700: loss: 3942.012
iteration 1800: loss: 3941.219
====> Epoch: 019 Train loss: 3939.4024  took : 53.49819827079773
====> Test loss: 3943.0663
iteration 0000: loss: 3951.518
iteration 0100: loss: 3934.835
iteration 0200: loss: 3936.844
iteration 0300: loss: 3942.330
iteration 0400: loss: 3934.118
iteration 0500: loss: 3933.398
iteration 0600: loss: 3936.068
iteration 0700: loss: 3938.066
iteration 0800: loss: 3938.407
iteration 0900: loss: 3934.377
iteration 1000: loss: 3946.634
iteration 1100: loss: 3939.351
iteration 1200: loss: 3944.555
iteration 1300: loss: 3935.013
iteration 1400: loss: 3940.191
iteration 1500: loss: 3935.507
iteration 1600: loss: 3939.976
iteration 1700: loss: 3937.160
iteration 1800: loss: 3943.758
====> Epoch: 020 Train loss: 3939.1744  took : 52.93570828437805
====> Test loss: 3942.9290
iteration 0000: loss: 3946.711
iteration 0100: loss: 3931.959
iteration 0200: loss: 3937.214
iteration 0300: loss: 3935.436
iteration 0400: loss: 3940.331
iteration 0500: loss: 3936.585
iteration 0600: loss: 3935.194
iteration 0700: loss: 3935.081
iteration 0800: loss: 3939.694
iteration 0900: loss: 3935.438
iteration 1000: loss: 3937.671
iteration 1100: loss: 3939.878
iteration 1200: loss: 3939.729
iteration 1300: loss: 3933.494
iteration 1400: loss: 3935.562
iteration 1500: loss: 3932.984
iteration 1600: loss: 3940.029
iteration 1700: loss: 3933.738
iteration 1800: loss: 3935.795
====> Epoch: 021 Train loss: 3939.0166  took : 52.801920890808105
====> Test loss: 3943.3766
iteration 0000: loss: 3938.422
iteration 0100: loss: 3935.333
iteration 0200: loss: 3940.494
iteration 0300: loss: 3944.755
iteration 0400: loss: 3943.776
iteration 0500: loss: 3934.414
iteration 0600: loss: 3937.418
iteration 0700: loss: 3939.660
iteration 0800: loss: 3938.856
iteration 0900: loss: 3938.653
iteration 1000: loss: 3941.029
iteration 1100: loss: 3931.935
iteration 1200: loss: 3938.501
iteration 1300: loss: 3941.926
iteration 1400: loss: 3942.045
iteration 1500: loss: 3935.720
iteration 1600: loss: 3939.232
iteration 1700: loss: 3939.919
iteration 1800: loss: 3939.928
====> Epoch: 022 Train loss: 3938.9517  took : 52.97820162773132
====> Test loss: 3942.1562
iteration 0000: loss: 3946.095
iteration 0100: loss: 3939.589
iteration 0200: loss: 3940.729
iteration 0300: loss: 3939.688
iteration 0400: loss: 3938.667
iteration 0500: loss: 3944.438
iteration 0600: loss: 3934.938
iteration 0700: loss: 3935.014
iteration 0800: loss: 3939.129
iteration 0900: loss: 3939.613
iteration 1000: loss: 3940.993
iteration 1100: loss: 3937.155
iteration 1200: loss: 3937.990
iteration 1300: loss: 3942.140
iteration 1400: loss: 3943.604
iteration 1500: loss: 3931.933
iteration 1600: loss: 3938.922
iteration 1700: loss: 3934.999
iteration 1800: loss: 3939.815
====> Epoch: 023 Train loss: 3938.9322  took : 52.84089231491089
====> Test loss: 3942.9567
iteration 0000: loss: 3938.200
iteration 0100: loss: 3936.557
iteration 0200: loss: 3936.874
iteration 0300: loss: 3947.913
iteration 0400: loss: 3946.064
iteration 0500: loss: 3939.604
iteration 0600: loss: 3940.130
iteration 0700: loss: 3944.920
iteration 0800: loss: 3941.448
iteration 0900: loss: 3939.706
iteration 1000: loss: 3935.910
iteration 1100: loss: 3940.012
iteration 1200: loss: 3939.390
iteration 1300: loss: 3934.375
iteration 1400: loss: 3946.406
iteration 1500: loss: 3938.848
iteration 1600: loss: 3944.711
iteration 1700: loss: 3934.656
iteration 1800: loss: 3937.732
====> Epoch: 024 Train loss: 3939.1759  took : 52.8385865688324
====> Test loss: 3942.3210
iteration 0000: loss: 3933.637
iteration 0100: loss: 3936.179
iteration 0200: loss: 3939.447
iteration 0300: loss: 3934.777
iteration 0400: loss: 3939.448
iteration 0500: loss: 3935.984
iteration 0600: loss: 3932.719
iteration 0700: loss: 3941.964
iteration 0800: loss: 3937.844
iteration 0900: loss: 3937.128
iteration 1000: loss: 3940.732
iteration 1100: loss: 3943.710
iteration 1200: loss: 3936.202
iteration 1300: loss: 3941.139
iteration 1400: loss: 3943.959
iteration 1500: loss: 3935.798
iteration 1600: loss: 3938.375
iteration 1700: loss: 3937.243
iteration 1800: loss: 3934.413
====> Epoch: 025 Train loss: 3938.5688  took : 53.029433488845825
====> Test loss: 3943.0103
iteration 0000: loss: 3940.207
iteration 0100: loss: 3940.180
iteration 0200: loss: 3937.639
iteration 0300: loss: 3939.491
iteration 0400: loss: 3933.862
iteration 0500: loss: 3939.422
iteration 0600: loss: 3938.908
iteration 0700: loss: 3934.849
iteration 0800: loss: 3936.301
iteration 0900: loss: 3936.015
iteration 1000: loss: 3936.878
iteration 1100: loss: 3938.132
iteration 1200: loss: 3937.649
iteration 1300: loss: 3939.209
iteration 1400: loss: 3937.612
iteration 1500: loss: 3935.831
iteration 1600: loss: 3938.336
iteration 1700: loss: 3940.286
iteration 1800: loss: 3935.148
====> Epoch: 026 Train loss: 3938.4694  took : 52.9571316242218
====> Test loss: 3942.6594
iteration 0000: loss: 3937.347
iteration 0100: loss: 3936.776
iteration 0200: loss: 3933.304
iteration 0300: loss: 3935.526
iteration 0400: loss: 3932.625
iteration 0500: loss: 3938.339
iteration 0600: loss: 3938.270
iteration 0700: loss: 3937.667
iteration 0800: loss: 3934.897
iteration 0900: loss: 3934.996
iteration 1000: loss: 3946.848
iteration 1100: loss: 3934.983
iteration 1200: loss: 3939.964
iteration 1300: loss: 3936.125
iteration 1400: loss: 3947.278
iteration 1500: loss: 3937.524
iteration 1600: loss: 3934.629
iteration 1700: loss: 3940.104
iteration 1800: loss: 3939.381
====> Epoch: 027 Train loss: 3938.4463  took : 53.08857011795044
====> Test loss: 3942.5161
iteration 0000: loss: 3941.044
iteration 0100: loss: 3931.482
iteration 0200: loss: 3936.274
iteration 0300: loss: 3932.930
iteration 0400: loss: 3940.154
iteration 0500: loss: 3936.533
iteration 0600: loss: 3935.315
iteration 0700: loss: 3934.057
iteration 0800: loss: 3939.392
iteration 0900: loss: 3939.610
iteration 1000: loss: 3934.496
iteration 1100: loss: 3938.679
iteration 1200: loss: 3939.801
iteration 1300: loss: 3933.731
iteration 1400: loss: 3940.816
iteration 1500: loss: 3941.297
iteration 1600: loss: 3945.559
iteration 1700: loss: 3943.382
iteration 1800: loss: 3937.047
====> Epoch: 028 Train loss: 3938.3422  took : 53.14354705810547
====> Test loss: 3942.7591
iteration 0000: loss: 3936.702
iteration 0100: loss: 3936.589
iteration 0200: loss: 3936.953
iteration 0300: loss: 3935.301
iteration 0400: loss: 3951.664
iteration 0500: loss: 3938.037
iteration 0600: loss: 3935.659
iteration 0700: loss: 3936.505
iteration 0800: loss: 3938.772
iteration 0900: loss: 3938.643
iteration 1000: loss: 3939.019
iteration 1100: loss: 3931.949
iteration 1200: loss: 3939.032
iteration 1300: loss: 3936.814
iteration 1400: loss: 3939.670
iteration 1500: loss: 3936.088
iteration 1600: loss: 3936.868
iteration 1700: loss: 3939.227
iteration 1800: loss: 3939.900
====> Epoch: 029 Train loss: 3938.7833  took : 53.01921796798706
====> Test loss: 3942.1076
iteration 0000: loss: 3936.757
iteration 0100: loss: 3938.713
iteration 0200: loss: 3934.582
iteration 0300: loss: 3935.998
iteration 0400: loss: 3937.543
iteration 0500: loss: 3937.280
iteration 0600: loss: 3939.014
iteration 0700: loss: 3940.669
iteration 0800: loss: 3938.749
iteration 0900: loss: 3939.654
iteration 1000: loss: 3940.031
iteration 1100: loss: 3935.832
iteration 1200: loss: 3932.254
iteration 1300: loss: 3938.816
iteration 1400: loss: 3937.120
iteration 1500: loss: 3941.321
iteration 1600: loss: 3939.983
iteration 1700: loss: 3933.042
iteration 1800: loss: 3939.707
====> Epoch: 030 Train loss: 3938.1933  took : 53.282119035720825
====> Test loss: 3942.9337
iteration 0000: loss: 3944.607
iteration 0100: loss: 3939.207
iteration 0200: loss: 3936.043
iteration 0300: loss: 3931.285
iteration 0400: loss: 3933.495
iteration 0500: loss: 3945.353
iteration 0600: loss: 3939.055
iteration 0700: loss: 3937.115
iteration 0800: loss: 3932.995
iteration 0900: loss: 3937.777
iteration 1000: loss: 3935.478
iteration 1100: loss: 3939.870
iteration 1200: loss: 3929.333
iteration 1300: loss: 3943.522
iteration 1400: loss: 3939.680
iteration 1500: loss: 3940.821
iteration 1600: loss: 3937.570
iteration 1700: loss: 3937.937
iteration 1800: loss: 3944.318
====> Epoch: 031 Train loss: 3938.3393  took : 52.97962760925293
====> Test loss: 3942.6235
iteration 0000: loss: 3929.463
iteration 0100: loss: 3934.407
iteration 0200: loss: 3937.335
iteration 0300: loss: 3938.132
iteration 0400: loss: 3937.570
iteration 0500: loss: 3940.118
iteration 0600: loss: 3935.294
iteration 0700: loss: 3940.293
iteration 0800: loss: 3937.134
iteration 0900: loss: 3934.658
iteration 1000: loss: 3939.939
iteration 1100: loss: 3939.108
iteration 1200: loss: 3936.791
iteration 1300: loss: 3938.001
iteration 1400: loss: 3939.722
iteration 1500: loss: 3940.878
iteration 1600: loss: 3941.196
iteration 1700: loss: 3935.021
iteration 1800: loss: 3937.153
====> Epoch: 032 Train loss: 3938.2565  took : 52.75560712814331
====> Test loss: 3942.2596
iteration 0000: loss: 3931.261
iteration 0100: loss: 3939.270
iteration 0200: loss: 3940.190
iteration 0300: loss: 3931.553
iteration 0400: loss: 3933.665
iteration 0500: loss: 3941.501
iteration 0600: loss: 3939.519
iteration 0700: loss: 3940.998
iteration 0800: loss: 3941.361
iteration 0900: loss: 3939.953
iteration 1000: loss: 3941.094
iteration 1100: loss: 3939.465
iteration 1200: loss: 3945.422
iteration 1300: loss: 3940.663
iteration 1400: loss: 3931.315
iteration 1500: loss: 3940.345
iteration 1600: loss: 3934.872
iteration 1700: loss: 3938.866
iteration 1800: loss: 3949.851
====> Epoch: 033 Train loss: 3938.0712  took : 53.048272371292114
====> Test loss: 3941.9940
iteration 0000: loss: 3936.839
iteration 0100: loss: 3932.355
iteration 0200: loss: 3934.190
iteration 0300: loss: 3932.042
iteration 0400: loss: 3937.074
iteration 0500: loss: 3938.710
iteration 0600: loss: 3935.282
iteration 0700: loss: 3939.879
iteration 0800: loss: 3935.444
iteration 0900: loss: 3940.643
iteration 1000: loss: 3938.756
iteration 1100: loss: 3943.967
iteration 1200: loss: 3937.588
iteration 1300: loss: 3932.922
iteration 1400: loss: 3939.135
iteration 1500: loss: 3935.709
iteration 1600: loss: 3938.576
iteration 1700: loss: 3938.443
iteration 1800: loss: 3938.601
====> Epoch: 034 Train loss: 3938.0119  took : 52.79104995727539
====> Test loss: 3942.5036
iteration 0000: loss: 3938.143
iteration 0100: loss: 3929.651
iteration 0200: loss: 3941.080
iteration 0300: loss: 3939.319
iteration 0400: loss: 3942.866
iteration 0500: loss: 3933.893
iteration 0600: loss: 3942.941
iteration 0700: loss: 3937.162
iteration 0800: loss: 3944.462
iteration 0900: loss: 3934.875
iteration 1000: loss: 3936.150
iteration 1100: loss: 3936.985
iteration 1200: loss: 3940.131
iteration 1300: loss: 3942.030
iteration 1400: loss: 3939.637
iteration 1500: loss: 3942.582
iteration 1600: loss: 3932.876
iteration 1700: loss: 3937.275
iteration 1800: loss: 3938.333
====> Epoch: 035 Train loss: 3938.0347  took : 53.18070912361145
====> Test loss: 3942.2122
iteration 0000: loss: 3938.740
iteration 0100: loss: 3938.185
iteration 0200: loss: 3936.624
iteration 0300: loss: 3936.148
iteration 0400: loss: 3936.022
iteration 0500: loss: 3937.805
iteration 0600: loss: 3936.032
iteration 0700: loss: 3935.113
iteration 0800: loss: 3936.637
iteration 0900: loss: 3941.428
iteration 1000: loss: 3934.182
iteration 1100: loss: 3934.162
iteration 1200: loss: 3941.326
iteration 1300: loss: 3938.416
iteration 1400: loss: 3943.181
iteration 1500: loss: 3936.675
iteration 1600: loss: 3937.407
iteration 1700: loss: 3931.923
iteration 1800: loss: 3939.362
====> Epoch: 036 Train loss: 3937.8574  took : 52.98412537574768
====> Test loss: 3941.9319
iteration 0000: loss: 3939.850
iteration 0100: loss: 3937.640
iteration 0200: loss: 3935.091
iteration 0300: loss: 3936.300
iteration 0400: loss: 3935.742
iteration 0500: loss: 3937.780
iteration 0600: loss: 3937.103
iteration 0700: loss: 3938.347
iteration 0800: loss: 3932.235
iteration 0900: loss: 3936.925
iteration 1000: loss: 3937.282
iteration 1100: loss: 3941.993
iteration 1200: loss: 3940.760
iteration 1300: loss: 3938.285
iteration 1400: loss: 3935.413
iteration 1500: loss: 3939.263
iteration 1600: loss: 3931.720
iteration 1700: loss: 3940.709
iteration 1800: loss: 3933.638
====> Epoch: 037 Train loss: 3937.9437  took : 52.95751929283142
====> Test loss: 3943.6753
iteration 0000: loss: 3941.942
iteration 0100: loss: 3938.743
iteration 0200: loss: 3940.909
iteration 0300: loss: 3938.491
iteration 0400: loss: 3936.219
iteration 0500: loss: 3936.985
iteration 0600: loss: 3933.207
iteration 0700: loss: 3934.962
iteration 0800: loss: 3938.687
iteration 0900: loss: 3944.705
iteration 1000: loss: 3940.218
iteration 1100: loss: 3936.044
iteration 1200: loss: 3933.543
iteration 1300: loss: 3931.340
iteration 1400: loss: 3936.993
iteration 1500: loss: 3938.802
iteration 1600: loss: 3934.931
iteration 1700: loss: 3931.912
iteration 1800: loss: 3936.278
====> Epoch: 038 Train loss: 3937.8941  took : 53.016942262649536
====> Test loss: 3941.9779
iteration 0000: loss: 3934.841
iteration 0100: loss: 3939.317
iteration 0200: loss: 3938.117
iteration 0300: loss: 3937.453
iteration 0400: loss: 3942.720
iteration 0500: loss: 3940.170
iteration 0600: loss: 3935.565
iteration 0700: loss: 3941.547
iteration 0800: loss: 3942.550
iteration 0900: loss: 3937.939
iteration 1000: loss: 3929.777
iteration 1100: loss: 3937.109
iteration 1200: loss: 3940.116
iteration 1300: loss: 3944.470
iteration 1400: loss: 3933.926
iteration 1500: loss: 3934.796
iteration 1600: loss: 3941.378
iteration 1700: loss: 3932.563
iteration 1800: loss: 3931.695
====> Epoch: 039 Train loss: 3937.9635  took : 52.93200349807739
====> Test loss: 3941.7377
iteration 0000: loss: 3932.600
iteration 0100: loss: 3934.531
iteration 0200: loss: 3946.516
iteration 0300: loss: 3945.747
iteration 0400: loss: 3941.993
iteration 0500: loss: 3936.636
iteration 0600: loss: 3933.504
iteration 0700: loss: 3939.589
iteration 0800: loss: 3930.789
iteration 0900: loss: 3935.255
iteration 1000: loss: 3936.691
iteration 1100: loss: 3936.622
iteration 1200: loss: 3938.213
iteration 1300: loss: 3944.075
iteration 1400: loss: 3937.425
iteration 1500: loss: 3934.775
iteration 1600: loss: 3937.062
iteration 1700: loss: 3937.336
iteration 1800: loss: 3945.359
====> Epoch: 040 Train loss: 3937.9555  took : 52.76429605484009
====> Test loss: 3942.1160
iteration 0000: loss: 3936.918
iteration 0100: loss: 3943.518
iteration 0200: loss: 3938.141
iteration 0300: loss: 3927.527
iteration 0400: loss: 3938.824
iteration 0500: loss: 3939.204
iteration 0600: loss: 3944.911
iteration 0700: loss: 3944.463
iteration 0800: loss: 3936.863
iteration 0900: loss: 3936.843
iteration 1000: loss: 3939.846
iteration 1100: loss: 3931.395
iteration 1200: loss: 3939.669
iteration 1300: loss: 3936.213
iteration 1400: loss: 3939.697
iteration 1500: loss: 3936.277
iteration 1600: loss: 3941.980
iteration 1700: loss: 3935.761
iteration 1800: loss: 3940.262
====> Epoch: 041 Train loss: 3937.6502  took : 52.85463309288025
====> Test loss: 3941.5717
iteration 0000: loss: 3937.783
iteration 0100: loss: 3934.671
iteration 0200: loss: 3933.022
iteration 0300: loss: 3939.046
iteration 0400: loss: 3936.649
iteration 0500: loss: 3933.427
iteration 0600: loss: 3933.401
iteration 0700: loss: 3934.221
iteration 0800: loss: 3938.150
iteration 0900: loss: 3943.176
iteration 1000: loss: 3940.595
iteration 1100: loss: 3942.303
iteration 1200: loss: 3937.695
iteration 1300: loss: 3939.993
iteration 1400: loss: 3941.801
iteration 1500: loss: 3937.090
iteration 1600: loss: 3931.646
iteration 1700: loss: 3935.994
iteration 1800: loss: 3938.356
====> Epoch: 042 Train loss: 3937.6721  took : 52.89711093902588
====> Test loss: 3941.8971
iteration 0000: loss: 3938.288
iteration 0100: loss: 3946.230
iteration 0200: loss: 3932.084
iteration 0300: loss: 3936.756
iteration 0400: loss: 3940.081
iteration 0500: loss: 3935.479
iteration 0600: loss: 3931.684
iteration 0700: loss: 3935.309
iteration 0800: loss: 3933.437
iteration 0900: loss: 3936.545
iteration 1000: loss: 3932.045
iteration 1100: loss: 3939.660
iteration 1200: loss: 3938.443
iteration 1300: loss: 3937.254
iteration 1400: loss: 3937.458
iteration 1500: loss: 3942.661
iteration 1600: loss: 3938.087
iteration 1700: loss: 3939.821
iteration 1800: loss: 3940.805
====> Epoch: 043 Train loss: 3937.5501  took : 52.979756593704224
====> Test loss: 3942.0728
iteration 0000: loss: 3936.315
iteration 0100: loss: 3936.516
iteration 0200: loss: 3940.738
iteration 0300: loss: 3933.702
iteration 0400: loss: 3939.011
iteration 0500: loss: 3946.933
iteration 0600: loss: 3936.739
iteration 0700: loss: 3943.819
iteration 0800: loss: 3935.713
iteration 0900: loss: 3935.946
iteration 1000: loss: 3938.996
iteration 1100: loss: 3942.235
iteration 1200: loss: 3936.268
iteration 1300: loss: 3932.895
iteration 1400: loss: 3941.397
iteration 1500: loss: 3939.777
iteration 1600: loss: 3935.783
iteration 1700: loss: 3941.868
iteration 1800: loss: 3923.983
====> Epoch: 044 Train loss: 3937.6191  took : 52.945637464523315
====> Test loss: 3942.0661
iteration 0000: loss: 3935.291
iteration 0100: loss: 3931.775
iteration 0200: loss: 3937.882
iteration 0300: loss: 3942.140
iteration 0400: loss: 3934.181
iteration 0500: loss: 3933.039
iteration 0600: loss: 3934.238
iteration 0700: loss: 3948.481
iteration 0800: loss: 3937.013
iteration 0900: loss: 3939.194
iteration 1000: loss: 3935.363
iteration 1100: loss: 3944.333
iteration 1200: loss: 3935.742
iteration 1300: loss: 3942.453
iteration 1400: loss: 3942.417
iteration 1500: loss: 3935.189
iteration 1600: loss: 3938.632
iteration 1700: loss: 3935.374
iteration 1800: loss: 3934.672
====> Epoch: 045 Train loss: 3937.5436  took : 52.980379819869995
====> Test loss: 3941.9491
iteration 0000: loss: 3934.002
iteration 0100: loss: 3937.437
iteration 0200: loss: 3940.092
iteration 0300: loss: 3935.948
iteration 0400: loss: 3936.204
iteration 0500: loss: 3938.443
iteration 0600: loss: 3940.536
iteration 0700: loss: 3941.222
iteration 0800: loss: 3933.376
iteration 0900: loss: 3935.248
iteration 1000: loss: 3935.934
iteration 1100: loss: 3938.548
iteration 1200: loss: 3936.606
iteration 1300: loss: 3940.927
iteration 1400: loss: 3942.173
iteration 1500: loss: 3934.376
iteration 1600: loss: 3935.193
iteration 1700: loss: 3940.140
iteration 1800: loss: 3936.834
====> Epoch: 046 Train loss: 3937.4947  took : 53.10318112373352
====> Test loss: 3942.1790
iteration 0000: loss: 3937.541
iteration 0100: loss: 3943.695
iteration 0200: loss: 3939.774
iteration 0300: loss: 3937.323
iteration 0400: loss: 3934.000
iteration 0500: loss: 3934.345
iteration 0600: loss: 3950.946
iteration 0700: loss: 3937.835
iteration 0800: loss: 3941.999
iteration 0900: loss: 3945.004
iteration 1000: loss: 3940.020
iteration 1100: loss: 3941.047
iteration 1200: loss: 3939.659
iteration 1300: loss: 3936.224
iteration 1400: loss: 3934.183
iteration 1500: loss: 3933.638
iteration 1600: loss: 3943.096
iteration 1700: loss: 3934.065
iteration 1800: loss: 3943.272
====> Epoch: 047 Train loss: 3937.5139  took : 52.84685015678406
====> Test loss: 3941.8925
iteration 0000: loss: 3928.867
iteration 0100: loss: 3933.763
iteration 0200: loss: 3935.062
iteration 0300: loss: 3935.110
iteration 0400: loss: 3936.828
iteration 0500: loss: 3931.367
iteration 0600: loss: 3938.353
iteration 0700: loss: 3935.838
iteration 0800: loss: 3943.995
iteration 0900: loss: 3935.263
iteration 1000: loss: 3938.349
iteration 1100: loss: 3931.367
iteration 1200: loss: 3936.762
iteration 1300: loss: 3935.436
iteration 1400: loss: 3938.613
iteration 1500: loss: 3939.567
iteration 1600: loss: 3938.686
iteration 1700: loss: 3937.007
iteration 1800: loss: 3936.769
====> Epoch: 048 Train loss: 3937.3334  took : 52.85534882545471
====> Test loss: 3941.7667
iteration 0000: loss: 3935.291
iteration 0100: loss: 3935.551
iteration 0200: loss: 3935.997
iteration 0300: loss: 3938.181
iteration 0400: loss: 3931.450
iteration 0500: loss: 3937.268
iteration 0600: loss: 3933.028
iteration 0700: loss: 3934.987
iteration 0800: loss: 3931.092
iteration 0900: loss: 3933.766
iteration 1000: loss: 3938.499
iteration 1100: loss: 3932.025
iteration 1200: loss: 3938.180
iteration 1300: loss: 3940.672
iteration 1400: loss: 3928.498
iteration 1500: loss: 3938.089
iteration 1600: loss: 3934.544
iteration 1700: loss: 3942.093
iteration 1800: loss: 3942.599
====> Epoch: 049 Train loss: 3937.3135  took : 52.90909934043884
====> Test loss: 3941.3814
iteration 0000: loss: 3936.764
iteration 0100: loss: 3939.673
iteration 0200: loss: 3939.672
iteration 0300: loss: 3938.216
iteration 0400: loss: 3940.720
iteration 0500: loss: 3938.837
iteration 0600: loss: 3943.619
iteration 0700: loss: 3941.542
iteration 0800: loss: 3939.694
iteration 0900: loss: 3946.553
iteration 1000: loss: 3937.640
iteration 1100: loss: 3939.073
iteration 1200: loss: 3940.070
iteration 1300: loss: 3937.237
iteration 1400: loss: 3938.789
iteration 1500: loss: 3946.691
iteration 1600: loss: 3935.497
iteration 1700: loss: 3940.213
iteration 1800: loss: 3934.099
====> Epoch: 050 Train loss: 3937.3353  took : 53.042638540267944
====> Test loss: 3941.0148
====> [MM-VAE] Time: 3161.363s or 00:52:41
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  16
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_16
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_16
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1990.250
iteration 0100: loss: 1570.636
iteration 0200: loss: 1562.616
iteration 0300: loss: 1565.454
iteration 0400: loss: 1551.585
iteration 0500: loss: 1545.408
iteration 0600: loss: 1550.794
iteration 0700: loss: 1545.810
iteration 0800: loss: 1540.501
iteration 0900: loss: 1540.931
====> Epoch: 001 Train loss: 1555.8955  took : 8.469189643859863
====> Test loss: 1538.0425
iteration 0000: loss: 1540.243
iteration 0100: loss: 1537.798
iteration 0200: loss: 1541.225
iteration 0300: loss: 1535.648
iteration 0400: loss: 1536.588
iteration 0500: loss: 1530.924
iteration 0600: loss: 1532.594
iteration 0700: loss: 1528.481
iteration 0800: loss: 1526.025
iteration 0900: loss: 1529.928
====> Epoch: 002 Train loss: 1532.2957  took : 8.478020191192627
====> Test loss: 1530.4062
iteration 0000: loss: 1530.083
iteration 0100: loss: 1528.159
iteration 0200: loss: 1528.435
iteration 0300: loss: 1527.502
iteration 0400: loss: 1524.281
iteration 0500: loss: 1526.655
iteration 0600: loss: 1524.732
iteration 0700: loss: 1527.798
iteration 0800: loss: 1523.067
iteration 0900: loss: 1522.771
====> Epoch: 003 Train loss: 1526.6186  took : 8.419829368591309
====> Test loss: 1527.1583
iteration 0000: loss: 1524.449
iteration 0100: loss: 1526.834
iteration 0200: loss: 1525.909
iteration 0300: loss: 1523.431
iteration 0400: loss: 1523.959
iteration 0500: loss: 1527.668
iteration 0600: loss: 1525.168
iteration 0700: loss: 1523.302
iteration 0800: loss: 1522.554
iteration 0900: loss: 1522.244
====> Epoch: 004 Train loss: 1524.0024  took : 8.475045442581177
====> Test loss: 1525.4344
iteration 0000: loss: 1521.793
iteration 0100: loss: 1523.641
iteration 0200: loss: 1523.307
iteration 0300: loss: 1521.183
iteration 0400: loss: 1523.058
iteration 0500: loss: 1518.883
iteration 0600: loss: 1520.739
iteration 0700: loss: 1519.547
iteration 0800: loss: 1522.401
iteration 0900: loss: 1521.594
====> Epoch: 005 Train loss: 1522.4016  took : 8.428639650344849
====> Test loss: 1523.9281
iteration 0000: loss: 1520.956
iteration 0100: loss: 1518.835
iteration 0200: loss: 1523.190
iteration 0300: loss: 1520.470
iteration 0400: loss: 1521.551
iteration 0500: loss: 1520.473
iteration 0600: loss: 1521.839
iteration 0700: loss: 1518.981
iteration 0800: loss: 1522.901
iteration 0900: loss: 1517.569
====> Epoch: 006 Train loss: 1521.1705  took : 8.405360698699951
====> Test loss: 1523.1469
iteration 0000: loss: 1520.002
iteration 0100: loss: 1520.975
iteration 0200: loss: 1520.510
iteration 0300: loss: 1517.139
iteration 0400: loss: 1520.708
iteration 0500: loss: 1518.170
iteration 0600: loss: 1521.800
iteration 0700: loss: 1520.518
iteration 0800: loss: 1518.081
iteration 0900: loss: 1521.144
====> Epoch: 007 Train loss: 1520.2832  took : 8.469301462173462
====> Test loss: 1522.4150
iteration 0000: loss: 1522.547
iteration 0100: loss: 1518.035
iteration 0200: loss: 1515.500
iteration 0300: loss: 1523.767
iteration 0400: loss: 1522.598
iteration 0500: loss: 1520.120
iteration 0600: loss: 1517.914
iteration 0700: loss: 1521.104
iteration 0800: loss: 1521.531
iteration 0900: loss: 1516.383
====> Epoch: 008 Train loss: 1519.6254  took : 8.505966186523438
====> Test loss: 1521.7309
iteration 0000: loss: 1516.614
iteration 0100: loss: 1519.026
iteration 0200: loss: 1521.175
iteration 0300: loss: 1517.700
iteration 0400: loss: 1517.492
iteration 0500: loss: 1522.644
iteration 0600: loss: 1515.921
iteration 0700: loss: 1519.112
iteration 0800: loss: 1519.371
iteration 0900: loss: 1520.146
====> Epoch: 009 Train loss: 1519.0881  took : 8.490123510360718
====> Test loss: 1521.5466
iteration 0000: loss: 1519.246
iteration 0100: loss: 1514.914
iteration 0200: loss: 1518.930
iteration 0300: loss: 1519.951
iteration 0400: loss: 1519.252
iteration 0500: loss: 1518.914
iteration 0600: loss: 1518.137
iteration 0700: loss: 1515.951
iteration 0800: loss: 1514.391
iteration 0900: loss: 1514.519
====> Epoch: 010 Train loss: 1518.7238  took : 8.419156312942505
====> Test loss: 1521.0781
iteration 0000: loss: 1520.934
iteration 0100: loss: 1523.581
iteration 0200: loss: 1519.472
iteration 0300: loss: 1518.487
iteration 0400: loss: 1517.077
iteration 0500: loss: 1519.887
iteration 0600: loss: 1518.685
iteration 0700: loss: 1520.751
iteration 0800: loss: 1515.964
iteration 0900: loss: 1516.949
====> Epoch: 011 Train loss: 1518.3258  took : 8.43383264541626
====> Test loss: 1520.8595
iteration 0000: loss: 1518.734
iteration 0100: loss: 1518.888
iteration 0200: loss: 1518.071
iteration 0300: loss: 1514.670
iteration 0400: loss: 1515.765
iteration 0500: loss: 1518.312
iteration 0600: loss: 1519.528
iteration 0700: loss: 1519.049
iteration 0800: loss: 1521.325
iteration 0900: loss: 1519.367
====> Epoch: 012 Train loss: 1518.0234  took : 8.428274393081665
====> Test loss: 1520.7006
iteration 0000: loss: 1519.155
iteration 0100: loss: 1514.999
iteration 0200: loss: 1517.364
iteration 0300: loss: 1516.288
iteration 0400: loss: 1516.419
iteration 0500: loss: 1520.944
iteration 0600: loss: 1518.491
iteration 0700: loss: 1517.519
iteration 0800: loss: 1516.975
iteration 0900: loss: 1517.281
====> Epoch: 013 Train loss: 1517.7410  took : 8.441138505935669
====> Test loss: 1520.4807
iteration 0000: loss: 1517.265
iteration 0100: loss: 1517.822
iteration 0200: loss: 1517.452
iteration 0300: loss: 1517.064
iteration 0400: loss: 1518.828
iteration 0500: loss: 1514.092
iteration 0600: loss: 1517.920
iteration 0700: loss: 1514.971
iteration 0800: loss: 1515.926
iteration 0900: loss: 1518.502
====> Epoch: 014 Train loss: 1517.5178  took : 8.407740592956543
====> Test loss: 1520.2490
iteration 0000: loss: 1518.856
iteration 0100: loss: 1516.958
iteration 0200: loss: 1517.775
iteration 0300: loss: 1520.714
iteration 0400: loss: 1519.043
iteration 0500: loss: 1517.316
iteration 0600: loss: 1515.425
iteration 0700: loss: 1517.798
iteration 0800: loss: 1514.991
iteration 0900: loss: 1516.402
====> Epoch: 015 Train loss: 1517.2448  took : 8.437483549118042
====> Test loss: 1520.1894
iteration 0000: loss: 1515.098
iteration 0100: loss: 1515.674
iteration 0200: loss: 1519.926
iteration 0300: loss: 1515.962
iteration 0400: loss: 1516.262
iteration 0500: loss: 1515.956
iteration 0600: loss: 1518.161
iteration 0700: loss: 1518.003
iteration 0800: loss: 1516.229
iteration 0900: loss: 1517.109
====> Epoch: 016 Train loss: 1516.9962  took : 8.409361839294434
====> Test loss: 1519.8322
iteration 0000: loss: 1517.290
iteration 0100: loss: 1519.703
iteration 0200: loss: 1518.193
iteration 0300: loss: 1517.588
iteration 0400: loss: 1518.620
iteration 0500: loss: 1514.967
iteration 0600: loss: 1519.009
iteration 0700: loss: 1514.841
iteration 0800: loss: 1517.667
iteration 0900: loss: 1515.080
====> Epoch: 017 Train loss: 1516.8181  took : 8.5344717502594
====> Test loss: 1519.8538
iteration 0000: loss: 1514.678
iteration 0100: loss: 1516.105
iteration 0200: loss: 1517.419
iteration 0300: loss: 1516.135
iteration 0400: loss: 1514.763
iteration 0500: loss: 1515.599
iteration 0600: loss: 1518.392
iteration 0700: loss: 1514.283
iteration 0800: loss: 1517.244
iteration 0900: loss: 1516.630
====> Epoch: 018 Train loss: 1516.6624  took : 8.543036699295044
====> Test loss: 1519.6959
iteration 0000: loss: 1517.811
iteration 0100: loss: 1516.141
iteration 0200: loss: 1516.004
iteration 0300: loss: 1514.252
iteration 0400: loss: 1515.991
iteration 0500: loss: 1517.144
iteration 0600: loss: 1517.190
iteration 0700: loss: 1514.575
iteration 0800: loss: 1516.447
iteration 0900: loss: 1514.794
====> Epoch: 019 Train loss: 1516.4989  took : 8.416118383407593
====> Test loss: 1519.5563
iteration 0000: loss: 1515.958
iteration 0100: loss: 1514.624
iteration 0200: loss: 1514.827
iteration 0300: loss: 1519.636
iteration 0400: loss: 1515.394
iteration 0500: loss: 1515.847
iteration 0600: loss: 1518.065
iteration 0700: loss: 1514.853
iteration 0800: loss: 1516.929
iteration 0900: loss: 1517.564
====> Epoch: 020 Train loss: 1516.3877  took : 8.574106454849243
====> Test loss: 1519.6101
iteration 0000: loss: 1513.863
iteration 0100: loss: 1515.171
iteration 0200: loss: 1515.756
iteration 0300: loss: 1518.221
iteration 0400: loss: 1519.878
iteration 0500: loss: 1517.608
iteration 0600: loss: 1515.298
iteration 0700: loss: 1514.198
iteration 0800: loss: 1517.099
iteration 0900: loss: 1520.219
====> Epoch: 021 Train loss: 1516.2501  took : 8.494685888290405
====> Test loss: 1519.2976
iteration 0000: loss: 1517.625
iteration 0100: loss: 1518.493
iteration 0200: loss: 1516.557
iteration 0300: loss: 1513.491
iteration 0400: loss: 1515.051
iteration 0500: loss: 1515.686
iteration 0600: loss: 1518.002
iteration 0700: loss: 1517.545
iteration 0800: loss: 1516.092
iteration 0900: loss: 1517.105
====> Epoch: 022 Train loss: 1516.1254  took : 8.55881929397583
====> Test loss: 1519.4095
iteration 0000: loss: 1514.617
iteration 0100: loss: 1517.889
iteration 0200: loss: 1515.392
iteration 0300: loss: 1517.472
iteration 0400: loss: 1513.979
iteration 0500: loss: 1514.677
iteration 0600: loss: 1515.886
iteration 0700: loss: 1516.629
iteration 0800: loss: 1517.864
iteration 0900: loss: 1515.253
====> Epoch: 023 Train loss: 1516.0168  took : 8.450072765350342
====> Test loss: 1519.3033
iteration 0000: loss: 1516.809
iteration 0100: loss: 1516.163
iteration 0200: loss: 1517.741
iteration 0300: loss: 1519.756
iteration 0400: loss: 1518.546
iteration 0500: loss: 1518.523
iteration 0600: loss: 1516.646
iteration 0700: loss: 1514.003
iteration 0800: loss: 1518.710
iteration 0900: loss: 1517.865
====> Epoch: 024 Train loss: 1515.9060  took : 8.487154245376587
====> Test loss: 1519.0618
iteration 0000: loss: 1517.104
iteration 0100: loss: 1514.153
iteration 0200: loss: 1515.815
iteration 0300: loss: 1515.527
iteration 0400: loss: 1513.783
iteration 0500: loss: 1515.196
iteration 0600: loss: 1513.863
iteration 0700: loss: 1517.296
iteration 0800: loss: 1514.520
iteration 0900: loss: 1518.510
====> Epoch: 025 Train loss: 1515.7830  took : 8.495494604110718
====> Test loss: 1519.0674
iteration 0000: loss: 1516.907
iteration 0100: loss: 1516.337
iteration 0200: loss: 1515.510
iteration 0300: loss: 1517.382
iteration 0400: loss: 1515.311
iteration 0500: loss: 1516.665
iteration 0600: loss: 1517.325
iteration 0700: loss: 1514.635
iteration 0800: loss: 1517.075
iteration 0900: loss: 1516.002
====> Epoch: 026 Train loss: 1515.6983  took : 8.495964765548706
====> Test loss: 1518.9943
iteration 0000: loss: 1514.844
iteration 0100: loss: 1513.913
iteration 0200: loss: 1514.215
iteration 0300: loss: 1515.194
iteration 0400: loss: 1513.226
iteration 0500: loss: 1513.070
iteration 0600: loss: 1517.780
iteration 0700: loss: 1516.701
iteration 0800: loss: 1515.092
iteration 0900: loss: 1519.729
====> Epoch: 027 Train loss: 1515.6192  took : 8.437501668930054
====> Test loss: 1519.0062
iteration 0000: loss: 1514.160
iteration 0100: loss: 1517.196
iteration 0200: loss: 1514.800
iteration 0300: loss: 1513.589
iteration 0400: loss: 1517.400
iteration 0500: loss: 1514.028
iteration 0600: loss: 1515.471
iteration 0700: loss: 1513.739
iteration 0800: loss: 1515.013
iteration 0900: loss: 1514.012
====> Epoch: 028 Train loss: 1515.5544  took : 8.552350044250488
====> Test loss: 1518.8727
iteration 0000: loss: 1516.927
iteration 0100: loss: 1515.033
iteration 0200: loss: 1516.317
iteration 0300: loss: 1518.267
iteration 0400: loss: 1513.914
iteration 0500: loss: 1516.930
iteration 0600: loss: 1515.123
iteration 0700: loss: 1517.225
iteration 0800: loss: 1515.156
iteration 0900: loss: 1517.067
====> Epoch: 029 Train loss: 1515.4508  took : 8.509491682052612
====> Test loss: 1518.6713
iteration 0000: loss: 1514.641
iteration 0100: loss: 1513.827
iteration 0200: loss: 1516.894
iteration 0300: loss: 1515.081
iteration 0400: loss: 1516.041
iteration 0500: loss: 1515.488
iteration 0600: loss: 1513.767
iteration 0700: loss: 1519.008
iteration 0800: loss: 1513.979
iteration 0900: loss: 1515.826
====> Epoch: 030 Train loss: 1515.3759  took : 8.463207960128784
====> Test loss: 1518.7843
iteration 0000: loss: 1517.725
iteration 0100: loss: 1516.634
iteration 0200: loss: 1516.686
iteration 0300: loss: 1514.845
iteration 0400: loss: 1515.020
iteration 0500: loss: 1512.749
iteration 0600: loss: 1514.395
iteration 0700: loss: 1514.389
iteration 0800: loss: 1516.256
iteration 0900: loss: 1515.870
====> Epoch: 031 Train loss: 1515.2894  took : 8.471359014511108
====> Test loss: 1518.7263
iteration 0000: loss: 1514.698
iteration 0100: loss: 1516.052
iteration 0200: loss: 1513.741
iteration 0300: loss: 1516.038
iteration 0400: loss: 1517.649
iteration 0500: loss: 1515.510
iteration 0600: loss: 1513.855
iteration 0700: loss: 1515.479
iteration 0800: loss: 1518.321
iteration 0900: loss: 1514.652
====> Epoch: 032 Train loss: 1515.2193  took : 8.437520265579224
====> Test loss: 1518.5866
iteration 0000: loss: 1515.942
iteration 0100: loss: 1515.460
iteration 0200: loss: 1515.012
iteration 0300: loss: 1515.889
iteration 0400: loss: 1516.154
iteration 0500: loss: 1512.263
iteration 0600: loss: 1518.557
iteration 0700: loss: 1515.210
iteration 0800: loss: 1514.255
iteration 0900: loss: 1512.382
====> Epoch: 033 Train loss: 1515.1727  took : 8.480000495910645
====> Test loss: 1518.5314
iteration 0000: loss: 1514.472
iteration 0100: loss: 1515.453
iteration 0200: loss: 1516.097
iteration 0300: loss: 1518.275
iteration 0400: loss: 1510.955
iteration 0500: loss: 1517.073
iteration 0600: loss: 1513.675
iteration 0700: loss: 1513.838
iteration 0800: loss: 1513.564
iteration 0900: loss: 1515.977
====> Epoch: 034 Train loss: 1515.1029  took : 8.536102294921875
====> Test loss: 1518.6523
iteration 0000: loss: 1515.718
iteration 0100: loss: 1514.449
iteration 0200: loss: 1515.179
iteration 0300: loss: 1517.401
iteration 0400: loss: 1514.891
iteration 0500: loss: 1515.886
iteration 0600: loss: 1513.853
iteration 0700: loss: 1516.578
iteration 0800: loss: 1514.924
iteration 0900: loss: 1518.103
====> Epoch: 035 Train loss: 1515.0366  took : 8.448576927185059
====> Test loss: 1518.5169
iteration 0000: loss: 1512.961
iteration 0100: loss: 1513.996
iteration 0200: loss: 1514.759
iteration 0300: loss: 1517.288
iteration 0400: loss: 1513.559
iteration 0500: loss: 1516.976
iteration 0600: loss: 1513.838
iteration 0700: loss: 1512.977
iteration 0800: loss: 1514.343
iteration 0900: loss: 1513.099
====> Epoch: 036 Train loss: 1514.9297  took : 8.538219451904297
====> Test loss: 1518.5320
iteration 0000: loss: 1514.930
iteration 0100: loss: 1513.926
iteration 0200: loss: 1514.718
iteration 0300: loss: 1514.020
iteration 0400: loss: 1513.808
iteration 0500: loss: 1515.086
iteration 0600: loss: 1514.971
iteration 0700: loss: 1514.803
iteration 0800: loss: 1516.909
iteration 0900: loss: 1515.626
====> Epoch: 037 Train loss: 1514.8991  took : 8.446383237838745
====> Test loss: 1518.2904
iteration 0000: loss: 1514.851
iteration 0100: loss: 1514.565
iteration 0200: loss: 1517.467
iteration 0300: loss: 1512.887
iteration 0400: loss: 1514.849
iteration 0500: loss: 1513.877
iteration 0600: loss: 1517.526
iteration 0700: loss: 1515.219
iteration 0800: loss: 1516.106
iteration 0900: loss: 1514.493
====> Epoch: 038 Train loss: 1514.8469  took : 8.45850944519043
====> Test loss: 1518.3531
iteration 0000: loss: 1514.034
iteration 0100: loss: 1514.741
iteration 0200: loss: 1517.775
iteration 0300: loss: 1515.441
iteration 0400: loss: 1513.344
iteration 0500: loss: 1517.005
iteration 0600: loss: 1512.610
iteration 0700: loss: 1516.474
iteration 0800: loss: 1514.994
iteration 0900: loss: 1515.807
====> Epoch: 039 Train loss: 1514.8039  took : 8.549417972564697
====> Test loss: 1518.4275
iteration 0000: loss: 1516.775
iteration 0100: loss: 1515.749
iteration 0200: loss: 1515.372
iteration 0300: loss: 1513.232
iteration 0400: loss: 1515.836
iteration 0500: loss: 1514.498
iteration 0600: loss: 1515.027
iteration 0700: loss: 1513.617
iteration 0800: loss: 1515.001
iteration 0900: loss: 1515.294
====> Epoch: 040 Train loss: 1514.7418  took : 8.469963550567627
====> Test loss: 1518.4073
iteration 0000: loss: 1514.986
iteration 0100: loss: 1516.274
iteration 0200: loss: 1515.683
iteration 0300: loss: 1516.926
iteration 0400: loss: 1513.541
iteration 0500: loss: 1515.415
iteration 0600: loss: 1514.926
iteration 0700: loss: 1512.791
iteration 0800: loss: 1516.107
iteration 0900: loss: 1515.159
====> Epoch: 041 Train loss: 1514.7079  took : 8.524258852005005
====> Test loss: 1518.3439
iteration 0000: loss: 1515.358
iteration 0100: loss: 1511.999
iteration 0200: loss: 1518.042
iteration 0300: loss: 1514.910
iteration 0400: loss: 1514.328
iteration 0500: loss: 1511.207
iteration 0600: loss: 1516.423
iteration 0700: loss: 1515.684
iteration 0800: loss: 1514.772
iteration 0900: loss: 1515.900
====> Epoch: 042 Train loss: 1514.6474  took : 8.522145986557007
====> Test loss: 1518.2899
iteration 0000: loss: 1516.350
iteration 0100: loss: 1514.717
iteration 0200: loss: 1513.826
iteration 0300: loss: 1514.739
iteration 0400: loss: 1517.597
iteration 0500: loss: 1515.361
iteration 0600: loss: 1512.656
iteration 0700: loss: 1513.718
iteration 0800: loss: 1514.121
iteration 0900: loss: 1516.661
====> Epoch: 043 Train loss: 1514.5703  took : 8.422207117080688
====> Test loss: 1518.3126
iteration 0000: loss: 1515.729
iteration 0100: loss: 1514.016
iteration 0200: loss: 1513.215
iteration 0300: loss: 1513.904
iteration 0400: loss: 1515.145
iteration 0500: loss: 1515.256
iteration 0600: loss: 1516.564
iteration 0700: loss: 1513.868
iteration 0800: loss: 1515.868
iteration 0900: loss: 1514.489
====> Epoch: 044 Train loss: 1514.5222  took : 8.43676209449768
====> Test loss: 1518.1925
iteration 0000: loss: 1512.969
iteration 0100: loss: 1512.402
iteration 0200: loss: 1513.607
iteration 0300: loss: 1513.910
iteration 0400: loss: 1515.757
iteration 0500: loss: 1514.053
iteration 0600: loss: 1516.704
iteration 0700: loss: 1514.330
iteration 0800: loss: 1513.483
iteration 0900: loss: 1517.544
====> Epoch: 045 Train loss: 1514.4751  took : 8.452972650527954
====> Test loss: 1518.1215
iteration 0000: loss: 1513.624
iteration 0100: loss: 1513.504
iteration 0200: loss: 1515.034
iteration 0300: loss: 1515.056
iteration 0400: loss: 1513.146
iteration 0500: loss: 1514.835
iteration 0600: loss: 1516.155
iteration 0700: loss: 1514.950
iteration 0800: loss: 1515.226
iteration 0900: loss: 1515.378
====> Epoch: 046 Train loss: 1514.4479  took : 8.52349305152893
====> Test loss: 1518.1020
iteration 0000: loss: 1514.870
iteration 0100: loss: 1514.349
iteration 0200: loss: 1510.115
iteration 0300: loss: 1512.098
iteration 0400: loss: 1516.923
iteration 0500: loss: 1513.215
iteration 0600: loss: 1513.560
iteration 0700: loss: 1513.183
iteration 0800: loss: 1512.340
iteration 0900: loss: 1516.203
====> Epoch: 047 Train loss: 1514.4147  took : 8.534980535507202
====> Test loss: 1518.0789
iteration 0000: loss: 1515.683
iteration 0100: loss: 1515.274
iteration 0200: loss: 1515.627
iteration 0300: loss: 1515.013
iteration 0400: loss: 1513.201
iteration 0500: loss: 1514.870
iteration 0600: loss: 1514.139
iteration 0700: loss: 1514.629
iteration 0800: loss: 1511.126
iteration 0900: loss: 1514.433
====> Epoch: 048 Train loss: 1514.3383  took : 8.431592464447021
====> Test loss: 1518.2107
iteration 0000: loss: 1515.002
iteration 0100: loss: 1514.578
iteration 0200: loss: 1514.369
iteration 0300: loss: 1517.788
iteration 0400: loss: 1515.764
iteration 0500: loss: 1513.502
iteration 0600: loss: 1512.308
iteration 0700: loss: 1514.162
iteration 0800: loss: 1516.193
iteration 0900: loss: 1514.338
====> Epoch: 049 Train loss: 1514.3231  took : 8.482444524765015
====> Test loss: 1517.9836
iteration 0000: loss: 1514.677
iteration 0100: loss: 1510.477
iteration 0200: loss: 1514.445
iteration 0300: loss: 1512.333
iteration 0400: loss: 1513.466
iteration 0500: loss: 1514.287
iteration 0600: loss: 1515.234
iteration 0700: loss: 1513.535
iteration 0800: loss: 1513.739
iteration 0900: loss: 1514.168
====> Epoch: 050 Train loss: 1514.2816  took : 8.534369230270386
====> Test loss: 1518.1341
====> [MM-VAE] Time: 506.171s or 00:08:26
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  16
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_16
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_16
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.539
iteration 0100: loss: 2088.257
iteration 0200: loss: 2044.317
iteration 0300: loss: 2016.132
iteration 0400: loss: 2005.272
iteration 0500: loss: 2009.937
iteration 0600: loss: 1994.250
iteration 0700: loss: 1994.816
iteration 0800: loss: 1993.038
iteration 0900: loss: 1990.279
====> Epoch: 001 Train loss: 2023.0461  took : 12.86725401878357
====> Test loss: 1991.1171
iteration 0000: loss: 1990.151
iteration 0100: loss: 1989.191
iteration 0200: loss: 1985.911
iteration 0300: loss: 1981.911
iteration 0400: loss: 1990.208
iteration 0500: loss: 1982.648
iteration 0600: loss: 1984.900
iteration 0700: loss: 1984.663
iteration 0800: loss: 1979.046
iteration 0900: loss: 1978.952
====> Epoch: 002 Train loss: 1981.9613  took : 13.252472400665283
====> Test loss: 1980.2294
iteration 0000: loss: 1974.280
iteration 0100: loss: 1979.591
iteration 0200: loss: 1974.156
iteration 0300: loss: 1979.080
iteration 0400: loss: 1973.445
iteration 0500: loss: 1975.637
iteration 0600: loss: 1975.665
iteration 0700: loss: 1972.494
iteration 0800: loss: 1970.813
iteration 0900: loss: 1974.168
====> Epoch: 003 Train loss: 1973.6008  took : 11.956330299377441
====> Test loss: 1973.5092
iteration 0000: loss: 1969.484
iteration 0100: loss: 1969.481
iteration 0200: loss: 1966.759
iteration 0300: loss: 1964.969
iteration 0400: loss: 1963.104
iteration 0500: loss: 1961.006
iteration 0600: loss: 1967.631
iteration 0700: loss: 1965.099
iteration 0800: loss: 1960.423
iteration 0900: loss: 1969.617
====> Epoch: 004 Train loss: 1966.5278  took : 12.892227411270142
====> Test loss: 1967.0064
iteration 0000: loss: 1960.027
iteration 0100: loss: 1961.512
iteration 0200: loss: 1961.657
iteration 0300: loss: 1967.395
iteration 0400: loss: 1962.579
iteration 0500: loss: 1963.146
iteration 0600: loss: 1961.627
iteration 0700: loss: 1964.890
iteration 0800: loss: 1964.163
iteration 0900: loss: 1965.812
====> Epoch: 005 Train loss: 1962.4832  took : 13.242291688919067
====> Test loss: 1963.7611
iteration 0000: loss: 1963.208
iteration 0100: loss: 1957.477
iteration 0200: loss: 1959.948
iteration 0300: loss: 1962.286
iteration 0400: loss: 1962.250
iteration 0500: loss: 1960.043
iteration 0600: loss: 1960.178
iteration 0700: loss: 1958.447
iteration 0800: loss: 1959.683
iteration 0900: loss: 1957.978
====> Epoch: 006 Train loss: 1960.0713  took : 12.195138216018677
====> Test loss: 1962.2737
iteration 0000: loss: 1954.947
iteration 0100: loss: 1959.750
iteration 0200: loss: 1962.495
iteration 0300: loss: 1959.101
iteration 0400: loss: 1959.023
iteration 0500: loss: 1959.568
iteration 0600: loss: 1959.227
iteration 0700: loss: 1958.646
iteration 0800: loss: 1959.517
iteration 0900: loss: 1957.059
====> Epoch: 007 Train loss: 1958.8553  took : 12.860893249511719
====> Test loss: 1960.7177
iteration 0000: loss: 1958.810
iteration 0100: loss: 1958.273
iteration 0200: loss: 1958.769
iteration 0300: loss: 1957.186
iteration 0400: loss: 1956.995
iteration 0500: loss: 1957.797
iteration 0600: loss: 1954.497
iteration 0700: loss: 1959.970
iteration 0800: loss: 1957.545
iteration 0900: loss: 1961.039
====> Epoch: 008 Train loss: 1957.7564  took : 12.628051280975342
====> Test loss: 1959.3153
iteration 0000: loss: 1957.647
iteration 0100: loss: 1955.955
iteration 0200: loss: 1960.217
iteration 0300: loss: 1959.324
iteration 0400: loss: 1955.494
iteration 0500: loss: 1953.815
iteration 0600: loss: 1955.647
iteration 0700: loss: 1956.846
iteration 0800: loss: 1956.560
iteration 0900: loss: 1961.191
====> Epoch: 009 Train loss: 1956.8400  took : 11.66838002204895
====> Test loss: 1959.0077
iteration 0000: loss: 1955.208
iteration 0100: loss: 1953.962
iteration 0200: loss: 1955.492
iteration 0300: loss: 1956.493
iteration 0400: loss: 1955.300
iteration 0500: loss: 1953.792
iteration 0600: loss: 1955.865
iteration 0700: loss: 1957.663
iteration 0800: loss: 1952.923
iteration 0900: loss: 1954.574
====> Epoch: 010 Train loss: 1956.2688  took : 13.401798725128174
====> Test loss: 1958.3145
iteration 0000: loss: 1957.255
iteration 0100: loss: 1957.878
iteration 0200: loss: 1955.232
iteration 0300: loss: 1956.247
iteration 0400: loss: 1955.575
iteration 0500: loss: 1958.003
iteration 0600: loss: 1953.800
iteration 0700: loss: 1955.873
iteration 0800: loss: 1953.489
iteration 0900: loss: 1954.229
====> Epoch: 011 Train loss: 1955.8469  took : 12.139398097991943
====> Test loss: 1958.6596
iteration 0000: loss: 1956.871
iteration 0100: loss: 1956.867
iteration 0200: loss: 1955.116
iteration 0300: loss: 1958.361
iteration 0400: loss: 1956.208
iteration 0500: loss: 1955.844
iteration 0600: loss: 1956.691
iteration 0700: loss: 1952.667
iteration 0800: loss: 1954.935
iteration 0900: loss: 1954.577
====> Epoch: 012 Train loss: 1955.3041  took : 12.315258502960205
====> Test loss: 1957.2598
iteration 0000: loss: 1954.478
iteration 0100: loss: 1955.335
iteration 0200: loss: 1954.133
iteration 0300: loss: 1957.722
iteration 0400: loss: 1956.226
iteration 0500: loss: 1956.035
iteration 0600: loss: 1955.474
iteration 0700: loss: 1956.619
iteration 0800: loss: 1953.046
iteration 0900: loss: 1955.413
====> Epoch: 013 Train loss: 1954.7288  took : 12.251480102539062
====> Test loss: 1956.1474
iteration 0000: loss: 1954.004
iteration 0100: loss: 1955.552
iteration 0200: loss: 1957.695
iteration 0300: loss: 1955.071
iteration 0400: loss: 1954.660
iteration 0500: loss: 1952.501
iteration 0600: loss: 1951.904
iteration 0700: loss: 1955.257
iteration 0800: loss: 1952.443
iteration 0900: loss: 1953.696
====> Epoch: 014 Train loss: 1954.4002  took : 12.637820482254028
====> Test loss: 1956.5909
iteration 0000: loss: 1954.444
iteration 0100: loss: 1953.271
iteration 0200: loss: 1954.844
iteration 0300: loss: 1954.716
iteration 0400: loss: 1954.419
iteration 0500: loss: 1951.359
iteration 0600: loss: 1953.390
iteration 0700: loss: 1954.190
iteration 0800: loss: 1950.564
iteration 0900: loss: 1954.552
====> Epoch: 015 Train loss: 1953.9485  took : 11.848618745803833
====> Test loss: 1956.0331
iteration 0000: loss: 1953.601
iteration 0100: loss: 1951.178
iteration 0200: loss: 1953.183
iteration 0300: loss: 1952.379
iteration 0400: loss: 1955.157
iteration 0500: loss: 1951.013
iteration 0600: loss: 1952.399
iteration 0700: loss: 1952.854
iteration 0800: loss: 1952.743
iteration 0900: loss: 1953.498
====> Epoch: 016 Train loss: 1953.2440  took : 13.184360980987549
====> Test loss: 1955.1198
iteration 0000: loss: 1953.414
iteration 0100: loss: 1952.240
iteration 0200: loss: 1955.028
iteration 0300: loss: 1952.051
iteration 0400: loss: 1953.304
iteration 0500: loss: 1951.585
iteration 0600: loss: 1951.788
iteration 0700: loss: 1952.205
iteration 0800: loss: 1951.160
iteration 0900: loss: 1952.689
====> Epoch: 017 Train loss: 1952.6791  took : 12.339004516601562
====> Test loss: 1954.9076
iteration 0000: loss: 1953.838
iteration 0100: loss: 1951.680
iteration 0200: loss: 1951.561
iteration 0300: loss: 1950.939
iteration 0400: loss: 1954.496
iteration 0500: loss: 1952.422
iteration 0600: loss: 1952.084
iteration 0700: loss: 1955.127
iteration 0800: loss: 1951.340
iteration 0900: loss: 1952.166
====> Epoch: 018 Train loss: 1952.3761  took : 13.15478777885437
====> Test loss: 1954.4942
iteration 0000: loss: 1951.973
iteration 0100: loss: 1950.887
iteration 0200: loss: 1953.044
iteration 0300: loss: 1953.884
iteration 0400: loss: 1952.015
iteration 0500: loss: 1954.642
iteration 0600: loss: 1950.916
iteration 0700: loss: 1950.705
iteration 0800: loss: 1950.345
iteration 0900: loss: 1952.610
====> Epoch: 019 Train loss: 1951.9014  took : 13.164799928665161
====> Test loss: 1953.9840
iteration 0000: loss: 1952.980
iteration 0100: loss: 1953.424
iteration 0200: loss: 1950.976
iteration 0300: loss: 1950.768
iteration 0400: loss: 1952.209
iteration 0500: loss: 1953.625
iteration 0600: loss: 1950.382
iteration 0700: loss: 1952.995
iteration 0800: loss: 1950.068
iteration 0900: loss: 1951.110
====> Epoch: 020 Train loss: 1951.2393  took : 12.187813997268677
====> Test loss: 1953.1186
iteration 0000: loss: 1950.452
iteration 0100: loss: 1950.829
iteration 0200: loss: 1951.661
iteration 0300: loss: 1950.873
iteration 0400: loss: 1952.742
iteration 0500: loss: 1949.890
iteration 0600: loss: 1950.962
iteration 0700: loss: 1950.148
iteration 0800: loss: 1952.316
iteration 0900: loss: 1950.237
====> Epoch: 021 Train loss: 1950.6159  took : 12.210638284683228
====> Test loss: 1952.9237
iteration 0000: loss: 1952.889
iteration 0100: loss: 1951.624
iteration 0200: loss: 1950.268
iteration 0300: loss: 1948.911
iteration 0400: loss: 1950.809
iteration 0500: loss: 1951.087
iteration 0600: loss: 1950.391
iteration 0700: loss: 1950.373
iteration 0800: loss: 1950.509
iteration 0900: loss: 1948.503
====> Epoch: 022 Train loss: 1950.4658  took : 13.149516582489014
====> Test loss: 1951.9055
iteration 0000: loss: 1950.951
iteration 0100: loss: 1949.839
iteration 0200: loss: 1950.785
iteration 0300: loss: 1951.162
iteration 0400: loss: 1948.714
iteration 0500: loss: 1949.567
iteration 0600: loss: 1949.409
iteration 0700: loss: 1948.129
iteration 0800: loss: 1949.194
iteration 0900: loss: 1952.108
====> Epoch: 023 Train loss: 1950.1407  took : 12.161386966705322
====> Test loss: 1951.9390
iteration 0000: loss: 1950.360
iteration 0100: loss: 1950.358
iteration 0200: loss: 1950.639
iteration 0300: loss: 1950.341
iteration 0400: loss: 1951.099
iteration 0500: loss: 1950.661
iteration 0600: loss: 1948.185
iteration 0700: loss: 1949.416
iteration 0800: loss: 1948.326
iteration 0900: loss: 1949.307
====> Epoch: 024 Train loss: 1949.8725  took : 12.769034624099731
====> Test loss: 1951.7124
iteration 0000: loss: 1948.681
iteration 0100: loss: 1949.662
iteration 0200: loss: 1950.809
iteration 0300: loss: 1950.491
iteration 0400: loss: 1949.889
iteration 0500: loss: 1949.046
iteration 0600: loss: 1948.558
iteration 0700: loss: 1948.379
iteration 0800: loss: 1949.774
iteration 0900: loss: 1948.731
====> Epoch: 025 Train loss: 1949.6574  took : 13.06560730934143
====> Test loss: 1952.1398
iteration 0000: loss: 1951.496
iteration 0100: loss: 1949.838
iteration 0200: loss: 1948.468
iteration 0300: loss: 1949.393
iteration 0400: loss: 1948.967
iteration 0500: loss: 1949.564
iteration 0600: loss: 1951.049
iteration 0700: loss: 1950.543
iteration 0800: loss: 1949.057
iteration 0900: loss: 1949.911
====> Epoch: 026 Train loss: 1949.6495  took : 12.411250829696655
====> Test loss: 1951.2828
iteration 0000: loss: 1950.882
iteration 0100: loss: 1948.900
iteration 0200: loss: 1947.811
iteration 0300: loss: 1948.081
iteration 0400: loss: 1947.709
iteration 0500: loss: 1949.372
iteration 0600: loss: 1950.198
iteration 0700: loss: 1950.907
iteration 0800: loss: 1949.092
iteration 0900: loss: 1948.017
====> Epoch: 027 Train loss: 1949.2811  took : 13.160656690597534
====> Test loss: 1951.1841
iteration 0000: loss: 1948.350
iteration 0100: loss: 1950.545
iteration 0200: loss: 1948.900
iteration 0300: loss: 1949.316
iteration 0400: loss: 1949.738
iteration 0500: loss: 1950.303
iteration 0600: loss: 1950.604
iteration 0700: loss: 1948.528
iteration 0800: loss: 1949.322
iteration 0900: loss: 1947.865
====> Epoch: 028 Train loss: 1949.1839  took : 12.533390522003174
====> Test loss: 1950.8549
iteration 0000: loss: 1948.576
iteration 0100: loss: 1950.403
iteration 0200: loss: 1948.273
iteration 0300: loss: 1949.012
iteration 0400: loss: 1948.573
iteration 0500: loss: 1949.359
iteration 0600: loss: 1948.120
iteration 0700: loss: 1951.996
iteration 0800: loss: 1948.216
iteration 0900: loss: 1949.561
====> Epoch: 029 Train loss: 1949.0977  took : 12.90815806388855
====> Test loss: 1951.4149
iteration 0000: loss: 1948.409
iteration 0100: loss: 1947.623
iteration 0200: loss: 1949.662
iteration 0300: loss: 1949.761
iteration 0400: loss: 1952.281
iteration 0500: loss: 1948.217
iteration 0600: loss: 1948.827
iteration 0700: loss: 1947.958
iteration 0800: loss: 1948.332
iteration 0900: loss: 1949.474
====> Epoch: 030 Train loss: 1948.8132  took : 11.631114959716797
====> Test loss: 1950.7715
iteration 0000: loss: 1948.614
iteration 0100: loss: 1947.875
iteration 0200: loss: 1948.297
iteration 0300: loss: 1949.945
iteration 0400: loss: 1947.732
iteration 0500: loss: 1947.874
iteration 0600: loss: 1948.435
iteration 0700: loss: 1948.301
iteration 0800: loss: 1948.823
iteration 0900: loss: 1949.219
====> Epoch: 031 Train loss: 1948.7923  took : 12.84989047050476
====> Test loss: 1951.3101
iteration 0000: loss: 1950.499
iteration 0100: loss: 1947.337
iteration 0200: loss: 1947.836
iteration 0300: loss: 1949.312
iteration 0400: loss: 1947.907
iteration 0500: loss: 1949.970
iteration 0600: loss: 1948.349
iteration 0700: loss: 1949.832
iteration 0800: loss: 1949.044
iteration 0900: loss: 1950.658
====> Epoch: 032 Train loss: 1948.8699  took : 12.824127674102783
====> Test loss: 1951.2064
iteration 0000: loss: 1950.968
iteration 0100: loss: 1949.901
iteration 0200: loss: 1948.986
iteration 0300: loss: 1947.376
iteration 0400: loss: 1949.187
iteration 0500: loss: 1950.226
iteration 0600: loss: 1947.748
iteration 0700: loss: 1949.138
iteration 0800: loss: 1948.221
iteration 0900: loss: 1950.199
====> Epoch: 033 Train loss: 1948.8144  took : 13.178588628768921
====> Test loss: 1950.4770
iteration 0000: loss: 1947.931
iteration 0100: loss: 1949.014
iteration 0200: loss: 1950.217
iteration 0300: loss: 1949.330
iteration 0400: loss: 1949.242
iteration 0500: loss: 1947.804
iteration 0600: loss: 1948.815
iteration 0700: loss: 1948.177
iteration 0800: loss: 1948.851
iteration 0900: loss: 1948.354
====> Epoch: 034 Train loss: 1948.7958  took : 12.614515542984009
====> Test loss: 1950.6135
iteration 0000: loss: 1948.778
iteration 0100: loss: 1948.713
iteration 0200: loss: 1949.414
iteration 0300: loss: 1949.083
iteration 0400: loss: 1948.181
iteration 0500: loss: 1948.530
iteration 0600: loss: 1949.781
iteration 0700: loss: 1949.000
iteration 0800: loss: 1949.393
iteration 0900: loss: 1948.323
====> Epoch: 035 Train loss: 1948.6783  took : 12.95142412185669
====> Test loss: 1950.8409
iteration 0000: loss: 1948.634
iteration 0100: loss: 1949.634
iteration 0200: loss: 1948.879
iteration 0300: loss: 1947.082
iteration 0400: loss: 1948.431
iteration 0500: loss: 1948.378
iteration 0600: loss: 1948.718
iteration 0700: loss: 1948.550
iteration 0800: loss: 1948.212
iteration 0900: loss: 1948.861
====> Epoch: 036 Train loss: 1948.7849  took : 12.6563560962677
====> Test loss: 1950.6087
iteration 0000: loss: 1948.218
iteration 0100: loss: 1947.600
iteration 0200: loss: 1948.425
iteration 0300: loss: 1948.933
iteration 0400: loss: 1948.103
iteration 0500: loss: 1948.822
iteration 0600: loss: 1947.755
iteration 0700: loss: 1948.498
iteration 0800: loss: 1948.053
iteration 0900: loss: 1947.800
====> Epoch: 037 Train loss: 1948.6240  took : 11.72130560874939
====> Test loss: 1950.5370
iteration 0000: loss: 1948.582
iteration 0100: loss: 1947.746
iteration 0200: loss: 1948.526
iteration 0300: loss: 1947.868
iteration 0400: loss: 1947.836
iteration 0500: loss: 1948.094
iteration 0600: loss: 1948.593
iteration 0700: loss: 1948.014
iteration 0800: loss: 1949.179
iteration 0900: loss: 1948.288
====> Epoch: 038 Train loss: 1948.4794  took : 12.130517482757568
====> Test loss: 1950.5046
iteration 0000: loss: 1948.682
iteration 0100: loss: 1949.243
iteration 0200: loss: 1947.935
iteration 0300: loss: 1947.761
iteration 0400: loss: 1949.761
iteration 0500: loss: 1947.892
iteration 0600: loss: 1948.305
iteration 0700: loss: 1948.364
iteration 0800: loss: 1947.517
iteration 0900: loss: 1947.327
====> Epoch: 039 Train loss: 1948.5600  took : 11.96639108657837
====> Test loss: 1950.5602
iteration 0000: loss: 1947.292
iteration 0100: loss: 1947.718
iteration 0200: loss: 1949.732
iteration 0300: loss: 1947.089
iteration 0400: loss: 1947.481
iteration 0500: loss: 1950.025
iteration 0600: loss: 1947.785
iteration 0700: loss: 1948.993
iteration 0800: loss: 1948.290
iteration 0900: loss: 1948.782
====> Epoch: 040 Train loss: 1948.6386  took : 12.195083379745483
====> Test loss: 1950.8295
iteration 0000: loss: 1949.037
iteration 0100: loss: 1948.013
iteration 0200: loss: 1949.404
iteration 0300: loss: 1947.460
iteration 0400: loss: 1947.284
iteration 0500: loss: 1948.609
iteration 0600: loss: 1947.050
iteration 0700: loss: 1947.740
iteration 0800: loss: 1947.816
iteration 0900: loss: 1948.255
====> Epoch: 041 Train loss: 1948.4824  took : 13.104858875274658
====> Test loss: 1950.5435
iteration 0000: loss: 1947.545
iteration 0100: loss: 1947.258
iteration 0200: loss: 1948.490
iteration 0300: loss: 1948.376
iteration 0400: loss: 1950.239
iteration 0500: loss: 1948.785
iteration 0600: loss: 1949.104
iteration 0700: loss: 1947.526
iteration 0800: loss: 1948.583
iteration 0900: loss: 1947.783
====> Epoch: 042 Train loss: 1948.3309  took : 12.50744080543518
====> Test loss: 1949.7953
iteration 0000: loss: 1947.042
iteration 0100: loss: 1946.895
iteration 0200: loss: 1949.065
iteration 0300: loss: 1947.408
iteration 0400: loss: 1949.656
iteration 0500: loss: 1949.502
iteration 0600: loss: 1947.929
iteration 0700: loss: 1948.638
iteration 0800: loss: 1948.681
iteration 0900: loss: 1947.683
====> Epoch: 043 Train loss: 1948.3667  took : 12.580873012542725
====> Test loss: 1950.2737
iteration 0000: loss: 1947.453
iteration 0100: loss: 1948.403
iteration 0200: loss: 1949.087
iteration 0300: loss: 1947.831
iteration 0400: loss: 1947.780
iteration 0500: loss: 1948.521
iteration 0600: loss: 1947.576
iteration 0700: loss: 1947.570
iteration 0800: loss: 1947.285
iteration 0900: loss: 1948.646
====> Epoch: 044 Train loss: 1948.2343  took : 12.68453860282898
====> Test loss: 1950.6507
iteration 0000: loss: 1948.582
iteration 0100: loss: 1948.950
iteration 0200: loss: 1948.144
iteration 0300: loss: 1948.194
iteration 0400: loss: 1949.554
iteration 0500: loss: 1948.251
iteration 0600: loss: 1948.002
iteration 0700: loss: 1947.747
iteration 0800: loss: 1947.786
iteration 0900: loss: 1947.521
====> Epoch: 045 Train loss: 1948.2327  took : 13.25052785873413
====> Test loss: 1950.3404
iteration 0000: loss: 1948.486
iteration 0100: loss: 1949.353
iteration 0200: loss: 1948.792
iteration 0300: loss: 1947.847
iteration 0400: loss: 1948.712
iteration 0500: loss: 1948.476
iteration 0600: loss: 1947.806
iteration 0700: loss: 1948.220
iteration 0800: loss: 1948.843
iteration 0900: loss: 1949.601
====> Epoch: 046 Train loss: 1948.2351  took : 11.79543137550354
====> Test loss: 1950.1496
iteration 0000: loss: 1947.376
iteration 0100: loss: 1947.992
iteration 0200: loss: 1948.774
iteration 0300: loss: 1948.619
iteration 0400: loss: 1947.973
iteration 0500: loss: 1947.217
iteration 0600: loss: 1947.677
iteration 0700: loss: 1948.696
iteration 0800: loss: 1948.150
iteration 0900: loss: 1948.220
====> Epoch: 047 Train loss: 1948.3205  took : 12.88976001739502
====> Test loss: 1950.2279
iteration 0000: loss: 1948.389
iteration 0100: loss: 1948.064
iteration 0200: loss: 1947.974
iteration 0300: loss: 1947.613
iteration 0400: loss: 1947.441
iteration 0500: loss: 1949.025
iteration 0600: loss: 1947.921
iteration 0700: loss: 1948.563
iteration 0800: loss: 1948.608
iteration 0900: loss: 1947.773
====> Epoch: 048 Train loss: 1948.1808  took : 12.616236448287964
====> Test loss: 1950.4544
iteration 0000: loss: 1948.411
iteration 0100: loss: 1948.947
iteration 0200: loss: 1948.779
iteration 0300: loss: 1947.545
iteration 0400: loss: 1948.924
iteration 0500: loss: 1948.521
iteration 0600: loss: 1949.596
iteration 0700: loss: 1949.793
iteration 0800: loss: 1947.774
iteration 0900: loss: 1947.547
====> Epoch: 049 Train loss: 1948.3887  took : 11.882873058319092
====> Test loss: 1950.2123
iteration 0000: loss: 1948.266
iteration 0100: loss: 1947.841
iteration 0200: loss: 1947.608
iteration 0300: loss: 1947.722
iteration 0400: loss: 1947.287
iteration 0500: loss: 1947.333
iteration 0600: loss: 1948.770
iteration 0700: loss: 1948.142
iteration 0800: loss: 1947.467
iteration 0900: loss: 1947.521
====> Epoch: 050 Train loss: 1948.0642  took : 13.13316798210144
====> Test loss: 1949.9554
====> [MM-VAE] Time: 699.573s or 00:11:39
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  16
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_16
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_16
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5218.590
iteration 0100: loss: 4099.555
iteration 0200: loss: 4078.527
iteration 0300: loss: 4028.966
iteration 0400: loss: 4018.804
iteration 0500: loss: 4015.553
iteration 0600: loss: 4012.492
iteration 0700: loss: 4013.086
iteration 0800: loss: 3998.809
iteration 0900: loss: 4013.481
iteration 1000: loss: 4008.295
iteration 1100: loss: 3983.041
iteration 1200: loss: 4008.792
iteration 1300: loss: 4008.086
iteration 1400: loss: 3986.729
iteration 1500: loss: 3994.938
iteration 1600: loss: 3986.066
iteration 1700: loss: 3983.107
iteration 1800: loss: 4000.791
====> Epoch: 001 Train loss: 4021.1891  took : 53.26545691490173
====> Test loss: 3984.8712
iteration 0000: loss: 3979.908
iteration 0100: loss: 3988.986
iteration 0200: loss: 3973.051
iteration 0300: loss: 3976.912
iteration 0400: loss: 3979.077
iteration 0500: loss: 3974.665
iteration 0600: loss: 3975.510
iteration 0700: loss: 3966.671
iteration 0800: loss: 3965.285
iteration 0900: loss: 3968.355
iteration 1000: loss: 3960.090
iteration 1100: loss: 3961.140
iteration 1200: loss: 3959.905
iteration 1300: loss: 3957.863
iteration 1400: loss: 3958.160
iteration 1500: loss: 3966.616
iteration 1600: loss: 3963.028
iteration 1700: loss: 3967.105
iteration 1800: loss: 3964.506
====> Epoch: 002 Train loss: 3968.4671  took : 52.67209243774414
====> Test loss: 3961.0810
iteration 0000: loss: 3961.627
iteration 0100: loss: 3953.458
iteration 0200: loss: 3949.711
iteration 0300: loss: 3951.601
iteration 0400: loss: 3950.436
iteration 0500: loss: 3947.486
iteration 0600: loss: 3950.803
iteration 0700: loss: 3950.643
iteration 0800: loss: 3951.013
iteration 0900: loss: 3947.914
iteration 1000: loss: 3957.540
iteration 1100: loss: 3947.220
iteration 1200: loss: 3942.797
iteration 1300: loss: 3947.991
iteration 1400: loss: 3944.138
iteration 1500: loss: 3944.011
iteration 1600: loss: 3943.726
iteration 1700: loss: 3943.471
iteration 1800: loss: 3942.247
====> Epoch: 003 Train loss: 3951.9245  took : 53.115681886672974
====> Test loss: 3950.5965
iteration 0000: loss: 3948.642
iteration 0100: loss: 3951.267
iteration 0200: loss: 3940.986
iteration 0300: loss: 3948.728
iteration 0400: loss: 3943.723
iteration 0500: loss: 3945.096
iteration 0600: loss: 3945.301
iteration 0700: loss: 3952.540
iteration 0800: loss: 3947.115
iteration 0900: loss: 3959.868
iteration 1000: loss: 3949.187
iteration 1100: loss: 3949.174
iteration 1200: loss: 3944.355
iteration 1300: loss: 3946.285
iteration 1400: loss: 3946.191
iteration 1500: loss: 3951.024
iteration 1600: loss: 3947.114
iteration 1700: loss: 3944.603
iteration 1800: loss: 3944.625
====> Epoch: 004 Train loss: 3946.3612  took : 52.89491939544678
====> Test loss: 3948.7091
iteration 0000: loss: 3940.739
iteration 0100: loss: 3947.037
iteration 0200: loss: 3948.526
iteration 0300: loss: 3941.165
iteration 0400: loss: 3939.969
iteration 0500: loss: 3941.568
iteration 0600: loss: 3947.811
iteration 0700: loss: 3945.365
iteration 0800: loss: 3948.707
iteration 0900: loss: 3943.059
iteration 1000: loss: 3942.202
iteration 1100: loss: 3939.643
iteration 1200: loss: 3944.238
iteration 1300: loss: 3943.450
iteration 1400: loss: 3939.916
iteration 1500: loss: 3941.717
iteration 1600: loss: 3944.638
iteration 1700: loss: 3946.720
iteration 1800: loss: 3933.917
====> Epoch: 005 Train loss: 3944.5729  took : 52.874287843704224
====> Test loss: 3948.4388
iteration 0000: loss: 3944.626
iteration 0100: loss: 3940.951
iteration 0200: loss: 3938.882
iteration 0300: loss: 3943.547
iteration 0400: loss: 3945.274
iteration 0500: loss: 3943.943
iteration 0600: loss: 3943.284
iteration 0700: loss: 3948.769
iteration 0800: loss: 3942.376
iteration 0900: loss: 3939.525
iteration 1000: loss: 3944.618
iteration 1100: loss: 3945.809
iteration 1200: loss: 3946.269
iteration 1300: loss: 3937.497
iteration 1400: loss: 3940.763
iteration 1500: loss: 3945.275
iteration 1600: loss: 3947.401
iteration 1700: loss: 3946.466
iteration 1800: loss: 3941.138
====> Epoch: 006 Train loss: 3943.7765  took : 52.8986439704895
====> Test loss: 3947.7223
iteration 0000: loss: 3944.414
iteration 0100: loss: 3939.791
iteration 0200: loss: 3946.728
iteration 0300: loss: 3945.875
iteration 0400: loss: 3938.597
iteration 0500: loss: 3942.914
iteration 0600: loss: 3939.910
iteration 0700: loss: 3943.114
iteration 0800: loss: 3944.164
iteration 0900: loss: 3938.440
iteration 1000: loss: 3942.236
iteration 1100: loss: 3940.745
iteration 1200: loss: 3946.097
iteration 1300: loss: 3940.400
iteration 1400: loss: 3948.308
iteration 1500: loss: 3941.946
iteration 1600: loss: 3941.284
iteration 1700: loss: 3944.254
iteration 1800: loss: 3942.061
====> Epoch: 007 Train loss: 3943.2035  took : 52.77680850028992
====> Test loss: 3946.3318
iteration 0000: loss: 3942.977
iteration 0100: loss: 3941.037
iteration 0200: loss: 3941.513
iteration 0300: loss: 3945.505
iteration 0400: loss: 3944.440
iteration 0500: loss: 3942.697
iteration 0600: loss: 3940.790
iteration 0700: loss: 3946.596
iteration 0800: loss: 3938.177
iteration 0900: loss: 3938.019
iteration 1000: loss: 3953.325
iteration 1100: loss: 3939.277
iteration 1200: loss: 3943.512
iteration 1300: loss: 3933.289
iteration 1400: loss: 3943.268
iteration 1500: loss: 3941.899
iteration 1600: loss: 3946.471
iteration 1700: loss: 3937.765
iteration 1800: loss: 3941.989
====> Epoch: 008 Train loss: 3942.8554  took : 52.85561561584473
====> Test loss: 3946.5866
iteration 0000: loss: 3946.385
iteration 0100: loss: 3946.584
iteration 0200: loss: 3949.843
iteration 0300: loss: 3947.077
iteration 0400: loss: 3942.080
iteration 0500: loss: 3943.648
iteration 0600: loss: 3942.241
iteration 0700: loss: 3939.926
iteration 0800: loss: 3946.313
iteration 0900: loss: 3942.845
iteration 1000: loss: 3944.584
iteration 1100: loss: 3949.604
iteration 1200: loss: 3940.816
iteration 1300: loss: 3944.882
iteration 1400: loss: 3937.329
iteration 1500: loss: 3940.369
iteration 1600: loss: 3941.714
iteration 1700: loss: 3942.199
iteration 1800: loss: 3938.379
====> Epoch: 009 Train loss: 3942.6642  took : 52.88986372947693
====> Test loss: 3945.6848
iteration 0000: loss: 3940.156
iteration 0100: loss: 3949.139
iteration 0200: loss: 3940.496
iteration 0300: loss: 3939.277
iteration 0400: loss: 3946.818
iteration 0500: loss: 3939.180
iteration 0600: loss: 3935.971
iteration 0700: loss: 3938.120
iteration 0800: loss: 3939.361
iteration 0900: loss: 3952.079
iteration 1000: loss: 3943.490
iteration 1100: loss: 3943.828
iteration 1200: loss: 3943.607
iteration 1300: loss: 3940.005
iteration 1400: loss: 3946.662
iteration 1500: loss: 3945.109
iteration 1600: loss: 3944.126
iteration 1700: loss: 3940.339
iteration 1800: loss: 3941.062
====> Epoch: 010 Train loss: 3942.2592  took : 53.070271015167236
====> Test loss: 3945.5013
iteration 0000: loss: 3947.161
iteration 0100: loss: 3945.424
iteration 0200: loss: 3946.298
iteration 0300: loss: 3937.595
iteration 0400: loss: 3941.023
iteration 0500: loss: 3941.965
iteration 0600: loss: 3941.905
iteration 0700: loss: 3939.567
iteration 0800: loss: 3943.511
iteration 0900: loss: 3949.867
iteration 1000: loss: 3937.688
iteration 1100: loss: 3944.063
iteration 1200: loss: 3937.930
iteration 1300: loss: 3938.771
iteration 1400: loss: 3942.199
iteration 1500: loss: 3935.275
iteration 1600: loss: 3944.147
iteration 1700: loss: 3936.253
iteration 1800: loss: 3939.251
====> Epoch: 011 Train loss: 3942.0338  took : 52.77767491340637
====> Test loss: 3945.0318
iteration 0000: loss: 3941.939
iteration 0100: loss: 3940.512
iteration 0200: loss: 3934.719
iteration 0300: loss: 3935.667
iteration 0400: loss: 3941.308
iteration 0500: loss: 3941.915
iteration 0600: loss: 3946.702
iteration 0700: loss: 3939.617
iteration 0800: loss: 3944.200
iteration 0900: loss: 3939.516
iteration 1000: loss: 3937.812
iteration 1100: loss: 3947.207
iteration 1200: loss: 3939.417
iteration 1300: loss: 3941.901
iteration 1400: loss: 3935.996
iteration 1500: loss: 3941.794
iteration 1600: loss: 3940.862
iteration 1700: loss: 3945.639
iteration 1800: loss: 3944.417
====> Epoch: 012 Train loss: 3941.8738  took : 52.80014657974243
====> Test loss: 3945.3745
iteration 0000: loss: 3941.048
iteration 0100: loss: 3944.187
iteration 0200: loss: 3944.214
iteration 0300: loss: 3941.484
iteration 0400: loss: 3943.935
iteration 0500: loss: 3938.898
iteration 0600: loss: 3948.285
iteration 0700: loss: 3938.494
iteration 0800: loss: 3937.092
iteration 0900: loss: 3943.255
iteration 1000: loss: 3940.953
iteration 1100: loss: 3944.711
iteration 1200: loss: 3940.121
iteration 1300: loss: 3935.347
iteration 1400: loss: 3938.432
iteration 1500: loss: 3941.450
iteration 1600: loss: 3945.168
iteration 1700: loss: 3939.034
iteration 1800: loss: 3948.225
====> Epoch: 013 Train loss: 3941.7047  took : 52.86659383773804
====> Test loss: 3945.2053
iteration 0000: loss: 3942.103
iteration 0100: loss: 3944.877
iteration 0200: loss: 3943.964
iteration 0300: loss: 3940.470
iteration 0400: loss: 3942.877
iteration 0500: loss: 3944.123
iteration 0600: loss: 3943.434
iteration 0700: loss: 3951.412
iteration 0800: loss: 3939.159
iteration 0900: loss: 3940.984
iteration 1000: loss: 3943.764
iteration 1100: loss: 3942.284
iteration 1200: loss: 3944.392
iteration 1300: loss: 3940.813
iteration 1400: loss: 3944.185
iteration 1500: loss: 3942.301
iteration 1600: loss: 3937.999
iteration 1700: loss: 3944.363
iteration 1800: loss: 3939.276
====> Epoch: 014 Train loss: 3941.6083  took : 52.96280550956726
====> Test loss: 3945.6441
iteration 0000: loss: 3943.437
iteration 0100: loss: 3940.971
iteration 0200: loss: 3940.514
iteration 0300: loss: 3937.669
iteration 0400: loss: 3944.157
iteration 0500: loss: 3935.692
iteration 0600: loss: 3941.939
iteration 0700: loss: 3941.539
iteration 0800: loss: 3943.676
iteration 0900: loss: 3939.821
iteration 1000: loss: 3942.436
iteration 1100: loss: 3938.571
iteration 1200: loss: 3938.989
iteration 1300: loss: 3940.898
iteration 1400: loss: 3940.489
iteration 1500: loss: 3938.354
iteration 1600: loss: 3939.925
iteration 1700: loss: 3940.399
iteration 1800: loss: 3942.202
====> Epoch: 015 Train loss: 3941.4072  took : 52.998470306396484
====> Test loss: 3944.5149
iteration 0000: loss: 3941.684
iteration 0100: loss: 3935.783
iteration 0200: loss: 3942.842
iteration 0300: loss: 3936.364
iteration 0400: loss: 3946.505
iteration 0500: loss: 3944.296
iteration 0600: loss: 3943.705
iteration 0700: loss: 3948.097
iteration 0800: loss: 3942.292
iteration 0900: loss: 3941.706
iteration 1000: loss: 3937.115
iteration 1100: loss: 3946.316
iteration 1200: loss: 3939.542
iteration 1300: loss: 3944.762
iteration 1400: loss: 3943.695
iteration 1500: loss: 3944.618
iteration 1600: loss: 3935.869
iteration 1700: loss: 3947.305
iteration 1800: loss: 3936.789
====> Epoch: 016 Train loss: 3941.3835  took : 52.91208791732788
====> Test loss: 3944.9800
iteration 0000: loss: 3939.416
iteration 0100: loss: 3940.453
iteration 0200: loss: 3938.434
iteration 0300: loss: 3943.389
iteration 0400: loss: 3939.575
iteration 0500: loss: 3938.594
iteration 0600: loss: 3947.308
iteration 0700: loss: 3939.231
iteration 0800: loss: 3947.161
iteration 0900: loss: 3944.463
iteration 1000: loss: 3939.533
iteration 1100: loss: 3940.865
iteration 1200: loss: 3936.242
iteration 1300: loss: 3939.977
iteration 1400: loss: 3943.296
iteration 1500: loss: 3939.834
iteration 1600: loss: 3940.192
iteration 1700: loss: 3936.506
iteration 1800: loss: 3940.214
====> Epoch: 017 Train loss: 3941.3810  took : 53.16982674598694
====> Test loss: 3944.9331
iteration 0000: loss: 3937.638
iteration 0100: loss: 3941.918
iteration 0200: loss: 3936.934
iteration 0300: loss: 3938.731
iteration 0400: loss: 3946.947
iteration 0500: loss: 3939.958
iteration 0600: loss: 3939.705
iteration 0700: loss: 3938.928
iteration 0800: loss: 3936.863
iteration 0900: loss: 3936.675
iteration 1000: loss: 3944.773
iteration 1100: loss: 3941.793
iteration 1200: loss: 3942.500
iteration 1300: loss: 3939.759
iteration 1400: loss: 3936.876
iteration 1500: loss: 3935.295
iteration 1600: loss: 3940.733
iteration 1700: loss: 3939.271
iteration 1800: loss: 3943.948
====> Epoch: 018 Train loss: 3941.0334  took : 52.78183126449585
====> Test loss: 3944.7265
iteration 0000: loss: 3937.506
iteration 0100: loss: 3938.524
iteration 0200: loss: 3945.593
iteration 0300: loss: 3948.742
iteration 0400: loss: 3939.826
iteration 0500: loss: 3939.206
iteration 0600: loss: 3946.851
iteration 0700: loss: 3938.511
iteration 0800: loss: 3940.440
iteration 0900: loss: 3936.307
iteration 1000: loss: 3940.350
iteration 1100: loss: 3938.931
iteration 1200: loss: 3939.943
iteration 1300: loss: 3945.726
iteration 1400: loss: 3944.694
iteration 1500: loss: 3941.125
iteration 1600: loss: 3949.298
iteration 1700: loss: 3943.345
iteration 1800: loss: 3942.685
====> Epoch: 019 Train loss: 3941.1709  took : 53.073105335235596
====> Test loss: 3944.9359
iteration 0000: loss: 3939.750
iteration 0100: loss: 3937.565
iteration 0200: loss: 3936.593
iteration 0300: loss: 3942.400
iteration 0400: loss: 3939.009
iteration 0500: loss: 3940.451
iteration 0600: loss: 3938.395
iteration 0700: loss: 3948.157
iteration 0800: loss: 3943.582
iteration 0900: loss: 3938.481
iteration 1000: loss: 3936.216
iteration 1100: loss: 3941.262
iteration 1200: loss: 3945.246
iteration 1300: loss: 3941.762
iteration 1400: loss: 3936.107
iteration 1500: loss: 3945.052
iteration 1600: loss: 3941.622
iteration 1700: loss: 3936.753
iteration 1800: loss: 3939.966
====> Epoch: 020 Train loss: 3941.1626  took : 53.16232657432556
====> Test loss: 3945.2806
iteration 0000: loss: 3936.635
iteration 0100: loss: 3945.706
iteration 0200: loss: 3947.588
iteration 0300: loss: 3942.441
iteration 0400: loss: 3949.905
iteration 0500: loss: 3942.784
iteration 0600: loss: 3940.860
iteration 0700: loss: 3948.873
iteration 0800: loss: 3939.931
iteration 0900: loss: 3940.592
iteration 1000: loss: 3939.611
iteration 1100: loss: 3941.695
iteration 1200: loss: 3937.751
iteration 1300: loss: 3936.324
iteration 1400: loss: 3942.617
iteration 1500: loss: 3947.832
iteration 1600: loss: 3950.292
iteration 1700: loss: 3943.178
iteration 1800: loss: 3943.270
====> Epoch: 021 Train loss: 3941.0743  took : 52.77876424789429
====> Test loss: 3944.2942
iteration 0000: loss: 3943.035
iteration 0100: loss: 3945.533
iteration 0200: loss: 3941.560
iteration 0300: loss: 3942.072
iteration 0400: loss: 3935.605
iteration 0500: loss: 3934.639
iteration 0600: loss: 3941.219
iteration 0700: loss: 3940.745
iteration 0800: loss: 3944.493
iteration 0900: loss: 3944.560
iteration 1000: loss: 3943.604
iteration 1100: loss: 3940.668
iteration 1200: loss: 3941.133
iteration 1300: loss: 3941.383
iteration 1400: loss: 3936.355
iteration 1500: loss: 3940.197
iteration 1600: loss: 3939.966
iteration 1700: loss: 3936.486
iteration 1800: loss: 3942.298
====> Epoch: 022 Train loss: 3940.9706  took : 52.81586241722107
====> Test loss: 3945.0104
iteration 0000: loss: 3939.297
iteration 0100: loss: 3933.610
iteration 0200: loss: 3943.215
iteration 0300: loss: 3937.728
iteration 0400: loss: 3944.282
iteration 0500: loss: 3939.056
iteration 0600: loss: 3939.964
iteration 0700: loss: 3933.927
iteration 0800: loss: 3943.013
iteration 0900: loss: 3934.498
iteration 1000: loss: 3944.462
iteration 1100: loss: 3938.660
iteration 1200: loss: 3942.281
iteration 1300: loss: 3938.948
iteration 1400: loss: 3937.619
iteration 1500: loss: 3942.597
iteration 1600: loss: 3938.058
iteration 1700: loss: 3939.552
iteration 1800: loss: 3937.091
====> Epoch: 023 Train loss: 3940.8195  took : 52.90988874435425
====> Test loss: 3944.1319
iteration 0000: loss: 3935.054
iteration 0100: loss: 3942.631
iteration 0200: loss: 3939.802
iteration 0300: loss: 3945.121
iteration 0400: loss: 3941.718
iteration 0500: loss: 3945.014
iteration 0600: loss: 3938.355
iteration 0700: loss: 3938.806
iteration 0800: loss: 3941.290
iteration 0900: loss: 3941.529
iteration 1000: loss: 3940.030
iteration 1100: loss: 3940.456
iteration 1200: loss: 3939.531
iteration 1300: loss: 3944.373
iteration 1400: loss: 3938.152
iteration 1500: loss: 3937.738
iteration 1600: loss: 3940.377
iteration 1700: loss: 3935.303
iteration 1800: loss: 3945.332
====> Epoch: 024 Train loss: 3940.7176  took : 52.74850106239319
====> Test loss: 3944.5199
iteration 0000: loss: 3936.518
iteration 0100: loss: 3941.920
iteration 0200: loss: 3937.825
iteration 0300: loss: 3946.296
iteration 0400: loss: 3937.455
iteration 0500: loss: 3942.133
iteration 0600: loss: 3935.778
iteration 0700: loss: 3942.439
iteration 0800: loss: 3939.402
iteration 0900: loss: 3938.237
iteration 1000: loss: 3941.453
iteration 1100: loss: 3938.695
iteration 1200: loss: 3941.100
iteration 1300: loss: 3946.277
iteration 1400: loss: 3941.540
iteration 1500: loss: 3939.698
iteration 1600: loss: 3945.356
iteration 1700: loss: 3935.667
iteration 1800: loss: 3941.213
====> Epoch: 025 Train loss: 3940.6898  took : 52.80489182472229
====> Test loss: 3944.8166
iteration 0000: loss: 3940.258
iteration 0100: loss: 3942.209
iteration 0200: loss: 3943.867
iteration 0300: loss: 3937.036
iteration 0400: loss: 3938.176
iteration 0500: loss: 3938.247
iteration 0600: loss: 3936.921
iteration 0700: loss: 3946.924
iteration 0800: loss: 3940.843
iteration 0900: loss: 3946.322
iteration 1000: loss: 3934.791
iteration 1100: loss: 3935.703
iteration 1200: loss: 3940.579
iteration 1300: loss: 3946.067
iteration 1400: loss: 3940.329
iteration 1500: loss: 3938.688
iteration 1600: loss: 3939.824
iteration 1700: loss: 3944.474
iteration 1800: loss: 3943.151
====> Epoch: 026 Train loss: 3940.6038  took : 52.87615156173706
====> Test loss: 3944.1298
iteration 0000: loss: 3934.971
iteration 0100: loss: 3940.342
iteration 0200: loss: 3933.760
iteration 0300: loss: 3945.684
iteration 0400: loss: 3940.824
iteration 0500: loss: 3934.993
iteration 0600: loss: 3939.893
iteration 0700: loss: 3942.032
iteration 0800: loss: 3939.827
iteration 0900: loss: 3938.543
iteration 1000: loss: 3939.709
iteration 1100: loss: 3942.374
iteration 1200: loss: 3940.841
iteration 1300: loss: 3939.502
iteration 1400: loss: 3938.800
iteration 1500: loss: 3939.762
iteration 1600: loss: 3938.994
iteration 1700: loss: 3937.821
iteration 1800: loss: 3947.609
====> Epoch: 027 Train loss: 3940.5975  took : 52.760560512542725
====> Test loss: 3943.6193
iteration 0000: loss: 3934.679
iteration 0100: loss: 3941.172
iteration 0200: loss: 3939.786
iteration 0300: loss: 3946.897
iteration 0400: loss: 3935.503
iteration 0500: loss: 3937.931
iteration 0600: loss: 3944.331
iteration 0700: loss: 3941.165
iteration 0800: loss: 3942.835
iteration 0900: loss: 3942.122
iteration 1000: loss: 3942.863
iteration 1100: loss: 3938.098
iteration 1200: loss: 3942.181
iteration 1300: loss: 3944.718
iteration 1400: loss: 3940.937
iteration 1500: loss: 3942.909
iteration 1600: loss: 3937.920
iteration 1700: loss: 3937.655
iteration 1800: loss: 3946.878
====> Epoch: 028 Train loss: 3940.5590  took : 53.00503063201904
====> Test loss: 3944.5269
iteration 0000: loss: 3944.440
iteration 0100: loss: 3940.660
iteration 0200: loss: 3947.162
iteration 0300: loss: 3935.732
iteration 0400: loss: 3936.779
iteration 0500: loss: 3942.277
iteration 0600: loss: 3938.553
iteration 0700: loss: 3938.747
iteration 0800: loss: 3940.409
iteration 0900: loss: 3941.782
iteration 1000: loss: 3937.793
iteration 1100: loss: 3940.644
iteration 1200: loss: 3943.415
iteration 1300: loss: 3941.143
iteration 1400: loss: 3942.294
iteration 1500: loss: 3937.101
iteration 1600: loss: 3932.707
iteration 1700: loss: 3942.869
iteration 1800: loss: 3937.122
====> Epoch: 029 Train loss: 3940.4649  took : 53.05037331581116
====> Test loss: 3944.0271
iteration 0000: loss: 3940.468
iteration 0100: loss: 3944.280
iteration 0200: loss: 3940.703
iteration 0300: loss: 3935.787
iteration 0400: loss: 3933.249
iteration 0500: loss: 3947.323
iteration 0600: loss: 3940.715
iteration 0700: loss: 3946.352
iteration 0800: loss: 3945.024
iteration 0900: loss: 3939.399
iteration 1000: loss: 3943.736
iteration 1100: loss: 3934.383
iteration 1200: loss: 3939.706
iteration 1300: loss: 3935.640
iteration 1400: loss: 3939.971
iteration 1500: loss: 3937.753
iteration 1600: loss: 3944.736
iteration 1700: loss: 3942.865
iteration 1800: loss: 3939.548
====> Epoch: 030 Train loss: 3940.4145  took : 52.94994592666626
====> Test loss: 3943.6642
iteration 0000: loss: 3937.877
iteration 0100: loss: 3936.003
iteration 0200: loss: 3938.934
iteration 0300: loss: 3938.346
iteration 0400: loss: 3941.096
iteration 0500: loss: 3933.514
iteration 0600: loss: 3942.137
iteration 0700: loss: 3943.047
iteration 0800: loss: 3940.198
iteration 0900: loss: 3939.227
iteration 1000: loss: 3934.778
iteration 1100: loss: 3941.488
iteration 1200: loss: 3938.772
iteration 1300: loss: 3939.354
iteration 1400: loss: 3942.166
iteration 1500: loss: 3939.388
iteration 1600: loss: 3933.742
iteration 1700: loss: 3940.715
iteration 1800: loss: 3936.960
====> Epoch: 031 Train loss: 3940.3507  took : 52.613441467285156
====> Test loss: 3944.0602
iteration 0000: loss: 3936.197
iteration 0100: loss: 3946.106
iteration 0200: loss: 3937.310
iteration 0300: loss: 3944.766
iteration 0400: loss: 3935.529
iteration 0500: loss: 3940.673
iteration 0600: loss: 3935.879
iteration 0700: loss: 3939.135
iteration 0800: loss: 3943.042
iteration 0900: loss: 3942.843
iteration 1000: loss: 3943.291
iteration 1100: loss: 3941.303
iteration 1200: loss: 3937.716
iteration 1300: loss: 3936.811
iteration 1400: loss: 3944.295
iteration 1500: loss: 3937.710
iteration 1600: loss: 3943.444
iteration 1700: loss: 3932.799
iteration 1800: loss: 3936.468
====> Epoch: 032 Train loss: 3940.4202  took : 52.995718240737915
====> Test loss: 3943.8676
iteration 0000: loss: 3937.091
iteration 0100: loss: 3943.202
iteration 0200: loss: 3944.993
iteration 0300: loss: 3937.924
iteration 0400: loss: 3940.347
iteration 0500: loss: 3941.989
iteration 0600: loss: 3941.193
iteration 0700: loss: 3935.187
iteration 0800: loss: 3943.186
iteration 0900: loss: 3941.640
iteration 1000: loss: 3942.051
iteration 1100: loss: 3936.705
iteration 1200: loss: 3937.853
iteration 1300: loss: 3939.100
iteration 1400: loss: 3936.942
iteration 1500: loss: 3943.383
iteration 1600: loss: 3944.434
iteration 1700: loss: 3939.211
iteration 1800: loss: 3944.707
====> Epoch: 033 Train loss: 3940.3751  took : 53.0649299621582
====> Test loss: 3943.8291
iteration 0000: loss: 3939.022
iteration 0100: loss: 3937.247
iteration 0200: loss: 3941.983
iteration 0300: loss: 3939.664
iteration 0400: loss: 3939.167
iteration 0500: loss: 3936.652
iteration 0600: loss: 3943.053
iteration 0700: loss: 3933.977
iteration 0800: loss: 3938.402
iteration 0900: loss: 3940.011
iteration 1000: loss: 3937.912
iteration 1100: loss: 3940.476
iteration 1200: loss: 3938.209
iteration 1300: loss: 3936.133
iteration 1400: loss: 3936.410
iteration 1500: loss: 3939.098
iteration 1600: loss: 3946.364
iteration 1700: loss: 3937.710
iteration 1800: loss: 3942.577
====> Epoch: 034 Train loss: 3940.2598  took : 52.81785798072815
====> Test loss: 3943.6064
iteration 0000: loss: 3935.671
iteration 0100: loss: 3936.815
iteration 0200: loss: 3935.685
iteration 0300: loss: 3940.574
iteration 0400: loss: 3937.706
iteration 0500: loss: 3946.228
iteration 0600: loss: 3947.836
iteration 0700: loss: 3938.218
iteration 0800: loss: 3940.152
iteration 0900: loss: 3934.703
iteration 1000: loss: 3943.727
iteration 1100: loss: 3939.085
iteration 1200: loss: 3943.441
iteration 1300: loss: 3938.545
iteration 1400: loss: 3946.983
iteration 1500: loss: 3936.988
iteration 1600: loss: 3943.008
iteration 1700: loss: 3943.629
iteration 1800: loss: 3942.885
====> Epoch: 035 Train loss: 3940.3263  took : 53.027231216430664
====> Test loss: 3943.3531
iteration 0000: loss: 3943.000
iteration 0100: loss: 3933.839
iteration 0200: loss: 3939.821
iteration 0300: loss: 3940.575
iteration 0400: loss: 3941.689
iteration 0500: loss: 3947.594
iteration 0600: loss: 3935.045
iteration 0700: loss: 3945.361
iteration 0800: loss: 3936.978
iteration 0900: loss: 3937.415
iteration 1000: loss: 3941.457
iteration 1100: loss: 3947.096
iteration 1200: loss: 3942.118
iteration 1300: loss: 3943.360
iteration 1400: loss: 3946.688
iteration 1500: loss: 3941.484
iteration 1600: loss: 3938.723
iteration 1700: loss: 3937.119
iteration 1800: loss: 3946.338
====> Epoch: 036 Train loss: 3940.1955  took : 52.772006034851074
====> Test loss: 3943.7513
iteration 0000: loss: 3942.201
iteration 0100: loss: 3939.429
iteration 0200: loss: 3940.068
iteration 0300: loss: 3945.757
iteration 0400: loss: 3938.187
iteration 0500: loss: 3943.678
iteration 0600: loss: 3942.065
iteration 0700: loss: 3933.528
iteration 0800: loss: 3939.553
iteration 0900: loss: 3942.014
iteration 1000: loss: 3941.380
iteration 1100: loss: 3934.117
iteration 1200: loss: 3942.298
iteration 1300: loss: 3939.329
iteration 1400: loss: 3942.302
iteration 1500: loss: 3937.229
iteration 1600: loss: 3942.942
iteration 1700: loss: 3944.579
iteration 1800: loss: 3935.667
====> Epoch: 037 Train loss: 3940.0806  took : 52.80020308494568
====> Test loss: 3943.5019
iteration 0000: loss: 3937.980
iteration 0100: loss: 3939.557
iteration 0200: loss: 3941.098
iteration 0300: loss: 3939.418
iteration 0400: loss: 3938.804
iteration 0500: loss: 3940.771
iteration 0600: loss: 3939.261
iteration 0700: loss: 3938.432
iteration 0800: loss: 3938.619
iteration 0900: loss: 3941.006
iteration 1000: loss: 3940.793
iteration 1100: loss: 3946.079
iteration 1200: loss: 3942.855
iteration 1300: loss: 3942.110
iteration 1400: loss: 3948.494
iteration 1500: loss: 3938.556
iteration 1600: loss: 3942.202
iteration 1700: loss: 3940.970
iteration 1800: loss: 3938.184
====> Epoch: 038 Train loss: 3940.0960  took : 52.81695532798767
====> Test loss: 3943.4346
iteration 0000: loss: 3936.938
iteration 0100: loss: 3935.100
iteration 0200: loss: 3937.566
iteration 0300: loss: 3939.694
iteration 0400: loss: 3945.006
iteration 0500: loss: 3939.085
iteration 0600: loss: 3933.711
iteration 0700: loss: 3946.976
iteration 0800: loss: 3943.817
iteration 0900: loss: 3941.142
iteration 1000: loss: 3939.235
iteration 1100: loss: 3938.222
iteration 1200: loss: 3938.809
iteration 1300: loss: 3941.660
iteration 1400: loss: 3938.574
iteration 1500: loss: 3943.546
iteration 1600: loss: 3947.994
iteration 1700: loss: 3943.924
iteration 1800: loss: 3942.315
====> Epoch: 039 Train loss: 3940.0338  took : 52.871342182159424
====> Test loss: 3943.4900
iteration 0000: loss: 3935.829
iteration 0100: loss: 3940.568
iteration 0200: loss: 3939.801
iteration 0300: loss: 3942.897
iteration 0400: loss: 3945.690
iteration 0500: loss: 3937.922
iteration 0600: loss: 3941.202
iteration 0700: loss: 3938.274
iteration 0800: loss: 3945.813
iteration 0900: loss: 3940.417
iteration 1000: loss: 3938.225
iteration 1100: loss: 3943.455
iteration 1200: loss: 3943.464
iteration 1300: loss: 3941.115
iteration 1400: loss: 3939.603
iteration 1500: loss: 3943.482
iteration 1600: loss: 3942.717
iteration 1700: loss: 3947.230
iteration 1800: loss: 3936.839
====> Epoch: 040 Train loss: 3940.1003  took : 52.94154119491577
====> Test loss: 3943.2561
iteration 0000: loss: 3939.666
iteration 0100: loss: 3944.045
iteration 0200: loss: 3945.056
iteration 0300: loss: 3940.353
iteration 0400: loss: 3933.271
iteration 0500: loss: 3936.739
iteration 0600: loss: 3937.829
iteration 0700: loss: 3937.125
iteration 0800: loss: 3938.512
iteration 0900: loss: 3934.581
iteration 1000: loss: 3940.567
iteration 1100: loss: 3941.129
iteration 1200: loss: 3945.566
iteration 1300: loss: 3937.624
iteration 1400: loss: 3947.874
iteration 1500: loss: 3942.213
iteration 1600: loss: 3935.853
iteration 1700: loss: 3942.276
iteration 1800: loss: 3935.865
====> Epoch: 041 Train loss: 3940.0505  took : 52.63515019416809
====> Test loss: 3943.4929
iteration 0000: loss: 3939.461
iteration 0100: loss: 3943.270
iteration 0200: loss: 3944.135
iteration 0300: loss: 3939.616
iteration 0400: loss: 3939.031
iteration 0500: loss: 3936.804
iteration 0600: loss: 3943.073
iteration 0700: loss: 3939.019
iteration 0800: loss: 3940.068
iteration 0900: loss: 3938.623
iteration 1000: loss: 3938.068
iteration 1100: loss: 3944.829
iteration 1200: loss: 3935.901
iteration 1300: loss: 3938.051
iteration 1400: loss: 3943.972
iteration 1500: loss: 3939.746
iteration 1600: loss: 3938.787
iteration 1700: loss: 3942.441
iteration 1800: loss: 3937.435
====> Epoch: 042 Train loss: 3939.9463  took : 52.63359999656677
====> Test loss: 3943.4117
iteration 0000: loss: 3951.005
iteration 0100: loss: 3943.515
iteration 0200: loss: 3938.684
iteration 0300: loss: 3942.183
iteration 0400: loss: 3940.989
iteration 0500: loss: 3946.296
iteration 0600: loss: 3936.975
iteration 0700: loss: 3939.298
iteration 0800: loss: 3937.495
iteration 0900: loss: 3952.930
iteration 1000: loss: 3935.998
iteration 1100: loss: 3937.000
iteration 1200: loss: 3937.134
iteration 1300: loss: 3934.961
iteration 1400: loss: 3949.531
iteration 1500: loss: 3935.784
iteration 1600: loss: 3938.860
iteration 1700: loss: 3936.728
iteration 1800: loss: 3937.461
====> Epoch: 043 Train loss: 3940.0418  took : 52.87227010726929
====> Test loss: 3943.6855
iteration 0000: loss: 3940.918
iteration 0100: loss: 3942.559
iteration 0200: loss: 3935.905
iteration 0300: loss: 3942.595
iteration 0400: loss: 3946.883
iteration 0500: loss: 3935.436
iteration 0600: loss: 3937.821
iteration 0700: loss: 3937.311
iteration 0800: loss: 3935.183
iteration 0900: loss: 3938.871
iteration 1000: loss: 3940.015
iteration 1100: loss: 3935.499
iteration 1200: loss: 3935.781
iteration 1300: loss: 3939.512
iteration 1400: loss: 3937.783
iteration 1500: loss: 3938.884
iteration 1600: loss: 3945.383
iteration 1700: loss: 3950.948
iteration 1800: loss: 3940.785
====> Epoch: 044 Train loss: 3939.9779  took : 52.811134576797485
====> Test loss: 3943.8660
iteration 0000: loss: 3939.672
iteration 0100: loss: 3946.257
iteration 0200: loss: 3941.794
iteration 0300: loss: 3940.731
iteration 0400: loss: 3945.243
iteration 0500: loss: 3938.963
iteration 0600: loss: 3941.698
iteration 0700: loss: 3936.290
iteration 0800: loss: 3936.127
iteration 0900: loss: 3940.458
iteration 1000: loss: 3934.048
iteration 1100: loss: 3938.332
iteration 1200: loss: 3936.590
iteration 1300: loss: 3941.848
iteration 1400: loss: 3934.899
iteration 1500: loss: 3937.729
iteration 1600: loss: 3939.975
iteration 1700: loss: 3943.168
iteration 1800: loss: 3936.754
====> Epoch: 045 Train loss: 3939.8210  took : 52.98769783973694
====> Test loss: 3943.2476
iteration 0000: loss: 3934.899
iteration 0100: loss: 3937.348
iteration 0200: loss: 3936.719
iteration 0300: loss: 3938.912
iteration 0400: loss: 3941.051
iteration 0500: loss: 3944.508
iteration 0600: loss: 3935.234
iteration 0700: loss: 3937.332
iteration 0800: loss: 3944.039
iteration 0900: loss: 3942.127
iteration 1000: loss: 3939.828
iteration 1100: loss: 3942.679
iteration 1200: loss: 3938.186
iteration 1300: loss: 3939.898
iteration 1400: loss: 3941.940
iteration 1500: loss: 3936.791
iteration 1600: loss: 3936.383
iteration 1700: loss: 3940.295
iteration 1800: loss: 3938.406
====> Epoch: 046 Train loss: 3939.7767  took : 52.86463379859924
====> Test loss: 3943.0852
iteration 0000: loss: 3935.975
iteration 0100: loss: 3934.498
iteration 0200: loss: 3940.576
iteration 0300: loss: 3938.898
iteration 0400: loss: 3935.617
iteration 0500: loss: 3935.692
iteration 0600: loss: 3939.185
iteration 0700: loss: 3935.661
iteration 0800: loss: 3936.199
iteration 0900: loss: 3944.030
iteration 1000: loss: 3941.662
iteration 1100: loss: 3937.645
iteration 1200: loss: 3945.887
iteration 1300: loss: 3937.446
iteration 1400: loss: 3937.500
iteration 1500: loss: 3938.426
iteration 1600: loss: 3941.004
iteration 1700: loss: 3944.310
iteration 1800: loss: 3939.056
====> Epoch: 047 Train loss: 3939.8164  took : 52.65445566177368
====> Test loss: 3943.5805
iteration 0000: loss: 3942.601
iteration 0100: loss: 3931.544
iteration 0200: loss: 3941.106
iteration 0300: loss: 3947.755
iteration 0400: loss: 3938.934
iteration 0500: loss: 3939.234
iteration 0600: loss: 3941.802
iteration 0700: loss: 3943.288
iteration 0800: loss: 3941.953
iteration 0900: loss: 3938.245
iteration 1000: loss: 3947.202
iteration 1100: loss: 3940.604
iteration 1200: loss: 3937.297
iteration 1300: loss: 3944.990
iteration 1400: loss: 3944.121
iteration 1500: loss: 3933.943
iteration 1600: loss: 3935.630
iteration 1700: loss: 3940.488
iteration 1800: loss: 3940.251
====> Epoch: 048 Train loss: 3939.8949  took : 52.8775520324707
====> Test loss: 3943.4293
iteration 0000: loss: 3939.675
iteration 0100: loss: 3938.725
iteration 0200: loss: 3941.971
iteration 0300: loss: 3946.749
iteration 0400: loss: 3941.519
iteration 0500: loss: 3946.428
iteration 0600: loss: 3939.006
iteration 0700: loss: 3946.591
iteration 0800: loss: 3939.259
iteration 0900: loss: 3939.114
iteration 1000: loss: 3934.202
iteration 1100: loss: 3942.652
iteration 1200: loss: 3935.880
iteration 1300: loss: 3936.653
iteration 1400: loss: 3939.532
iteration 1500: loss: 3946.285
iteration 1600: loss: 3938.947
iteration 1700: loss: 3941.173
iteration 1800: loss: 3942.233
====> Epoch: 049 Train loss: 3939.8220  took : 52.90402889251709
====> Test loss: 3943.4490
iteration 0000: loss: 3941.546
iteration 0100: loss: 3943.004
iteration 0200: loss: 3932.217
iteration 0300: loss: 3944.535
iteration 0400: loss: 3938.794
iteration 0500: loss: 3938.035
iteration 0600: loss: 3935.845
iteration 0700: loss: 3939.707
iteration 0800: loss: 3935.305
iteration 0900: loss: 3940.740
iteration 1000: loss: 3938.961
iteration 1100: loss: 3941.586
iteration 1200: loss: 3945.749
iteration 1300: loss: 3942.570
iteration 1400: loss: 3944.387
iteration 1500: loss: 3942.032
iteration 1600: loss: 3937.423
iteration 1700: loss: 3936.597
iteration 1800: loss: 3941.491
====> Epoch: 050 Train loss: 3939.7116  took : 52.81683564186096
====> Test loss: 3943.2275
====> [MM-VAE] Time: 3108.961s or 00:51:48
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  17
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_17
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_17
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1988.597
iteration 0100: loss: 1565.427
iteration 0200: loss: 1565.641
iteration 0300: loss: 1559.573
iteration 0400: loss: 1546.639
iteration 0500: loss: 1547.184
iteration 0600: loss: 1543.248
iteration 0700: loss: 1535.882
iteration 0800: loss: 1540.316
iteration 0900: loss: 1540.499
====> Epoch: 001 Train loss: 1553.9055  took : 8.399267435073853
====> Test loss: 1536.5032
iteration 0000: loss: 1537.247
iteration 0100: loss: 1540.304
iteration 0200: loss: 1529.929
iteration 0300: loss: 1529.480
iteration 0400: loss: 1532.827
iteration 0500: loss: 1534.551
iteration 0600: loss: 1531.340
iteration 0700: loss: 1525.979
iteration 0800: loss: 1524.693
iteration 0900: loss: 1528.486
====> Epoch: 002 Train loss: 1530.8790  took : 8.524431943893433
====> Test loss: 1529.3762
iteration 0000: loss: 1528.723
iteration 0100: loss: 1530.363
iteration 0200: loss: 1529.248
iteration 0300: loss: 1529.792
iteration 0400: loss: 1526.301
iteration 0500: loss: 1524.568
iteration 0600: loss: 1521.154
iteration 0700: loss: 1522.349
iteration 0800: loss: 1528.892
iteration 0900: loss: 1521.114
====> Epoch: 003 Train loss: 1525.8361  took : 8.449737548828125
====> Test loss: 1526.3059
iteration 0000: loss: 1526.042
iteration 0100: loss: 1521.778
iteration 0200: loss: 1523.475
iteration 0300: loss: 1523.055
iteration 0400: loss: 1526.463
iteration 0500: loss: 1521.172
iteration 0600: loss: 1521.125
iteration 0700: loss: 1520.116
iteration 0800: loss: 1521.796
iteration 0900: loss: 1521.701
====> Epoch: 004 Train loss: 1523.3782  took : 8.445319652557373
====> Test loss: 1524.6406
iteration 0000: loss: 1523.243
iteration 0100: loss: 1522.439
iteration 0200: loss: 1520.951
iteration 0300: loss: 1525.004
iteration 0400: loss: 1527.857
iteration 0500: loss: 1521.467
iteration 0600: loss: 1523.248
iteration 0700: loss: 1519.650
iteration 0800: loss: 1520.939
iteration 0900: loss: 1522.481
====> Epoch: 005 Train loss: 1521.8242  took : 8.502331256866455
====> Test loss: 1523.4705
iteration 0000: loss: 1519.890
iteration 0100: loss: 1518.426
iteration 0200: loss: 1520.439
iteration 0300: loss: 1520.814
iteration 0400: loss: 1519.431
iteration 0500: loss: 1520.214
iteration 0600: loss: 1522.697
iteration 0700: loss: 1520.010
iteration 0800: loss: 1521.092
iteration 0900: loss: 1522.490
====> Epoch: 006 Train loss: 1520.6257  took : 8.520400762557983
====> Test loss: 1522.4399
iteration 0000: loss: 1520.156
iteration 0100: loss: 1522.042
iteration 0200: loss: 1523.446
iteration 0300: loss: 1520.371
iteration 0400: loss: 1522.310
iteration 0500: loss: 1522.629
iteration 0600: loss: 1518.986
iteration 0700: loss: 1519.184
iteration 0800: loss: 1523.474
iteration 0900: loss: 1521.133
====> Epoch: 007 Train loss: 1519.8082  took : 8.475960969924927
====> Test loss: 1521.7609
iteration 0000: loss: 1516.517
iteration 0100: loss: 1519.814
iteration 0200: loss: 1518.025
iteration 0300: loss: 1517.887
iteration 0400: loss: 1521.406
iteration 0500: loss: 1520.309
iteration 0600: loss: 1520.225
iteration 0700: loss: 1524.137
iteration 0800: loss: 1514.098
iteration 0900: loss: 1521.673
====> Epoch: 008 Train loss: 1519.1640  took : 8.455146074295044
====> Test loss: 1521.2998
iteration 0000: loss: 1518.780
iteration 0100: loss: 1517.482
iteration 0200: loss: 1520.032
iteration 0300: loss: 1520.461
iteration 0400: loss: 1519.294
iteration 0500: loss: 1518.307
iteration 0600: loss: 1521.163
iteration 0700: loss: 1518.968
iteration 0800: loss: 1517.097
iteration 0900: loss: 1517.068
====> Epoch: 009 Train loss: 1518.6559  took : 8.478217363357544
====> Test loss: 1521.0742
iteration 0000: loss: 1521.239
iteration 0100: loss: 1518.558
iteration 0200: loss: 1517.450
iteration 0300: loss: 1519.310
iteration 0400: loss: 1516.669
iteration 0500: loss: 1516.844
iteration 0600: loss: 1516.638
iteration 0700: loss: 1518.515
iteration 0800: loss: 1517.485
iteration 0900: loss: 1518.235
====> Epoch: 010 Train loss: 1518.2146  took : 8.40902829170227
====> Test loss: 1520.6144
iteration 0000: loss: 1517.362
iteration 0100: loss: 1516.452
iteration 0200: loss: 1518.944
iteration 0300: loss: 1516.397
iteration 0400: loss: 1517.935
iteration 0500: loss: 1517.400
iteration 0600: loss: 1516.343
iteration 0700: loss: 1517.994
iteration 0800: loss: 1516.371
iteration 0900: loss: 1516.861
====> Epoch: 011 Train loss: 1517.8476  took : 8.414594888687134
====> Test loss: 1520.4015
iteration 0000: loss: 1520.670
iteration 0100: loss: 1519.196
iteration 0200: loss: 1516.740
iteration 0300: loss: 1518.484
iteration 0400: loss: 1518.797
iteration 0500: loss: 1516.216
iteration 0600: loss: 1516.767
iteration 0700: loss: 1517.867
iteration 0800: loss: 1514.753
iteration 0900: loss: 1517.686
====> Epoch: 012 Train loss: 1517.5479  took : 8.416321754455566
====> Test loss: 1520.2334
iteration 0000: loss: 1517.341
iteration 0100: loss: 1519.146
iteration 0200: loss: 1517.970
iteration 0300: loss: 1516.156
iteration 0400: loss: 1517.941
iteration 0500: loss: 1518.439
iteration 0600: loss: 1516.161
iteration 0700: loss: 1515.925
iteration 0800: loss: 1517.534
iteration 0900: loss: 1514.423
====> Epoch: 013 Train loss: 1517.2915  took : 8.491251468658447
====> Test loss: 1520.0811
iteration 0000: loss: 1519.109
iteration 0100: loss: 1516.874
iteration 0200: loss: 1520.196
iteration 0300: loss: 1514.891
iteration 0400: loss: 1515.349
iteration 0500: loss: 1519.424
iteration 0600: loss: 1513.047
iteration 0700: loss: 1521.921
iteration 0800: loss: 1516.576
iteration 0900: loss: 1517.441
====> Epoch: 014 Train loss: 1517.0373  took : 8.533490180969238
====> Test loss: 1519.8860
iteration 0000: loss: 1516.778
iteration 0100: loss: 1516.807
iteration 0200: loss: 1513.931
iteration 0300: loss: 1514.974
iteration 0400: loss: 1514.755
iteration 0500: loss: 1516.148
iteration 0600: loss: 1515.034
iteration 0700: loss: 1517.005
iteration 0800: loss: 1519.521
iteration 0900: loss: 1513.156
====> Epoch: 015 Train loss: 1516.8151  took : 8.492518424987793
====> Test loss: 1519.6287
iteration 0000: loss: 1515.768
iteration 0100: loss: 1515.473
iteration 0200: loss: 1514.893
iteration 0300: loss: 1515.533
iteration 0400: loss: 1515.823
iteration 0500: loss: 1516.639
iteration 0600: loss: 1515.896
iteration 0700: loss: 1517.739
iteration 0800: loss: 1517.914
iteration 0900: loss: 1516.658
====> Epoch: 016 Train loss: 1516.6459  took : 8.44536018371582
====> Test loss: 1519.4858
iteration 0000: loss: 1516.992
iteration 0100: loss: 1516.356
iteration 0200: loss: 1516.640
iteration 0300: loss: 1517.244
iteration 0400: loss: 1513.997
iteration 0500: loss: 1517.202
iteration 0600: loss: 1514.656
iteration 0700: loss: 1519.834
iteration 0800: loss: 1515.448
iteration 0900: loss: 1518.782
====> Epoch: 017 Train loss: 1516.4299  took : 8.503585577011108
====> Test loss: 1519.3848
iteration 0000: loss: 1516.125
iteration 0100: loss: 1515.700
iteration 0200: loss: 1516.638
iteration 0300: loss: 1514.408
iteration 0400: loss: 1518.524
iteration 0500: loss: 1514.905
iteration 0600: loss: 1516.823
iteration 0700: loss: 1516.190
iteration 0800: loss: 1514.212
iteration 0900: loss: 1515.327
====> Epoch: 018 Train loss: 1516.2694  took : 8.53477168083191
====> Test loss: 1519.3044
iteration 0000: loss: 1518.309
iteration 0100: loss: 1516.593
iteration 0200: loss: 1516.071
iteration 0300: loss: 1516.075
iteration 0400: loss: 1512.286
iteration 0500: loss: 1516.755
iteration 0600: loss: 1515.567
iteration 0700: loss: 1516.314
iteration 0800: loss: 1515.000
iteration 0900: loss: 1513.898
====> Epoch: 019 Train loss: 1516.1237  took : 8.539041519165039
====> Test loss: 1519.1498
iteration 0000: loss: 1519.639
iteration 0100: loss: 1519.128
iteration 0200: loss: 1516.656
iteration 0300: loss: 1515.287
iteration 0400: loss: 1516.984
iteration 0500: loss: 1515.936
iteration 0600: loss: 1514.699
iteration 0700: loss: 1513.780
iteration 0800: loss: 1514.912
iteration 0900: loss: 1516.681
====> Epoch: 020 Train loss: 1516.0063  took : 8.4440336227417
====> Test loss: 1519.1231
iteration 0000: loss: 1516.973
iteration 0100: loss: 1516.878
iteration 0200: loss: 1516.353
iteration 0300: loss: 1518.875
iteration 0400: loss: 1513.813
iteration 0500: loss: 1514.739
iteration 0600: loss: 1516.993
iteration 0700: loss: 1516.514
iteration 0800: loss: 1515.848
iteration 0900: loss: 1516.025
====> Epoch: 021 Train loss: 1515.8635  took : 8.497324228286743
====> Test loss: 1519.1400
iteration 0000: loss: 1513.957
iteration 0100: loss: 1515.540
iteration 0200: loss: 1514.805
iteration 0300: loss: 1518.554
iteration 0400: loss: 1516.094
iteration 0500: loss: 1517.610
iteration 0600: loss: 1515.245
iteration 0700: loss: 1516.707
iteration 0800: loss: 1516.358
iteration 0900: loss: 1517.547
====> Epoch: 022 Train loss: 1515.7175  took : 8.524251699447632
====> Test loss: 1518.8847
iteration 0000: loss: 1515.594
iteration 0100: loss: 1511.464
iteration 0200: loss: 1514.188
iteration 0300: loss: 1517.520
iteration 0400: loss: 1513.688
iteration 0500: loss: 1517.985
iteration 0600: loss: 1517.039
iteration 0700: loss: 1516.337
iteration 0800: loss: 1515.174
iteration 0900: loss: 1514.567
====> Epoch: 023 Train loss: 1515.6218  took : 8.414448022842407
====> Test loss: 1518.8700
iteration 0000: loss: 1517.907
iteration 0100: loss: 1514.365
iteration 0200: loss: 1513.373
iteration 0300: loss: 1514.441
iteration 0400: loss: 1515.917
iteration 0500: loss: 1515.844
iteration 0600: loss: 1515.306
iteration 0700: loss: 1518.240
iteration 0800: loss: 1514.035
iteration 0900: loss: 1514.678
====> Epoch: 024 Train loss: 1515.5008  took : 8.456027030944824
====> Test loss: 1518.7870
iteration 0000: loss: 1515.659
iteration 0100: loss: 1514.487
iteration 0200: loss: 1515.101
iteration 0300: loss: 1515.141
iteration 0400: loss: 1515.944
iteration 0500: loss: 1516.558
iteration 0600: loss: 1515.975
iteration 0700: loss: 1513.822
iteration 0800: loss: 1517.126
iteration 0900: loss: 1519.218
====> Epoch: 025 Train loss: 1515.4284  took : 8.510989665985107
====> Test loss: 1518.8188
iteration 0000: loss: 1514.120
iteration 0100: loss: 1517.487
iteration 0200: loss: 1515.582
iteration 0300: loss: 1515.190
iteration 0400: loss: 1515.137
iteration 0500: loss: 1515.662
iteration 0600: loss: 1514.445
iteration 0700: loss: 1516.962
iteration 0800: loss: 1516.582
iteration 0900: loss: 1513.344
====> Epoch: 026 Train loss: 1515.2861  took : 8.44050121307373
====> Test loss: 1518.6096
iteration 0000: loss: 1519.667
iteration 0100: loss: 1511.947
iteration 0200: loss: 1514.194
iteration 0300: loss: 1513.977
iteration 0400: loss: 1516.071
iteration 0500: loss: 1514.980
iteration 0600: loss: 1513.252
iteration 0700: loss: 1516.363
iteration 0800: loss: 1517.617
iteration 0900: loss: 1513.463
====> Epoch: 027 Train loss: 1515.2177  took : 8.442068099975586
====> Test loss: 1518.5229
iteration 0000: loss: 1516.493
iteration 0100: loss: 1517.622
iteration 0200: loss: 1514.469
iteration 0300: loss: 1515.413
iteration 0400: loss: 1516.155
iteration 0500: loss: 1516.638
iteration 0600: loss: 1518.058
iteration 0700: loss: 1512.228
iteration 0800: loss: 1516.278
iteration 0900: loss: 1515.394
====> Epoch: 028 Train loss: 1515.1046  took : 8.481769800186157
====> Test loss: 1518.4734
iteration 0000: loss: 1515.292
iteration 0100: loss: 1514.719
iteration 0200: loss: 1514.548
iteration 0300: loss: 1514.564
iteration 0400: loss: 1516.360
iteration 0500: loss: 1515.429
iteration 0600: loss: 1514.511
iteration 0700: loss: 1513.264
iteration 0800: loss: 1515.130
iteration 0900: loss: 1516.406
====> Epoch: 029 Train loss: 1515.0420  took : 8.557303428649902
====> Test loss: 1518.4324
iteration 0000: loss: 1514.325
iteration 0100: loss: 1514.989
iteration 0200: loss: 1516.371
iteration 0300: loss: 1516.068
iteration 0400: loss: 1515.016
iteration 0500: loss: 1516.025
iteration 0600: loss: 1515.216
iteration 0700: loss: 1516.416
iteration 0800: loss: 1513.483
iteration 0900: loss: 1513.430
====> Epoch: 030 Train loss: 1514.9674  took : 8.507122278213501
====> Test loss: 1518.3824
iteration 0000: loss: 1514.156
iteration 0100: loss: 1516.545
iteration 0200: loss: 1514.036
iteration 0300: loss: 1513.111
iteration 0400: loss: 1514.456
iteration 0500: loss: 1515.346
iteration 0600: loss: 1516.612
iteration 0700: loss: 1515.759
iteration 0800: loss: 1516.601
iteration 0900: loss: 1516.192
====> Epoch: 031 Train loss: 1514.8399  took : 8.429169178009033
====> Test loss: 1518.3602
iteration 0000: loss: 1515.394
iteration 0100: loss: 1513.346
iteration 0200: loss: 1514.266
iteration 0300: loss: 1513.251
iteration 0400: loss: 1515.466
iteration 0500: loss: 1515.672
iteration 0600: loss: 1514.651
iteration 0700: loss: 1514.318
iteration 0800: loss: 1515.521
iteration 0900: loss: 1516.150
====> Epoch: 032 Train loss: 1514.7936  took : 8.557109832763672
====> Test loss: 1518.0640
iteration 0000: loss: 1511.066
iteration 0100: loss: 1514.575
iteration 0200: loss: 1514.576
iteration 0300: loss: 1514.810
iteration 0400: loss: 1515.137
iteration 0500: loss: 1513.576
iteration 0600: loss: 1515.198
iteration 0700: loss: 1511.715
iteration 0800: loss: 1514.541
iteration 0900: loss: 1514.678
====> Epoch: 033 Train loss: 1514.7199  took : 8.477967023849487
====> Test loss: 1518.1161
iteration 0000: loss: 1513.551
iteration 0100: loss: 1514.250
iteration 0200: loss: 1515.373
iteration 0300: loss: 1514.479
iteration 0400: loss: 1516.621
iteration 0500: loss: 1514.363
iteration 0600: loss: 1514.537
iteration 0700: loss: 1515.036
iteration 0800: loss: 1515.692
iteration 0900: loss: 1514.469
====> Epoch: 034 Train loss: 1514.6373  took : 8.556517124176025
====> Test loss: 1518.2961
iteration 0000: loss: 1514.662
iteration 0100: loss: 1513.708
iteration 0200: loss: 1513.214
iteration 0300: loss: 1516.533
iteration 0400: loss: 1514.832
iteration 0500: loss: 1514.597
iteration 0600: loss: 1514.244
iteration 0700: loss: 1514.124
iteration 0800: loss: 1513.102
iteration 0900: loss: 1515.445
====> Epoch: 035 Train loss: 1514.5846  took : 8.477692365646362
====> Test loss: 1518.1536
iteration 0000: loss: 1515.158
iteration 0100: loss: 1515.609
iteration 0200: loss: 1513.965
iteration 0300: loss: 1513.086
iteration 0400: loss: 1513.546
iteration 0500: loss: 1516.055
iteration 0600: loss: 1514.809
iteration 0700: loss: 1518.382
iteration 0800: loss: 1513.818
iteration 0900: loss: 1514.625
====> Epoch: 036 Train loss: 1514.5277  took : 8.417989730834961
====> Test loss: 1518.0563
iteration 0000: loss: 1514.755
iteration 0100: loss: 1514.339
iteration 0200: loss: 1516.038
iteration 0300: loss: 1513.913
iteration 0400: loss: 1512.731
iteration 0500: loss: 1512.360
iteration 0600: loss: 1515.544
iteration 0700: loss: 1516.729
iteration 0800: loss: 1513.890
iteration 0900: loss: 1516.269
====> Epoch: 037 Train loss: 1514.4517  took : 8.505376100540161
====> Test loss: 1517.9065
iteration 0000: loss: 1514.282
iteration 0100: loss: 1513.670
iteration 0200: loss: 1514.351
iteration 0300: loss: 1513.216
iteration 0400: loss: 1514.720
iteration 0500: loss: 1513.470
iteration 0600: loss: 1514.153
iteration 0700: loss: 1515.225
iteration 0800: loss: 1516.192
iteration 0900: loss: 1514.281
====> Epoch: 038 Train loss: 1514.4039  took : 8.446313381195068
====> Test loss: 1518.0649
iteration 0000: loss: 1514.592
iteration 0100: loss: 1515.734
iteration 0200: loss: 1513.850
iteration 0300: loss: 1512.569
iteration 0400: loss: 1515.180
iteration 0500: loss: 1515.529
iteration 0600: loss: 1514.301
iteration 0700: loss: 1513.996
iteration 0800: loss: 1514.824
iteration 0900: loss: 1513.785
====> Epoch: 039 Train loss: 1514.3200  took : 8.420017719268799
====> Test loss: 1517.9509
iteration 0000: loss: 1514.234
iteration 0100: loss: 1513.116
iteration 0200: loss: 1514.627
iteration 0300: loss: 1516.680
iteration 0400: loss: 1514.965
iteration 0500: loss: 1513.973
iteration 0600: loss: 1515.469
iteration 0700: loss: 1517.519
iteration 0800: loss: 1515.375
iteration 0900: loss: 1514.933
====> Epoch: 040 Train loss: 1514.4201  took : 8.421112537384033
====> Test loss: 1517.9276
iteration 0000: loss: 1513.811
iteration 0100: loss: 1514.755
iteration 0200: loss: 1514.701
iteration 0300: loss: 1515.208
iteration 0400: loss: 1514.353
iteration 0500: loss: 1514.044
iteration 0600: loss: 1514.345
iteration 0700: loss: 1513.205
iteration 0800: loss: 1516.479
iteration 0900: loss: 1514.710
====> Epoch: 041 Train loss: 1514.2260  took : 8.531774520874023
====> Test loss: 1517.8266
iteration 0000: loss: 1516.701
iteration 0100: loss: 1514.259
iteration 0200: loss: 1516.123
iteration 0300: loss: 1514.172
iteration 0400: loss: 1515.240
iteration 0500: loss: 1513.408
iteration 0600: loss: 1513.683
iteration 0700: loss: 1512.967
iteration 0800: loss: 1513.332
iteration 0900: loss: 1514.376
====> Epoch: 042 Train loss: 1514.2038  took : 8.472341060638428
====> Test loss: 1518.0461
iteration 0000: loss: 1512.802
iteration 0100: loss: 1512.714
iteration 0200: loss: 1515.820
iteration 0300: loss: 1513.862
iteration 0400: loss: 1514.902
iteration 0500: loss: 1515.085
iteration 0600: loss: 1513.720
iteration 0700: loss: 1510.565
iteration 0800: loss: 1511.511
iteration 0900: loss: 1512.963
====> Epoch: 043 Train loss: 1514.1491  took : 8.498774528503418
====> Test loss: 1517.7608
iteration 0000: loss: 1515.233
iteration 0100: loss: 1514.327
iteration 0200: loss: 1514.421
iteration 0300: loss: 1514.136
iteration 0400: loss: 1513.607
iteration 0500: loss: 1514.944
iteration 0600: loss: 1514.101
iteration 0700: loss: 1513.932
iteration 0800: loss: 1514.235
iteration 0900: loss: 1515.263
====> Epoch: 044 Train loss: 1514.0503  took : 8.44662070274353
====> Test loss: 1517.7803
iteration 0000: loss: 1516.167
iteration 0100: loss: 1514.383
iteration 0200: loss: 1515.049
iteration 0300: loss: 1512.512
iteration 0400: loss: 1513.771
iteration 0500: loss: 1514.950
iteration 0600: loss: 1515.981
iteration 0700: loss: 1516.863
iteration 0800: loss: 1512.589
iteration 0900: loss: 1516.039
====> Epoch: 045 Train loss: 1514.0000  took : 8.41656756401062
====> Test loss: 1517.7762
iteration 0000: loss: 1514.964
iteration 0100: loss: 1515.127
iteration 0200: loss: 1513.641
iteration 0300: loss: 1515.343
iteration 0400: loss: 1512.784
iteration 0500: loss: 1513.445
iteration 0600: loss: 1514.248
iteration 0700: loss: 1513.571
iteration 0800: loss: 1514.066
iteration 0900: loss: 1515.820
====> Epoch: 046 Train loss: 1513.9349  took : 8.427369594573975
====> Test loss: 1517.7334
iteration 0000: loss: 1512.864
iteration 0100: loss: 1511.489
iteration 0200: loss: 1514.804
iteration 0300: loss: 1510.739
iteration 0400: loss: 1510.856
iteration 0500: loss: 1513.248
iteration 0600: loss: 1512.810
iteration 0700: loss: 1511.766
iteration 0800: loss: 1512.830
iteration 0900: loss: 1514.137
====> Epoch: 047 Train loss: 1513.8771  took : 8.554056406021118
====> Test loss: 1517.6863
iteration 0000: loss: 1513.772
iteration 0100: loss: 1515.391
iteration 0200: loss: 1513.114
iteration 0300: loss: 1513.274
iteration 0400: loss: 1514.861
iteration 0500: loss: 1512.525
iteration 0600: loss: 1512.607
iteration 0700: loss: 1514.096
iteration 0800: loss: 1515.081
iteration 0900: loss: 1514.859
====> Epoch: 048 Train loss: 1513.8605  took : 8.46750521659851
====> Test loss: 1517.6555
iteration 0000: loss: 1512.294
iteration 0100: loss: 1514.224
iteration 0200: loss: 1515.512
iteration 0300: loss: 1514.463
iteration 0400: loss: 1511.977
iteration 0500: loss: 1512.414
iteration 0600: loss: 1514.558
iteration 0700: loss: 1514.554
iteration 0800: loss: 1512.120
iteration 0900: loss: 1512.139
====> Epoch: 049 Train loss: 1513.7958  took : 8.472127199172974
====> Test loss: 1517.4728
iteration 0000: loss: 1513.743
iteration 0100: loss: 1514.490
iteration 0200: loss: 1515.888
iteration 0300: loss: 1514.127
iteration 0400: loss: 1516.593
iteration 0500: loss: 1513.008
iteration 0600: loss: 1513.827
iteration 0700: loss: 1513.743
iteration 0800: loss: 1515.110
iteration 0900: loss: 1510.212
====> Epoch: 050 Train loss: 1513.7276  took : 8.526207208633423
====> Test loss: 1517.8129
====> [MM-VAE] Time: 506.831s or 00:08:26
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  17
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_17
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_17
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.059
iteration 0100: loss: 2094.125
iteration 0200: loss: 2042.443
iteration 0300: loss: 2017.707
iteration 0400: loss: 2009.080
iteration 0500: loss: 1998.510
iteration 0600: loss: 1992.028
iteration 0700: loss: 1990.972
iteration 0800: loss: 1985.447
iteration 0900: loss: 1995.043
====> Epoch: 001 Train loss: 2020.8194  took : 13.248999118804932
====> Test loss: 1990.4512
iteration 0000: loss: 1989.743
iteration 0100: loss: 1983.832
iteration 0200: loss: 1977.344
iteration 0300: loss: 1986.444
iteration 0400: loss: 1979.392
iteration 0500: loss: 1978.414
iteration 0600: loss: 1986.471
iteration 0700: loss: 1982.490
iteration 0800: loss: 1972.898
iteration 0900: loss: 1972.035
====> Epoch: 002 Train loss: 1981.1307  took : 13.105682611465454
====> Test loss: 1978.9733
iteration 0000: loss: 1975.625
iteration 0100: loss: 1974.855
iteration 0200: loss: 1967.789
iteration 0300: loss: 1970.507
iteration 0400: loss: 1971.486
iteration 0500: loss: 1969.955
iteration 0600: loss: 1969.652
iteration 0700: loss: 1967.297
iteration 0800: loss: 1971.328
iteration 0900: loss: 1968.517
====> Epoch: 003 Train loss: 1970.4109  took : 12.305980205535889
====> Test loss: 1968.9581
iteration 0000: loss: 1964.191
iteration 0100: loss: 1966.412
iteration 0200: loss: 1964.840
iteration 0300: loss: 1965.458
iteration 0400: loss: 1964.312
iteration 0500: loss: 1959.739
iteration 0600: loss: 1960.780
iteration 0700: loss: 1960.898
iteration 0800: loss: 1961.296
iteration 0900: loss: 1957.170
====> Epoch: 004 Train loss: 1962.5517  took : 12.104365587234497
====> Test loss: 1961.5544
iteration 0000: loss: 1958.955
iteration 0100: loss: 1959.944
iteration 0200: loss: 1957.661
iteration 0300: loss: 1954.613
iteration 0400: loss: 1960.527
iteration 0500: loss: 1957.041
iteration 0600: loss: 1957.692
iteration 0700: loss: 1955.075
iteration 0800: loss: 1954.752
iteration 0900: loss: 1954.941
====> Epoch: 005 Train loss: 1956.9337  took : 13.314244747161865
====> Test loss: 1958.2630
iteration 0000: loss: 1956.757
iteration 0100: loss: 1955.359
iteration 0200: loss: 1953.159
iteration 0300: loss: 1954.370
iteration 0400: loss: 1952.587
iteration 0500: loss: 1952.203
iteration 0600: loss: 1953.568
iteration 0700: loss: 1952.587
iteration 0800: loss: 1956.091
iteration 0900: loss: 1952.905
====> Epoch: 006 Train loss: 1954.0136  took : 12.568996667861938
====> Test loss: 1955.5301
iteration 0000: loss: 1953.765
iteration 0100: loss: 1951.045
iteration 0200: loss: 1949.998
iteration 0300: loss: 1953.447
iteration 0400: loss: 1951.543
iteration 0500: loss: 1950.745
iteration 0600: loss: 1954.919
iteration 0700: loss: 1951.407
iteration 0800: loss: 1951.474
iteration 0900: loss: 1950.826
====> Epoch: 007 Train loss: 1952.0756  took : 12.42957878112793
====> Test loss: 1953.8291
iteration 0000: loss: 1952.114
iteration 0100: loss: 1952.116
iteration 0200: loss: 1949.775
iteration 0300: loss: 1952.418
iteration 0400: loss: 1948.808
iteration 0500: loss: 1950.202
iteration 0600: loss: 1950.612
iteration 0700: loss: 1950.899
iteration 0800: loss: 1950.411
iteration 0900: loss: 1951.507
====> Epoch: 008 Train loss: 1951.0448  took : 12.411109447479248
====> Test loss: 1952.8613
iteration 0000: loss: 1950.465
iteration 0100: loss: 1951.757
iteration 0200: loss: 1949.402
iteration 0300: loss: 1951.108
iteration 0400: loss: 1949.767
iteration 0500: loss: 1950.571
iteration 0600: loss: 1948.868
iteration 0700: loss: 1949.279
iteration 0800: loss: 1948.707
iteration 0900: loss: 1948.947
====> Epoch: 009 Train loss: 1950.3585  took : 12.782328605651855
====> Test loss: 1952.6478
iteration 0000: loss: 1948.836
iteration 0100: loss: 1950.168
iteration 0200: loss: 1949.635
iteration 0300: loss: 1950.526
iteration 0400: loss: 1949.659
iteration 0500: loss: 1949.365
iteration 0600: loss: 1947.965
iteration 0700: loss: 1950.217
iteration 0800: loss: 1948.521
iteration 0900: loss: 1950.989
====> Epoch: 010 Train loss: 1949.9200  took : 12.147151947021484
====> Test loss: 1952.2983
iteration 0000: loss: 1952.348
iteration 0100: loss: 1950.169
iteration 0200: loss: 1950.378
iteration 0300: loss: 1949.781
iteration 0400: loss: 1948.622
iteration 0500: loss: 1950.689
iteration 0600: loss: 1951.196
iteration 0700: loss: 1949.312
iteration 0800: loss: 1950.169
iteration 0900: loss: 1948.454
====> Epoch: 011 Train loss: 1949.5773  took : 12.567107677459717
====> Test loss: 1951.7105
iteration 0000: loss: 1948.649
iteration 0100: loss: 1948.857
iteration 0200: loss: 1948.490
iteration 0300: loss: 1949.609
iteration 0400: loss: 1950.314
iteration 0500: loss: 1949.121
iteration 0600: loss: 1948.805
iteration 0700: loss: 1950.988
iteration 0800: loss: 1950.815
iteration 0900: loss: 1949.674
====> Epoch: 012 Train loss: 1949.6160  took : 13.179490327835083
====> Test loss: 1951.5445
iteration 0000: loss: 1947.972
iteration 0100: loss: 1948.154
iteration 0200: loss: 1949.511
iteration 0300: loss: 1948.846
iteration 0400: loss: 1950.804
iteration 0500: loss: 1948.292
iteration 0600: loss: 1949.568
iteration 0700: loss: 1949.893
iteration 0800: loss: 1948.754
iteration 0900: loss: 1948.575
====> Epoch: 013 Train loss: 1949.1990  took : 12.320982456207275
====> Test loss: 1951.5135
iteration 0000: loss: 1949.295
iteration 0100: loss: 1950.057
iteration 0200: loss: 1949.213
iteration 0300: loss: 1949.403
iteration 0400: loss: 1948.901
iteration 0500: loss: 1949.294
iteration 0600: loss: 1948.997
iteration 0700: loss: 1948.053
iteration 0800: loss: 1948.777
iteration 0900: loss: 1949.699
====> Epoch: 014 Train loss: 1949.1473  took : 13.100423097610474
====> Test loss: 1951.2926
iteration 0000: loss: 1950.047
iteration 0100: loss: 1948.159
iteration 0200: loss: 1949.793
iteration 0300: loss: 1948.205
iteration 0400: loss: 1948.855
iteration 0500: loss: 1950.369
iteration 0600: loss: 1950.721
iteration 0700: loss: 1948.751
iteration 0800: loss: 1949.702
iteration 0900: loss: 1948.572
====> Epoch: 015 Train loss: 1948.9901  took : 13.398313760757446
====> Test loss: 1951.3963
iteration 0000: loss: 1947.408
iteration 0100: loss: 1948.315
iteration 0200: loss: 1950.431
iteration 0300: loss: 1952.780
iteration 0400: loss: 1950.175
iteration 0500: loss: 1948.966
iteration 0600: loss: 1948.397
iteration 0700: loss: 1950.883
iteration 0800: loss: 1948.213
iteration 0900: loss: 1948.398
====> Epoch: 016 Train loss: 1949.1140  took : 11.874930381774902
====> Test loss: 1951.4514
iteration 0000: loss: 1950.671
iteration 0100: loss: 1948.285
iteration 0200: loss: 1948.651
iteration 0300: loss: 1948.154
iteration 0400: loss: 1948.877
iteration 0500: loss: 1949.720
iteration 0600: loss: 1949.069
iteration 0700: loss: 1948.748
iteration 0800: loss: 1948.851
iteration 0900: loss: 1949.412
====> Epoch: 017 Train loss: 1949.0484  took : 12.25141954421997
====> Test loss: 1951.5296
iteration 0000: loss: 1948.488
iteration 0100: loss: 1948.851
iteration 0200: loss: 1950.098
iteration 0300: loss: 1948.649
iteration 0400: loss: 1949.930
iteration 0500: loss: 1948.459
iteration 0600: loss: 1948.526
iteration 0700: loss: 1948.651
iteration 0800: loss: 1948.336
iteration 0900: loss: 1949.023
====> Epoch: 018 Train loss: 1948.8799  took : 11.96803331375122
====> Test loss: 1950.8219
iteration 0000: loss: 1950.567
iteration 0100: loss: 1949.472
iteration 0200: loss: 1948.177
iteration 0300: loss: 1948.468
iteration 0400: loss: 1948.126
iteration 0500: loss: 1951.551
iteration 0600: loss: 1948.752
iteration 0700: loss: 1950.651
iteration 0800: loss: 1948.340
iteration 0900: loss: 1948.353
====> Epoch: 019 Train loss: 1948.8022  took : 12.63116455078125
====> Test loss: 1951.1643
iteration 0000: loss: 1948.585
iteration 0100: loss: 1949.700
iteration 0200: loss: 1948.744
iteration 0300: loss: 1950.424
iteration 0400: loss: 1949.262
iteration 0500: loss: 1950.283
iteration 0600: loss: 1948.548
iteration 0700: loss: 1949.637
iteration 0800: loss: 1947.626
iteration 0900: loss: 1948.135
====> Epoch: 020 Train loss: 1948.8159  took : 12.596515417098999
====> Test loss: 1950.8133
iteration 0000: loss: 1949.514
iteration 0100: loss: 1948.861
iteration 0200: loss: 1949.314
iteration 0300: loss: 1947.825
iteration 0400: loss: 1947.477
iteration 0500: loss: 1949.282
iteration 0600: loss: 1947.942
iteration 0700: loss: 1949.118
iteration 0800: loss: 1948.346
iteration 0900: loss: 1947.646
====> Epoch: 021 Train loss: 1948.6779  took : 12.306844711303711
====> Test loss: 1950.9881
iteration 0000: loss: 1948.128
iteration 0100: loss: 1949.269
iteration 0200: loss: 1947.962
iteration 0300: loss: 1949.670
iteration 0400: loss: 1947.873
iteration 0500: loss: 1948.652
iteration 0600: loss: 1950.391
iteration 0700: loss: 1947.911
iteration 0800: loss: 1949.794
iteration 0900: loss: 1948.478
====> Epoch: 022 Train loss: 1948.7536  took : 11.707900762557983
====> Test loss: 1950.8990
iteration 0000: loss: 1950.065
iteration 0100: loss: 1948.552
iteration 0200: loss: 1948.007
iteration 0300: loss: 1951.549
iteration 0400: loss: 1951.290
iteration 0500: loss: 1948.643
iteration 0600: loss: 1948.919
iteration 0700: loss: 1947.479
iteration 0800: loss: 1948.227
iteration 0900: loss: 1949.441
====> Epoch: 023 Train loss: 1948.7249  took : 13.233306646347046
====> Test loss: 1951.2461
iteration 0000: loss: 1948.310
iteration 0100: loss: 1948.026
iteration 0200: loss: 1949.336
iteration 0300: loss: 1949.167
iteration 0400: loss: 1948.430
iteration 0500: loss: 1947.657
iteration 0600: loss: 1948.296
iteration 0700: loss: 1949.482
iteration 0800: loss: 1948.115
iteration 0900: loss: 1949.209
====> Epoch: 024 Train loss: 1948.5429  took : 12.099088668823242
====> Test loss: 1950.4761
iteration 0000: loss: 1947.487
iteration 0100: loss: 1948.921
iteration 0200: loss: 1947.820
iteration 0300: loss: 1948.969
iteration 0400: loss: 1948.298
iteration 0500: loss: 1947.799
iteration 0600: loss: 1949.496
iteration 0700: loss: 1947.399
iteration 0800: loss: 1948.461
iteration 0900: loss: 1948.926
====> Epoch: 025 Train loss: 1948.3914  took : 12.543384313583374
====> Test loss: 1950.3422
iteration 0000: loss: 1947.715
iteration 0100: loss: 1951.062
iteration 0200: loss: 1948.798
iteration 0300: loss: 1948.075
iteration 0400: loss: 1948.520
iteration 0500: loss: 1949.204
iteration 0600: loss: 1947.892
iteration 0700: loss: 1948.183
iteration 0800: loss: 1948.864
iteration 0900: loss: 1948.455
====> Epoch: 026 Train loss: 1948.4212  took : 12.488461256027222
====> Test loss: 1950.2183
iteration 0000: loss: 1947.225
iteration 0100: loss: 1948.061
iteration 0200: loss: 1947.684
iteration 0300: loss: 1948.739
iteration 0400: loss: 1948.938
iteration 0500: loss: 1947.511
iteration 0600: loss: 1948.089
iteration 0700: loss: 1948.592
iteration 0800: loss: 1947.264
iteration 0900: loss: 1948.682
====> Epoch: 027 Train loss: 1948.3167  took : 12.530539274215698
====> Test loss: 1950.6664
iteration 0000: loss: 1948.869
iteration 0100: loss: 1948.204
iteration 0200: loss: 1947.709
iteration 0300: loss: 1947.846
iteration 0400: loss: 1949.484
iteration 0500: loss: 1948.376
iteration 0600: loss: 1947.884
iteration 0700: loss: 1949.190
iteration 0800: loss: 1948.824
iteration 0900: loss: 1949.952
====> Epoch: 028 Train loss: 1948.4425  took : 12.045874834060669
====> Test loss: 1950.4322
iteration 0000: loss: 1946.962
iteration 0100: loss: 1947.924
iteration 0200: loss: 1948.480
iteration 0300: loss: 1948.595
iteration 0400: loss: 1948.820
iteration 0500: loss: 1948.486
iteration 0600: loss: 1948.538
iteration 0700: loss: 1947.859
iteration 0800: loss: 1948.411
iteration 0900: loss: 1948.654
====> Epoch: 029 Train loss: 1948.4381  took : 12.811390161514282
====> Test loss: 1950.5449
iteration 0000: loss: 1948.442
iteration 0100: loss: 1948.766
iteration 0200: loss: 1947.482
iteration 0300: loss: 1947.395
iteration 0400: loss: 1948.567
iteration 0500: loss: 1947.833
iteration 0600: loss: 1947.916
iteration 0700: loss: 1949.013
iteration 0800: loss: 1949.805
iteration 0900: loss: 1948.216
====> Epoch: 030 Train loss: 1948.4205  took : 12.944764614105225
====> Test loss: 1950.4272
iteration 0000: loss: 1947.352
iteration 0100: loss: 1948.885
iteration 0200: loss: 1948.668
iteration 0300: loss: 1949.726
iteration 0400: loss: 1946.817
iteration 0500: loss: 1947.411
iteration 0600: loss: 1947.840
iteration 0700: loss: 1949.131
iteration 0800: loss: 1947.831
iteration 0900: loss: 1948.902
====> Epoch: 031 Train loss: 1948.3212  took : 12.929301023483276
====> Test loss: 1950.5197
iteration 0000: loss: 1947.539
iteration 0100: loss: 1947.651
iteration 0200: loss: 1949.610
iteration 0300: loss: 1948.816
iteration 0400: loss: 1949.383
iteration 0500: loss: 1949.159
iteration 0600: loss: 1948.798
iteration 0700: loss: 1948.316
iteration 0800: loss: 1949.800
iteration 0900: loss: 1948.153
====> Epoch: 032 Train loss: 1948.3729  took : 12.981807470321655
====> Test loss: 1950.7504
iteration 0000: loss: 1948.795
iteration 0100: loss: 1949.070
iteration 0200: loss: 1947.579
iteration 0300: loss: 1948.552
iteration 0400: loss: 1948.658
iteration 0500: loss: 1947.613
iteration 0600: loss: 1948.547
iteration 0700: loss: 1948.629
iteration 0800: loss: 1947.738
iteration 0900: loss: 1947.814
====> Epoch: 033 Train loss: 1948.3846  took : 12.39589810371399
====> Test loss: 1950.2782
iteration 0000: loss: 1947.544
iteration 0100: loss: 1948.854
iteration 0200: loss: 1948.396
iteration 0300: loss: 1948.193
iteration 0400: loss: 1949.575
iteration 0500: loss: 1947.836
iteration 0600: loss: 1949.449
iteration 0700: loss: 1947.824
iteration 0800: loss: 1947.772
iteration 0900: loss: 1948.209
====> Epoch: 034 Train loss: 1948.4812  took : 12.046708345413208
====> Test loss: 1950.2360
iteration 0000: loss: 1948.184
iteration 0100: loss: 1949.114
iteration 0200: loss: 1947.288
iteration 0300: loss: 1948.143
iteration 0400: loss: 1948.281
iteration 0500: loss: 1947.913
iteration 0600: loss: 1948.997
iteration 0700: loss: 1947.581
iteration 0800: loss: 1948.131
iteration 0900: loss: 1947.925
====> Epoch: 035 Train loss: 1948.3682  took : 11.99326229095459
====> Test loss: 1950.2910
iteration 0000: loss: 1948.079
iteration 0100: loss: 1947.290
iteration 0200: loss: 1949.144
iteration 0300: loss: 1948.192
iteration 0400: loss: 1947.494
iteration 0500: loss: 1948.878
iteration 0600: loss: 1947.416
iteration 0700: loss: 1948.261
iteration 0800: loss: 1948.332
iteration 0900: loss: 1947.675
====> Epoch: 036 Train loss: 1948.3170  took : 12.847582817077637
====> Test loss: 1950.3773
iteration 0000: loss: 1947.685
iteration 0100: loss: 1947.282
iteration 0200: loss: 1948.066
iteration 0300: loss: 1948.683
iteration 0400: loss: 1948.138
iteration 0500: loss: 1947.249
iteration 0600: loss: 1948.981
iteration 0700: loss: 1948.680
iteration 0800: loss: 1949.671
iteration 0900: loss: 1948.703
====> Epoch: 037 Train loss: 1948.2508  took : 13.178636074066162
====> Test loss: 1950.4771
iteration 0000: loss: 1947.457
iteration 0100: loss: 1948.650
iteration 0200: loss: 1947.895
iteration 0300: loss: 1947.797
iteration 0400: loss: 1949.427
iteration 0500: loss: 1947.825
iteration 0600: loss: 1948.369
iteration 0700: loss: 1948.516
iteration 0800: loss: 1948.601
iteration 0900: loss: 1948.573
====> Epoch: 038 Train loss: 1948.1802  took : 13.280277252197266
====> Test loss: 1950.2083
iteration 0000: loss: 1947.846
iteration 0100: loss: 1949.160
iteration 0200: loss: 1948.458
iteration 0300: loss: 1949.273
iteration 0400: loss: 1948.646
iteration 0500: loss: 1948.760
iteration 0600: loss: 1947.520
iteration 0700: loss: 1949.048
iteration 0800: loss: 1948.167
iteration 0900: loss: 1947.054
====> Epoch: 039 Train loss: 1948.2451  took : 13.408365726470947
====> Test loss: 1950.0505
iteration 0000: loss: 1947.017
iteration 0100: loss: 1948.248
iteration 0200: loss: 1949.480
iteration 0300: loss: 1949.137
iteration 0400: loss: 1947.064
iteration 0500: loss: 1947.880
iteration 0600: loss: 1946.715
iteration 0700: loss: 1949.364
iteration 0800: loss: 1947.616
iteration 0900: loss: 1947.780
====> Epoch: 040 Train loss: 1948.3794  took : 13.240875959396362
====> Test loss: 1950.3584
iteration 0000: loss: 1947.227
iteration 0100: loss: 1947.392
iteration 0200: loss: 1949.052
iteration 0300: loss: 1950.348
iteration 0400: loss: 1948.371
iteration 0500: loss: 1948.284
iteration 0600: loss: 1948.259
iteration 0700: loss: 1947.356
iteration 0800: loss: 1948.154
iteration 0900: loss: 1948.122
====> Epoch: 041 Train loss: 1948.4482  took : 13.027998685836792
====> Test loss: 1950.5102
iteration 0000: loss: 1947.940
iteration 0100: loss: 1947.851
iteration 0200: loss: 1947.967
iteration 0300: loss: 1948.417
iteration 0400: loss: 1949.563
iteration 0500: loss: 1949.628
iteration 0600: loss: 1948.175
iteration 0700: loss: 1949.244
iteration 0800: loss: 1948.039
iteration 0900: loss: 1948.915
====> Epoch: 042 Train loss: 1948.4035  took : 12.732445001602173
====> Test loss: 1950.4817
iteration 0000: loss: 1948.540
iteration 0100: loss: 1948.593
iteration 0200: loss: 1947.849
iteration 0300: loss: 1948.646
iteration 0400: loss: 1948.254
iteration 0500: loss: 1947.973
iteration 0600: loss: 1948.818
iteration 0700: loss: 1948.077
iteration 0800: loss: 1947.998
iteration 0900: loss: 1949.450
====> Epoch: 043 Train loss: 1948.1078  took : 13.088191270828247
====> Test loss: 1950.4544
iteration 0000: loss: 1947.543
iteration 0100: loss: 1948.400
iteration 0200: loss: 1948.262
iteration 0300: loss: 1949.920
iteration 0400: loss: 1949.665
iteration 0500: loss: 1948.202
iteration 0600: loss: 1949.454
iteration 0700: loss: 1947.330
iteration 0800: loss: 1946.907
iteration 0900: loss: 1946.710
====> Epoch: 044 Train loss: 1948.3563  took : 11.864078521728516
====> Test loss: 1950.2800
iteration 0000: loss: 1948.040
iteration 0100: loss: 1947.851
iteration 0200: loss: 1950.420
iteration 0300: loss: 1948.702
iteration 0400: loss: 1948.102
iteration 0500: loss: 1948.996
iteration 0600: loss: 1948.704
iteration 0700: loss: 1948.231
iteration 0800: loss: 1947.771
iteration 0900: loss: 1947.581
====> Epoch: 045 Train loss: 1948.1660  took : 12.274196147918701
====> Test loss: 1950.3804
iteration 0000: loss: 1948.054
iteration 0100: loss: 1948.356
iteration 0200: loss: 1949.025
iteration 0300: loss: 1948.496
iteration 0400: loss: 1948.793
iteration 0500: loss: 1947.813
iteration 0600: loss: 1949.422
iteration 0700: loss: 1947.971
iteration 0800: loss: 1947.709
iteration 0900: loss: 1947.523
====> Epoch: 046 Train loss: 1948.0888  took : 13.252346277236938
====> Test loss: 1950.1652
iteration 0000: loss: 1947.014
iteration 0100: loss: 1947.554
iteration 0200: loss: 1947.204
iteration 0300: loss: 1948.416
iteration 0400: loss: 1948.479
iteration 0500: loss: 1947.837
iteration 0600: loss: 1948.625
iteration 0700: loss: 1947.612
iteration 0800: loss: 1948.172
iteration 0900: loss: 1949.790
====> Epoch: 047 Train loss: 1948.1770  took : 12.76122260093689
====> Test loss: 1950.9112
iteration 0000: loss: 1947.603
iteration 0100: loss: 1948.261
iteration 0200: loss: 1948.532
iteration 0300: loss: 1950.224
iteration 0400: loss: 1946.934
iteration 0500: loss: 1947.924
iteration 0600: loss: 1947.450
iteration 0700: loss: 1949.162
iteration 0800: loss: 1947.646
iteration 0900: loss: 1948.021
====> Epoch: 048 Train loss: 1948.4859  took : 12.698523759841919
====> Test loss: 1950.1889
iteration 0000: loss: 1947.710
iteration 0100: loss: 1947.433
iteration 0200: loss: 1949.838
iteration 0300: loss: 1948.229
iteration 0400: loss: 1948.236
iteration 0500: loss: 1948.877
iteration 0600: loss: 1947.731
iteration 0700: loss: 1947.985
iteration 0800: loss: 1949.016
iteration 0900: loss: 1948.461
====> Epoch: 049 Train loss: 1948.0912  took : 11.965035200119019
====> Test loss: 1950.1575
iteration 0000: loss: 1947.574
iteration 0100: loss: 1948.494
iteration 0200: loss: 1948.359
iteration 0300: loss: 1947.938
iteration 0400: loss: 1948.136
iteration 0500: loss: 1948.628
iteration 0600: loss: 1948.209
iteration 0700: loss: 1948.437
iteration 0800: loss: 1948.718
iteration 0900: loss: 1947.490
====> Epoch: 050 Train loss: 1948.1125  took : 12.83717131614685
====> Test loss: 1949.9708
====> [MM-VAE] Time: 702.993s or 00:11:42
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  17
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_17
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_17
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5218.977
iteration 0100: loss: 4102.061
iteration 0200: loss: 4094.827
iteration 0300: loss: 4057.911
iteration 0400: loss: 4011.526
iteration 0500: loss: 4028.278
iteration 0600: loss: 4015.101
iteration 0700: loss: 4019.340
iteration 0800: loss: 4006.702
iteration 0900: loss: 4013.895
iteration 1000: loss: 4012.216
iteration 1100: loss: 3986.401
iteration 1200: loss: 3999.497
iteration 1300: loss: 3988.679
iteration 1400: loss: 3994.849
iteration 1500: loss: 3997.105
iteration 1600: loss: 4002.703
iteration 1700: loss: 3992.414
iteration 1800: loss: 3988.071
====> Epoch: 001 Train loss: 4025.8306  took : 52.97320771217346
====> Test loss: 3993.5766
iteration 0000: loss: 3993.379
iteration 0100: loss: 3992.303
iteration 0200: loss: 3987.488
iteration 0300: loss: 3995.324
iteration 0400: loss: 3990.459
iteration 0500: loss: 3998.446
iteration 0600: loss: 3985.656
iteration 0700: loss: 3978.451
iteration 0800: loss: 3987.344
iteration 0900: loss: 3978.931
iteration 1000: loss: 3970.048
iteration 1100: loss: 3983.294
iteration 1200: loss: 3981.355
iteration 1300: loss: 3978.893
iteration 1400: loss: 3987.841
iteration 1500: loss: 3976.813
iteration 1600: loss: 3970.892
iteration 1700: loss: 3979.545
iteration 1800: loss: 3974.258
====> Epoch: 002 Train loss: 3982.8728  took : 53.09047174453735
====> Test loss: 3982.4996
iteration 0000: loss: 3970.065
iteration 0100: loss: 3991.213
iteration 0200: loss: 3979.855
iteration 0300: loss: 3983.694
iteration 0400: loss: 3977.423
iteration 0500: loss: 3986.058
iteration 0600: loss: 3976.722
iteration 0700: loss: 3979.429
iteration 0800: loss: 3981.904
iteration 0900: loss: 3965.226
iteration 1000: loss: 3976.288
iteration 1100: loss: 3974.213
iteration 1200: loss: 3974.734
iteration 1300: loss: 3981.121
iteration 1400: loss: 3973.802
iteration 1500: loss: 3968.099
iteration 1600: loss: 3965.812
iteration 1700: loss: 3967.042
iteration 1800: loss: 3977.643
====> Epoch: 003 Train loss: 3975.7455  took : 53.050843954086304
====> Test loss: 3978.7621
iteration 0000: loss: 3991.712
iteration 0100: loss: 3981.890
iteration 0200: loss: 3972.410
iteration 0300: loss: 3962.796
iteration 0400: loss: 3977.485
iteration 0500: loss: 3974.324
iteration 0600: loss: 3977.794
iteration 0700: loss: 3970.674
iteration 0800: loss: 3968.966
iteration 0900: loss: 3978.798
iteration 1000: loss: 3971.837
iteration 1100: loss: 3976.344
iteration 1200: loss: 3966.394
iteration 1300: loss: 3971.676
iteration 1400: loss: 3977.752
iteration 1500: loss: 3970.845
iteration 1600: loss: 3966.899
iteration 1700: loss: 3977.987
iteration 1800: loss: 3975.781
====> Epoch: 004 Train loss: 3972.4266  took : 53.211610555648804
====> Test loss: 3978.1266
iteration 0000: loss: 3970.424
iteration 0100: loss: 3971.847
iteration 0200: loss: 3977.450
iteration 0300: loss: 3976.044
iteration 0400: loss: 3968.357
iteration 0500: loss: 3963.489
iteration 0600: loss: 3978.252
iteration 0700: loss: 3968.949
iteration 0800: loss: 3972.020
iteration 0900: loss: 3976.565
iteration 1000: loss: 3962.757
iteration 1100: loss: 3962.447
iteration 1200: loss: 3970.507
iteration 1300: loss: 3982.741
iteration 1400: loss: 3970.632
iteration 1500: loss: 3973.397
iteration 1600: loss: 3969.845
iteration 1700: loss: 3969.382
iteration 1800: loss: 3982.136
====> Epoch: 005 Train loss: 3970.8074  took : 52.932849168777466
====> Test loss: 3975.5485
iteration 0000: loss: 3963.483
iteration 0100: loss: 3961.426
iteration 0200: loss: 3972.443
iteration 0300: loss: 3977.315
iteration 0400: loss: 3962.977
iteration 0500: loss: 3972.088
iteration 0600: loss: 3977.099
iteration 0700: loss: 3967.474
iteration 0800: loss: 3963.770
iteration 0900: loss: 3968.648
iteration 1000: loss: 3973.116
iteration 1100: loss: 3959.286
iteration 1200: loss: 3967.294
iteration 1300: loss: 3971.102
iteration 1400: loss: 3968.989
iteration 1500: loss: 3962.776
iteration 1600: loss: 3958.264
iteration 1700: loss: 3984.661
iteration 1800: loss: 3972.948
====> Epoch: 006 Train loss: 3969.4159  took : 53.1077024936676
====> Test loss: 3975.0829
iteration 0000: loss: 3975.269
iteration 0100: loss: 3978.809
iteration 0200: loss: 3967.104
iteration 0300: loss: 3970.669
iteration 0400: loss: 3981.358
iteration 0500: loss: 3966.268
iteration 0600: loss: 3986.179
iteration 0700: loss: 3974.034
iteration 0800: loss: 3971.300
iteration 0900: loss: 3963.500
iteration 1000: loss: 3976.988
iteration 1100: loss: 3961.485
iteration 1200: loss: 3968.933
iteration 1300: loss: 3959.140
iteration 1400: loss: 3966.564
iteration 1500: loss: 3966.011
iteration 1600: loss: 3966.148
iteration 1700: loss: 3960.708
iteration 1800: loss: 3965.644
====> Epoch: 007 Train loss: 3968.4914  took : 53.301358222961426
====> Test loss: 3973.5831
iteration 0000: loss: 3969.554
iteration 0100: loss: 3960.946
iteration 0200: loss: 3973.853
iteration 0300: loss: 3977.246
iteration 0400: loss: 3967.687
iteration 0500: loss: 3959.052
iteration 0600: loss: 3968.146
iteration 0700: loss: 3964.627
iteration 0800: loss: 3969.858
iteration 0900: loss: 3977.251
iteration 1000: loss: 3964.002
iteration 1100: loss: 3967.324
iteration 1200: loss: 3968.954
iteration 1300: loss: 3966.584
iteration 1400: loss: 3956.871
iteration 1500: loss: 3971.788
iteration 1600: loss: 3961.348
iteration 1700: loss: 3971.281
iteration 1800: loss: 3964.119
====> Epoch: 008 Train loss: 3967.3978  took : 53.09984850883484
====> Test loss: 3973.1239
iteration 0000: loss: 3963.846
iteration 0100: loss: 3961.938
iteration 0200: loss: 3967.630
iteration 0300: loss: 3961.048
iteration 0400: loss: 3983.576
iteration 0500: loss: 3973.220
iteration 0600: loss: 3966.148
iteration 0700: loss: 3964.712
iteration 0800: loss: 3973.961
iteration 0900: loss: 3954.862
iteration 1000: loss: 3965.638
iteration 1100: loss: 3957.594
iteration 1200: loss: 3965.774
iteration 1300: loss: 3970.002
iteration 1400: loss: 3970.173
iteration 1500: loss: 3959.938
iteration 1600: loss: 3963.577
iteration 1700: loss: 3965.833
iteration 1800: loss: 3962.447
====> Epoch: 009 Train loss: 3966.7716  took : 53.134960651397705
====> Test loss: 3972.9478
iteration 0000: loss: 3973.429
iteration 0100: loss: 3960.247
iteration 0200: loss: 3970.175
iteration 0300: loss: 3971.342
iteration 0400: loss: 3963.744
iteration 0500: loss: 3975.862
iteration 0600: loss: 3957.864
iteration 0700: loss: 3969.095
iteration 0800: loss: 3961.609
iteration 0900: loss: 3972.373
iteration 1000: loss: 3966.287
iteration 1100: loss: 3975.974
iteration 1200: loss: 3956.562
iteration 1300: loss: 3960.309
iteration 1400: loss: 3969.874
iteration 1500: loss: 3967.863
iteration 1600: loss: 3957.136
iteration 1700: loss: 3954.346
iteration 1800: loss: 3964.587
====> Epoch: 010 Train loss: 3966.5584  took : 53.05673551559448
====> Test loss: 3974.0048
iteration 0000: loss: 3967.757
iteration 0100: loss: 3957.888
iteration 0200: loss: 3957.445
iteration 0300: loss: 3964.447
iteration 0400: loss: 3968.427
iteration 0500: loss: 3966.708
iteration 0600: loss: 3969.598
iteration 0700: loss: 3964.868
iteration 0800: loss: 3959.164
iteration 0900: loss: 3972.497
iteration 1000: loss: 3963.975
iteration 1100: loss: 3966.872
iteration 1200: loss: 3960.984
iteration 1300: loss: 3971.811
iteration 1400: loss: 3964.432
iteration 1500: loss: 3961.697
iteration 1600: loss: 3975.506
iteration 1700: loss: 3946.473
iteration 1800: loss: 3966.256
====> Epoch: 011 Train loss: 3965.8319  took : 52.879000186920166
====> Test loss: 3971.2788
iteration 0000: loss: 3973.011
iteration 0100: loss: 3970.923
iteration 0200: loss: 3967.597
iteration 0300: loss: 3959.461
iteration 0400: loss: 3967.006
iteration 0500: loss: 3962.500
iteration 0600: loss: 3962.158
iteration 0700: loss: 3962.385
iteration 0800: loss: 3965.090
iteration 0900: loss: 3974.561
iteration 1000: loss: 3964.286
iteration 1100: loss: 3969.613
iteration 1200: loss: 3964.391
iteration 1300: loss: 3970.218
iteration 1400: loss: 3966.110
iteration 1500: loss: 3964.967
iteration 1600: loss: 3960.373
iteration 1700: loss: 3961.513
iteration 1800: loss: 3978.700
====> Epoch: 012 Train loss: 3965.1145  took : 52.92510414123535
====> Test loss: 3971.6109
iteration 0000: loss: 3975.896
iteration 0100: loss: 3964.092
iteration 0200: loss: 3960.307
iteration 0300: loss: 3958.566
iteration 0400: loss: 3967.365
iteration 0500: loss: 3956.686
iteration 0600: loss: 3961.264
iteration 0700: loss: 3962.971
iteration 0800: loss: 3969.792
iteration 0900: loss: 3975.808
iteration 1000: loss: 3967.290
iteration 1100: loss: 3965.620
iteration 1200: loss: 3975.741
iteration 1300: loss: 3961.168
iteration 1400: loss: 3966.178
iteration 1500: loss: 3968.859
iteration 1600: loss: 3972.346
iteration 1700: loss: 3962.023
iteration 1800: loss: 3958.498
====> Epoch: 013 Train loss: 3965.1265  took : 53.329554319381714
====> Test loss: 3970.9599
iteration 0000: loss: 3958.396
iteration 0100: loss: 3957.938
iteration 0200: loss: 3970.660
iteration 0300: loss: 3968.751
iteration 0400: loss: 3959.639
iteration 0500: loss: 3963.997
iteration 0600: loss: 3973.481
iteration 0700: loss: 3969.309
iteration 0800: loss: 3955.384
iteration 0900: loss: 3957.208
iteration 1000: loss: 3972.521
iteration 1100: loss: 3962.902
iteration 1200: loss: 3962.588
iteration 1300: loss: 3969.913
iteration 1400: loss: 3969.663
iteration 1500: loss: 3961.565
iteration 1600: loss: 3961.838
iteration 1700: loss: 3962.570
iteration 1800: loss: 3960.875
====> Epoch: 014 Train loss: 3964.4464  took : 53.11101198196411
====> Test loss: 3971.1727
iteration 0000: loss: 3971.254
iteration 0100: loss: 3955.872
iteration 0200: loss: 3966.792
iteration 0300: loss: 3965.397
iteration 0400: loss: 3956.003
iteration 0500: loss: 3956.125
iteration 0600: loss: 3964.205
iteration 0700: loss: 3971.759
iteration 0800: loss: 3963.958
iteration 0900: loss: 3963.793
iteration 1000: loss: 3961.653
iteration 1100: loss: 3966.665
iteration 1200: loss: 3970.538
iteration 1300: loss: 3957.090
iteration 1400: loss: 3960.269
iteration 1500: loss: 3963.504
iteration 1600: loss: 3966.360
iteration 1700: loss: 3955.267
iteration 1800: loss: 3960.411
====> Epoch: 015 Train loss: 3964.3960  took : 52.937445640563965
====> Test loss: 3971.9566
iteration 0000: loss: 3963.206
iteration 0100: loss: 3956.273
iteration 0200: loss: 3969.085
iteration 0300: loss: 3959.695
iteration 0400: loss: 3967.024
iteration 0500: loss: 3966.049
iteration 0600: loss: 3972.014
iteration 0700: loss: 3976.109
iteration 0800: loss: 3963.267
iteration 0900: loss: 3959.440
iteration 1000: loss: 3957.334
iteration 1100: loss: 3966.280
iteration 1200: loss: 3972.537
iteration 1300: loss: 3976.374
iteration 1400: loss: 3965.511
iteration 1500: loss: 3963.183
iteration 1600: loss: 3965.803
iteration 1700: loss: 3966.862
iteration 1800: loss: 3969.172
====> Epoch: 016 Train loss: 3964.1785  took : 53.257025480270386
====> Test loss: 3971.6117
iteration 0000: loss: 3968.426
iteration 0100: loss: 3961.178
iteration 0200: loss: 3960.351
iteration 0300: loss: 3961.143
iteration 0400: loss: 3967.382
iteration 0500: loss: 3965.206
iteration 0600: loss: 3968.059
iteration 0700: loss: 3970.652
iteration 0800: loss: 3954.849
iteration 0900: loss: 3973.351
iteration 1000: loss: 3967.773
iteration 1100: loss: 3965.641
iteration 1200: loss: 3969.460
iteration 1300: loss: 3960.967
iteration 1400: loss: 3965.294
iteration 1500: loss: 3954.611
iteration 1600: loss: 3961.277
iteration 1700: loss: 3960.432
iteration 1800: loss: 3964.353
====> Epoch: 017 Train loss: 3964.1470  took : 53.05510878562927
====> Test loss: 3971.4635
iteration 0000: loss: 3961.500
iteration 0100: loss: 3970.843
iteration 0200: loss: 3967.355
iteration 0300: loss: 3962.479
iteration 0400: loss: 3972.319
iteration 0500: loss: 3957.669
iteration 0600: loss: 3955.267
iteration 0700: loss: 3962.461
iteration 0800: loss: 3957.723
iteration 0900: loss: 3962.955
iteration 1000: loss: 3970.156
iteration 1100: loss: 3954.760
iteration 1200: loss: 3965.969
iteration 1300: loss: 3962.518
iteration 1400: loss: 3963.551
iteration 1500: loss: 3963.173
iteration 1600: loss: 3965.369
iteration 1700: loss: 3965.637
iteration 1800: loss: 3968.822
====> Epoch: 018 Train loss: 3963.4763  took : 53.29120993614197
====> Test loss: 3970.2282
iteration 0000: loss: 3963.441
iteration 0100: loss: 3972.646
iteration 0200: loss: 3971.230
iteration 0300: loss: 3953.887
iteration 0400: loss: 3966.316
iteration 0500: loss: 3971.303
iteration 0600: loss: 3957.606
iteration 0700: loss: 3960.078
iteration 0800: loss: 3959.018
iteration 0900: loss: 3970.710
iteration 1000: loss: 3971.189
iteration 1100: loss: 3971.497
iteration 1200: loss: 3959.718
iteration 1300: loss: 3966.306
iteration 1400: loss: 3956.662
iteration 1500: loss: 3960.984
iteration 1600: loss: 3963.348
iteration 1700: loss: 3965.274
iteration 1800: loss: 3962.081
====> Epoch: 019 Train loss: 3963.9053  took : 53.0076470375061
====> Test loss: 3970.6750
iteration 0000: loss: 3966.221
iteration 0100: loss: 3973.465
iteration 0200: loss: 3959.800
iteration 0300: loss: 3957.810
iteration 0400: loss: 3960.701
iteration 0500: loss: 3956.658
iteration 0600: loss: 3966.240
iteration 0700: loss: 3963.803
iteration 0800: loss: 3969.124
iteration 0900: loss: 3959.227
iteration 1000: loss: 3969.476
iteration 1100: loss: 3964.850
iteration 1200: loss: 3957.823
iteration 1300: loss: 3962.409
iteration 1400: loss: 3960.758
iteration 1500: loss: 3959.714
iteration 1600: loss: 3966.926
iteration 1700: loss: 3959.055
iteration 1800: loss: 3967.328
====> Epoch: 020 Train loss: 3963.2392  took : 53.09035587310791
====> Test loss: 3970.2517
iteration 0000: loss: 3969.209
iteration 0100: loss: 3964.989
iteration 0200: loss: 3963.523
iteration 0300: loss: 3961.355
iteration 0400: loss: 3960.964
iteration 0500: loss: 3957.102
iteration 0600: loss: 3964.352
iteration 0700: loss: 3970.068
iteration 0800: loss: 3965.564
iteration 0900: loss: 3964.794
iteration 1000: loss: 3967.676
iteration 1100: loss: 3977.339
iteration 1200: loss: 3959.832
iteration 1300: loss: 3968.046
iteration 1400: loss: 3961.596
iteration 1500: loss: 3960.322
iteration 1600: loss: 3967.799
iteration 1700: loss: 3967.472
iteration 1800: loss: 3961.629
====> Epoch: 021 Train loss: 3963.5023  took : 53.054811000823975
====> Test loss: 3970.6798
iteration 0000: loss: 3966.845
iteration 0100: loss: 3967.681
iteration 0200: loss: 3964.068
iteration 0300: loss: 3959.488
iteration 0400: loss: 3965.034
iteration 0500: loss: 3964.908
iteration 0600: loss: 3952.968
iteration 0700: loss: 3949.846
iteration 0800: loss: 3966.084
iteration 0900: loss: 3965.962
iteration 1000: loss: 3960.085
iteration 1100: loss: 3960.486
iteration 1200: loss: 3960.887
iteration 1300: loss: 3958.678
iteration 1400: loss: 3964.281
iteration 1500: loss: 3961.054
iteration 1600: loss: 3961.704
iteration 1700: loss: 3952.895
iteration 1800: loss: 3963.843
====> Epoch: 022 Train loss: 3962.9318  took : 52.97063899040222
====> Test loss: 3970.1292
iteration 0000: loss: 3966.208
iteration 0100: loss: 3961.754
iteration 0200: loss: 3961.051
iteration 0300: loss: 3972.928
iteration 0400: loss: 3970.552
iteration 0500: loss: 3959.714
iteration 0600: loss: 3965.867
iteration 0700: loss: 3967.477
iteration 0800: loss: 3959.191
iteration 0900: loss: 3968.038
iteration 1000: loss: 3958.013
iteration 1100: loss: 3958.479
iteration 1200: loss: 3958.091
iteration 1300: loss: 3974.336
iteration 1400: loss: 3964.838
iteration 1500: loss: 3966.831
iteration 1600: loss: 3959.331
iteration 1700: loss: 3961.336
iteration 1800: loss: 3956.681
====> Epoch: 023 Train loss: 3962.8455  took : 52.997716188430786
====> Test loss: 3970.2173
iteration 0000: loss: 3965.821
iteration 0100: loss: 3974.134
iteration 0200: loss: 3968.776
iteration 0300: loss: 3969.676
iteration 0400: loss: 3958.326
iteration 0500: loss: 3948.992
iteration 0600: loss: 3957.196
iteration 0700: loss: 3966.583
iteration 0800: loss: 3958.152
iteration 0900: loss: 3964.322
iteration 1000: loss: 3963.401
iteration 1100: loss: 3962.373
iteration 1200: loss: 3955.384
iteration 1300: loss: 3959.580
iteration 1400: loss: 3960.685
iteration 1500: loss: 3971.692
iteration 1600: loss: 3961.490
iteration 1700: loss: 3963.329
iteration 1800: loss: 3957.570
====> Epoch: 024 Train loss: 3962.6999  took : 52.74921727180481
====> Test loss: 3969.7450
iteration 0000: loss: 3964.826
iteration 0100: loss: 3967.530
iteration 0200: loss: 3955.175
iteration 0300: loss: 3962.560
iteration 0400: loss: 3960.136
iteration 0500: loss: 3958.722
iteration 0600: loss: 3966.180
iteration 0700: loss: 3961.502
iteration 0800: loss: 3958.663
iteration 0900: loss: 3958.345
iteration 1000: loss: 3972.460
iteration 1100: loss: 3970.994
iteration 1200: loss: 3963.679
iteration 1300: loss: 3957.703
iteration 1400: loss: 3955.911
iteration 1500: loss: 3959.108
iteration 1600: loss: 3963.177
iteration 1700: loss: 3966.041
iteration 1800: loss: 3966.338
====> Epoch: 025 Train loss: 3962.5433  took : 52.69083905220032
====> Test loss: 3970.5297
iteration 0000: loss: 3963.689
iteration 0100: loss: 3968.302
iteration 0200: loss: 3960.884
iteration 0300: loss: 3962.658
iteration 0400: loss: 3963.357
iteration 0500: loss: 3961.649
iteration 0600: loss: 3962.847
iteration 0700: loss: 3960.314
iteration 0800: loss: 3964.996
iteration 0900: loss: 3969.638
iteration 1000: loss: 3955.287
iteration 1100: loss: 3971.695
iteration 1200: loss: 3959.643
iteration 1300: loss: 3962.898
iteration 1400: loss: 3964.136
iteration 1500: loss: 3966.925
iteration 1600: loss: 3965.391
iteration 1700: loss: 3963.821
iteration 1800: loss: 3955.044
====> Epoch: 026 Train loss: 3962.5358  took : 52.93172335624695
====> Test loss: 3969.1018
iteration 0000: loss: 3956.370
iteration 0100: loss: 3962.525
iteration 0200: loss: 3964.614
iteration 0300: loss: 3974.113
iteration 0400: loss: 3964.262
iteration 0500: loss: 3957.270
iteration 0600: loss: 3963.686
iteration 0700: loss: 3965.812
iteration 0800: loss: 3959.817
iteration 0900: loss: 3961.362
iteration 1000: loss: 3959.025
iteration 1100: loss: 3967.608
iteration 1200: loss: 3963.935
iteration 1300: loss: 3958.864
iteration 1400: loss: 3958.091
iteration 1500: loss: 3960.075
iteration 1600: loss: 3962.414
iteration 1700: loss: 3956.854
iteration 1800: loss: 3952.998
====> Epoch: 027 Train loss: 3962.3430  took : 52.94438147544861
====> Test loss: 3970.0240
iteration 0000: loss: 3955.529
iteration 0100: loss: 3968.003
iteration 0200: loss: 3970.098
iteration 0300: loss: 3969.279
iteration 0400: loss: 3973.178
iteration 0500: loss: 3961.537
iteration 0600: loss: 3961.708
iteration 0700: loss: 3964.039
iteration 0800: loss: 3964.516
iteration 0900: loss: 3970.548
iteration 1000: loss: 3959.821
iteration 1100: loss: 3965.740
iteration 1200: loss: 3971.254
iteration 1300: loss: 3963.619
iteration 1400: loss: 3967.379
iteration 1500: loss: 3956.363
iteration 1600: loss: 3956.393
iteration 1700: loss: 3967.137
iteration 1800: loss: 3964.479
====> Epoch: 028 Train loss: 3962.5011  took : 53.42458534240723
====> Test loss: 3970.4880
iteration 0000: loss: 3971.147
iteration 0100: loss: 3977.870
iteration 0200: loss: 3958.498
iteration 0300: loss: 3960.760
iteration 0400: loss: 3963.387
iteration 0500: loss: 3964.924
iteration 0600: loss: 3956.994
iteration 0700: loss: 3961.547
iteration 0800: loss: 3963.555
iteration 0900: loss: 3963.624
iteration 1000: loss: 3967.410
iteration 1100: loss: 3952.000
iteration 1200: loss: 3956.716
iteration 1300: loss: 3957.726
iteration 1400: loss: 3958.361
iteration 1500: loss: 3963.581
iteration 1600: loss: 3967.782
iteration 1700: loss: 3954.918
iteration 1800: loss: 3970.965
====> Epoch: 029 Train loss: 3962.5976  took : 53.08516502380371
====> Test loss: 3969.8681
iteration 0000: loss: 3963.241
iteration 0100: loss: 3969.715
iteration 0200: loss: 3962.979
iteration 0300: loss: 3955.271
iteration 0400: loss: 3948.893
iteration 0500: loss: 3970.203
iteration 0600: loss: 3962.260
iteration 0700: loss: 3953.957
iteration 0800: loss: 3961.548
iteration 0900: loss: 3967.305
iteration 1000: loss: 3964.701
iteration 1100: loss: 3969.965
iteration 1200: loss: 3956.949
iteration 1300: loss: 3965.388
iteration 1400: loss: 3966.353
iteration 1500: loss: 3958.249
iteration 1600: loss: 3970.073
iteration 1700: loss: 3962.523
iteration 1800: loss: 3968.127
====> Epoch: 030 Train loss: 3961.8523  took : 53.249958515167236
====> Test loss: 3969.0859
iteration 0000: loss: 3961.487
iteration 0100: loss: 3959.728
iteration 0200: loss: 3960.970
iteration 0300: loss: 3957.242
iteration 0400: loss: 3961.418
iteration 0500: loss: 3967.305
iteration 0600: loss: 3958.296
iteration 0700: loss: 3966.690
iteration 0800: loss: 3951.816
iteration 0900: loss: 3966.445
iteration 1000: loss: 3959.708
iteration 1100: loss: 3962.091
iteration 1200: loss: 3963.758
iteration 1300: loss: 3967.580
iteration 1400: loss: 3968.833
iteration 1500: loss: 3970.177
iteration 1600: loss: 3962.061
iteration 1700: loss: 3959.743
iteration 1800: loss: 3958.555
====> Epoch: 031 Train loss: 3962.2256  took : 53.00617074966431
====> Test loss: 3969.2574
iteration 0000: loss: 3962.019
iteration 0100: loss: 3963.046
iteration 0200: loss: 3960.540
iteration 0300: loss: 3958.208
iteration 0400: loss: 3955.433
iteration 0500: loss: 3963.786
iteration 0600: loss: 3952.560
iteration 0700: loss: 3962.641
iteration 0800: loss: 3954.688
iteration 0900: loss: 3966.592
iteration 1000: loss: 3967.276
iteration 1100: loss: 3954.153
iteration 1200: loss: 3963.391
iteration 1300: loss: 3972.716
iteration 1400: loss: 3956.300
iteration 1500: loss: 3972.467
iteration 1600: loss: 3971.062
iteration 1700: loss: 3951.972
iteration 1800: loss: 3958.533
====> Epoch: 032 Train loss: 3961.7810  took : 52.7157027721405
====> Test loss: 3969.3954
iteration 0000: loss: 3964.748
iteration 0100: loss: 3963.157
iteration 0200: loss: 3959.279
iteration 0300: loss: 3968.683
iteration 0400: loss: 3959.449
iteration 0500: loss: 3966.677
iteration 0600: loss: 3966.586
iteration 0700: loss: 3967.561
iteration 0800: loss: 3966.539
iteration 0900: loss: 3958.359
iteration 1000: loss: 3963.372
iteration 1100: loss: 3960.010
iteration 1200: loss: 3965.991
iteration 1300: loss: 3958.792
iteration 1400: loss: 3964.056
iteration 1500: loss: 3962.901
iteration 1600: loss: 3964.574
iteration 1700: loss: 3970.968
iteration 1800: loss: 3965.947
====> Epoch: 033 Train loss: 3962.2714  took : 52.927714347839355
====> Test loss: 3970.2268
iteration 0000: loss: 3960.812
iteration 0100: loss: 3959.312
iteration 0200: loss: 3957.841
iteration 0300: loss: 3958.212
iteration 0400: loss: 3955.163
iteration 0500: loss: 3970.358
iteration 0600: loss: 3954.742
iteration 0700: loss: 3967.338
iteration 0800: loss: 3957.632
iteration 0900: loss: 3977.567
iteration 1000: loss: 3970.766
iteration 1100: loss: 3962.900
iteration 1200: loss: 3961.810
iteration 1300: loss: 3970.536
iteration 1400: loss: 3952.096
iteration 1500: loss: 3963.373
iteration 1600: loss: 3965.569
iteration 1700: loss: 3962.054
iteration 1800: loss: 3956.535
====> Epoch: 034 Train loss: 3962.2812  took : 52.868841886520386
====> Test loss: 3968.6512
iteration 0000: loss: 3965.161
iteration 0100: loss: 3970.536
iteration 0200: loss: 3955.785
iteration 0300: loss: 3956.913
iteration 0400: loss: 3956.573
iteration 0500: loss: 3965.680
iteration 0600: loss: 3960.450
iteration 0700: loss: 3963.391
iteration 0800: loss: 3966.816
iteration 0900: loss: 3964.606
iteration 1000: loss: 3962.837
iteration 1100: loss: 3963.254
iteration 1200: loss: 3961.249
iteration 1300: loss: 3953.978
iteration 1400: loss: 3958.355
iteration 1500: loss: 3949.852
iteration 1600: loss: 3957.361
iteration 1700: loss: 3958.957
iteration 1800: loss: 3962.698
====> Epoch: 035 Train loss: 3962.3527  took : 53.063467502593994
====> Test loss: 3970.1668
iteration 0000: loss: 3965.256
iteration 0100: loss: 3962.323
iteration 0200: loss: 3956.109
iteration 0300: loss: 3965.352
iteration 0400: loss: 3961.233
iteration 0500: loss: 3962.517
iteration 0600: loss: 3961.995
iteration 0700: loss: 3961.647
iteration 0800: loss: 3958.613
iteration 0900: loss: 3960.195
iteration 1000: loss: 3962.638
iteration 1100: loss: 3957.249
iteration 1200: loss: 3961.146
iteration 1300: loss: 3961.952
iteration 1400: loss: 3962.301
iteration 1500: loss: 3961.760
iteration 1600: loss: 3962.917
iteration 1700: loss: 3961.420
iteration 1800: loss: 3956.828
====> Epoch: 036 Train loss: 3961.8664  took : 52.91619610786438
====> Test loss: 3969.9135
iteration 0000: loss: 3957.597
iteration 0100: loss: 3960.524
iteration 0200: loss: 3951.091
iteration 0300: loss: 3964.997
iteration 0400: loss: 3970.749
iteration 0500: loss: 3962.092
iteration 0600: loss: 3965.081
iteration 0700: loss: 3961.946
iteration 0800: loss: 3956.658
iteration 0900: loss: 3956.592
iteration 1000: loss: 3956.173
iteration 1100: loss: 3955.166
iteration 1200: loss: 3962.965
iteration 1300: loss: 3952.274
iteration 1400: loss: 3959.703
iteration 1500: loss: 3956.471
iteration 1600: loss: 3962.230
iteration 1700: loss: 3958.750
iteration 1800: loss: 3970.264
====> Epoch: 037 Train loss: 3961.8666  took : 53.09381628036499
====> Test loss: 3971.3067
iteration 0000: loss: 3955.039
iteration 0100: loss: 3955.830
iteration 0200: loss: 3954.770
iteration 0300: loss: 3960.955
iteration 0400: loss: 3949.506
iteration 0500: loss: 3973.280
iteration 0600: loss: 3971.128
iteration 0700: loss: 3951.054
iteration 0800: loss: 3963.221
iteration 0900: loss: 3963.990
iteration 1000: loss: 3966.963
iteration 1100: loss: 3963.537
iteration 1200: loss: 3972.760
iteration 1300: loss: 3955.656
iteration 1400: loss: 3960.358
iteration 1500: loss: 3969.652
iteration 1600: loss: 3965.933
iteration 1700: loss: 3964.322
iteration 1800: loss: 3965.933
====> Epoch: 038 Train loss: 3962.1406  took : 52.91494560241699
====> Test loss: 3969.2417
iteration 0000: loss: 3965.445
iteration 0100: loss: 3958.129
iteration 0200: loss: 3965.863
iteration 0300: loss: 3960.342
iteration 0400: loss: 3953.162
iteration 0500: loss: 3957.946
iteration 0600: loss: 3959.806
iteration 0700: loss: 3956.863
iteration 0800: loss: 3958.364
iteration 0900: loss: 3957.906
iteration 1000: loss: 3960.095
iteration 1100: loss: 3950.088
iteration 1200: loss: 3956.456
iteration 1300: loss: 3956.281
iteration 1400: loss: 3971.831
iteration 1500: loss: 3968.653
iteration 1600: loss: 3958.624
iteration 1700: loss: 3962.218
iteration 1800: loss: 3961.698
====> Epoch: 039 Train loss: 3960.9611  took : 53.09674286842346
====> Test loss: 3968.9996
iteration 0000: loss: 3953.542
iteration 0100: loss: 3959.023
iteration 0200: loss: 3953.155
iteration 0300: loss: 3960.207
iteration 0400: loss: 3963.264
iteration 0500: loss: 3958.534
iteration 0600: loss: 3954.734
iteration 0700: loss: 3963.040
iteration 0800: loss: 3950.086
iteration 0900: loss: 3961.628
iteration 1000: loss: 3963.755
iteration 1100: loss: 3953.482
iteration 1200: loss: 3956.864
iteration 1300: loss: 3968.254
iteration 1400: loss: 3957.907
iteration 1500: loss: 3957.950
iteration 1600: loss: 3957.031
iteration 1700: loss: 3970.846
iteration 1800: loss: 3955.143
====> Epoch: 040 Train loss: 3961.1490  took : 53.10385513305664
====> Test loss: 3969.0279
iteration 0000: loss: 3957.123
iteration 0100: loss: 3959.217
iteration 0200: loss: 3961.542
iteration 0300: loss: 3966.073
iteration 0400: loss: 3963.959
iteration 0500: loss: 3961.227
iteration 0600: loss: 3953.330
iteration 0700: loss: 3967.955
iteration 0800: loss: 3964.541
iteration 0900: loss: 3962.317
iteration 1000: loss: 3964.159
iteration 1100: loss: 3961.986
iteration 1200: loss: 3967.019
iteration 1300: loss: 3959.927
iteration 1400: loss: 3967.486
iteration 1500: loss: 3965.633
iteration 1600: loss: 3969.635
iteration 1700: loss: 3966.620
iteration 1800: loss: 3959.001
====> Epoch: 041 Train loss: 3961.0228  took : 52.96337127685547
====> Test loss: 3969.7420
iteration 0000: loss: 3955.774
iteration 0100: loss: 3955.598
iteration 0200: loss: 3954.492
iteration 0300: loss: 3964.589
iteration 0400: loss: 3965.357
iteration 0500: loss: 3958.526
iteration 0600: loss: 3956.140
iteration 0700: loss: 3968.649
iteration 0800: loss: 3954.107
iteration 0900: loss: 3966.136
iteration 1000: loss: 3964.462
iteration 1100: loss: 3963.730
iteration 1200: loss: 3961.726
iteration 1300: loss: 3966.221
iteration 1400: loss: 3960.812
iteration 1500: loss: 3962.729
iteration 1600: loss: 3963.717
iteration 1700: loss: 3962.567
iteration 1800: loss: 3962.562
====> Epoch: 042 Train loss: 3961.3186  took : 53.10682034492493
====> Test loss: 3969.3530
iteration 0000: loss: 3957.641
iteration 0100: loss: 3966.550
iteration 0200: loss: 3959.048
iteration 0300: loss: 3955.195
iteration 0400: loss: 3967.490
iteration 0500: loss: 3961.898
iteration 0600: loss: 3969.837
iteration 0700: loss: 3958.940
iteration 0800: loss: 3950.359
iteration 0900: loss: 3966.579
iteration 1000: loss: 3958.660
iteration 1100: loss: 3967.921
iteration 1200: loss: 3959.069
iteration 1300: loss: 3968.088
iteration 1400: loss: 3964.255
iteration 1500: loss: 3951.447
iteration 1600: loss: 3962.363
iteration 1700: loss: 3953.285
iteration 1800: loss: 3952.405
====> Epoch: 043 Train loss: 3961.0219  took : 53.225322246551514
====> Test loss: 3969.0869
iteration 0000: loss: 3957.290
iteration 0100: loss: 3948.613
iteration 0200: loss: 3961.709
iteration 0300: loss: 3959.892
iteration 0400: loss: 3957.449
iteration 0500: loss: 3959.489
iteration 0600: loss: 3957.319
iteration 0700: loss: 3950.478
iteration 0800: loss: 3969.252
iteration 0900: loss: 3957.915
iteration 1000: loss: 3954.171
iteration 1100: loss: 3956.281
iteration 1200: loss: 3949.974
iteration 1300: loss: 3959.798
iteration 1400: loss: 3958.075
iteration 1500: loss: 3970.240
iteration 1600: loss: 3961.260
iteration 1700: loss: 3956.308
iteration 1800: loss: 3970.968
====> Epoch: 044 Train loss: 3960.8642  took : 53.06339764595032
====> Test loss: 3968.4695
iteration 0000: loss: 3957.657
iteration 0100: loss: 3962.840
iteration 0200: loss: 3952.993
iteration 0300: loss: 3960.348
iteration 0400: loss: 3952.834
iteration 0500: loss: 3956.173
iteration 0600: loss: 3965.468
iteration 0700: loss: 3955.196
iteration 0800: loss: 3961.456
iteration 0900: loss: 3962.838
iteration 1000: loss: 3965.855
iteration 1100: loss: 3959.612
iteration 1200: loss: 3960.957
iteration 1300: loss: 3959.153
iteration 1400: loss: 3966.672
iteration 1500: loss: 3968.965
iteration 1600: loss: 3956.826
iteration 1700: loss: 3961.087
iteration 1800: loss: 3971.111
====> Epoch: 045 Train loss: 3960.5837  took : 53.022656202316284
====> Test loss: 3969.2533
iteration 0000: loss: 3968.964
iteration 0100: loss: 3954.757
iteration 0200: loss: 3955.261
iteration 0300: loss: 3956.602
iteration 0400: loss: 3960.906
iteration 0500: loss: 3971.758
iteration 0600: loss: 3954.983
iteration 0700: loss: 3963.577
iteration 0800: loss: 3964.910
iteration 0900: loss: 3964.119
iteration 1000: loss: 3959.891
iteration 1100: loss: 3950.482
iteration 1200: loss: 3963.919
iteration 1300: loss: 3950.138
iteration 1400: loss: 3958.439
iteration 1500: loss: 3965.415
iteration 1600: loss: 3964.088
iteration 1700: loss: 3960.236
iteration 1800: loss: 3964.795
====> Epoch: 046 Train loss: 3960.4489  took : 53.09397530555725
====> Test loss: 3968.6339
iteration 0000: loss: 3960.033
iteration 0100: loss: 3957.855
iteration 0200: loss: 3960.239
iteration 0300: loss: 3956.443
iteration 0400: loss: 3958.618
iteration 0500: loss: 3960.017
iteration 0600: loss: 3963.828
iteration 0700: loss: 3963.249
iteration 0800: loss: 3957.593
iteration 0900: loss: 3964.155
iteration 1000: loss: 3963.677
iteration 1100: loss: 3957.395
iteration 1200: loss: 3961.219
iteration 1300: loss: 3959.632
iteration 1400: loss: 3955.864
iteration 1500: loss: 3971.318
iteration 1600: loss: 3960.821
iteration 1700: loss: 3966.586
iteration 1800: loss: 3957.493
====> Epoch: 047 Train loss: 3960.5197  took : 52.76218390464783
====> Test loss: 3968.8775
iteration 0000: loss: 3956.454
iteration 0100: loss: 3957.804
iteration 0200: loss: 3957.765
iteration 0300: loss: 3961.053
iteration 0400: loss: 3963.404
iteration 0500: loss: 3959.183
iteration 0600: loss: 3959.885
iteration 0700: loss: 3961.542
iteration 0800: loss: 3964.284
iteration 0900: loss: 3969.347
iteration 1000: loss: 3961.358
iteration 1100: loss: 3965.413
iteration 1200: loss: 3963.665
iteration 1300: loss: 3956.439
iteration 1400: loss: 3957.904
iteration 1500: loss: 3968.658
iteration 1600: loss: 3965.145
iteration 1700: loss: 3967.530
iteration 1800: loss: 3959.336
====> Epoch: 048 Train loss: 3960.8819  took : 52.91248178482056
====> Test loss: 3968.2507
iteration 0000: loss: 3965.513
iteration 0100: loss: 3957.623
iteration 0200: loss: 3964.041
iteration 0300: loss: 3953.503
iteration 0400: loss: 3960.040
iteration 0500: loss: 3957.191
iteration 0600: loss: 3966.787
iteration 0700: loss: 3961.814
iteration 0800: loss: 3963.933
iteration 0900: loss: 3967.552
iteration 1000: loss: 3966.378
iteration 1100: loss: 3969.804
iteration 1200: loss: 3957.688
iteration 1300: loss: 3959.267
iteration 1400: loss: 3963.714
iteration 1500: loss: 3959.915
iteration 1600: loss: 3953.507
iteration 1700: loss: 3962.489
iteration 1800: loss: 3953.715
====> Epoch: 049 Train loss: 3960.1343  took : 53.265376329422
====> Test loss: 3969.0357
iteration 0000: loss: 3957.413
iteration 0100: loss: 3962.414
iteration 0200: loss: 3961.253
iteration 0300: loss: 3962.021
iteration 0400: loss: 3966.757
iteration 0500: loss: 3953.986
iteration 0600: loss: 3953.285
iteration 0700: loss: 3952.987
iteration 0800: loss: 3958.812
iteration 0900: loss: 3962.369
iteration 1000: loss: 3957.832
iteration 1100: loss: 3961.617
iteration 1200: loss: 3959.751
iteration 1300: loss: 3952.571
iteration 1400: loss: 3953.063
iteration 1500: loss: 3963.309
iteration 1600: loss: 3966.669
iteration 1700: loss: 3958.513
iteration 1800: loss: 3962.818
====> Epoch: 050 Train loss: 3960.8529  took : 53.034250020980835
====> Test loss: 3969.2263
====> [MM-VAE] Time: 3148.357s or 00:52:28
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  18
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_18
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_18
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1988.637
iteration 0100: loss: 1573.687
iteration 0200: loss: 1562.088
iteration 0300: loss: 1554.341
iteration 0400: loss: 1546.549
iteration 0500: loss: 1544.296
iteration 0600: loss: 1543.097
iteration 0700: loss: 1541.536
iteration 0800: loss: 1542.440
iteration 0900: loss: 1537.420
====> Epoch: 001 Train loss: 1555.8711  took : 8.470056295394897
====> Test loss: 1539.3417
iteration 0000: loss: 1544.391
iteration 0100: loss: 1534.717
iteration 0200: loss: 1536.508
iteration 0300: loss: 1534.597
iteration 0400: loss: 1532.187
iteration 0500: loss: 1536.104
iteration 0600: loss: 1528.404
iteration 0700: loss: 1527.903
iteration 0800: loss: 1529.368
iteration 0900: loss: 1535.372
====> Epoch: 002 Train loss: 1532.9642  took : 8.4238440990448
====> Test loss: 1530.9419
iteration 0000: loss: 1529.398
iteration 0100: loss: 1526.244
iteration 0200: loss: 1529.517
iteration 0300: loss: 1528.302
iteration 0400: loss: 1529.728
iteration 0500: loss: 1524.953
iteration 0600: loss: 1526.625
iteration 0700: loss: 1523.393
iteration 0800: loss: 1523.903
iteration 0900: loss: 1524.186
====> Epoch: 003 Train loss: 1527.4097  took : 8.531584739685059
====> Test loss: 1527.9254
iteration 0000: loss: 1526.514
iteration 0100: loss: 1529.457
iteration 0200: loss: 1524.780
iteration 0300: loss: 1523.450
iteration 0400: loss: 1527.789
iteration 0500: loss: 1523.709
iteration 0600: loss: 1526.511
iteration 0700: loss: 1523.322
iteration 0800: loss: 1523.355
iteration 0900: loss: 1523.744
====> Epoch: 004 Train loss: 1524.8436  took : 8.487730503082275
====> Test loss: 1525.8840
iteration 0000: loss: 1522.733
iteration 0100: loss: 1525.508
iteration 0200: loss: 1523.752
iteration 0300: loss: 1522.687
iteration 0400: loss: 1523.651
iteration 0500: loss: 1521.969
iteration 0600: loss: 1523.208
iteration 0700: loss: 1523.042
iteration 0800: loss: 1524.366
iteration 0900: loss: 1523.799
====> Epoch: 005 Train loss: 1523.3158  took : 8.549575090408325
====> Test loss: 1524.9247
iteration 0000: loss: 1525.747
iteration 0100: loss: 1522.251
iteration 0200: loss: 1523.260
iteration 0300: loss: 1524.308
iteration 0400: loss: 1523.328
iteration 0500: loss: 1526.897
iteration 0600: loss: 1521.669
iteration 0700: loss: 1519.291
iteration 0800: loss: 1524.330
iteration 0900: loss: 1523.636
====> Epoch: 006 Train loss: 1522.1396  took : 8.412379264831543
====> Test loss: 1523.9589
iteration 0000: loss: 1524.628
iteration 0100: loss: 1522.994
iteration 0200: loss: 1524.219
iteration 0300: loss: 1520.937
iteration 0400: loss: 1524.014
iteration 0500: loss: 1518.781
iteration 0600: loss: 1521.299
iteration 0700: loss: 1519.858
iteration 0800: loss: 1517.807
iteration 0900: loss: 1522.477
====> Epoch: 007 Train loss: 1521.2350  took : 8.50758171081543
====> Test loss: 1523.1087
iteration 0000: loss: 1522.968
iteration 0100: loss: 1518.880
iteration 0200: loss: 1523.812
iteration 0300: loss: 1521.487
iteration 0400: loss: 1522.978
iteration 0500: loss: 1517.798
iteration 0600: loss: 1517.853
iteration 0700: loss: 1520.702
iteration 0800: loss: 1521.179
iteration 0900: loss: 1523.343
====> Epoch: 008 Train loss: 1520.5550  took : 8.518355131149292
====> Test loss: 1522.6754
iteration 0000: loss: 1518.123
iteration 0100: loss: 1519.477
iteration 0200: loss: 1518.484
iteration 0300: loss: 1519.490
iteration 0400: loss: 1521.939
iteration 0500: loss: 1521.424
iteration 0600: loss: 1518.996
iteration 0700: loss: 1517.094
iteration 0800: loss: 1516.403
iteration 0900: loss: 1520.323
====> Epoch: 009 Train loss: 1519.9947  took : 8.459772825241089
====> Test loss: 1522.1857
iteration 0000: loss: 1519.206
iteration 0100: loss: 1521.683
iteration 0200: loss: 1519.758
iteration 0300: loss: 1515.715
iteration 0400: loss: 1519.009
iteration 0500: loss: 1521.772
iteration 0600: loss: 1519.647
iteration 0700: loss: 1518.162
iteration 0800: loss: 1522.272
iteration 0900: loss: 1520.033
====> Epoch: 010 Train loss: 1519.5117  took : 8.437772989273071
====> Test loss: 1521.7764
iteration 0000: loss: 1518.147
iteration 0100: loss: 1520.721
iteration 0200: loss: 1521.902
iteration 0300: loss: 1516.632
iteration 0400: loss: 1517.049
iteration 0500: loss: 1518.022
iteration 0600: loss: 1519.366
iteration 0700: loss: 1518.338
iteration 0800: loss: 1520.085
iteration 0900: loss: 1520.372
====> Epoch: 011 Train loss: 1519.0950  took : 8.39064884185791
====> Test loss: 1521.5685
iteration 0000: loss: 1516.963
iteration 0100: loss: 1519.055
iteration 0200: loss: 1519.829
iteration 0300: loss: 1518.609
iteration 0400: loss: 1520.801
iteration 0500: loss: 1522.398
iteration 0600: loss: 1518.863
iteration 0700: loss: 1519.931
iteration 0800: loss: 1518.886
iteration 0900: loss: 1517.516
====> Epoch: 012 Train loss: 1518.7443  took : 8.36491322517395
====> Test loss: 1521.1551
iteration 0000: loss: 1517.063
iteration 0100: loss: 1520.162
iteration 0200: loss: 1517.364
iteration 0300: loss: 1519.012
iteration 0400: loss: 1517.773
iteration 0500: loss: 1518.554
iteration 0600: loss: 1516.264
iteration 0700: loss: 1520.373
iteration 0800: loss: 1518.135
iteration 0900: loss: 1520.942
====> Epoch: 013 Train loss: 1518.4573  took : 8.474075317382812
====> Test loss: 1520.9817
iteration 0000: loss: 1517.598
iteration 0100: loss: 1518.851
iteration 0200: loss: 1515.731
iteration 0300: loss: 1517.008
iteration 0400: loss: 1522.623
iteration 0500: loss: 1520.091
iteration 0600: loss: 1517.942
iteration 0700: loss: 1519.737
iteration 0800: loss: 1517.681
iteration 0900: loss: 1517.288
====> Epoch: 014 Train loss: 1518.2251  took : 8.409904956817627
====> Test loss: 1520.7417
iteration 0000: loss: 1519.089
iteration 0100: loss: 1518.886
iteration 0200: loss: 1516.483
iteration 0300: loss: 1520.566
iteration 0400: loss: 1518.895
iteration 0500: loss: 1515.444
iteration 0600: loss: 1515.623
iteration 0700: loss: 1518.041
iteration 0800: loss: 1516.458
iteration 0900: loss: 1517.510
====> Epoch: 015 Train loss: 1517.9604  took : 8.398154735565186
====> Test loss: 1520.7999
iteration 0000: loss: 1517.486
iteration 0100: loss: 1517.354
iteration 0200: loss: 1519.869
iteration 0300: loss: 1517.997
iteration 0400: loss: 1515.816
iteration 0500: loss: 1516.761
iteration 0600: loss: 1519.166
iteration 0700: loss: 1518.022
iteration 0800: loss: 1518.161
iteration 0900: loss: 1520.896
====> Epoch: 016 Train loss: 1517.7681  took : 8.45019245147705
====> Test loss: 1520.4878
iteration 0000: loss: 1519.475
iteration 0100: loss: 1517.192
iteration 0200: loss: 1517.963
iteration 0300: loss: 1519.238
iteration 0400: loss: 1516.927
iteration 0500: loss: 1517.944
iteration 0600: loss: 1519.118
iteration 0700: loss: 1518.155
iteration 0800: loss: 1520.723
iteration 0900: loss: 1517.639
====> Epoch: 017 Train loss: 1517.5777  took : 8.410564661026001
====> Test loss: 1520.5718
iteration 0000: loss: 1517.416
iteration 0100: loss: 1518.370
iteration 0200: loss: 1519.351
iteration 0300: loss: 1517.686
iteration 0400: loss: 1516.410
iteration 0500: loss: 1518.215
iteration 0600: loss: 1516.694
iteration 0700: loss: 1518.943
iteration 0800: loss: 1518.470
iteration 0900: loss: 1519.782
====> Epoch: 018 Train loss: 1517.3894  took : 8.50266170501709
====> Test loss: 1520.1255
iteration 0000: loss: 1518.228
iteration 0100: loss: 1516.455
iteration 0200: loss: 1518.220
iteration 0300: loss: 1518.644
iteration 0400: loss: 1516.302
iteration 0500: loss: 1515.641
iteration 0600: loss: 1517.267
iteration 0700: loss: 1515.913
iteration 0800: loss: 1518.344
iteration 0900: loss: 1521.021
====> Epoch: 019 Train loss: 1517.1939  took : 8.483798742294312
====> Test loss: 1519.9305
iteration 0000: loss: 1517.665
iteration 0100: loss: 1518.336
iteration 0200: loss: 1516.246
iteration 0300: loss: 1517.502
iteration 0400: loss: 1517.617
iteration 0500: loss: 1516.458
iteration 0600: loss: 1515.326
iteration 0700: loss: 1514.854
iteration 0800: loss: 1516.998
iteration 0900: loss: 1515.433
====> Epoch: 020 Train loss: 1517.1059  took : 8.316234827041626
====> Test loss: 1519.9604
iteration 0000: loss: 1518.736
iteration 0100: loss: 1515.642
iteration 0200: loss: 1519.293
iteration 0300: loss: 1516.646
iteration 0400: loss: 1515.087
iteration 0500: loss: 1518.678
iteration 0600: loss: 1519.864
iteration 0700: loss: 1518.397
iteration 0800: loss: 1518.921
iteration 0900: loss: 1514.772
====> Epoch: 021 Train loss: 1516.9418  took : 8.569194555282593
====> Test loss: 1519.7395
iteration 0000: loss: 1518.972
iteration 0100: loss: 1518.488
iteration 0200: loss: 1515.922
iteration 0300: loss: 1517.773
iteration 0400: loss: 1519.752
iteration 0500: loss: 1515.844
iteration 0600: loss: 1514.176
iteration 0700: loss: 1516.037
iteration 0800: loss: 1518.461
iteration 0900: loss: 1515.195
====> Epoch: 022 Train loss: 1516.7963  took : 8.516303062438965
====> Test loss: 1519.7471
iteration 0000: loss: 1519.235
iteration 0100: loss: 1518.220
iteration 0200: loss: 1518.840
iteration 0300: loss: 1517.326
iteration 0400: loss: 1514.864
iteration 0500: loss: 1518.551
iteration 0600: loss: 1515.552
iteration 0700: loss: 1517.354
iteration 0800: loss: 1514.920
iteration 0900: loss: 1519.108
====> Epoch: 023 Train loss: 1516.6735  took : 8.522627353668213
====> Test loss: 1519.7739
iteration 0000: loss: 1519.041
iteration 0100: loss: 1514.641
iteration 0200: loss: 1515.468
iteration 0300: loss: 1515.008
iteration 0400: loss: 1515.348
iteration 0500: loss: 1517.757
iteration 0600: loss: 1517.055
iteration 0700: loss: 1516.528
iteration 0800: loss: 1516.663
iteration 0900: loss: 1517.548
====> Epoch: 024 Train loss: 1516.5961  took : 8.446415424346924
====> Test loss: 1519.5096
iteration 0000: loss: 1516.459
iteration 0100: loss: 1517.307
iteration 0200: loss: 1518.333
iteration 0300: loss: 1516.762
iteration 0400: loss: 1516.922
iteration 0500: loss: 1518.176
iteration 0600: loss: 1519.415
iteration 0700: loss: 1518.106
iteration 0800: loss: 1518.366
iteration 0900: loss: 1515.329
====> Epoch: 025 Train loss: 1516.4791  took : 8.504497051239014
====> Test loss: 1519.3445
iteration 0000: loss: 1515.499
iteration 0100: loss: 1516.646
iteration 0200: loss: 1514.288
iteration 0300: loss: 1515.006
iteration 0400: loss: 1516.019
iteration 0500: loss: 1515.537
iteration 0600: loss: 1516.677
iteration 0700: loss: 1519.811
iteration 0800: loss: 1516.654
iteration 0900: loss: 1515.573
====> Epoch: 026 Train loss: 1516.3591  took : 8.407370567321777
====> Test loss: 1519.4149
iteration 0000: loss: 1516.227
iteration 0100: loss: 1513.444
iteration 0200: loss: 1514.367
iteration 0300: loss: 1515.599
iteration 0400: loss: 1515.635
iteration 0500: loss: 1513.149
iteration 0600: loss: 1515.448
iteration 0700: loss: 1513.019
iteration 0800: loss: 1518.860
iteration 0900: loss: 1516.312
====> Epoch: 027 Train loss: 1516.3032  took : 8.417766809463501
====> Test loss: 1519.2606
iteration 0000: loss: 1517.668
iteration 0100: loss: 1516.233
iteration 0200: loss: 1516.674
iteration 0300: loss: 1516.716
iteration 0400: loss: 1516.638
iteration 0500: loss: 1514.228
iteration 0600: loss: 1517.844
iteration 0700: loss: 1514.006
iteration 0800: loss: 1517.722
iteration 0900: loss: 1518.589
====> Epoch: 028 Train loss: 1516.2338  took : 8.537824153900146
====> Test loss: 1519.3008
iteration 0000: loss: 1516.070
iteration 0100: loss: 1516.573
iteration 0200: loss: 1517.633
iteration 0300: loss: 1514.549
iteration 0400: loss: 1514.965
iteration 0500: loss: 1516.627
iteration 0600: loss: 1515.373
iteration 0700: loss: 1516.563
iteration 0800: loss: 1519.532
iteration 0900: loss: 1516.165
====> Epoch: 029 Train loss: 1516.1462  took : 8.539551019668579
====> Test loss: 1519.1376
iteration 0000: loss: 1513.541
iteration 0100: loss: 1513.555
iteration 0200: loss: 1517.406
iteration 0300: loss: 1519.807
iteration 0400: loss: 1515.902
iteration 0500: loss: 1516.222
iteration 0600: loss: 1516.258
iteration 0700: loss: 1516.986
iteration 0800: loss: 1516.070
iteration 0900: loss: 1517.256
====> Epoch: 030 Train loss: 1516.0296  took : 8.44919204711914
====> Test loss: 1519.1737
iteration 0000: loss: 1513.750
iteration 0100: loss: 1512.508
iteration 0200: loss: 1515.539
iteration 0300: loss: 1514.253
iteration 0400: loss: 1513.884
iteration 0500: loss: 1514.568
iteration 0600: loss: 1515.010
iteration 0700: loss: 1515.413
iteration 0800: loss: 1515.022
iteration 0900: loss: 1516.226
====> Epoch: 031 Train loss: 1515.9973  took : 8.438627481460571
====> Test loss: 1519.0670
iteration 0000: loss: 1515.213
iteration 0100: loss: 1515.695
iteration 0200: loss: 1517.577
iteration 0300: loss: 1514.330
iteration 0400: loss: 1514.792
iteration 0500: loss: 1514.498
iteration 0600: loss: 1514.576
iteration 0700: loss: 1514.438
iteration 0800: loss: 1517.524
iteration 0900: loss: 1513.256
====> Epoch: 032 Train loss: 1515.8756  took : 8.46301531791687
====> Test loss: 1518.9993
iteration 0000: loss: 1514.067
iteration 0100: loss: 1517.678
iteration 0200: loss: 1514.887
iteration 0300: loss: 1516.956
iteration 0400: loss: 1514.727
iteration 0500: loss: 1515.514
iteration 0600: loss: 1516.721
iteration 0700: loss: 1519.445
iteration 0800: loss: 1518.198
iteration 0900: loss: 1512.482
====> Epoch: 033 Train loss: 1515.8206  took : 8.527339458465576
====> Test loss: 1518.9208
iteration 0000: loss: 1517.344
iteration 0100: loss: 1518.212
iteration 0200: loss: 1515.781
iteration 0300: loss: 1515.236
iteration 0400: loss: 1515.456
iteration 0500: loss: 1516.596
iteration 0600: loss: 1518.864
iteration 0700: loss: 1516.483
iteration 0800: loss: 1517.038
iteration 0900: loss: 1517.429
====> Epoch: 034 Train loss: 1515.7526  took : 8.556787967681885
====> Test loss: 1518.8319
iteration 0000: loss: 1518.427
iteration 0100: loss: 1515.244
iteration 0200: loss: 1515.080
iteration 0300: loss: 1512.753
iteration 0400: loss: 1516.620
iteration 0500: loss: 1515.737
iteration 0600: loss: 1513.667
iteration 0700: loss: 1518.736
iteration 0800: loss: 1513.841
iteration 0900: loss: 1515.122
====> Epoch: 035 Train loss: 1515.6523  took : 8.52567744255066
====> Test loss: 1519.0140
iteration 0000: loss: 1513.732
iteration 0100: loss: 1517.318
iteration 0200: loss: 1514.270
iteration 0300: loss: 1515.496
iteration 0400: loss: 1515.771
iteration 0500: loss: 1516.728
iteration 0600: loss: 1514.631
iteration 0700: loss: 1516.854
iteration 0800: loss: 1515.751
iteration 0900: loss: 1511.450
====> Epoch: 036 Train loss: 1515.6145  took : 8.509172677993774
====> Test loss: 1518.8221
iteration 0000: loss: 1517.687
iteration 0100: loss: 1515.375
iteration 0200: loss: 1516.275
iteration 0300: loss: 1512.489
iteration 0400: loss: 1513.009
iteration 0500: loss: 1514.292
iteration 0600: loss: 1515.397
iteration 0700: loss: 1514.856
iteration 0800: loss: 1517.382
iteration 0900: loss: 1512.909
====> Epoch: 037 Train loss: 1515.5459  took : 8.543058156967163
====> Test loss: 1518.7954
iteration 0000: loss: 1515.516
iteration 0100: loss: 1516.086
iteration 0200: loss: 1517.646
iteration 0300: loss: 1514.407
iteration 0400: loss: 1512.642
iteration 0500: loss: 1513.659
iteration 0600: loss: 1514.738
iteration 0700: loss: 1515.148
iteration 0800: loss: 1514.934
iteration 0900: loss: 1515.497
====> Epoch: 038 Train loss: 1515.4882  took : 8.426873207092285
====> Test loss: 1518.7835
iteration 0000: loss: 1515.235
iteration 0100: loss: 1512.744
iteration 0200: loss: 1513.443
iteration 0300: loss: 1512.997
iteration 0400: loss: 1515.456
iteration 0500: loss: 1514.923
iteration 0600: loss: 1517.083
iteration 0700: loss: 1516.737
iteration 0800: loss: 1514.794
iteration 0900: loss: 1514.447
====> Epoch: 039 Train loss: 1515.4492  took : 8.481389045715332
====> Test loss: 1518.6284
iteration 0000: loss: 1517.533
iteration 0100: loss: 1515.896
iteration 0200: loss: 1513.095
iteration 0300: loss: 1515.263
iteration 0400: loss: 1517.862
iteration 0500: loss: 1516.296
iteration 0600: loss: 1515.194
iteration 0700: loss: 1514.670
iteration 0800: loss: 1514.560
iteration 0900: loss: 1517.773
====> Epoch: 040 Train loss: 1515.3496  took : 8.530126571655273
====> Test loss: 1518.5596
iteration 0000: loss: 1517.082
iteration 0100: loss: 1515.960
iteration 0200: loss: 1516.408
iteration 0300: loss: 1513.831
iteration 0400: loss: 1514.858
iteration 0500: loss: 1514.745
iteration 0600: loss: 1514.695
iteration 0700: loss: 1514.841
iteration 0800: loss: 1516.136
iteration 0900: loss: 1514.463
====> Epoch: 041 Train loss: 1515.3567  took : 8.475767374038696
====> Test loss: 1518.5348
iteration 0000: loss: 1514.638
iteration 0100: loss: 1513.994
iteration 0200: loss: 1513.411
iteration 0300: loss: 1516.395
iteration 0400: loss: 1513.788
iteration 0500: loss: 1516.323
iteration 0600: loss: 1517.727
iteration 0700: loss: 1514.487
iteration 0800: loss: 1515.436
iteration 0900: loss: 1516.652
====> Epoch: 042 Train loss: 1515.2765  took : 8.520093202590942
====> Test loss: 1518.7336
iteration 0000: loss: 1517.382
iteration 0100: loss: 1513.938
iteration 0200: loss: 1515.428
iteration 0300: loss: 1515.027
iteration 0400: loss: 1516.207
iteration 0500: loss: 1518.008
iteration 0600: loss: 1516.940
iteration 0700: loss: 1515.581
iteration 0800: loss: 1517.727
iteration 0900: loss: 1514.391
====> Epoch: 043 Train loss: 1515.2481  took : 8.489325523376465
====> Test loss: 1518.4796
iteration 0000: loss: 1516.849
iteration 0100: loss: 1516.597
iteration 0200: loss: 1513.087
iteration 0300: loss: 1513.612
iteration 0400: loss: 1515.767
iteration 0500: loss: 1515.257
iteration 0600: loss: 1517.617
iteration 0700: loss: 1513.658
iteration 0800: loss: 1515.607
iteration 0900: loss: 1515.226
====> Epoch: 044 Train loss: 1515.1974  took : 8.53326964378357
====> Test loss: 1518.3927
iteration 0000: loss: 1517.554
iteration 0100: loss: 1516.237
iteration 0200: loss: 1514.389
iteration 0300: loss: 1515.399
iteration 0400: loss: 1514.894
iteration 0500: loss: 1513.925
iteration 0600: loss: 1517.319
iteration 0700: loss: 1515.157
iteration 0800: loss: 1514.913
iteration 0900: loss: 1516.192
====> Epoch: 045 Train loss: 1515.1307  took : 8.536465167999268
====> Test loss: 1518.3362
iteration 0000: loss: 1516.668
iteration 0100: loss: 1514.718
iteration 0200: loss: 1513.609
iteration 0300: loss: 1513.466
iteration 0400: loss: 1515.363
iteration 0500: loss: 1517.281
iteration 0600: loss: 1514.297
iteration 0700: loss: 1513.281
iteration 0800: loss: 1515.436
iteration 0900: loss: 1514.831
====> Epoch: 046 Train loss: 1515.0612  took : 8.404797315597534
====> Test loss: 1518.5349
iteration 0000: loss: 1514.388
iteration 0100: loss: 1513.236
iteration 0200: loss: 1516.320
iteration 0300: loss: 1514.300
iteration 0400: loss: 1515.641
iteration 0500: loss: 1516.732
iteration 0600: loss: 1515.859
iteration 0700: loss: 1515.137
iteration 0800: loss: 1513.230
iteration 0900: loss: 1513.251
====> Epoch: 047 Train loss: 1515.0385  took : 8.498447895050049
====> Test loss: 1518.2928
iteration 0000: loss: 1514.587
iteration 0100: loss: 1513.455
iteration 0200: loss: 1514.223
iteration 0300: loss: 1518.719
iteration 0400: loss: 1515.495
iteration 0500: loss: 1513.986
iteration 0600: loss: 1516.314
iteration 0700: loss: 1515.358
iteration 0800: loss: 1516.135
iteration 0900: loss: 1514.496
====> Epoch: 048 Train loss: 1514.9944  took : 8.517608642578125
====> Test loss: 1518.4419
iteration 0000: loss: 1513.911
iteration 0100: loss: 1514.596
iteration 0200: loss: 1514.263
iteration 0300: loss: 1516.234
iteration 0400: loss: 1513.674
iteration 0500: loss: 1513.929
iteration 0600: loss: 1513.626
iteration 0700: loss: 1513.585
iteration 0800: loss: 1515.120
iteration 0900: loss: 1513.492
====> Epoch: 049 Train loss: 1514.9481  took : 8.4230215549469
====> Test loss: 1518.3735
iteration 0000: loss: 1514.609
iteration 0100: loss: 1517.732
iteration 0200: loss: 1517.106
iteration 0300: loss: 1514.859
iteration 0400: loss: 1515.001
iteration 0500: loss: 1516.699
iteration 0600: loss: 1515.901
iteration 0700: loss: 1512.910
iteration 0800: loss: 1514.917
iteration 0900: loss: 1513.760
====> Epoch: 050 Train loss: 1514.9547  took : 8.528480291366577
====> Test loss: 1518.1676
====> [MM-VAE] Time: 509.206s or 00:08:29
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  18
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_18
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_18
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2622.516
iteration 0100: loss: 2099.417
iteration 0200: loss: 2037.340
iteration 0300: loss: 2013.953
iteration 0400: loss: 2002.263
iteration 0500: loss: 1997.218
iteration 0600: loss: 2004.850
iteration 0700: loss: 1995.127
iteration 0800: loss: 1989.360
iteration 0900: loss: 1990.258
====> Epoch: 001 Train loss: 2020.6780  took : 12.709596872329712
====> Test loss: 1994.3652
iteration 0000: loss: 1986.124
iteration 0100: loss: 1990.342
iteration 0200: loss: 1989.137
iteration 0300: loss: 1987.879
iteration 0400: loss: 1985.113
iteration 0500: loss: 1975.468
iteration 0600: loss: 1982.768
iteration 0700: loss: 1982.614
iteration 0800: loss: 1973.615
iteration 0900: loss: 1976.564
====> Epoch: 002 Train loss: 1982.6632  took : 12.570162773132324
====> Test loss: 1981.1014
iteration 0000: loss: 1984.016
iteration 0100: loss: 1980.063
iteration 0200: loss: 1975.477
iteration 0300: loss: 1976.273
iteration 0400: loss: 1972.223
iteration 0500: loss: 1971.448
iteration 0600: loss: 1971.024
iteration 0700: loss: 1970.339
iteration 0800: loss: 1973.784
iteration 0900: loss: 1966.972
====> Epoch: 003 Train loss: 1973.8074  took : 12.574028730392456
====> Test loss: 1972.2651
iteration 0000: loss: 1970.212
iteration 0100: loss: 1969.284
iteration 0200: loss: 1964.859
iteration 0300: loss: 1967.542
iteration 0400: loss: 1966.527
iteration 0500: loss: 1967.021
iteration 0600: loss: 1966.322
iteration 0700: loss: 1964.299
iteration 0800: loss: 1969.038
iteration 0900: loss: 1960.233
====> Epoch: 004 Train loss: 1966.8503  took : 12.780854225158691
====> Test loss: 1966.2376
iteration 0000: loss: 1965.429
iteration 0100: loss: 1962.385
iteration 0200: loss: 1962.023
iteration 0300: loss: 1965.216
iteration 0400: loss: 1963.608
iteration 0500: loss: 1965.697
iteration 0600: loss: 1958.282
iteration 0700: loss: 1956.597
iteration 0800: loss: 1958.580
iteration 0900: loss: 1957.749
====> Epoch: 005 Train loss: 1961.7170  took : 12.505683660507202
====> Test loss: 1961.6146
iteration 0000: loss: 1961.986
iteration 0100: loss: 1959.749
iteration 0200: loss: 1957.954
iteration 0300: loss: 1959.930
iteration 0400: loss: 1957.805
iteration 0500: loss: 1959.888
iteration 0600: loss: 1956.854
iteration 0700: loss: 1959.530
iteration 0800: loss: 1957.463
iteration 0900: loss: 1956.538
====> Epoch: 006 Train loss: 1958.0491  took : 12.761112689971924
====> Test loss: 1959.8471
iteration 0000: loss: 1957.702
iteration 0100: loss: 1954.577
iteration 0200: loss: 1956.433
iteration 0300: loss: 1953.376
iteration 0400: loss: 1956.801
iteration 0500: loss: 1956.899
iteration 0600: loss: 1955.396
iteration 0700: loss: 1957.252
iteration 0800: loss: 1953.598
iteration 0900: loss: 1956.384
====> Epoch: 007 Train loss: 1956.0964  took : 12.024172306060791
====> Test loss: 1958.4533
iteration 0000: loss: 1956.322
iteration 0100: loss: 1955.770
iteration 0200: loss: 1955.762
iteration 0300: loss: 1954.449
iteration 0400: loss: 1954.057
iteration 0500: loss: 1955.188
iteration 0600: loss: 1956.406
iteration 0700: loss: 1953.539
iteration 0800: loss: 1954.196
iteration 0900: loss: 1952.655
====> Epoch: 008 Train loss: 1954.7955  took : 12.269074201583862
====> Test loss: 1956.3649
iteration 0000: loss: 1953.991
iteration 0100: loss: 1956.583
iteration 0200: loss: 1953.321
iteration 0300: loss: 1951.406
iteration 0400: loss: 1952.728
iteration 0500: loss: 1953.345
iteration 0600: loss: 1957.399
iteration 0700: loss: 1954.069
iteration 0800: loss: 1953.963
iteration 0900: loss: 1952.651
====> Epoch: 009 Train loss: 1953.6302  took : 12.199134826660156
====> Test loss: 1955.2114
iteration 0000: loss: 1954.611
iteration 0100: loss: 1952.927
iteration 0200: loss: 1952.417
iteration 0300: loss: 1954.446
iteration 0400: loss: 1951.291
iteration 0500: loss: 1952.118
iteration 0600: loss: 1949.783
iteration 0700: loss: 1951.173
iteration 0800: loss: 1952.744
iteration 0900: loss: 1951.109
====> Epoch: 010 Train loss: 1952.5807  took : 12.443593978881836
====> Test loss: 1954.7230
iteration 0000: loss: 1952.967
iteration 0100: loss: 1953.241
iteration 0200: loss: 1953.944
iteration 0300: loss: 1953.000
iteration 0400: loss: 1951.060
iteration 0500: loss: 1950.919
iteration 0600: loss: 1952.161
iteration 0700: loss: 1953.006
iteration 0800: loss: 1951.482
iteration 0900: loss: 1952.051
====> Epoch: 011 Train loss: 1951.9871  took : 12.856170654296875
====> Test loss: 1953.8652
iteration 0000: loss: 1953.446
iteration 0100: loss: 1952.087
iteration 0200: loss: 1951.494
iteration 0300: loss: 1950.097
iteration 0400: loss: 1949.524
iteration 0500: loss: 1951.781
iteration 0600: loss: 1951.744
iteration 0700: loss: 1951.985
iteration 0800: loss: 1952.058
iteration 0900: loss: 1952.815
====> Epoch: 012 Train loss: 1951.1801  took : 13.322581052780151
====> Test loss: 1953.3748
iteration 0000: loss: 1948.919
iteration 0100: loss: 1953.846
iteration 0200: loss: 1950.312
iteration 0300: loss: 1948.194
iteration 0400: loss: 1949.388
iteration 0500: loss: 1952.165
iteration 0600: loss: 1950.440
iteration 0700: loss: 1950.674
iteration 0800: loss: 1952.254
iteration 0900: loss: 1950.358
====> Epoch: 013 Train loss: 1950.7167  took : 12.125635147094727
====> Test loss: 1952.5073
iteration 0000: loss: 1951.696
iteration 0100: loss: 1951.712
iteration 0200: loss: 1952.544
iteration 0300: loss: 1952.294
iteration 0400: loss: 1950.809
iteration 0500: loss: 1949.870
iteration 0600: loss: 1950.081
iteration 0700: loss: 1950.310
iteration 0800: loss: 1949.434
iteration 0900: loss: 1949.910
====> Epoch: 014 Train loss: 1950.4752  took : 12.06998085975647
====> Test loss: 1952.5004
iteration 0000: loss: 1951.575
iteration 0100: loss: 1950.183
iteration 0200: loss: 1950.169
iteration 0300: loss: 1949.473
iteration 0400: loss: 1950.462
iteration 0500: loss: 1948.809
iteration 0600: loss: 1948.981
iteration 0700: loss: 1950.231
iteration 0800: loss: 1949.056
iteration 0900: loss: 1949.904
====> Epoch: 015 Train loss: 1950.1410  took : 12.07659912109375
====> Test loss: 1952.1572
iteration 0000: loss: 1950.731
iteration 0100: loss: 1949.516
iteration 0200: loss: 1948.990
iteration 0300: loss: 1949.668
iteration 0400: loss: 1948.993
iteration 0500: loss: 1950.628
iteration 0600: loss: 1949.097
iteration 0700: loss: 1948.600
iteration 0800: loss: 1950.449
iteration 0900: loss: 1950.533
====> Epoch: 016 Train loss: 1949.7944  took : 13.417656898498535
====> Test loss: 1951.4504
iteration 0000: loss: 1948.502
iteration 0100: loss: 1949.415
iteration 0200: loss: 1949.273
iteration 0300: loss: 1950.154
iteration 0400: loss: 1948.985
iteration 0500: loss: 1950.313
iteration 0600: loss: 1948.966
iteration 0700: loss: 1951.718
iteration 0800: loss: 1949.305
iteration 0900: loss: 1949.003
====> Epoch: 017 Train loss: 1949.6390  took : 12.530211925506592
====> Test loss: 1951.6427
iteration 0000: loss: 1949.419
iteration 0100: loss: 1948.530
iteration 0200: loss: 1948.951
iteration 0300: loss: 1948.927
iteration 0400: loss: 1948.701
iteration 0500: loss: 1949.032
iteration 0600: loss: 1949.959
iteration 0700: loss: 1950.008
iteration 0800: loss: 1948.731
iteration 0900: loss: 1948.769
====> Epoch: 018 Train loss: 1949.5079  took : 12.605032682418823
====> Test loss: 1951.0537
iteration 0000: loss: 1947.985
iteration 0100: loss: 1950.627
iteration 0200: loss: 1948.405
iteration 0300: loss: 1949.879
iteration 0400: loss: 1948.056
iteration 0500: loss: 1949.823
iteration 0600: loss: 1949.682
iteration 0700: loss: 1950.036
iteration 0800: loss: 1949.444
iteration 0900: loss: 1948.948
====> Epoch: 019 Train loss: 1949.3462  took : 12.722928285598755
====> Test loss: 1950.9796
iteration 0000: loss: 1949.678
iteration 0100: loss: 1950.708
iteration 0200: loss: 1948.377
iteration 0300: loss: 1950.554
iteration 0400: loss: 1948.665
iteration 0500: loss: 1948.895
iteration 0600: loss: 1948.803
iteration 0700: loss: 1948.510
iteration 0800: loss: 1950.381
iteration 0900: loss: 1950.104
====> Epoch: 020 Train loss: 1949.4181  took : 12.381662130355835
====> Test loss: 1951.3687
iteration 0000: loss: 1950.146
iteration 0100: loss: 1949.315
iteration 0200: loss: 1949.130
iteration 0300: loss: 1948.400
iteration 0400: loss: 1950.528
iteration 0500: loss: 1948.812
iteration 0600: loss: 1950.094
iteration 0700: loss: 1948.554
iteration 0800: loss: 1948.653
iteration 0900: loss: 1949.452
====> Epoch: 021 Train loss: 1949.4168  took : 13.376125574111938
====> Test loss: 1951.0333
iteration 0000: loss: 1948.303
iteration 0100: loss: 1949.713
iteration 0200: loss: 1949.146
iteration 0300: loss: 1948.264
iteration 0400: loss: 1949.041
iteration 0500: loss: 1949.820
iteration 0600: loss: 1950.129
iteration 0700: loss: 1949.081
iteration 0800: loss: 1948.934
iteration 0900: loss: 1948.435
====> Epoch: 022 Train loss: 1949.2231  took : 12.047387838363647
====> Test loss: 1951.4477
iteration 0000: loss: 1949.321
iteration 0100: loss: 1949.003
iteration 0200: loss: 1948.573
iteration 0300: loss: 1948.704
iteration 0400: loss: 1948.836
iteration 0500: loss: 1949.790
iteration 0600: loss: 1948.592
iteration 0700: loss: 1947.694
iteration 0800: loss: 1948.490
iteration 0900: loss: 1950.516
====> Epoch: 023 Train loss: 1949.2571  took : 13.28764033317566
====> Test loss: 1950.9064
iteration 0000: loss: 1947.944
iteration 0100: loss: 1950.717
iteration 0200: loss: 1949.390
iteration 0300: loss: 1949.338
iteration 0400: loss: 1949.663
iteration 0500: loss: 1948.783
iteration 0600: loss: 1949.005
iteration 0700: loss: 1949.134
iteration 0800: loss: 1950.542
iteration 0900: loss: 1949.616
====> Epoch: 024 Train loss: 1949.0996  took : 12.575930833816528
====> Test loss: 1951.7428
iteration 0000: loss: 1949.284
iteration 0100: loss: 1947.544
iteration 0200: loss: 1950.212
iteration 0300: loss: 1948.820
iteration 0400: loss: 1951.092
iteration 0500: loss: 1951.515
iteration 0600: loss: 1949.969
iteration 0700: loss: 1948.580
iteration 0800: loss: 1950.351
iteration 0900: loss: 1948.107
====> Epoch: 025 Train loss: 1949.0913  took : 13.382786512374878
====> Test loss: 1950.9056
iteration 0000: loss: 1949.996
iteration 0100: loss: 1948.800
iteration 0200: loss: 1948.433
iteration 0300: loss: 1947.204
iteration 0400: loss: 1948.499
iteration 0500: loss: 1948.191
iteration 0600: loss: 1948.895
iteration 0700: loss: 1947.727
iteration 0800: loss: 1948.757
iteration 0900: loss: 1947.319
====> Epoch: 026 Train loss: 1948.9348  took : 12.167200326919556
====> Test loss: 1951.3340
iteration 0000: loss: 1949.348
iteration 0100: loss: 1949.154
iteration 0200: loss: 1949.810
iteration 0300: loss: 1949.123
iteration 0400: loss: 1949.391
iteration 0500: loss: 1947.812
iteration 0600: loss: 1947.772
iteration 0700: loss: 1947.382
iteration 0800: loss: 1947.724
iteration 0900: loss: 1949.268
====> Epoch: 027 Train loss: 1948.8479  took : 12.781481504440308
====> Test loss: 1950.9554
iteration 0000: loss: 1948.552
iteration 0100: loss: 1947.944
iteration 0200: loss: 1949.530
iteration 0300: loss: 1948.003
iteration 0400: loss: 1948.395
iteration 0500: loss: 1950.199
iteration 0600: loss: 1949.907
iteration 0700: loss: 1948.402
iteration 0800: loss: 1949.028
iteration 0900: loss: 1948.738
====> Epoch: 028 Train loss: 1948.7074  took : 12.28023886680603
====> Test loss: 1950.5977
iteration 0000: loss: 1949.047
iteration 0100: loss: 1950.222
iteration 0200: loss: 1948.872
iteration 0300: loss: 1949.889
iteration 0400: loss: 1948.020
iteration 0500: loss: 1948.915
iteration 0600: loss: 1949.686
iteration 0700: loss: 1947.365
iteration 0800: loss: 1948.650
iteration 0900: loss: 1949.914
====> Epoch: 029 Train loss: 1948.7929  took : 12.875118255615234
====> Test loss: 1950.7404
iteration 0000: loss: 1948.729
iteration 0100: loss: 1948.676
iteration 0200: loss: 1947.940
iteration 0300: loss: 1948.445
iteration 0400: loss: 1948.920
iteration 0500: loss: 1949.375
iteration 0600: loss: 1947.621
iteration 0700: loss: 1949.912
iteration 0800: loss: 1948.186
iteration 0900: loss: 1947.955
====> Epoch: 030 Train loss: 1948.7532  took : 12.80642294883728
====> Test loss: 1950.5298
iteration 0000: loss: 1948.523
iteration 0100: loss: 1949.385
iteration 0200: loss: 1948.377
iteration 0300: loss: 1949.885
iteration 0400: loss: 1948.212
iteration 0500: loss: 1950.088
iteration 0600: loss: 1948.214
iteration 0700: loss: 1950.070
iteration 0800: loss: 1947.782
iteration 0900: loss: 1948.511
====> Epoch: 031 Train loss: 1948.6549  took : 12.529293775558472
====> Test loss: 1950.3301
iteration 0000: loss: 1949.375
iteration 0100: loss: 1948.141
iteration 0200: loss: 1948.567
iteration 0300: loss: 1950.082
iteration 0400: loss: 1948.895
iteration 0500: loss: 1948.186
iteration 0600: loss: 1948.624
iteration 0700: loss: 1949.035
iteration 0800: loss: 1948.279
iteration 0900: loss: 1949.259
====> Epoch: 032 Train loss: 1948.6614  took : 13.103112936019897
====> Test loss: 1950.4396
iteration 0000: loss: 1947.317
iteration 0100: loss: 1949.230
iteration 0200: loss: 1948.305
iteration 0300: loss: 1948.306
iteration 0400: loss: 1947.753
iteration 0500: loss: 1949.452
iteration 0600: loss: 1948.604
iteration 0700: loss: 1948.946
iteration 0800: loss: 1949.488
iteration 0900: loss: 1950.017
====> Epoch: 033 Train loss: 1948.7001  took : 12.997651100158691
====> Test loss: 1950.5262
iteration 0000: loss: 1948.053
iteration 0100: loss: 1950.876
iteration 0200: loss: 1949.776
iteration 0300: loss: 1949.526
iteration 0400: loss: 1949.172
iteration 0500: loss: 1948.536
iteration 0600: loss: 1948.896
iteration 0700: loss: 1949.234
iteration 0800: loss: 1949.473
iteration 0900: loss: 1948.254
====> Epoch: 034 Train loss: 1948.6389  took : 12.924346208572388
====> Test loss: 1950.2571
iteration 0000: loss: 1948.223
iteration 0100: loss: 1950.442
iteration 0200: loss: 1949.408
iteration 0300: loss: 1947.233
iteration 0400: loss: 1949.684
iteration 0500: loss: 1949.054
iteration 0600: loss: 1947.708
iteration 0700: loss: 1948.072
iteration 0800: loss: 1947.469
iteration 0900: loss: 1948.418
====> Epoch: 035 Train loss: 1948.6157  took : 12.657289981842041
====> Test loss: 1950.5385
iteration 0000: loss: 1949.210
iteration 0100: loss: 1950.188
iteration 0200: loss: 1949.573
iteration 0300: loss: 1948.211
iteration 0400: loss: 1949.409
iteration 0500: loss: 1949.210
iteration 0600: loss: 1949.961
iteration 0700: loss: 1948.370
iteration 0800: loss: 1948.929
iteration 0900: loss: 1948.102
====> Epoch: 036 Train loss: 1948.5931  took : 12.445310831069946
====> Test loss: 1950.3737
iteration 0000: loss: 1949.577
iteration 0100: loss: 1948.960
iteration 0200: loss: 1947.963
iteration 0300: loss: 1949.398
iteration 0400: loss: 1949.310
iteration 0500: loss: 1948.778
iteration 0600: loss: 1948.368
iteration 0700: loss: 1948.141
iteration 0800: loss: 1947.402
iteration 0900: loss: 1949.077
====> Epoch: 037 Train loss: 1948.7236  took : 12.294081449508667
====> Test loss: 1950.4489
iteration 0000: loss: 1949.612
iteration 0100: loss: 1948.718
iteration 0200: loss: 1947.362
iteration 0300: loss: 1948.428
iteration 0400: loss: 1950.032
iteration 0500: loss: 1949.062
iteration 0600: loss: 1948.895
iteration 0700: loss: 1947.603
iteration 0800: loss: 1947.613
iteration 0900: loss: 1949.615
====> Epoch: 038 Train loss: 1948.6305  took : 13.573979139328003
====> Test loss: 1950.7791
iteration 0000: loss: 1948.375
iteration 0100: loss: 1948.451
iteration 0200: loss: 1949.064
iteration 0300: loss: 1949.290
iteration 0400: loss: 1947.004
iteration 0500: loss: 1949.390
iteration 0600: loss: 1947.180
iteration 0700: loss: 1947.740
iteration 0800: loss: 1948.830
iteration 0900: loss: 1947.242
====> Epoch: 039 Train loss: 1948.6238  took : 12.069249868392944
====> Test loss: 1950.6490
iteration 0000: loss: 1948.080
iteration 0100: loss: 1951.130
iteration 0200: loss: 1948.660
iteration 0300: loss: 1947.876
iteration 0400: loss: 1948.389
iteration 0500: loss: 1948.181
iteration 0600: loss: 1948.178
iteration 0700: loss: 1947.694
iteration 0800: loss: 1947.378
iteration 0900: loss: 1947.909
====> Epoch: 040 Train loss: 1948.5470  took : 12.406879186630249
====> Test loss: 1950.2001
iteration 0000: loss: 1948.241
iteration 0100: loss: 1947.229
iteration 0200: loss: 1949.120
iteration 0300: loss: 1948.327
iteration 0400: loss: 1947.683
iteration 0500: loss: 1947.605
iteration 0600: loss: 1950.473
iteration 0700: loss: 1948.027
iteration 0800: loss: 1951.818
iteration 0900: loss: 1947.857
====> Epoch: 041 Train loss: 1948.6294  took : 12.116602659225464
====> Test loss: 1950.5197
iteration 0000: loss: 1949.726
iteration 0100: loss: 1948.894
iteration 0200: loss: 1948.532
iteration 0300: loss: 1947.855
iteration 0400: loss: 1947.982
iteration 0500: loss: 1947.994
iteration 0600: loss: 1948.106
iteration 0700: loss: 1951.538
iteration 0800: loss: 1950.139
iteration 0900: loss: 1949.203
====> Epoch: 042 Train loss: 1948.6031  took : 12.137563467025757
====> Test loss: 1950.5481
iteration 0000: loss: 1948.552
iteration 0100: loss: 1947.790
iteration 0200: loss: 1948.449
iteration 0300: loss: 1948.910
iteration 0400: loss: 1949.022
iteration 0500: loss: 1950.204
iteration 0600: loss: 1949.095
iteration 0700: loss: 1947.624
iteration 0800: loss: 1949.768
iteration 0900: loss: 1948.661
====> Epoch: 043 Train loss: 1948.6752  took : 12.641780138015747
====> Test loss: 1950.3878
iteration 0000: loss: 1947.391
iteration 0100: loss: 1949.007
iteration 0200: loss: 1949.354
iteration 0300: loss: 1948.944
iteration 0400: loss: 1948.923
iteration 0500: loss: 1949.528
iteration 0600: loss: 1949.170
iteration 0700: loss: 1947.699
iteration 0800: loss: 1949.081
iteration 0900: loss: 1950.148
====> Epoch: 044 Train loss: 1948.5913  took : 12.260969161987305
====> Test loss: 1950.6800
iteration 0000: loss: 1948.018
iteration 0100: loss: 1947.936
iteration 0200: loss: 1949.068
iteration 0300: loss: 1948.793
iteration 0400: loss: 1948.263
iteration 0500: loss: 1948.358
iteration 0600: loss: 1947.938
iteration 0700: loss: 1949.000
iteration 0800: loss: 1947.901
iteration 0900: loss: 1950.735
====> Epoch: 045 Train loss: 1948.5924  took : 12.71454405784607
====> Test loss: 1950.3692
iteration 0000: loss: 1947.767
iteration 0100: loss: 1948.586
iteration 0200: loss: 1948.600
iteration 0300: loss: 1947.414
iteration 0400: loss: 1950.324
iteration 0500: loss: 1948.972
iteration 0600: loss: 1946.890
iteration 0700: loss: 1948.604
iteration 0800: loss: 1950.965
iteration 0900: loss: 1948.461
====> Epoch: 046 Train loss: 1948.4192  took : 13.306886911392212
====> Test loss: 1950.2218
iteration 0000: loss: 1949.206
iteration 0100: loss: 1948.077
iteration 0200: loss: 1947.069
iteration 0300: loss: 1947.287
iteration 0400: loss: 1948.973
iteration 0500: loss: 1947.678
iteration 0600: loss: 1948.408
iteration 0700: loss: 1947.803
iteration 0800: loss: 1949.381
iteration 0900: loss: 1947.664
====> Epoch: 047 Train loss: 1948.4484  took : 13.204283237457275
====> Test loss: 1950.1092
iteration 0000: loss: 1949.265
iteration 0100: loss: 1946.958
iteration 0200: loss: 1947.052
iteration 0300: loss: 1947.997
iteration 0400: loss: 1949.425
iteration 0500: loss: 1948.323
iteration 0600: loss: 1948.429
iteration 0700: loss: 1947.713
iteration 0800: loss: 1946.743
iteration 0900: loss: 1948.082
====> Epoch: 048 Train loss: 1948.4079  took : 12.720510005950928
====> Test loss: 1950.0284
iteration 0000: loss: 1949.330
iteration 0100: loss: 1948.531
iteration 0200: loss: 1947.857
iteration 0300: loss: 1950.107
iteration 0400: loss: 1948.410
iteration 0500: loss: 1948.968
iteration 0600: loss: 1949.729
iteration 0700: loss: 1948.431
iteration 0800: loss: 1949.371
iteration 0900: loss: 1949.621
====> Epoch: 049 Train loss: 1948.4671  took : 13.168452262878418
====> Test loss: 1950.3256
iteration 0000: loss: 1948.460
iteration 0100: loss: 1950.243
iteration 0200: loss: 1951.115
iteration 0300: loss: 1947.333
iteration 0400: loss: 1948.837
iteration 0500: loss: 1948.408
iteration 0600: loss: 1946.716
iteration 0700: loss: 1949.386
iteration 0800: loss: 1947.069
iteration 0900: loss: 1948.102
====> Epoch: 050 Train loss: 1948.3226  took : 12.415174007415771
====> Test loss: 1949.9974
====> [MM-VAE] Time: 703.179s or 00:11:43
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  18
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_18
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_18
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5218.397
iteration 0100: loss: 4154.783
iteration 0200: loss: 4079.724
iteration 0300: loss: 4057.187
iteration 0400: loss: 4022.774
iteration 0500: loss: 4022.693
iteration 0600: loss: 4021.039
iteration 0700: loss: 4009.406
iteration 0800: loss: 4020.380
iteration 0900: loss: 4007.082
iteration 1000: loss: 4002.264
iteration 1100: loss: 3995.607
iteration 1200: loss: 3991.983
iteration 1300: loss: 4021.289
iteration 1400: loss: 3991.083
iteration 1500: loss: 4008.146
iteration 1600: loss: 4006.213
iteration 1700: loss: 3995.590
iteration 1800: loss: 3981.770
====> Epoch: 001 Train loss: 4027.1237  took : 53.299524784088135
====> Test loss: 3992.2703
iteration 0000: loss: 3999.543
iteration 0100: loss: 3996.265
iteration 0200: loss: 3994.222
iteration 0300: loss: 3981.427
iteration 0400: loss: 3987.805
iteration 0500: loss: 3981.384
iteration 0600: loss: 3998.556
iteration 0700: loss: 3978.656
iteration 0800: loss: 3991.650
iteration 0900: loss: 3992.667
iteration 1000: loss: 3971.849
iteration 1100: loss: 3974.462
iteration 1200: loss: 3975.591
iteration 1300: loss: 3979.813
iteration 1400: loss: 3974.765
iteration 1500: loss: 3973.034
iteration 1600: loss: 3959.833
iteration 1700: loss: 3977.007
iteration 1800: loss: 3965.279
====> Epoch: 002 Train loss: 3978.8041  took : 53.69114708900452
====> Test loss: 3974.3415
iteration 0000: loss: 3961.073
iteration 0100: loss: 3971.941
iteration 0200: loss: 3962.565
iteration 0300: loss: 3964.775
iteration 0400: loss: 3978.811
iteration 0500: loss: 3964.182
iteration 0600: loss: 3972.766
iteration 0700: loss: 3961.369
iteration 0800: loss: 3965.311
iteration 0900: loss: 3966.664
iteration 1000: loss: 3958.696
iteration 1100: loss: 3965.487
iteration 1200: loss: 3961.736
iteration 1300: loss: 3964.438
iteration 1400: loss: 3956.824
iteration 1500: loss: 3963.792
iteration 1600: loss: 3959.685
iteration 1700: loss: 3967.842
iteration 1800: loss: 3957.793
====> Epoch: 003 Train loss: 3962.8555  took : 53.30873203277588
====> Test loss: 3959.4718
iteration 0000: loss: 3958.345
iteration 0100: loss: 3947.003
iteration 0200: loss: 3951.099
iteration 0300: loss: 3954.173
iteration 0400: loss: 3958.641
iteration 0500: loss: 3948.714
iteration 0600: loss: 3953.219
iteration 0700: loss: 3958.513
iteration 0800: loss: 3949.310
iteration 0900: loss: 3959.380
iteration 1000: loss: 3960.708
iteration 1100: loss: 3950.757
iteration 1200: loss: 3953.619
iteration 1300: loss: 3956.472
iteration 1400: loss: 3954.575
iteration 1500: loss: 3956.553
iteration 1600: loss: 3954.539
iteration 1700: loss: 3950.195
iteration 1800: loss: 3950.364
====> Epoch: 004 Train loss: 3953.5030  took : 53.312272787094116
====> Test loss: 3953.4439
iteration 0000: loss: 3954.574
iteration 0100: loss: 3941.050
iteration 0200: loss: 3954.811
iteration 0300: loss: 3949.927
iteration 0400: loss: 3946.984
iteration 0500: loss: 3953.843
iteration 0600: loss: 3946.336
iteration 0700: loss: 3947.704
iteration 0800: loss: 3950.369
iteration 0900: loss: 3947.870
iteration 1000: loss: 3951.635
iteration 1100: loss: 3948.750
iteration 1200: loss: 3946.709
iteration 1300: loss: 3945.624
iteration 1400: loss: 3948.750
iteration 1500: loss: 3947.135
iteration 1600: loss: 3945.973
iteration 1700: loss: 3948.059
iteration 1800: loss: 3952.889
====> Epoch: 005 Train loss: 3948.4171  took : 53.253523111343384
====> Test loss: 3950.4232
iteration 0000: loss: 3950.263
iteration 0100: loss: 3949.612
iteration 0200: loss: 3938.962
iteration 0300: loss: 3946.183
iteration 0400: loss: 3949.182
iteration 0500: loss: 3945.382
iteration 0600: loss: 3941.832
iteration 0700: loss: 3944.814
iteration 0800: loss: 3940.368
iteration 0900: loss: 3943.450
iteration 1000: loss: 3942.900
iteration 1100: loss: 3945.858
iteration 1200: loss: 3945.797
iteration 1300: loss: 3950.193
iteration 1400: loss: 3948.893
iteration 1500: loss: 3942.006
iteration 1600: loss: 3940.230
iteration 1700: loss: 3949.060
iteration 1800: loss: 3947.344
====> Epoch: 006 Train loss: 3945.3066  took : 53.20344018936157
====> Test loss: 3946.8902
iteration 0000: loss: 3944.554
iteration 0100: loss: 3954.006
iteration 0200: loss: 3947.255
iteration 0300: loss: 3943.747
iteration 0400: loss: 3950.519
iteration 0500: loss: 3943.665
iteration 0600: loss: 3944.932
iteration 0700: loss: 3941.277
iteration 0800: loss: 3943.532
iteration 0900: loss: 3939.086
iteration 1000: loss: 3946.385
iteration 1100: loss: 3943.181
iteration 1200: loss: 3939.077
iteration 1300: loss: 3943.611
iteration 1400: loss: 3941.016
iteration 1500: loss: 3944.233
iteration 1600: loss: 3943.803
iteration 1700: loss: 3942.338
iteration 1800: loss: 3942.666
====> Epoch: 007 Train loss: 3943.9302  took : 52.994884967803955
====> Test loss: 3947.2285
iteration 0000: loss: 3948.751
iteration 0100: loss: 3942.350
iteration 0200: loss: 3942.617
iteration 0300: loss: 3941.195
iteration 0400: loss: 3941.660
iteration 0500: loss: 3944.937
iteration 0600: loss: 3941.583
iteration 0700: loss: 3948.708
iteration 0800: loss: 3947.114
iteration 0900: loss: 3939.902
iteration 1000: loss: 3941.539
iteration 1100: loss: 3945.819
iteration 1200: loss: 3940.939
iteration 1300: loss: 3946.406
iteration 1400: loss: 3941.546
iteration 1500: loss: 3944.717
iteration 1600: loss: 3941.726
iteration 1700: loss: 3947.108
iteration 1800: loss: 3941.628
====> Epoch: 008 Train loss: 3943.1185  took : 53.5653612613678
====> Test loss: 3946.1205
iteration 0000: loss: 3948.860
iteration 0100: loss: 3945.669
iteration 0200: loss: 3946.694
iteration 0300: loss: 3944.512
iteration 0400: loss: 3938.669
iteration 0500: loss: 3940.772
iteration 0600: loss: 3937.342
iteration 0700: loss: 3941.353
iteration 0800: loss: 3940.506
iteration 0900: loss: 3939.410
iteration 1000: loss: 3945.941
iteration 1100: loss: 3943.075
iteration 1200: loss: 3947.974
iteration 1300: loss: 3942.213
iteration 1400: loss: 3946.397
iteration 1500: loss: 3939.691
iteration 1600: loss: 3941.031
iteration 1700: loss: 3943.449
iteration 1800: loss: 3939.926
====> Epoch: 009 Train loss: 3942.3862  took : 53.33347821235657
====> Test loss: 3947.3303
iteration 0000: loss: 3947.613
iteration 0100: loss: 3944.281
iteration 0200: loss: 3939.434
iteration 0300: loss: 3942.037
iteration 0400: loss: 3948.306
iteration 0500: loss: 3946.634
iteration 0600: loss: 3947.853
iteration 0700: loss: 3945.378
iteration 0800: loss: 3950.205
iteration 0900: loss: 3936.853
iteration 1000: loss: 3948.750
iteration 1100: loss: 3940.312
iteration 1200: loss: 3938.741
iteration 1300: loss: 3938.140
iteration 1400: loss: 3938.076
iteration 1500: loss: 3936.756
iteration 1600: loss: 3939.929
iteration 1700: loss: 3941.345
iteration 1800: loss: 3944.968
====> Epoch: 010 Train loss: 3941.7682  took : 53.23762273788452
====> Test loss: 3945.1041
iteration 0000: loss: 3941.874
iteration 0100: loss: 3939.205
iteration 0200: loss: 3944.130
iteration 0300: loss: 3935.751
iteration 0400: loss: 3939.381
iteration 0500: loss: 3939.781
iteration 0600: loss: 3936.958
iteration 0700: loss: 3944.215
iteration 0800: loss: 3943.566
iteration 0900: loss: 3938.991
iteration 1000: loss: 3947.482
iteration 1100: loss: 3942.837
iteration 1200: loss: 3942.841
iteration 1300: loss: 3946.229
iteration 1400: loss: 3941.252
iteration 1500: loss: 3942.224
iteration 1600: loss: 3945.176
iteration 1700: loss: 3942.194
iteration 1800: loss: 3940.396
====> Epoch: 011 Train loss: 3941.4392  took : 53.274903297424316
====> Test loss: 3945.0063
iteration 0000: loss: 3939.710
iteration 0100: loss: 3945.873
iteration 0200: loss: 3945.423
iteration 0300: loss: 3938.950
iteration 0400: loss: 3944.871
iteration 0500: loss: 3939.058
iteration 0600: loss: 3943.460
iteration 0700: loss: 3946.167
iteration 0800: loss: 3940.502
iteration 0900: loss: 3940.081
iteration 1000: loss: 3935.930
iteration 1100: loss: 3942.198
iteration 1200: loss: 3933.903
iteration 1300: loss: 3941.940
iteration 1400: loss: 3940.341
iteration 1500: loss: 3944.522
iteration 1600: loss: 3945.964
iteration 1700: loss: 3947.267
iteration 1800: loss: 3943.691
====> Epoch: 012 Train loss: 3941.1697  took : 52.992310762405396
====> Test loss: 3945.0380
iteration 0000: loss: 3943.486
iteration 0100: loss: 3942.240
iteration 0200: loss: 3934.699
iteration 0300: loss: 3939.417
iteration 0400: loss: 3943.613
iteration 0500: loss: 3944.423
iteration 0600: loss: 3940.870
iteration 0700: loss: 3935.146
iteration 0800: loss: 3943.741
iteration 0900: loss: 3935.463
iteration 1000: loss: 3944.135
iteration 1100: loss: 3942.868
iteration 1200: loss: 3942.047
iteration 1300: loss: 3938.309
iteration 1400: loss: 3941.718
iteration 1500: loss: 3941.719
iteration 1600: loss: 3940.907
iteration 1700: loss: 3938.958
iteration 1800: loss: 3940.937
====> Epoch: 013 Train loss: 3941.0292  took : 53.06685471534729
====> Test loss: 3944.7666
iteration 0000: loss: 3943.344
iteration 0100: loss: 3937.752
iteration 0200: loss: 3945.612
iteration 0300: loss: 3938.186
iteration 0400: loss: 3940.166
iteration 0500: loss: 3939.347
iteration 0600: loss: 3943.164
iteration 0700: loss: 3940.380
iteration 0800: loss: 3941.194
iteration 0900: loss: 3941.980
iteration 1000: loss: 3945.721
iteration 1100: loss: 3936.864
iteration 1200: loss: 3944.464
iteration 1300: loss: 3938.501
iteration 1400: loss: 3940.238
iteration 1500: loss: 3939.279
iteration 1600: loss: 3937.188
iteration 1700: loss: 3941.976
iteration 1800: loss: 3945.009
====> Epoch: 014 Train loss: 3940.7029  took : 53.049933433532715
====> Test loss: 3945.1285
iteration 0000: loss: 3945.093
iteration 0100: loss: 3938.863
iteration 0200: loss: 3943.354
iteration 0300: loss: 3943.251
iteration 0400: loss: 3943.270
iteration 0500: loss: 3942.252
iteration 0600: loss: 3935.853
iteration 0700: loss: 3940.229
iteration 0800: loss: 3937.218
iteration 0900: loss: 3941.463
iteration 1000: loss: 3944.217
iteration 1100: loss: 3943.327
iteration 1200: loss: 3938.022
iteration 1300: loss: 3945.582
iteration 1400: loss: 3939.902
iteration 1500: loss: 3948.129
iteration 1600: loss: 3937.333
iteration 1700: loss: 3940.687
iteration 1800: loss: 3935.701
====> Epoch: 015 Train loss: 3940.4998  took : 53.10030484199524
====> Test loss: 3943.5939
iteration 0000: loss: 3938.202
iteration 0100: loss: 3939.968
iteration 0200: loss: 3937.114
iteration 0300: loss: 3944.149
iteration 0400: loss: 3938.979
iteration 0500: loss: 3941.225
iteration 0600: loss: 3938.265
iteration 0700: loss: 3940.766
iteration 0800: loss: 3940.557
iteration 0900: loss: 3939.919
iteration 1000: loss: 3942.692
iteration 1100: loss: 3945.176
iteration 1200: loss: 3936.147
iteration 1300: loss: 3944.779
iteration 1400: loss: 3938.677
iteration 1500: loss: 3939.944
iteration 1600: loss: 3938.750
iteration 1700: loss: 3944.836
iteration 1800: loss: 3941.872
====> Epoch: 016 Train loss: 3940.2431  took : 53.014275550842285
====> Test loss: 3944.0246
iteration 0000: loss: 3940.870
iteration 0100: loss: 3934.608
iteration 0200: loss: 3934.845
iteration 0300: loss: 3934.909
iteration 0400: loss: 3946.599
iteration 0500: loss: 3952.050
iteration 0600: loss: 3938.187
iteration 0700: loss: 3942.489
iteration 0800: loss: 3940.159
iteration 0900: loss: 3935.519
iteration 1000: loss: 3939.179
iteration 1100: loss: 3938.656
iteration 1200: loss: 3936.379
iteration 1300: loss: 3937.346
iteration 1400: loss: 3940.209
iteration 1500: loss: 3940.270
iteration 1600: loss: 3940.975
iteration 1700: loss: 3943.208
iteration 1800: loss: 3940.153
====> Epoch: 017 Train loss: 3940.2206  took : 52.93592429161072
====> Test loss: 3943.7401
iteration 0000: loss: 3947.282
iteration 0100: loss: 3938.073
iteration 0200: loss: 3939.534
iteration 0300: loss: 3949.899
iteration 0400: loss: 3940.194
iteration 0500: loss: 3939.411
iteration 0600: loss: 3939.513
iteration 0700: loss: 3934.785
iteration 0800: loss: 3941.005
iteration 0900: loss: 3940.328
iteration 1000: loss: 3945.371
iteration 1100: loss: 3935.584
iteration 1200: loss: 3937.972
iteration 1300: loss: 3942.220
iteration 1400: loss: 3938.250
iteration 1500: loss: 3941.013
iteration 1600: loss: 3938.099
iteration 1700: loss: 3941.986
iteration 1800: loss: 3944.710
====> Epoch: 018 Train loss: 3940.0550  took : 53.0463809967041
====> Test loss: 3943.3627
iteration 0000: loss: 3937.788
iteration 0100: loss: 3934.539
iteration 0200: loss: 3939.608
iteration 0300: loss: 3936.786
iteration 0400: loss: 3935.573
iteration 0500: loss: 3943.692
iteration 0600: loss: 3938.932
iteration 0700: loss: 3938.426
iteration 0800: loss: 3940.649
iteration 0900: loss: 3944.332
iteration 1000: loss: 3932.336
iteration 1100: loss: 3937.423
iteration 1200: loss: 3935.644
iteration 1300: loss: 3937.274
iteration 1400: loss: 3942.594
iteration 1500: loss: 3940.204
iteration 1600: loss: 3937.927
iteration 1700: loss: 3940.006
iteration 1800: loss: 3938.467
====> Epoch: 019 Train loss: 3939.9399  took : 53.038835287094116
====> Test loss: 3943.5285
iteration 0000: loss: 3946.414
iteration 0100: loss: 3935.993
iteration 0200: loss: 3940.088
iteration 0300: loss: 3936.370
iteration 0400: loss: 3945.870
iteration 0500: loss: 3931.241
iteration 0600: loss: 3943.536
iteration 0700: loss: 3944.703
iteration 0800: loss: 3937.157
iteration 0900: loss: 3932.788
iteration 1000: loss: 3939.177
iteration 1100: loss: 3944.623
iteration 1200: loss: 3940.654
iteration 1300: loss: 3935.428
iteration 1400: loss: 3938.494
iteration 1500: loss: 3942.353
iteration 1600: loss: 3942.974
iteration 1700: loss: 3940.010
iteration 1800: loss: 3941.432
====> Epoch: 020 Train loss: 3939.8047  took : 52.99437499046326
====> Test loss: 3944.0176
iteration 0000: loss: 3936.407
iteration 0100: loss: 3946.487
iteration 0200: loss: 3937.855
iteration 0300: loss: 3945.457
iteration 0400: loss: 3942.616
iteration 0500: loss: 3934.836
iteration 0600: loss: 3944.978
iteration 0700: loss: 3938.982
iteration 0800: loss: 3940.506
iteration 0900: loss: 3938.642
iteration 1000: loss: 3938.984
iteration 1100: loss: 3939.611
iteration 1200: loss: 3944.768
iteration 1300: loss: 3935.638
iteration 1400: loss: 3936.941
iteration 1500: loss: 3937.402
iteration 1600: loss: 3935.093
iteration 1700: loss: 3940.432
iteration 1800: loss: 3943.187
====> Epoch: 021 Train loss: 3939.8292  took : 53.12366056442261
====> Test loss: 3943.6548
iteration 0000: loss: 3937.987
iteration 0100: loss: 3941.158
iteration 0200: loss: 3934.504
iteration 0300: loss: 3942.484
iteration 0400: loss: 3944.572
iteration 0500: loss: 3942.623
iteration 0600: loss: 3939.233
iteration 0700: loss: 3937.616
iteration 0800: loss: 3939.064
iteration 0900: loss: 3937.410
iteration 1000: loss: 3935.362
iteration 1100: loss: 3938.841
iteration 1200: loss: 3938.787
iteration 1300: loss: 3936.428
iteration 1400: loss: 3937.958
iteration 1500: loss: 3936.146
iteration 1600: loss: 3936.786
iteration 1700: loss: 3936.100
iteration 1800: loss: 3937.441
====> Epoch: 022 Train loss: 3939.4649  took : 53.11523365974426
====> Test loss: 3943.5595
iteration 0000: loss: 3939.132
iteration 0100: loss: 3938.852
iteration 0200: loss: 3940.540
iteration 0300: loss: 3935.626
iteration 0400: loss: 3938.957
iteration 0500: loss: 3939.405
iteration 0600: loss: 3943.409
iteration 0700: loss: 3938.295
iteration 0800: loss: 3937.060
iteration 0900: loss: 3934.488
iteration 1000: loss: 3937.928
iteration 1100: loss: 3935.038
iteration 1200: loss: 3937.272
iteration 1300: loss: 3934.260
iteration 1400: loss: 3937.201
iteration 1500: loss: 3934.491
iteration 1600: loss: 3947.857
iteration 1700: loss: 3937.990
iteration 1800: loss: 3938.544
====> Epoch: 023 Train loss: 3939.3200  took : 53.04213905334473
====> Test loss: 3943.4484
iteration 0000: loss: 3938.748
iteration 0100: loss: 3950.814
iteration 0200: loss: 3936.542
iteration 0300: loss: 3935.368
iteration 0400: loss: 3939.344
iteration 0500: loss: 3939.702
iteration 0600: loss: 3944.535
iteration 0700: loss: 3940.712
iteration 0800: loss: 3941.212
iteration 0900: loss: 3936.334
iteration 1000: loss: 3941.501
iteration 1100: loss: 3944.015
iteration 1200: loss: 3941.421
iteration 1300: loss: 3936.196
iteration 1400: loss: 3935.731
iteration 1500: loss: 3940.137
iteration 1600: loss: 3941.673
iteration 1700: loss: 3936.131
iteration 1800: loss: 3936.668
====> Epoch: 024 Train loss: 3939.1010  took : 53.11016774177551
====> Test loss: 3942.9011
iteration 0000: loss: 3939.927
iteration 0100: loss: 3937.644
iteration 0200: loss: 3944.487
iteration 0300: loss: 3938.235
iteration 0400: loss: 3939.094
iteration 0500: loss: 3933.451
iteration 0600: loss: 3937.767
iteration 0700: loss: 3946.189
iteration 0800: loss: 3937.609
iteration 0900: loss: 3938.844
iteration 1000: loss: 3941.784
iteration 1100: loss: 3936.203
iteration 1200: loss: 3942.308
iteration 1300: loss: 3939.721
iteration 1400: loss: 3938.488
iteration 1500: loss: 3937.350
iteration 1600: loss: 3939.682
iteration 1700: loss: 3940.840
iteration 1800: loss: 3937.954
====> Epoch: 025 Train loss: 3939.1179  took : 53.107444524765015
====> Test loss: 3942.6524
iteration 0000: loss: 3943.779
iteration 0100: loss: 3939.347
iteration 0200: loss: 3932.798
iteration 0300: loss: 3938.946
iteration 0400: loss: 3940.275
iteration 0500: loss: 3929.872
iteration 0600: loss: 3933.544
iteration 0700: loss: 3942.113
iteration 0800: loss: 3936.186
iteration 0900: loss: 3935.587
iteration 1000: loss: 3938.699
iteration 1100: loss: 3936.431
iteration 1200: loss: 3943.451
iteration 1300: loss: 3934.155
iteration 1400: loss: 3937.742
iteration 1500: loss: 3939.366
iteration 1600: loss: 3937.164
iteration 1700: loss: 3942.141
iteration 1800: loss: 3935.844
====> Epoch: 026 Train loss: 3939.0715  took : 52.83644676208496
====> Test loss: 3943.1822
iteration 0000: loss: 3937.414
iteration 0100: loss: 3934.599
iteration 0200: loss: 3943.798
iteration 0300: loss: 3933.965
iteration 0400: loss: 3937.578
iteration 0500: loss: 3938.812
iteration 0600: loss: 3938.739
iteration 0700: loss: 3939.756
iteration 0800: loss: 3939.528
iteration 0900: loss: 3939.324
iteration 1000: loss: 3939.458
iteration 1100: loss: 3937.718
iteration 1200: loss: 3939.093
iteration 1300: loss: 3941.604
iteration 1400: loss: 3939.213
iteration 1500: loss: 3943.397
iteration 1600: loss: 3940.798
iteration 1700: loss: 3933.368
iteration 1800: loss: 3939.521
====> Epoch: 027 Train loss: 3939.1635  took : 52.95326280593872
====> Test loss: 3943.3880
iteration 0000: loss: 3938.164
iteration 0100: loss: 3945.325
iteration 0200: loss: 3947.631
iteration 0300: loss: 3944.429
iteration 0400: loss: 3936.773
iteration 0500: loss: 3939.990
iteration 0600: loss: 3940.490
iteration 0700: loss: 3936.213
iteration 0800: loss: 3930.630
iteration 0900: loss: 3935.952
iteration 1000: loss: 3944.717
iteration 1100: loss: 3938.035
iteration 1200: loss: 3942.444
iteration 1300: loss: 3937.985
iteration 1400: loss: 3942.662
iteration 1500: loss: 3943.885
iteration 1600: loss: 3936.958
iteration 1700: loss: 3930.013
iteration 1800: loss: 3933.876
====> Epoch: 028 Train loss: 3938.8702  took : 52.980645179748535
====> Test loss: 3942.6249
iteration 0000: loss: 3941.186
iteration 0100: loss: 3936.859
iteration 0200: loss: 3937.028
iteration 0300: loss: 3931.976
iteration 0400: loss: 3931.787
iteration 0500: loss: 3942.059
iteration 0600: loss: 3938.044
iteration 0700: loss: 3942.116
iteration 0800: loss: 3946.065
iteration 0900: loss: 3934.009
iteration 1000: loss: 3941.022
iteration 1100: loss: 3941.218
iteration 1200: loss: 3938.886
iteration 1300: loss: 3942.138
iteration 1400: loss: 3943.359
iteration 1500: loss: 3935.312
iteration 1600: loss: 3939.541
iteration 1700: loss: 3939.287
iteration 1800: loss: 3937.107
====> Epoch: 029 Train loss: 3939.0683  took : 53.05553436279297
====> Test loss: 3942.8840
iteration 0000: loss: 3937.397
iteration 0100: loss: 3934.070
iteration 0200: loss: 3940.601
iteration 0300: loss: 3941.813
iteration 0400: loss: 3938.610
iteration 0500: loss: 3938.714
iteration 0600: loss: 3936.137
iteration 0700: loss: 3943.500
iteration 0800: loss: 3936.794
iteration 0900: loss: 3936.148
iteration 1000: loss: 3941.159
iteration 1100: loss: 3939.218
iteration 1200: loss: 3939.002
iteration 1300: loss: 3939.268
iteration 1400: loss: 3940.010
iteration 1500: loss: 3935.344
iteration 1600: loss: 3935.400
iteration 1700: loss: 3937.175
iteration 1800: loss: 3943.618
====> Epoch: 030 Train loss: 3938.6886  took : 52.99997019767761
====> Test loss: 3942.4846
iteration 0000: loss: 3938.629
iteration 0100: loss: 3937.586
iteration 0200: loss: 3944.528
iteration 0300: loss: 3939.700
iteration 0400: loss: 3942.213
iteration 0500: loss: 3938.353
iteration 0600: loss: 3941.448
iteration 0700: loss: 3937.621
iteration 0800: loss: 3939.004
iteration 0900: loss: 3937.096
iteration 1000: loss: 3939.523
iteration 1100: loss: 3941.230
iteration 1200: loss: 3941.565
iteration 1300: loss: 3940.796
iteration 1400: loss: 3941.997
iteration 1500: loss: 3939.943
iteration 1600: loss: 3945.112
iteration 1700: loss: 3941.327
iteration 1800: loss: 3944.817
====> Epoch: 031 Train loss: 3938.9029  took : 52.96780967712402
====> Test loss: 3943.3115
iteration 0000: loss: 3936.683
iteration 0100: loss: 3940.880
iteration 0200: loss: 3939.081
iteration 0300: loss: 3932.268
iteration 0400: loss: 3939.239
iteration 0500: loss: 3941.662
iteration 0600: loss: 3935.120
iteration 0700: loss: 3941.574
iteration 0800: loss: 3940.566
iteration 0900: loss: 3943.482
iteration 1000: loss: 3934.886
iteration 1100: loss: 3941.469
iteration 1200: loss: 3938.121
iteration 1300: loss: 3938.887
iteration 1400: loss: 3935.709
iteration 1500: loss: 3941.597
iteration 1600: loss: 3939.651
iteration 1700: loss: 3938.094
iteration 1800: loss: 3935.211
====> Epoch: 032 Train loss: 3938.6438  took : 52.95077848434448
====> Test loss: 3942.9216
iteration 0000: loss: 3937.577
iteration 0100: loss: 3939.168
iteration 0200: loss: 3930.456
iteration 0300: loss: 3937.446
iteration 0400: loss: 3941.829
iteration 0500: loss: 3936.264
iteration 0600: loss: 3934.254
iteration 0700: loss: 3933.771
iteration 0800: loss: 3940.002
iteration 0900: loss: 3940.915
iteration 1000: loss: 3936.810
iteration 1100: loss: 3938.939
iteration 1200: loss: 3939.183
iteration 1300: loss: 3941.226
iteration 1400: loss: 3946.364
iteration 1500: loss: 3933.435
iteration 1600: loss: 3937.583
iteration 1700: loss: 3932.203
iteration 1800: loss: 3933.718
====> Epoch: 033 Train loss: 3938.5349  took : 53.16681432723999
====> Test loss: 3942.5555
iteration 0000: loss: 3938.062
iteration 0100: loss: 3940.363
iteration 0200: loss: 3946.196
iteration 0300: loss: 3936.028
iteration 0400: loss: 3941.646
iteration 0500: loss: 3936.938
iteration 0600: loss: 3944.519
iteration 0700: loss: 3941.776
iteration 0800: loss: 3939.347
iteration 0900: loss: 3935.590
iteration 1000: loss: 3937.311
iteration 1100: loss: 3942.489
iteration 1200: loss: 3932.206
iteration 1300: loss: 3938.643
iteration 1400: loss: 3939.508
iteration 1500: loss: 3935.284
iteration 1600: loss: 3935.515
iteration 1700: loss: 3935.148
iteration 1800: loss: 3941.440
====> Epoch: 034 Train loss: 3938.4167  took : 53.00629806518555
====> Test loss: 3942.3401
iteration 0000: loss: 3939.883
iteration 0100: loss: 3937.599
iteration 0200: loss: 3945.773
iteration 0300: loss: 3936.360
iteration 0400: loss: 3937.953
iteration 0500: loss: 3938.194
iteration 0600: loss: 3936.595
iteration 0700: loss: 3944.101
iteration 0800: loss: 3939.057
iteration 0900: loss: 3934.352
iteration 1000: loss: 3938.587
iteration 1100: loss: 3938.374
iteration 1200: loss: 3934.015
iteration 1300: loss: 3937.404
iteration 1400: loss: 3942.245
iteration 1500: loss: 3938.650
iteration 1600: loss: 3938.425
iteration 1700: loss: 3942.829
iteration 1800: loss: 3936.377
====> Epoch: 035 Train loss: 3938.4366  took : 53.004263401031494
====> Test loss: 3942.4059
iteration 0000: loss: 3932.276
iteration 0100: loss: 3940.576
iteration 0200: loss: 3942.040
iteration 0300: loss: 3934.431
iteration 0400: loss: 3942.134
iteration 0500: loss: 3935.713
iteration 0600: loss: 3938.445
iteration 0700: loss: 3941.847
iteration 0800: loss: 3945.148
iteration 0900: loss: 3943.755
iteration 1000: loss: 3935.362
iteration 1100: loss: 3936.768
iteration 1200: loss: 3934.168
iteration 1300: loss: 3933.306
iteration 1400: loss: 3936.084
iteration 1500: loss: 3938.758
iteration 1600: loss: 3932.984
iteration 1700: loss: 3935.295
iteration 1800: loss: 3936.144
====> Epoch: 036 Train loss: 3938.5406  took : 52.891517877578735
====> Test loss: 3942.4479
iteration 0000: loss: 3940.856
iteration 0100: loss: 3941.659
iteration 0200: loss: 3938.604
iteration 0300: loss: 3939.220
iteration 0400: loss: 3929.108
iteration 0500: loss: 3938.242
iteration 0600: loss: 3937.374
iteration 0700: loss: 3938.363
iteration 0800: loss: 3944.240
iteration 0900: loss: 3936.067
iteration 1000: loss: 3949.612
iteration 1100: loss: 3940.336
iteration 1200: loss: 3937.854
iteration 1300: loss: 3939.475
iteration 1400: loss: 3941.003
iteration 1500: loss: 3938.727
iteration 1600: loss: 3935.391
iteration 1700: loss: 3938.699
iteration 1800: loss: 3940.858
====> Epoch: 037 Train loss: 3938.4078  took : 52.97197103500366
====> Test loss: 3942.3885
iteration 0000: loss: 3935.063
iteration 0100: loss: 3948.559
iteration 0200: loss: 3938.875
iteration 0300: loss: 3939.240
iteration 0400: loss: 3936.090
iteration 0500: loss: 3938.053
iteration 0600: loss: 3937.949
iteration 0700: loss: 3940.583
iteration 0800: loss: 3934.371
iteration 0900: loss: 3936.531
iteration 1000: loss: 3940.537
iteration 1100: loss: 3937.012
iteration 1200: loss: 3937.881
iteration 1300: loss: 3940.781
iteration 1400: loss: 3940.376
iteration 1500: loss: 3930.458
iteration 1600: loss: 3936.212
iteration 1700: loss: 3936.752
iteration 1800: loss: 3939.720
====> Epoch: 038 Train loss: 3938.3312  took : 52.982900619506836
====> Test loss: 3942.9750
iteration 0000: loss: 3932.531
iteration 0100: loss: 3936.812
iteration 0200: loss: 3936.278
iteration 0300: loss: 3939.780
iteration 0400: loss: 3932.190
iteration 0500: loss: 3937.030
iteration 0600: loss: 3931.595
iteration 0700: loss: 3941.517
iteration 0800: loss: 3932.336
iteration 0900: loss: 3941.306
iteration 1000: loss: 3939.370
iteration 1100: loss: 3940.198
iteration 1200: loss: 3932.458
iteration 1300: loss: 3944.856
iteration 1400: loss: 3939.128
iteration 1500: loss: 3938.718
iteration 1600: loss: 3935.988
iteration 1700: loss: 3933.338
iteration 1800: loss: 3940.203
====> Epoch: 039 Train loss: 3938.3006  took : 52.93149518966675
====> Test loss: 3941.9073
iteration 0000: loss: 3939.531
iteration 0100: loss: 3938.953
iteration 0200: loss: 3938.499
iteration 0300: loss: 3932.310
iteration 0400: loss: 3938.777
iteration 0500: loss: 3943.302
iteration 0600: loss: 3937.657
iteration 0700: loss: 3937.533
iteration 0800: loss: 3935.369
iteration 0900: loss: 3938.333
iteration 1000: loss: 3937.470
iteration 1100: loss: 3950.164
iteration 1200: loss: 3940.135
iteration 1300: loss: 3940.092
iteration 1400: loss: 3940.301
iteration 1500: loss: 3939.732
iteration 1600: loss: 3940.453
iteration 1700: loss: 3935.458
iteration 1800: loss: 3938.319
====> Epoch: 040 Train loss: 3938.2342  took : 53.26179051399231
====> Test loss: 3942.5610
iteration 0000: loss: 3937.139
iteration 0100: loss: 3936.669
iteration 0200: loss: 3936.427
iteration 0300: loss: 3944.042
iteration 0400: loss: 3933.607
iteration 0500: loss: 3934.844
iteration 0600: loss: 3935.487
iteration 0700: loss: 3937.646
iteration 0800: loss: 3939.804
iteration 0900: loss: 3943.557
iteration 1000: loss: 3934.721
iteration 1100: loss: 3936.715
iteration 1200: loss: 3937.095
iteration 1300: loss: 3937.151
iteration 1400: loss: 3934.844
iteration 1500: loss: 3941.388
iteration 1600: loss: 3938.260
iteration 1700: loss: 3937.360
iteration 1800: loss: 3938.198
====> Epoch: 041 Train loss: 3938.2763  took : 52.90194272994995
====> Test loss: 3942.8615
iteration 0000: loss: 3940.422
iteration 0100: loss: 3940.345
iteration 0200: loss: 3940.867
iteration 0300: loss: 3938.698
iteration 0400: loss: 3936.112
iteration 0500: loss: 3935.656
iteration 0600: loss: 3935.000
iteration 0700: loss: 3935.003
iteration 0800: loss: 3935.037
iteration 0900: loss: 3938.034
iteration 1000: loss: 3934.643
iteration 1100: loss: 3940.104
iteration 1200: loss: 3935.426
iteration 1300: loss: 3931.541
iteration 1400: loss: 3936.338
iteration 1500: loss: 3939.846
iteration 1600: loss: 3940.170
iteration 1700: loss: 3938.148
iteration 1800: loss: 3943.388
====> Epoch: 042 Train loss: 3938.0870  took : 52.869279623031616
====> Test loss: 3942.1337
iteration 0000: loss: 3942.713
iteration 0100: loss: 3937.722
iteration 0200: loss: 3938.961
iteration 0300: loss: 3935.582
iteration 0400: loss: 3939.123
iteration 0500: loss: 3936.660
iteration 0600: loss: 3938.693
iteration 0700: loss: 3937.589
iteration 0800: loss: 3937.718
iteration 0900: loss: 3945.111
iteration 1000: loss: 3939.247
iteration 1100: loss: 3941.812
iteration 1200: loss: 3940.549
iteration 1300: loss: 3934.937
iteration 1400: loss: 3938.091
iteration 1500: loss: 3940.218
iteration 1600: loss: 3944.292
iteration 1700: loss: 3943.427
iteration 1800: loss: 3937.932
====> Epoch: 043 Train loss: 3938.0728  took : 53.00434947013855
====> Test loss: 3942.0853
iteration 0000: loss: 3934.281
iteration 0100: loss: 3933.963
iteration 0200: loss: 3944.057
iteration 0300: loss: 3933.192
iteration 0400: loss: 3934.270
iteration 0500: loss: 3941.744
iteration 0600: loss: 3933.365
iteration 0700: loss: 3937.987
iteration 0800: loss: 3935.757
iteration 0900: loss: 3936.853
iteration 1000: loss: 3941.458
iteration 1100: loss: 3937.877
iteration 1200: loss: 3942.327
iteration 1300: loss: 3936.706
iteration 1400: loss: 3937.772
iteration 1500: loss: 3939.349
iteration 1600: loss: 3935.830
iteration 1700: loss: 3936.091
iteration 1800: loss: 3938.960
====> Epoch: 044 Train loss: 3937.9576  took : 53.021905183792114
====> Test loss: 3942.4328
iteration 0000: loss: 3936.380
iteration 0100: loss: 3938.206
iteration 0200: loss: 3938.881
iteration 0300: loss: 3935.108
iteration 0400: loss: 3933.486
iteration 0500: loss: 3942.090
iteration 0600: loss: 3937.019
iteration 0700: loss: 3939.240
iteration 0800: loss: 3941.765
iteration 0900: loss: 3932.097
iteration 1000: loss: 3942.298
iteration 1100: loss: 3937.289
iteration 1200: loss: 3936.619
iteration 1300: loss: 3934.789
iteration 1400: loss: 3936.056
iteration 1500: loss: 3942.696
iteration 1600: loss: 3936.292
iteration 1700: loss: 3937.125
iteration 1800: loss: 3931.305
====> Epoch: 045 Train loss: 3938.0294  took : 53.01771521568298
====> Test loss: 3942.4235
iteration 0000: loss: 3943.851
iteration 0100: loss: 3938.326
iteration 0200: loss: 3937.364
iteration 0300: loss: 3934.839
iteration 0400: loss: 3939.796
iteration 0500: loss: 3935.957
iteration 0600: loss: 3944.292
iteration 0700: loss: 3935.609
iteration 0800: loss: 3937.260
iteration 0900: loss: 3942.015
iteration 1000: loss: 3935.259
iteration 1100: loss: 3938.310
iteration 1200: loss: 3936.670
iteration 1300: loss: 3934.905
iteration 1400: loss: 3937.321
iteration 1500: loss: 3934.808
iteration 1600: loss: 3939.137
iteration 1700: loss: 3938.428
iteration 1800: loss: 3934.156
====> Epoch: 046 Train loss: 3937.9266  took : 52.88588738441467
====> Test loss: 3942.3228
iteration 0000: loss: 3940.488
iteration 0100: loss: 3936.605
iteration 0200: loss: 3936.597
iteration 0300: loss: 3942.545
iteration 0400: loss: 3942.001
iteration 0500: loss: 3937.881
iteration 0600: loss: 3937.585
iteration 0700: loss: 3936.946
iteration 0800: loss: 3936.675
iteration 0900: loss: 3940.716
iteration 1000: loss: 3931.078
iteration 1100: loss: 3934.704
iteration 1200: loss: 3938.062
iteration 1300: loss: 3933.992
iteration 1400: loss: 3940.343
iteration 1500: loss: 3943.798
iteration 1600: loss: 3940.714
iteration 1700: loss: 3941.839
iteration 1800: loss: 3939.977
====> Epoch: 047 Train loss: 3937.9303  took : 52.82191228866577
====> Test loss: 3941.8243
iteration 0000: loss: 3937.622
iteration 0100: loss: 3934.357
iteration 0200: loss: 3935.233
iteration 0300: loss: 3936.777
iteration 0400: loss: 3936.147
iteration 0500: loss: 3935.651
iteration 0600: loss: 3930.798
iteration 0700: loss: 3933.809
iteration 0800: loss: 3941.180
iteration 0900: loss: 3933.644
iteration 1000: loss: 3938.635
iteration 1100: loss: 3935.693
iteration 1200: loss: 3937.303
iteration 1300: loss: 3936.448
iteration 1400: loss: 3928.163
iteration 1500: loss: 3945.137
iteration 1600: loss: 3944.938
iteration 1700: loss: 3933.704
iteration 1800: loss: 3944.700
====> Epoch: 048 Train loss: 3937.9080  took : 52.93160820007324
====> Test loss: 3942.4985
iteration 0000: loss: 3944.150
iteration 0100: loss: 3935.409
iteration 0200: loss: 3940.938
iteration 0300: loss: 3940.348
iteration 0400: loss: 3939.507
iteration 0500: loss: 3937.121
iteration 0600: loss: 3932.114
iteration 0700: loss: 3937.544
iteration 0800: loss: 3939.033
iteration 0900: loss: 3936.781
iteration 1000: loss: 3935.903
iteration 1100: loss: 3938.690
iteration 1200: loss: 3936.916
iteration 1300: loss: 3939.914
iteration 1400: loss: 3939.173
iteration 1500: loss: 3938.536
iteration 1600: loss: 3939.309
iteration 1700: loss: 3937.190
iteration 1800: loss: 3938.584
====> Epoch: 049 Train loss: 3937.8689  took : 52.779149532318115
====> Test loss: 3941.9299
iteration 0000: loss: 3936.807
iteration 0100: loss: 3940.060
iteration 0200: loss: 3939.909
iteration 0300: loss: 3943.803
iteration 0400: loss: 3933.014
iteration 0500: loss: 3932.208
iteration 0600: loss: 3944.791
iteration 0700: loss: 3933.834
iteration 0800: loss: 3933.844
iteration 0900: loss: 3943.254
iteration 1000: loss: 3938.439
iteration 1100: loss: 3940.731
iteration 1200: loss: 3944.547
iteration 1300: loss: 3933.669
iteration 1400: loss: 3940.384
iteration 1500: loss: 3935.836
iteration 1600: loss: 3936.671
iteration 1700: loss: 3935.400
iteration 1800: loss: 3937.300
====> Epoch: 050 Train loss: 3937.7712  took : 52.962504625320435
====> Test loss: 3941.8566
====> [MM-VAE] Time: 3156.536s or 00:52:36
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  19
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_19
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_19
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 1989.534
iteration 0100: loss: 1568.757
iteration 0200: loss: 1563.510
iteration 0300: loss: 1561.681
iteration 0400: loss: 1549.403
iteration 0500: loss: 1542.267
iteration 0600: loss: 1538.115
iteration 0700: loss: 1544.988
iteration 0800: loss: 1542.669
iteration 0900: loss: 1537.973
====> Epoch: 001 Train loss: 1554.7854  took : 8.53183126449585
====> Test loss: 1538.0914
iteration 0000: loss: 1535.814
iteration 0100: loss: 1534.518
iteration 0200: loss: 1530.066
iteration 0300: loss: 1536.012
iteration 0400: loss: 1532.351
iteration 0500: loss: 1533.737
iteration 0600: loss: 1530.023
iteration 0700: loss: 1524.533
iteration 0800: loss: 1530.014
iteration 0900: loss: 1522.126
====> Epoch: 002 Train loss: 1531.3625  took : 8.52609658241272
====> Test loss: 1529.1441
iteration 0000: loss: 1531.443
iteration 0100: loss: 1526.845
iteration 0200: loss: 1525.675
iteration 0300: loss: 1522.851
iteration 0400: loss: 1524.994
iteration 0500: loss: 1525.325
iteration 0600: loss: 1525.212
iteration 0700: loss: 1524.694
iteration 0800: loss: 1523.249
iteration 0900: loss: 1522.197
====> Epoch: 003 Train loss: 1525.6624  took : 8.408541202545166
====> Test loss: 1526.2286
iteration 0000: loss: 1521.850
iteration 0100: loss: 1521.420
iteration 0200: loss: 1524.502
iteration 0300: loss: 1524.201
iteration 0400: loss: 1523.745
iteration 0500: loss: 1520.317
iteration 0600: loss: 1521.447
iteration 0700: loss: 1523.252
iteration 0800: loss: 1522.889
iteration 0900: loss: 1521.517
====> Epoch: 004 Train loss: 1523.2785  took : 8.49860668182373
====> Test loss: 1524.7261
iteration 0000: loss: 1518.462
iteration 0100: loss: 1523.148
iteration 0200: loss: 1522.712
iteration 0300: loss: 1522.006
iteration 0400: loss: 1519.192
iteration 0500: loss: 1523.503
iteration 0600: loss: 1519.122
iteration 0700: loss: 1519.332
iteration 0800: loss: 1519.995
iteration 0900: loss: 1523.598
====> Epoch: 005 Train loss: 1521.9099  took : 8.493305444717407
====> Test loss: 1523.5904
iteration 0000: loss: 1522.850
iteration 0100: loss: 1521.199
iteration 0200: loss: 1520.298
iteration 0300: loss: 1519.100
iteration 0400: loss: 1519.996
iteration 0500: loss: 1520.954
iteration 0600: loss: 1523.562
iteration 0700: loss: 1522.533
iteration 0800: loss: 1518.970
iteration 0900: loss: 1523.783
====> Epoch: 006 Train loss: 1520.9593  took : 8.50122857093811
====> Test loss: 1522.9811
iteration 0000: loss: 1519.606
iteration 0100: loss: 1518.724
iteration 0200: loss: 1521.397
iteration 0300: loss: 1518.809
iteration 0400: loss: 1517.352
iteration 0500: loss: 1522.347
iteration 0600: loss: 1521.952
iteration 0700: loss: 1517.712
iteration 0800: loss: 1521.704
iteration 0900: loss: 1519.768
====> Epoch: 007 Train loss: 1520.2441  took : 8.403062582015991
====> Test loss: 1522.3276
iteration 0000: loss: 1521.259
iteration 0100: loss: 1520.223
iteration 0200: loss: 1518.556
iteration 0300: loss: 1520.420
iteration 0400: loss: 1517.182
iteration 0500: loss: 1524.266
iteration 0600: loss: 1519.702
iteration 0700: loss: 1519.464
iteration 0800: loss: 1519.541
iteration 0900: loss: 1518.058
====> Epoch: 008 Train loss: 1519.6412  took : 8.498925685882568
====> Test loss: 1522.0265
iteration 0000: loss: 1518.570
iteration 0100: loss: 1521.229
iteration 0200: loss: 1517.188
iteration 0300: loss: 1518.966
iteration 0400: loss: 1520.180
iteration 0500: loss: 1520.202
iteration 0600: loss: 1518.618
iteration 0700: loss: 1518.198
iteration 0800: loss: 1517.106
iteration 0900: loss: 1519.440
====> Epoch: 009 Train loss: 1519.1738  took : 8.526987791061401
====> Test loss: 1521.6520
iteration 0000: loss: 1522.075
iteration 0100: loss: 1518.219
iteration 0200: loss: 1515.925
iteration 0300: loss: 1518.833
iteration 0400: loss: 1515.437
iteration 0500: loss: 1518.421
iteration 0600: loss: 1518.346
iteration 0700: loss: 1522.487
iteration 0800: loss: 1519.334
iteration 0900: loss: 1518.507
====> Epoch: 010 Train loss: 1518.7765  took : 8.534830331802368
====> Test loss: 1521.3294
iteration 0000: loss: 1520.882
iteration 0100: loss: 1515.832
iteration 0200: loss: 1516.999
iteration 0300: loss: 1516.988
iteration 0400: loss: 1516.324
iteration 0500: loss: 1520.418
iteration 0600: loss: 1519.622
iteration 0700: loss: 1519.300
iteration 0800: loss: 1515.343
iteration 0900: loss: 1517.935
====> Epoch: 011 Train loss: 1518.3168  took : 8.477915287017822
====> Test loss: 1521.1716
iteration 0000: loss: 1517.785
iteration 0100: loss: 1520.251
iteration 0200: loss: 1519.114
iteration 0300: loss: 1516.050
iteration 0400: loss: 1517.770
iteration 0500: loss: 1520.154
iteration 0600: loss: 1520.780
iteration 0700: loss: 1514.693
iteration 0800: loss: 1518.975
iteration 0900: loss: 1520.354
====> Epoch: 012 Train loss: 1517.9936  took : 8.399899244308472
====> Test loss: 1520.7058
iteration 0000: loss: 1516.761
iteration 0100: loss: 1517.450
iteration 0200: loss: 1520.956
iteration 0300: loss: 1516.784
iteration 0400: loss: 1516.512
iteration 0500: loss: 1517.009
iteration 0600: loss: 1518.595
iteration 0700: loss: 1514.707
iteration 0800: loss: 1516.015
iteration 0900: loss: 1520.141
====> Epoch: 013 Train loss: 1517.6927  took : 8.447043180465698
====> Test loss: 1520.5228
iteration 0000: loss: 1519.427
iteration 0100: loss: 1515.978
iteration 0200: loss: 1523.613
iteration 0300: loss: 1516.080
iteration 0400: loss: 1518.250
iteration 0500: loss: 1519.231
iteration 0600: loss: 1517.085
iteration 0700: loss: 1519.046
iteration 0800: loss: 1517.950
iteration 0900: loss: 1518.173
====> Epoch: 014 Train loss: 1517.4159  took : 8.50497055053711
====> Test loss: 1520.2544
iteration 0000: loss: 1517.691
iteration 0100: loss: 1513.796
iteration 0200: loss: 1516.281
iteration 0300: loss: 1519.330
iteration 0400: loss: 1516.042
iteration 0500: loss: 1519.139
iteration 0600: loss: 1516.677
iteration 0700: loss: 1516.526
iteration 0800: loss: 1519.523
iteration 0900: loss: 1516.952
====> Epoch: 015 Train loss: 1517.1658  took : 8.442659378051758
====> Test loss: 1520.0057
iteration 0000: loss: 1513.463
iteration 0100: loss: 1515.420
iteration 0200: loss: 1516.619
iteration 0300: loss: 1515.152
iteration 0400: loss: 1517.927
iteration 0500: loss: 1518.135
iteration 0600: loss: 1516.874
iteration 0700: loss: 1514.789
iteration 0800: loss: 1519.471
iteration 0900: loss: 1516.362
====> Epoch: 016 Train loss: 1516.9207  took : 8.494555234909058
====> Test loss: 1519.9281
iteration 0000: loss: 1514.593
iteration 0100: loss: 1516.531
iteration 0200: loss: 1518.058
iteration 0300: loss: 1516.856
iteration 0400: loss: 1514.227
iteration 0500: loss: 1519.762
iteration 0600: loss: 1513.527
iteration 0700: loss: 1516.871
iteration 0800: loss: 1517.293
iteration 0900: loss: 1514.809
====> Epoch: 017 Train loss: 1516.7761  took : 8.405289888381958
====> Test loss: 1519.6965
iteration 0000: loss: 1515.955
iteration 0100: loss: 1515.436
iteration 0200: loss: 1519.474
iteration 0300: loss: 1515.847
iteration 0400: loss: 1517.351
iteration 0500: loss: 1515.707
iteration 0600: loss: 1515.505
iteration 0700: loss: 1519.178
iteration 0800: loss: 1519.194
iteration 0900: loss: 1517.420
====> Epoch: 018 Train loss: 1516.5064  took : 8.471073389053345
====> Test loss: 1519.6956
iteration 0000: loss: 1512.237
iteration 0100: loss: 1513.934
iteration 0200: loss: 1517.602
iteration 0300: loss: 1519.615
iteration 0400: loss: 1517.907
iteration 0500: loss: 1514.660
iteration 0600: loss: 1519.740
iteration 0700: loss: 1514.561
iteration 0800: loss: 1516.664
iteration 0900: loss: 1517.570
====> Epoch: 019 Train loss: 1516.3844  took : 8.438975811004639
====> Test loss: 1519.5741
iteration 0000: loss: 1517.163
iteration 0100: loss: 1517.299
iteration 0200: loss: 1518.104
iteration 0300: loss: 1513.803
iteration 0400: loss: 1513.230
iteration 0500: loss: 1517.385
iteration 0600: loss: 1517.627
iteration 0700: loss: 1516.724
iteration 0800: loss: 1518.608
iteration 0900: loss: 1516.795
====> Epoch: 020 Train loss: 1516.2113  took : 8.50619912147522
====> Test loss: 1519.4411
iteration 0000: loss: 1514.007
iteration 0100: loss: 1517.041
iteration 0200: loss: 1516.449
iteration 0300: loss: 1514.396
iteration 0400: loss: 1518.521
iteration 0500: loss: 1516.597
iteration 0600: loss: 1515.076
iteration 0700: loss: 1517.768
iteration 0800: loss: 1516.308
iteration 0900: loss: 1514.423
====> Epoch: 021 Train loss: 1516.0516  took : 8.459494352340698
====> Test loss: 1519.1371
iteration 0000: loss: 1519.466
iteration 0100: loss: 1514.613
iteration 0200: loss: 1515.746
iteration 0300: loss: 1515.899
iteration 0400: loss: 1517.148
iteration 0500: loss: 1514.345
iteration 0600: loss: 1514.193
iteration 0700: loss: 1516.926
iteration 0800: loss: 1517.580
iteration 0900: loss: 1520.482
====> Epoch: 022 Train loss: 1515.9170  took : 8.418314456939697
====> Test loss: 1519.0890
iteration 0000: loss: 1515.087
iteration 0100: loss: 1516.250
iteration 0200: loss: 1515.999
iteration 0300: loss: 1519.113
iteration 0400: loss: 1518.388
iteration 0500: loss: 1516.005
iteration 0600: loss: 1514.932
iteration 0700: loss: 1517.444
iteration 0800: loss: 1513.634
iteration 0900: loss: 1515.428
====> Epoch: 023 Train loss: 1515.8277  took : 8.55508828163147
====> Test loss: 1519.2611
iteration 0000: loss: 1514.169
iteration 0100: loss: 1514.367
iteration 0200: loss: 1516.824
iteration 0300: loss: 1515.359
iteration 0400: loss: 1513.449
iteration 0500: loss: 1514.271
iteration 0600: loss: 1514.854
iteration 0700: loss: 1517.354
iteration 0800: loss: 1518.412
iteration 0900: loss: 1517.940
====> Epoch: 024 Train loss: 1515.7612  took : 8.438445568084717
====> Test loss: 1519.1443
iteration 0000: loss: 1516.159
iteration 0100: loss: 1515.260
iteration 0200: loss: 1516.370
iteration 0300: loss: 1516.544
iteration 0400: loss: 1516.440
iteration 0500: loss: 1515.588
iteration 0600: loss: 1515.606
iteration 0700: loss: 1514.185
iteration 0800: loss: 1515.741
iteration 0900: loss: 1514.985
====> Epoch: 025 Train loss: 1515.5833  took : 8.43761920928955
====> Test loss: 1518.8610
iteration 0000: loss: 1514.586
iteration 0100: loss: 1517.863
iteration 0200: loss: 1516.408
iteration 0300: loss: 1515.632
iteration 0400: loss: 1517.797
iteration 0500: loss: 1514.994
iteration 0600: loss: 1517.937
iteration 0700: loss: 1516.846
iteration 0800: loss: 1515.370
iteration 0900: loss: 1517.733
====> Epoch: 026 Train loss: 1515.5625  took : 8.495734453201294
====> Test loss: 1518.8959
iteration 0000: loss: 1514.576
iteration 0100: loss: 1514.071
iteration 0200: loss: 1513.859
iteration 0300: loss: 1514.319
iteration 0400: loss: 1516.551
iteration 0500: loss: 1514.570
iteration 0600: loss: 1513.729
iteration 0700: loss: 1512.643
iteration 0800: loss: 1513.773
iteration 0900: loss: 1513.578
====> Epoch: 027 Train loss: 1515.4364  took : 8.532503843307495
====> Test loss: 1518.9428
iteration 0000: loss: 1517.544
iteration 0100: loss: 1514.646
iteration 0200: loss: 1514.090
iteration 0300: loss: 1515.444
iteration 0400: loss: 1515.235
iteration 0500: loss: 1514.486
iteration 0600: loss: 1517.709
iteration 0700: loss: 1515.807
iteration 0800: loss: 1515.697
iteration 0900: loss: 1516.150
====> Epoch: 028 Train loss: 1515.3453  took : 8.470020532608032
====> Test loss: 1518.7031
iteration 0000: loss: 1513.412
iteration 0100: loss: 1515.085
iteration 0200: loss: 1515.504
iteration 0300: loss: 1515.213
iteration 0400: loss: 1517.534
iteration 0500: loss: 1514.998
iteration 0600: loss: 1516.020
iteration 0700: loss: 1516.122
iteration 0800: loss: 1515.605
iteration 0900: loss: 1516.810
====> Epoch: 029 Train loss: 1515.2756  took : 8.437634468078613
====> Test loss: 1518.6206
iteration 0000: loss: 1518.190
iteration 0100: loss: 1518.197
iteration 0200: loss: 1515.438
iteration 0300: loss: 1512.972
iteration 0400: loss: 1514.454
iteration 0500: loss: 1514.011
iteration 0600: loss: 1514.324
iteration 0700: loss: 1518.374
iteration 0800: loss: 1516.777
iteration 0900: loss: 1513.862
====> Epoch: 030 Train loss: 1515.1930  took : 8.527099370956421
====> Test loss: 1518.7161
iteration 0000: loss: 1517.490
iteration 0100: loss: 1514.766
iteration 0200: loss: 1515.153
iteration 0300: loss: 1515.077
iteration 0400: loss: 1515.966
iteration 0500: loss: 1513.825
iteration 0600: loss: 1513.206
iteration 0700: loss: 1515.667
iteration 0800: loss: 1516.102
iteration 0900: loss: 1513.390
====> Epoch: 031 Train loss: 1515.0671  took : 8.56670618057251
====> Test loss: 1518.6517
iteration 0000: loss: 1515.210
iteration 0100: loss: 1514.785
iteration 0200: loss: 1515.014
iteration 0300: loss: 1515.972
iteration 0400: loss: 1514.151
iteration 0500: loss: 1516.177
iteration 0600: loss: 1516.132
iteration 0700: loss: 1518.213
iteration 0800: loss: 1513.896
iteration 0900: loss: 1513.196
====> Epoch: 032 Train loss: 1515.1458  took : 8.538615226745605
====> Test loss: 1518.6404
iteration 0000: loss: 1514.436
iteration 0100: loss: 1514.876
iteration 0200: loss: 1516.660
iteration 0300: loss: 1517.849
iteration 0400: loss: 1515.954
iteration 0500: loss: 1514.852
iteration 0600: loss: 1514.593
iteration 0700: loss: 1513.227
iteration 0800: loss: 1517.608
iteration 0900: loss: 1514.846
====> Epoch: 033 Train loss: 1514.9486  took : 8.510444164276123
====> Test loss: 1518.5111
iteration 0000: loss: 1513.876
iteration 0100: loss: 1515.194
iteration 0200: loss: 1514.631
iteration 0300: loss: 1514.178
iteration 0400: loss: 1513.719
iteration 0500: loss: 1511.605
iteration 0600: loss: 1514.591
iteration 0700: loss: 1515.964
iteration 0800: loss: 1515.074
iteration 0900: loss: 1514.364
====> Epoch: 034 Train loss: 1514.8973  took : 8.403581380844116
====> Test loss: 1518.4181
iteration 0000: loss: 1517.576
iteration 0100: loss: 1513.726
iteration 0200: loss: 1513.719
iteration 0300: loss: 1517.800
iteration 0400: loss: 1517.111
iteration 0500: loss: 1515.448
iteration 0600: loss: 1514.817
iteration 0700: loss: 1515.515
iteration 0800: loss: 1515.187
iteration 0900: loss: 1513.473
====> Epoch: 035 Train loss: 1514.9228  took : 8.517786264419556
====> Test loss: 1518.5218
iteration 0000: loss: 1513.699
iteration 0100: loss: 1515.805
iteration 0200: loss: 1515.340
iteration 0300: loss: 1515.225
iteration 0400: loss: 1516.248
iteration 0500: loss: 1513.894
iteration 0600: loss: 1511.706
iteration 0700: loss: 1516.950
iteration 0800: loss: 1512.613
iteration 0900: loss: 1512.767
====> Epoch: 036 Train loss: 1514.8809  took : 8.440333366394043
====> Test loss: 1518.5450
iteration 0000: loss: 1514.508
iteration 0100: loss: 1513.735
iteration 0200: loss: 1514.987
iteration 0300: loss: 1517.123
iteration 0400: loss: 1515.184
iteration 0500: loss: 1516.703
iteration 0600: loss: 1515.839
iteration 0700: loss: 1517.011
iteration 0800: loss: 1511.621
iteration 0900: loss: 1515.525
====> Epoch: 037 Train loss: 1514.7889  took : 8.479170560836792
====> Test loss: 1518.7648
iteration 0000: loss: 1514.302
iteration 0100: loss: 1515.384
iteration 0200: loss: 1514.620
iteration 0300: loss: 1516.288
iteration 0400: loss: 1514.477
iteration 0500: loss: 1516.846
iteration 0600: loss: 1514.690
iteration 0700: loss: 1515.488
iteration 0800: loss: 1515.564
iteration 0900: loss: 1514.321
====> Epoch: 038 Train loss: 1514.7085  took : 8.465314388275146
====> Test loss: 1518.2410
iteration 0000: loss: 1514.537
iteration 0100: loss: 1513.253
iteration 0200: loss: 1514.261
iteration 0300: loss: 1512.312
iteration 0400: loss: 1512.744
iteration 0500: loss: 1515.708
iteration 0600: loss: 1512.377
iteration 0700: loss: 1517.694
iteration 0800: loss: 1516.993
iteration 0900: loss: 1511.899
====> Epoch: 039 Train loss: 1514.5979  took : 8.500868797302246
====> Test loss: 1518.3292
iteration 0000: loss: 1513.572
iteration 0100: loss: 1516.958
iteration 0200: loss: 1514.345
iteration 0300: loss: 1513.964
iteration 0400: loss: 1513.993
iteration 0500: loss: 1514.870
iteration 0600: loss: 1512.030
iteration 0700: loss: 1516.443
iteration 0800: loss: 1514.156
iteration 0900: loss: 1513.882
====> Epoch: 040 Train loss: 1514.5446  took : 8.491625547409058
====> Test loss: 1518.3058
iteration 0000: loss: 1512.443
iteration 0100: loss: 1515.173
iteration 0200: loss: 1515.047
iteration 0300: loss: 1515.887
iteration 0400: loss: 1515.205
iteration 0500: loss: 1512.834
iteration 0600: loss: 1513.732
iteration 0700: loss: 1516.003
iteration 0800: loss: 1513.498
iteration 0900: loss: 1517.110
====> Epoch: 041 Train loss: 1514.4772  took : 8.472262382507324
====> Test loss: 1518.1754
iteration 0000: loss: 1513.586
iteration 0100: loss: 1517.511
iteration 0200: loss: 1514.936
iteration 0300: loss: 1513.671
iteration 0400: loss: 1513.653
iteration 0500: loss: 1515.250
iteration 0600: loss: 1516.076
iteration 0700: loss: 1514.283
iteration 0800: loss: 1512.315
iteration 0900: loss: 1513.383
====> Epoch: 042 Train loss: 1514.4571  took : 8.519649028778076
====> Test loss: 1518.3093
iteration 0000: loss: 1511.896
iteration 0100: loss: 1516.531
iteration 0200: loss: 1514.808
iteration 0300: loss: 1513.613
iteration 0400: loss: 1516.997
iteration 0500: loss: 1515.956
iteration 0600: loss: 1518.023
iteration 0700: loss: 1515.623
iteration 0800: loss: 1517.140
iteration 0900: loss: 1510.614
====> Epoch: 043 Train loss: 1514.3850  took : 8.509393215179443
====> Test loss: 1518.1633
iteration 0000: loss: 1514.935
iteration 0100: loss: 1512.841
iteration 0200: loss: 1511.359
iteration 0300: loss: 1512.086
iteration 0400: loss: 1515.099
iteration 0500: loss: 1513.399
iteration 0600: loss: 1512.038
iteration 0700: loss: 1513.304
iteration 0800: loss: 1514.519
iteration 0900: loss: 1513.577
====> Epoch: 044 Train loss: 1514.3157  took : 8.432771682739258
====> Test loss: 1518.2605
iteration 0000: loss: 1513.518
iteration 0100: loss: 1514.469
iteration 0200: loss: 1513.005
iteration 0300: loss: 1512.702
iteration 0400: loss: 1513.177
iteration 0500: loss: 1515.891
iteration 0600: loss: 1515.889
iteration 0700: loss: 1511.531
iteration 0800: loss: 1514.090
iteration 0900: loss: 1512.755
====> Epoch: 045 Train loss: 1514.3574  took : 8.483779430389404
====> Test loss: 1518.2294
iteration 0000: loss: 1513.310
iteration 0100: loss: 1513.663
iteration 0200: loss: 1516.057
iteration 0300: loss: 1515.055
iteration 0400: loss: 1515.406
iteration 0500: loss: 1515.652
iteration 0600: loss: 1515.686
iteration 0700: loss: 1512.811
iteration 0800: loss: 1513.594
iteration 0900: loss: 1511.916
====> Epoch: 046 Train loss: 1514.2559  took : 8.526091814041138
====> Test loss: 1518.0722
iteration 0000: loss: 1513.420
iteration 0100: loss: 1512.340
iteration 0200: loss: 1515.110
iteration 0300: loss: 1513.565
iteration 0400: loss: 1515.447
iteration 0500: loss: 1514.867
iteration 0600: loss: 1515.019
iteration 0700: loss: 1515.369
iteration 0800: loss: 1514.430
iteration 0900: loss: 1513.677
====> Epoch: 047 Train loss: 1514.3412  took : 8.43817949295044
====> Test loss: 1518.1931
iteration 0000: loss: 1514.810
iteration 0100: loss: 1514.519
iteration 0200: loss: 1512.667
iteration 0300: loss: 1514.682
iteration 0400: loss: 1515.280
iteration 0500: loss: 1512.418
iteration 0600: loss: 1515.830
iteration 0700: loss: 1513.867
iteration 0800: loss: 1513.958
iteration 0900: loss: 1513.038
====> Epoch: 048 Train loss: 1514.2015  took : 8.446885108947754
====> Test loss: 1518.1788
iteration 0000: loss: 1514.679
iteration 0100: loss: 1514.268
iteration 0200: loss: 1513.627
iteration 0300: loss: 1516.280
iteration 0400: loss: 1514.578
iteration 0500: loss: 1514.371
iteration 0600: loss: 1515.741
iteration 0700: loss: 1515.738
iteration 0800: loss: 1514.939
iteration 0900: loss: 1514.748
====> Epoch: 049 Train loss: 1514.1585  took : 8.463662385940552
====> Test loss: 1518.0944
iteration 0000: loss: 1515.671
iteration 0100: loss: 1514.393
iteration 0200: loss: 1515.457
iteration 0300: loss: 1514.601
iteration 0400: loss: 1515.214
iteration 0500: loss: 1513.667
iteration 0600: loss: 1513.743
iteration 0700: loss: 1515.614
iteration 0800: loss: 1515.762
iteration 0900: loss: 1514.356
====> Epoch: 050 Train loss: 1514.0900  took : 8.46823501586914
====> Test loss: 1518.1038
====> [MM-VAE] Time: 507.088s or 00:08:27
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  19
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_19
Run Directory:
 ./rslt/parameter-search-32/VAE_OSCN/vae_oscn_seed_19
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]       1,229,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0
            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 3072]       1,231,872
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Model is initialized without loading.

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: dreg 
t_objectives: iwae
iteration 0000: loss: 2621.408
iteration 0100: loss: 2113.721
iteration 0200: loss: 2046.828
iteration 0300: loss: 2012.951
iteration 0400: loss: 2001.241
iteration 0500: loss: 1993.288
iteration 0600: loss: 1988.971
iteration 0700: loss: 1991.637
iteration 0800: loss: 1989.793
iteration 0900: loss: 1990.421
====> Epoch: 001 Train loss: 2019.8957  took : 13.070018291473389
====> Test loss: 1992.3104
iteration 0000: loss: 1985.060
iteration 0100: loss: 1987.698
iteration 0200: loss: 1987.533
iteration 0300: loss: 1987.254
iteration 0400: loss: 1982.721
iteration 0500: loss: 1983.249
iteration 0600: loss: 1980.024
iteration 0700: loss: 1978.011
iteration 0800: loss: 1978.402
iteration 0900: loss: 1973.730
====> Epoch: 002 Train loss: 1983.3569  took : 12.473447799682617
====> Test loss: 1981.3694
iteration 0000: loss: 1975.655
iteration 0100: loss: 1980.343
iteration 0200: loss: 1977.087
iteration 0300: loss: 1976.229
iteration 0400: loss: 1978.244
iteration 0500: loss: 1977.005
iteration 0600: loss: 1965.392
iteration 0700: loss: 1969.853
iteration 0800: loss: 1968.477
iteration 0900: loss: 1971.808
====> Epoch: 003 Train loss: 1973.3213  took : 13.487818002700806
====> Test loss: 1971.9712
iteration 0000: loss: 1971.650
iteration 0100: loss: 1965.152
iteration 0200: loss: 1972.921
iteration 0300: loss: 1964.697
iteration 0400: loss: 1963.645
iteration 0500: loss: 1970.516
iteration 0600: loss: 1968.193
iteration 0700: loss: 1963.641
iteration 0800: loss: 1966.301
iteration 0900: loss: 1966.794
====> Epoch: 004 Train loss: 1967.1578  took : 13.088668823242188
====> Test loss: 1967.7616
iteration 0000: loss: 1964.424
iteration 0100: loss: 1963.285
iteration 0200: loss: 1966.210
iteration 0300: loss: 1959.500
iteration 0400: loss: 1961.246
iteration 0500: loss: 1963.656
iteration 0600: loss: 1965.109
iteration 0700: loss: 1960.808
iteration 0800: loss: 1960.856
iteration 0900: loss: 1960.155
====> Epoch: 005 Train loss: 1963.2309  took : 13.515284538269043
====> Test loss: 1965.8308
iteration 0000: loss: 1965.556
iteration 0100: loss: 1960.953
iteration 0200: loss: 1964.074
iteration 0300: loss: 1958.129
iteration 0400: loss: 1959.824
iteration 0500: loss: 1962.395
iteration 0600: loss: 1955.562
iteration 0700: loss: 1959.243
iteration 0800: loss: 1958.288
iteration 0900: loss: 1965.534
====> Epoch: 006 Train loss: 1960.9082  took : 12.839798212051392
====> Test loss: 1963.1640
iteration 0000: loss: 1957.639
iteration 0100: loss: 1963.769
iteration 0200: loss: 1954.735
iteration 0300: loss: 1960.616
iteration 0400: loss: 1958.114
iteration 0500: loss: 1963.285
iteration 0600: loss: 1957.776
iteration 0700: loss: 1958.508
iteration 0800: loss: 1959.582
iteration 0900: loss: 1959.153
====> Epoch: 007 Train loss: 1959.5525  took : 13.297760725021362
====> Test loss: 1961.3452
iteration 0000: loss: 1962.332
iteration 0100: loss: 1957.742
iteration 0200: loss: 1958.609
iteration 0300: loss: 1955.578
iteration 0400: loss: 1959.296
iteration 0500: loss: 1957.933
iteration 0600: loss: 1957.522
iteration 0700: loss: 1960.970
iteration 0800: loss: 1958.217
iteration 0900: loss: 1954.761
====> Epoch: 008 Train loss: 1958.4125  took : 11.97859001159668
====> Test loss: 1960.0996
iteration 0000: loss: 1958.691
iteration 0100: loss: 1957.586
iteration 0200: loss: 1959.726
iteration 0300: loss: 1958.440
iteration 0400: loss: 1956.235
iteration 0500: loss: 1959.082
iteration 0600: loss: 1956.955
iteration 0700: loss: 1959.249
iteration 0800: loss: 1955.986
iteration 0900: loss: 1960.729
====> Epoch: 009 Train loss: 1957.4887  took : 13.184860467910767
====> Test loss: 1959.6067
iteration 0000: loss: 1955.827
iteration 0100: loss: 1956.156
iteration 0200: loss: 1957.143
iteration 0300: loss: 1957.554
iteration 0400: loss: 1958.645
iteration 0500: loss: 1957.239
iteration 0600: loss: 1957.930
iteration 0700: loss: 1955.587
iteration 0800: loss: 1956.299
iteration 0900: loss: 1955.571
====> Epoch: 010 Train loss: 1956.7802  took : 12.793106317520142
====> Test loss: 1961.2959
iteration 0000: loss: 1955.062
iteration 0100: loss: 1955.549
iteration 0200: loss: 1955.672
iteration 0300: loss: 1957.063
iteration 0400: loss: 1956.763
iteration 0500: loss: 1957.626
iteration 0600: loss: 1958.768
iteration 0700: loss: 1954.908
iteration 0800: loss: 1956.154
iteration 0900: loss: 1959.547
====> Epoch: 011 Train loss: 1956.2066  took : 13.02619457244873
====> Test loss: 1959.0186
iteration 0000: loss: 1956.797
iteration 0100: loss: 1955.730
iteration 0200: loss: 1955.411
iteration 0300: loss: 1956.621
iteration 0400: loss: 1958.510
iteration 0500: loss: 1959.476
iteration 0600: loss: 1956.477
iteration 0700: loss: 1957.916
iteration 0800: loss: 1955.688
iteration 0900: loss: 1954.333
====> Epoch: 012 Train loss: 1955.5227  took : 12.370185852050781
====> Test loss: 1958.2278
iteration 0000: loss: 1952.853
iteration 0100: loss: 1953.611
iteration 0200: loss: 1956.596
iteration 0300: loss: 1957.104
iteration 0400: loss: 1953.379
iteration 0500: loss: 1954.518
iteration 0600: loss: 1954.825
iteration 0700: loss: 1955.259
iteration 0800: loss: 1953.748
iteration 0900: loss: 1952.966
====> Epoch: 013 Train loss: 1955.1358  took : 11.799574375152588
====> Test loss: 1957.4876
iteration 0000: loss: 1955.598
iteration 0100: loss: 1953.583
iteration 0200: loss: 1953.608
iteration 0300: loss: 1954.456
iteration 0400: loss: 1955.767
iteration 0500: loss: 1954.169
iteration 0600: loss: 1956.172
iteration 0700: loss: 1954.830
iteration 0800: loss: 1956.173
iteration 0900: loss: 1953.059
====> Epoch: 014 Train loss: 1954.6243  took : 12.949942111968994
====> Test loss: 1956.8596
iteration 0000: loss: 1950.873
iteration 0100: loss: 1952.762
iteration 0200: loss: 1953.926
iteration 0300: loss: 1955.777
iteration 0400: loss: 1958.783
iteration 0500: loss: 1956.225
iteration 0600: loss: 1950.641
iteration 0700: loss: 1953.931
iteration 0800: loss: 1953.605
iteration 0900: loss: 1955.902
====> Epoch: 015 Train loss: 1954.4398  took : 12.266465187072754
====> Test loss: 1956.9060
iteration 0000: loss: 1955.034
iteration 0100: loss: 1955.238
iteration 0200: loss: 1955.976
iteration 0300: loss: 1954.127
iteration 0400: loss: 1954.227
iteration 0500: loss: 1955.228
iteration 0600: loss: 1953.840
iteration 0700: loss: 1951.620
iteration 0800: loss: 1956.397
iteration 0900: loss: 1953.054
====> Epoch: 016 Train loss: 1954.0815  took : 12.393649816513062
====> Test loss: 1957.2543
iteration 0000: loss: 1957.683
iteration 0100: loss: 1953.190
iteration 0200: loss: 1953.789
iteration 0300: loss: 1953.517
iteration 0400: loss: 1952.624
iteration 0500: loss: 1953.047
iteration 0600: loss: 1951.791
iteration 0700: loss: 1952.844
iteration 0800: loss: 1950.972
iteration 0900: loss: 1952.256
====> Epoch: 017 Train loss: 1953.4660  took : 13.473400115966797
====> Test loss: 1955.8113
iteration 0000: loss: 1951.543
iteration 0100: loss: 1953.799
iteration 0200: loss: 1953.142
iteration 0300: loss: 1952.735
iteration 0400: loss: 1952.985
iteration 0500: loss: 1952.376
iteration 0600: loss: 1954.155
iteration 0700: loss: 1954.291
iteration 0800: loss: 1951.931
iteration 0900: loss: 1952.209
====> Epoch: 018 Train loss: 1953.1365  took : 13.484624147415161
====> Test loss: 1955.1362
iteration 0000: loss: 1952.326
iteration 0100: loss: 1952.429
iteration 0200: loss: 1952.731
iteration 0300: loss: 1952.238
iteration 0400: loss: 1951.638
iteration 0500: loss: 1953.446
iteration 0600: loss: 1953.098
iteration 0700: loss: 1952.557
iteration 0800: loss: 1953.528
iteration 0900: loss: 1954.530
====> Epoch: 019 Train loss: 1952.8820  took : 12.78941559791565
====> Test loss: 1955.7061
iteration 0000: loss: 1950.554
iteration 0100: loss: 1951.372
iteration 0200: loss: 1953.494
iteration 0300: loss: 1952.973
iteration 0400: loss: 1952.542
iteration 0500: loss: 1949.902
iteration 0600: loss: 1951.401
iteration 0700: loss: 1950.781
iteration 0800: loss: 1952.036
iteration 0900: loss: 1951.875
====> Epoch: 020 Train loss: 1952.5376  took : 11.828307867050171
====> Test loss: 1955.8080
iteration 0000: loss: 1954.029
iteration 0100: loss: 1952.700
iteration 0200: loss: 1952.825
iteration 0300: loss: 1953.108
iteration 0400: loss: 1953.296
iteration 0500: loss: 1952.094
iteration 0600: loss: 1951.052
iteration 0700: loss: 1952.222
iteration 0800: loss: 1953.568
iteration 0900: loss: 1951.318
====> Epoch: 021 Train loss: 1952.5245  took : 12.512152194976807
====> Test loss: 1955.1896
iteration 0000: loss: 1951.621
iteration 0100: loss: 1949.898
iteration 0200: loss: 1949.987
iteration 0300: loss: 1951.919
iteration 0400: loss: 1951.002
iteration 0500: loss: 1952.494
iteration 0600: loss: 1952.227
iteration 0700: loss: 1953.088
iteration 0800: loss: 1954.381
iteration 0900: loss: 1951.938
====> Epoch: 022 Train loss: 1952.1692  took : 12.913763999938965
====> Test loss: 1954.5742
iteration 0000: loss: 1953.605
iteration 0100: loss: 1952.897
iteration 0200: loss: 1951.682
iteration 0300: loss: 1951.416
iteration 0400: loss: 1950.894
iteration 0500: loss: 1950.237
iteration 0600: loss: 1953.325
iteration 0700: loss: 1950.747
iteration 0800: loss: 1950.627
iteration 0900: loss: 1951.858
====> Epoch: 023 Train loss: 1951.8913  took : 13.144600868225098
====> Test loss: 1954.9679
iteration 0000: loss: 1954.795
iteration 0100: loss: 1949.097
iteration 0200: loss: 1953.077
iteration 0300: loss: 1951.500
iteration 0400: loss: 1951.206
iteration 0500: loss: 1952.599
iteration 0600: loss: 1953.487
iteration 0700: loss: 1950.359
iteration 0800: loss: 1950.784
iteration 0900: loss: 1950.139
====> Epoch: 024 Train loss: 1951.7665  took : 13.407974004745483
====> Test loss: 1953.8385
iteration 0000: loss: 1948.126
iteration 0100: loss: 1949.393
iteration 0200: loss: 1948.937
iteration 0300: loss: 1949.242
iteration 0400: loss: 1951.159
iteration 0500: loss: 1950.860
iteration 0600: loss: 1949.356
iteration 0700: loss: 1949.723
iteration 0800: loss: 1951.470
iteration 0900: loss: 1951.153
====> Epoch: 025 Train loss: 1951.3871  took : 12.246564626693726
====> Test loss: 1954.0668
iteration 0000: loss: 1951.835
iteration 0100: loss: 1952.970
iteration 0200: loss: 1953.593
iteration 0300: loss: 1951.269
iteration 0400: loss: 1950.272
iteration 0500: loss: 1950.040
iteration 0600: loss: 1952.773
iteration 0700: loss: 1953.186
iteration 0800: loss: 1952.320
iteration 0900: loss: 1951.421
====> Epoch: 026 Train loss: 1951.2960  took : 13.204117059707642
====> Test loss: 1953.6815
iteration 0000: loss: 1951.581
iteration 0100: loss: 1952.921
iteration 0200: loss: 1952.264
iteration 0300: loss: 1950.688
iteration 0400: loss: 1949.585
iteration 0500: loss: 1952.043
iteration 0600: loss: 1950.951
iteration 0700: loss: 1951.177
iteration 0800: loss: 1953.053
iteration 0900: loss: 1951.636
====> Epoch: 027 Train loss: 1951.2035  took : 12.480295419692993
====> Test loss: 1953.8796
iteration 0000: loss: 1954.236
iteration 0100: loss: 1952.635
iteration 0200: loss: 1951.935
iteration 0300: loss: 1952.443
iteration 0400: loss: 1954.184
iteration 0500: loss: 1951.282
iteration 0600: loss: 1951.125
iteration 0700: loss: 1950.823
iteration 0800: loss: 1949.090
iteration 0900: loss: 1950.670
====> Epoch: 028 Train loss: 1951.3511  took : 13.002854108810425
====> Test loss: 1953.9337
iteration 0000: loss: 1950.418
iteration 0100: loss: 1952.000
iteration 0200: loss: 1951.435
iteration 0300: loss: 1949.913
iteration 0400: loss: 1950.967
iteration 0500: loss: 1951.812
iteration 0600: loss: 1951.107
iteration 0700: loss: 1952.470
iteration 0800: loss: 1950.676
iteration 0900: loss: 1950.857
====> Epoch: 029 Train loss: 1951.1538  took : 12.08776307106018
====> Test loss: 1954.0464
iteration 0000: loss: 1952.566
iteration 0100: loss: 1950.651
iteration 0200: loss: 1949.863
iteration 0300: loss: 1950.541
iteration 0400: loss: 1952.682
iteration 0500: loss: 1950.287
iteration 0600: loss: 1950.283
iteration 0700: loss: 1950.949
iteration 0800: loss: 1951.901
iteration 0900: loss: 1951.491
====> Epoch: 030 Train loss: 1950.9102  took : 13.282338619232178
====> Test loss: 1953.0577
iteration 0000: loss: 1950.501
iteration 0100: loss: 1950.945
iteration 0200: loss: 1950.552
iteration 0300: loss: 1948.820
iteration 0400: loss: 1954.817
iteration 0500: loss: 1952.088
iteration 0600: loss: 1949.148
iteration 0700: loss: 1952.755
iteration 0800: loss: 1949.752
iteration 0900: loss: 1950.940
====> Epoch: 031 Train loss: 1950.7876  took : 13.445968866348267
====> Test loss: 1954.0589
iteration 0000: loss: 1951.582
iteration 0100: loss: 1950.397
iteration 0200: loss: 1949.496
iteration 0300: loss: 1950.522
iteration 0400: loss: 1951.953
iteration 0500: loss: 1950.761
iteration 0600: loss: 1950.052
iteration 0700: loss: 1949.683
iteration 0800: loss: 1950.972
iteration 0900: loss: 1950.626
====> Epoch: 032 Train loss: 1950.6625  took : 12.19977593421936
====> Test loss: 1952.6860
iteration 0000: loss: 1950.128
iteration 0100: loss: 1949.056
iteration 0200: loss: 1949.694
iteration 0300: loss: 1951.393
iteration 0400: loss: 1951.375
iteration 0500: loss: 1950.867
iteration 0600: loss: 1948.689
iteration 0700: loss: 1947.986
iteration 0800: loss: 1949.464
iteration 0900: loss: 1949.465
====> Epoch: 033 Train loss: 1950.3394  took : 12.441379308700562
====> Test loss: 1953.1263
iteration 0000: loss: 1951.156
iteration 0100: loss: 1949.822
iteration 0200: loss: 1952.000
iteration 0300: loss: 1950.151
iteration 0400: loss: 1949.671
iteration 0500: loss: 1951.004
iteration 0600: loss: 1951.185
iteration 0700: loss: 1949.594
iteration 0800: loss: 1952.492
iteration 0900: loss: 1949.689
====> Epoch: 034 Train loss: 1950.3083  took : 13.060232639312744
====> Test loss: 1952.4578
iteration 0000: loss: 1949.830
iteration 0100: loss: 1952.196
iteration 0200: loss: 1951.829
iteration 0300: loss: 1950.093
iteration 0400: loss: 1950.470
iteration 0500: loss: 1950.128
iteration 0600: loss: 1952.982
iteration 0700: loss: 1951.643
iteration 0800: loss: 1951.375
iteration 0900: loss: 1950.232
====> Epoch: 035 Train loss: 1950.3365  took : 12.647969961166382
====> Test loss: 1952.6445
iteration 0000: loss: 1950.060
iteration 0100: loss: 1949.998
iteration 0200: loss: 1951.844
iteration 0300: loss: 1948.411
iteration 0400: loss: 1950.058
iteration 0500: loss: 1950.547
iteration 0600: loss: 1949.352
iteration 0700: loss: 1950.615
iteration 0800: loss: 1949.253
iteration 0900: loss: 1950.199
====> Epoch: 036 Train loss: 1950.4051  took : 12.964781999588013
====> Test loss: 1952.6605
iteration 0000: loss: 1948.935
iteration 0100: loss: 1950.277
iteration 0200: loss: 1952.529
iteration 0300: loss: 1951.700
iteration 0400: loss: 1952.361
iteration 0500: loss: 1951.534
iteration 0600: loss: 1948.123
iteration 0700: loss: 1948.829
iteration 0800: loss: 1953.025
iteration 0900: loss: 1949.029
====> Epoch: 037 Train loss: 1950.1398  took : 13.31774640083313
====> Test loss: 1952.2971
iteration 0000: loss: 1949.551
iteration 0100: loss: 1949.360
iteration 0200: loss: 1949.571
iteration 0300: loss: 1950.827
iteration 0400: loss: 1951.486
iteration 0500: loss: 1950.086
iteration 0600: loss: 1950.651
iteration 0700: loss: 1950.492
iteration 0800: loss: 1951.268
iteration 0900: loss: 1948.669
====> Epoch: 038 Train loss: 1950.1902  took : 12.447680711746216
====> Test loss: 1952.1299
iteration 0000: loss: 1948.231
iteration 0100: loss: 1949.787
iteration 0200: loss: 1950.231
iteration 0300: loss: 1948.534
iteration 0400: loss: 1948.180
iteration 0500: loss: 1952.574
iteration 0600: loss: 1948.711
iteration 0700: loss: 1950.312
iteration 0800: loss: 1951.088
iteration 0900: loss: 1950.671
====> Epoch: 039 Train loss: 1950.1811  took : 13.338987112045288
====> Test loss: 1952.5783
iteration 0000: loss: 1948.427
iteration 0100: loss: 1950.377
iteration 0200: loss: 1949.487
iteration 0300: loss: 1949.212
iteration 0400: loss: 1949.871
iteration 0500: loss: 1951.562
iteration 0600: loss: 1949.641
iteration 0700: loss: 1949.591
iteration 0800: loss: 1949.338
iteration 0900: loss: 1949.615
====> Epoch: 040 Train loss: 1950.1634  took : 12.129352807998657
====> Test loss: 1952.3638
iteration 0000: loss: 1948.516
iteration 0100: loss: 1949.245
iteration 0200: loss: 1950.011
iteration 0300: loss: 1949.144
iteration 0400: loss: 1949.796
iteration 0500: loss: 1950.077
iteration 0600: loss: 1948.506
iteration 0700: loss: 1950.129
iteration 0800: loss: 1949.075
iteration 0900: loss: 1949.244
====> Epoch: 041 Train loss: 1950.0212  took : 12.662384033203125
====> Test loss: 1952.6103
iteration 0000: loss: 1950.908
iteration 0100: loss: 1949.234
iteration 0200: loss: 1949.392
iteration 0300: loss: 1950.317
iteration 0400: loss: 1949.415
iteration 0500: loss: 1951.498
iteration 0600: loss: 1948.641
iteration 0700: loss: 1950.944
iteration 0800: loss: 1950.475
iteration 0900: loss: 1949.307
====> Epoch: 042 Train loss: 1949.8650  took : 13.556150436401367
====> Test loss: 1952.0725
iteration 0000: loss: 1951.025
iteration 0100: loss: 1950.999
iteration 0200: loss: 1949.913
iteration 0300: loss: 1949.894
iteration 0400: loss: 1950.214
iteration 0500: loss: 1948.519
iteration 0600: loss: 1949.444
iteration 0700: loss: 1950.441
iteration 0800: loss: 1948.323
iteration 0900: loss: 1948.024
====> Epoch: 043 Train loss: 1949.8707  took : 12.800832748413086
====> Test loss: 1952.1460
iteration 0000: loss: 1948.997
iteration 0100: loss: 1950.672
iteration 0200: loss: 1949.972
iteration 0300: loss: 1948.484
iteration 0400: loss: 1950.332
iteration 0500: loss: 1949.844
iteration 0600: loss: 1948.988
iteration 0700: loss: 1949.720
iteration 0800: loss: 1950.662
iteration 0900: loss: 1947.989
====> Epoch: 044 Train loss: 1949.8054  took : 13.071063041687012
====> Test loss: 1951.7545
iteration 0000: loss: 1949.762
iteration 0100: loss: 1950.329
iteration 0200: loss: 1949.580
iteration 0300: loss: 1951.766
iteration 0400: loss: 1950.214
iteration 0500: loss: 1950.748
iteration 0600: loss: 1948.646
iteration 0700: loss: 1948.834
iteration 0800: loss: 1948.743
iteration 0900: loss: 1949.531
====> Epoch: 045 Train loss: 1949.8350  took : 11.89587140083313
====> Test loss: 1952.3256
iteration 0000: loss: 1949.061
iteration 0100: loss: 1949.161
iteration 0200: loss: 1950.010
iteration 0300: loss: 1948.729
iteration 0400: loss: 1950.094
iteration 0500: loss: 1948.802
iteration 0600: loss: 1951.356
iteration 0700: loss: 1949.247
iteration 0800: loss: 1948.607
iteration 0900: loss: 1950.122
====> Epoch: 046 Train loss: 1949.6768  took : 13.548081636428833
====> Test loss: 1952.0530
iteration 0000: loss: 1948.805
iteration 0100: loss: 1950.312
iteration 0200: loss: 1948.528
iteration 0300: loss: 1949.602
iteration 0400: loss: 1950.142
iteration 0500: loss: 1948.241
iteration 0600: loss: 1952.023
iteration 0700: loss: 1948.554
iteration 0800: loss: 1949.569
iteration 0900: loss: 1948.945
====> Epoch: 047 Train loss: 1949.6060  took : 13.46794843673706
====> Test loss: 1951.7266
iteration 0000: loss: 1947.901
iteration 0100: loss: 1948.126
iteration 0200: loss: 1950.837
iteration 0300: loss: 1947.591
iteration 0400: loss: 1949.745
iteration 0500: loss: 1948.976
iteration 0600: loss: 1949.446
iteration 0700: loss: 1949.972
iteration 0800: loss: 1948.158
iteration 0900: loss: 1950.186
====> Epoch: 048 Train loss: 1949.6953  took : 12.934349775314331
====> Test loss: 1952.1042
iteration 0000: loss: 1949.085
iteration 0100: loss: 1948.702
iteration 0200: loss: 1949.095
iteration 0300: loss: 1949.290
iteration 0400: loss: 1949.591
iteration 0500: loss: 1950.427
iteration 0600: loss: 1949.454
iteration 0700: loss: 1949.296
iteration 0800: loss: 1949.303
iteration 0900: loss: 1949.655
====> Epoch: 049 Train loss: 1949.6301  took : 12.499919414520264
====> Test loss: 1952.3614
iteration 0000: loss: 1951.165
iteration 0100: loss: 1950.113
iteration 0200: loss: 1950.428
iteration 0300: loss: 1949.019
iteration 0400: loss: 1949.183
iteration 0500: loss: 1949.808
iteration 0600: loss: 1949.139
iteration 0700: loss: 1948.937
iteration 0800: loss: 1950.560
iteration 0900: loss: 1952.224
====> Epoch: 050 Train loss: 1949.7004  took : 12.325640201568604
====> Test loss: 1951.6287
====> [MM-VAE] Time: 713.583s or 00:11:53
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
set seed is  19
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_19
Run Directory:
 ./rslt/parameter-search-32/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_19
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Model is initialized without loading.

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5218.585
iteration 0100: loss: 4124.376
iteration 0200: loss: 4086.019
iteration 0300: loss: 4037.984
iteration 0400: loss: 4028.756
iteration 0500: loss: 4013.989
iteration 0600: loss: 4014.347
iteration 0700: loss: 4002.403
iteration 0800: loss: 3998.508
iteration 0900: loss: 4011.775
iteration 1000: loss: 4000.574
iteration 1100: loss: 4001.288
iteration 1200: loss: 4012.379
iteration 1300: loss: 4015.352
iteration 1400: loss: 3994.650
iteration 1500: loss: 3987.449
iteration 1600: loss: 3984.919
iteration 1700: loss: 3991.310
iteration 1800: loss: 3990.807
====> Epoch: 001 Train loss: 4024.6248  took : 53.596628189086914
====> Test loss: 3990.5977
iteration 0000: loss: 3993.787
iteration 0100: loss: 3995.078
iteration 0200: loss: 3983.336
iteration 0300: loss: 3988.667
iteration 0400: loss: 3981.530
iteration 0500: loss: 3979.697
iteration 0600: loss: 3979.520
iteration 0700: loss: 3978.335
iteration 0800: loss: 3986.561
iteration 0900: loss: 3983.322
iteration 1000: loss: 3974.104
iteration 1100: loss: 3977.780
iteration 1200: loss: 3966.393
iteration 1300: loss: 3971.372
iteration 1400: loss: 3968.704
iteration 1500: loss: 3957.083
iteration 1600: loss: 3973.766
iteration 1700: loss: 3965.827
iteration 1800: loss: 3967.756
====> Epoch: 002 Train loss: 3976.7955  took : 53.47755837440491
====> Test loss: 3970.7710
iteration 0000: loss: 3962.360
iteration 0100: loss: 3960.829
iteration 0200: loss: 3961.977
iteration 0300: loss: 3970.324
iteration 0400: loss: 3951.226
iteration 0500: loss: 3966.806
iteration 0600: loss: 3958.441
iteration 0700: loss: 3954.867
iteration 0800: loss: 3960.239
iteration 0900: loss: 3954.901
iteration 1000: loss: 3955.623
iteration 1100: loss: 3962.287
iteration 1200: loss: 3962.395
iteration 1300: loss: 3954.347
iteration 1400: loss: 3958.139
iteration 1500: loss: 3946.061
iteration 1600: loss: 3963.369
iteration 1700: loss: 3953.731
iteration 1800: loss: 3952.370
====> Epoch: 003 Train loss: 3959.6634  took : 53.40838146209717
====> Test loss: 3956.2478
iteration 0000: loss: 3954.005
iteration 0100: loss: 3949.265
iteration 0200: loss: 3948.388
iteration 0300: loss: 3948.570
iteration 0400: loss: 3954.184
iteration 0500: loss: 3954.184
iteration 0600: loss: 3953.037
iteration 0700: loss: 3948.923
iteration 0800: loss: 3946.467
iteration 0900: loss: 3949.665
iteration 1000: loss: 3956.412
iteration 1100: loss: 3942.703
iteration 1200: loss: 3952.539
iteration 1300: loss: 3949.508
iteration 1400: loss: 3950.958
iteration 1500: loss: 3950.462
iteration 1600: loss: 3951.525
iteration 1700: loss: 3949.348
iteration 1800: loss: 3948.610
====> Epoch: 004 Train loss: 3949.7909  took : 53.30395770072937
====> Test loss: 3950.5218
iteration 0000: loss: 3946.579
iteration 0100: loss: 3942.233
iteration 0200: loss: 3948.436
iteration 0300: loss: 3945.648
iteration 0400: loss: 3945.579
iteration 0500: loss: 3950.244
iteration 0600: loss: 3945.800
iteration 0700: loss: 3949.807
iteration 0800: loss: 3945.409
iteration 0900: loss: 3942.242
iteration 1000: loss: 3946.068
iteration 1100: loss: 3940.591
iteration 1200: loss: 3954.953
iteration 1300: loss: 3937.716
iteration 1400: loss: 3946.404
iteration 1500: loss: 3941.102
iteration 1600: loss: 3939.956
iteration 1700: loss: 3949.123
iteration 1800: loss: 3947.979
====> Epoch: 005 Train loss: 3946.0411  took : 53.31617093086243
====> Test loss: 3948.2850
iteration 0000: loss: 3941.337
iteration 0100: loss: 3943.059
iteration 0200: loss: 3946.670
iteration 0300: loss: 3943.933
iteration 0400: loss: 3946.042
iteration 0500: loss: 3939.280
iteration 0600: loss: 3941.258
iteration 0700: loss: 3941.265
iteration 0800: loss: 3942.442
iteration 0900: loss: 3941.382
iteration 1000: loss: 3945.990
iteration 1100: loss: 3944.321
iteration 1200: loss: 3941.985
iteration 1300: loss: 3942.805
iteration 1400: loss: 3941.646
iteration 1500: loss: 3944.081
iteration 1600: loss: 3941.657
iteration 1700: loss: 3945.469
iteration 1800: loss: 3938.550
====> Epoch: 006 Train loss: 3944.0612  took : 53.51168727874756
====> Test loss: 3947.2373
iteration 0000: loss: 3946.596
iteration 0100: loss: 3946.340
iteration 0200: loss: 3937.776
iteration 0300: loss: 3943.361
iteration 0400: loss: 3937.932
iteration 0500: loss: 3949.833
iteration 0600: loss: 3943.148
iteration 0700: loss: 3945.111
iteration 0800: loss: 3941.225
iteration 0900: loss: 3943.052
iteration 1000: loss: 3951.847
iteration 1100: loss: 3940.528
iteration 1200: loss: 3934.268
iteration 1300: loss: 3943.485
iteration 1400: loss: 3945.327
iteration 1500: loss: 3938.190
iteration 1600: loss: 3952.489
iteration 1700: loss: 3939.442
iteration 1800: loss: 3942.392
====> Epoch: 007 Train loss: 3943.0240  took : 53.23192024230957
====> Test loss: 3945.4863
iteration 0000: loss: 3945.147
iteration 0100: loss: 3941.148
iteration 0200: loss: 3943.010
iteration 0300: loss: 3941.481
iteration 0400: loss: 3941.647
iteration 0500: loss: 3946.819
iteration 0600: loss: 3945.188
iteration 0700: loss: 3947.805
iteration 0800: loss: 3943.543
iteration 0900: loss: 3944.995
iteration 1000: loss: 3943.739
iteration 1100: loss: 3943.644
iteration 1200: loss: 3943.722
iteration 1300: loss: 3944.846
iteration 1400: loss: 3947.064
iteration 1500: loss: 3938.694
iteration 1600: loss: 3941.316
iteration 1700: loss: 3937.533
iteration 1800: loss: 3938.262
====> Epoch: 008 Train loss: 3942.3964  took : 53.177314043045044
====> Test loss: 3945.7285
iteration 0000: loss: 3940.667
iteration 0100: loss: 3940.113
iteration 0200: loss: 3946.915
iteration 0300: loss: 3944.664
iteration 0400: loss: 3941.287
iteration 0500: loss: 3945.177
iteration 0600: loss: 3944.429
iteration 0700: loss: 3946.187
iteration 0800: loss: 3941.740
iteration 0900: loss: 3941.022
iteration 1000: loss: 3941.206
iteration 1100: loss: 3936.737
iteration 1200: loss: 3940.608
iteration 1300: loss: 3941.429
iteration 1400: loss: 3932.579
iteration 1500: loss: 3941.609
iteration 1600: loss: 3950.555
iteration 1700: loss: 3942.006
iteration 1800: loss: 3937.602
====> Epoch: 009 Train loss: 3941.7616  took : 53.12979602813721
====> Test loss: 3944.9478
iteration 0000: loss: 3942.154
iteration 0100: loss: 3941.533
iteration 0200: loss: 3939.855
iteration 0300: loss: 3937.489
iteration 0400: loss: 3942.269
iteration 0500: loss: 3937.042
iteration 0600: loss: 3940.092
iteration 0700: loss: 3946.612
iteration 0800: loss: 3935.968
iteration 0900: loss: 3941.573
iteration 1000: loss: 3943.748
iteration 1100: loss: 3943.177
iteration 1200: loss: 3942.123
iteration 1300: loss: 3942.436
iteration 1400: loss: 3940.938
iteration 1500: loss: 3936.867
iteration 1600: loss: 3940.609
iteration 1700: loss: 3946.811
iteration 1800: loss: 3939.608
====> Epoch: 010 Train loss: 3941.5002  took : 52.98013782501221
====> Test loss: 3945.1107
iteration 0000: loss: 3936.548
iteration 0100: loss: 3939.763
iteration 0200: loss: 3940.908
iteration 0300: loss: 3944.167
iteration 0400: loss: 3938.824
iteration 0500: loss: 3943.441
iteration 0600: loss: 3933.574
iteration 0700: loss: 3937.408
iteration 0800: loss: 3947.859
iteration 0900: loss: 3941.371
iteration 1000: loss: 3938.117
iteration 1100: loss: 3940.041
iteration 1200: loss: 3938.689
iteration 1300: loss: 3934.523
iteration 1400: loss: 3938.893
iteration 1500: loss: 3938.961
iteration 1600: loss: 3940.260
iteration 1700: loss: 3945.769
iteration 1800: loss: 3940.352
====> Epoch: 011 Train loss: 3941.2820  took : 53.188831090927124
====> Test loss: 3944.7061
iteration 0000: loss: 3945.598
iteration 0100: loss: 3943.480
iteration 0200: loss: 3940.077
iteration 0300: loss: 3935.597
iteration 0400: loss: 3943.673
iteration 0500: loss: 3940.651
iteration 0600: loss: 3937.797
iteration 0700: loss: 3944.449
iteration 0800: loss: 3943.963
iteration 0900: loss: 3940.148
iteration 1000: loss: 3942.339
iteration 1100: loss: 3945.630
iteration 1200: loss: 3941.334
iteration 1300: loss: 3945.356
iteration 1400: loss: 3943.776
iteration 1500: loss: 3944.651
iteration 1600: loss: 3937.677
iteration 1700: loss: 3944.464
iteration 1800: loss: 3938.873
====> Epoch: 012 Train loss: 3940.9535  took : 53.20604586601257
====> Test loss: 3944.3593
iteration 0000: loss: 3935.854
iteration 0100: loss: 3938.297
iteration 0200: loss: 3938.207
iteration 0300: loss: 3943.972
iteration 0400: loss: 3933.434
iteration 0500: loss: 3943.588
iteration 0600: loss: 3942.188
iteration 0700: loss: 3943.290
iteration 0800: loss: 3939.962
iteration 0900: loss: 3942.433
iteration 1000: loss: 3949.714
iteration 1100: loss: 3949.107
iteration 1200: loss: 3942.650
iteration 1300: loss: 3941.179
iteration 1400: loss: 3939.271
iteration 1500: loss: 3933.662
iteration 1600: loss: 3941.437
iteration 1700: loss: 3941.288
iteration 1800: loss: 3942.462
====> Epoch: 013 Train loss: 3940.6355  took : 53.178837299346924
====> Test loss: 3944.3349
iteration 0000: loss: 3938.122
iteration 0100: loss: 3936.253
iteration 0200: loss: 3933.956
iteration 0300: loss: 3939.690
iteration 0400: loss: 3948.574
iteration 0500: loss: 3940.312
iteration 0600: loss: 3941.450
iteration 0700: loss: 3939.620
iteration 0800: loss: 3936.401
iteration 0900: loss: 3939.755
iteration 1000: loss: 3938.437
iteration 1100: loss: 3935.614
iteration 1200: loss: 3937.841
iteration 1300: loss: 3948.174
iteration 1400: loss: 3945.639
iteration 1500: loss: 3934.177
iteration 1600: loss: 3937.578
iteration 1700: loss: 3939.811
iteration 1800: loss: 3939.375
====> Epoch: 014 Train loss: 3940.5008  took : 53.09919309616089
====> Test loss: 3945.1103
iteration 0000: loss: 3941.842
iteration 0100: loss: 3935.905
iteration 0200: loss: 3932.800
iteration 0300: loss: 3938.815
iteration 0400: loss: 3935.122
iteration 0500: loss: 3931.455
iteration 0600: loss: 3935.910
iteration 0700: loss: 3943.243
iteration 0800: loss: 3937.754
iteration 0900: loss: 3937.893
iteration 1000: loss: 3934.190
iteration 1100: loss: 3935.907
iteration 1200: loss: 3945.032
iteration 1300: loss: 3943.708
iteration 1400: loss: 3947.277
iteration 1500: loss: 3947.342
iteration 1600: loss: 3943.324
iteration 1700: loss: 3938.036
iteration 1800: loss: 3936.392
====> Epoch: 015 Train loss: 3940.2639  took : 53.18830633163452
====> Test loss: 3944.8510
iteration 0000: loss: 3942.925
iteration 0100: loss: 3943.033
iteration 0200: loss: 3948.473
iteration 0300: loss: 3941.465
iteration 0400: loss: 3941.129
iteration 0500: loss: 3940.633
iteration 0600: loss: 3940.805
iteration 0700: loss: 3948.779
iteration 0800: loss: 3946.013
iteration 0900: loss: 3945.838
iteration 1000: loss: 3938.220
iteration 1100: loss: 3943.638
iteration 1200: loss: 3934.376
iteration 1300: loss: 3940.636
iteration 1400: loss: 3942.160
iteration 1500: loss: 3932.828
iteration 1600: loss: 3941.912
iteration 1700: loss: 3942.558
iteration 1800: loss: 3940.957
====> Epoch: 016 Train loss: 3940.1051  took : 53.16065859794617
====> Test loss: 3943.9434
iteration 0000: loss: 3938.306
iteration 0100: loss: 3939.630
iteration 0200: loss: 3937.409
iteration 0300: loss: 3937.531
iteration 0400: loss: 3942.445
iteration 0500: loss: 3942.309
iteration 0600: loss: 3936.962
iteration 0700: loss: 3936.694
iteration 0800: loss: 3939.128
iteration 0900: loss: 3940.135
iteration 1000: loss: 3943.978
iteration 1100: loss: 3934.553
iteration 1200: loss: 3939.398
iteration 1300: loss: 3940.221
iteration 1400: loss: 3944.246
iteration 1500: loss: 3941.449
iteration 1600: loss: 3939.215
iteration 1700: loss: 3937.788
iteration 1800: loss: 3939.374
====> Epoch: 017 Train loss: 3939.9128  took : 53.01094365119934
====> Test loss: 3943.8196
iteration 0000: loss: 3939.785
iteration 0100: loss: 3938.115
iteration 0200: loss: 3941.330
iteration 0300: loss: 3939.841
iteration 0400: loss: 3953.903
iteration 0500: loss: 3939.529
iteration 0600: loss: 3943.177
iteration 0700: loss: 3936.508
iteration 0800: loss: 3934.311
iteration 0900: loss: 3942.478
iteration 1000: loss: 3936.869
iteration 1100: loss: 3938.290
iteration 1200: loss: 3942.201
iteration 1300: loss: 3941.793
iteration 1400: loss: 3942.411
iteration 1500: loss: 3939.021
iteration 1600: loss: 3942.742
iteration 1700: loss: 3942.086
iteration 1800: loss: 3937.102
====> Epoch: 018 Train loss: 3939.8319  took : 53.16461944580078
====> Test loss: 3944.7714
iteration 0000: loss: 3937.416
iteration 0100: loss: 3934.983
iteration 0200: loss: 3939.019
iteration 0300: loss: 3937.856
iteration 0400: loss: 3936.780
iteration 0500: loss: 3934.605
iteration 0600: loss: 3939.404
iteration 0700: loss: 3935.638
iteration 0800: loss: 3939.133
iteration 0900: loss: 3934.297
iteration 1000: loss: 3940.042
iteration 1100: loss: 3941.727
iteration 1200: loss: 3944.076
iteration 1300: loss: 3941.655
iteration 1400: loss: 3948.530
iteration 1500: loss: 3938.316
iteration 1600: loss: 3948.150
iteration 1700: loss: 3940.213
iteration 1800: loss: 3938.127
====> Epoch: 019 Train loss: 3939.5967  took : 53.07792925834656
====> Test loss: 3945.0055
iteration 0000: loss: 3940.753
iteration 0100: loss: 3932.208
iteration 0200: loss: 3935.242
iteration 0300: loss: 3939.243
iteration 0400: loss: 3941.665
iteration 0500: loss: 3937.802
iteration 0600: loss: 3935.832
iteration 0700: loss: 3932.437
iteration 0800: loss: 3941.140
iteration 0900: loss: 3943.813
iteration 1000: loss: 3941.158
iteration 1100: loss: 3941.127
iteration 1200: loss: 3936.115
iteration 1300: loss: 3942.735
iteration 1400: loss: 3940.572
iteration 1500: loss: 3942.617
iteration 1600: loss: 3937.897
iteration 1700: loss: 3939.656
iteration 1800: loss: 3935.974
====> Epoch: 020 Train loss: 3939.7210  took : 53.18601751327515
====> Test loss: 3943.5347
iteration 0000: loss: 3939.605
iteration 0100: loss: 3939.855
iteration 0200: loss: 3943.832
iteration 0300: loss: 3940.461
iteration 0400: loss: 3940.186
iteration 0500: loss: 3939.926
iteration 0600: loss: 3936.927
iteration 0700: loss: 3940.277
iteration 0800: loss: 3941.578
iteration 0900: loss: 3930.631
iteration 1000: loss: 3930.428
iteration 1100: loss: 3935.848
iteration 1200: loss: 3939.712
iteration 1300: loss: 3942.373
iteration 1400: loss: 3945.394
iteration 1500: loss: 3942.801
iteration 1600: loss: 3936.951
iteration 1700: loss: 3940.770
iteration 1800: loss: 3933.159
====> Epoch: 021 Train loss: 3939.4054  took : 53.2177050113678
====> Test loss: 3943.1937
iteration 0000: loss: 3938.658
iteration 0100: loss: 3935.923
iteration 0200: loss: 3942.653
iteration 0300: loss: 3939.472
iteration 0400: loss: 3940.932
iteration 0500: loss: 3941.156
iteration 0600: loss: 3943.129
iteration 0700: loss: 3931.641
iteration 0800: loss: 3951.221
iteration 0900: loss: 3941.570
iteration 1000: loss: 3938.557
iteration 1100: loss: 3935.298
iteration 1200: loss: 3940.284
iteration 1300: loss: 3936.279
iteration 1400: loss: 3946.234
iteration 1500: loss: 3938.092
iteration 1600: loss: 3933.841
iteration 1700: loss: 3944.112
iteration 1800: loss: 3938.081
====> Epoch: 022 Train loss: 3939.4414  took : 52.95323848724365
====> Test loss: 3943.6610
iteration 0000: loss: 3937.118
iteration 0100: loss: 3939.072
iteration 0200: loss: 3933.840
iteration 0300: loss: 3933.520
iteration 0400: loss: 3939.809
iteration 0500: loss: 3934.900
iteration 0600: loss: 3938.202
iteration 0700: loss: 3933.096
iteration 0800: loss: 3937.656
iteration 0900: loss: 3952.790
iteration 1000: loss: 3934.822
iteration 1100: loss: 3944.875
iteration 1200: loss: 3940.879
iteration 1300: loss: 3942.583
iteration 1400: loss: 3938.005
iteration 1500: loss: 3937.605
iteration 1600: loss: 3939.984
iteration 1700: loss: 3941.832
iteration 1800: loss: 3940.002
====> Epoch: 023 Train loss: 3939.3533  took : 52.924755334854126
====> Test loss: 3942.9778
iteration 0000: loss: 3942.251
iteration 0100: loss: 3939.494
iteration 0200: loss: 3934.834
iteration 0300: loss: 3939.064
iteration 0400: loss: 3935.507
iteration 0500: loss: 3941.816
iteration 0600: loss: 3937.181
iteration 0700: loss: 3942.689
iteration 0800: loss: 3938.470
iteration 0900: loss: 3938.733
iteration 1000: loss: 3935.980
iteration 1100: loss: 3936.244
iteration 1200: loss: 3939.883
iteration 1300: loss: 3937.190
iteration 1400: loss: 3935.651
iteration 1500: loss: 3934.043
iteration 1600: loss: 3942.680
iteration 1700: loss: 3937.182
iteration 1800: loss: 3937.269
====> Epoch: 024 Train loss: 3939.0965  took : 53.17558693885803
====> Test loss: 3943.2941
iteration 0000: loss: 3946.416
iteration 0100: loss: 3946.202
iteration 0200: loss: 3936.057
iteration 0300: loss: 3939.575
iteration 0400: loss: 3933.529
iteration 0500: loss: 3942.020
iteration 0600: loss: 3940.887
iteration 0700: loss: 3937.454
iteration 0800: loss: 3933.875
iteration 0900: loss: 3937.131
iteration 1000: loss: 3933.691
iteration 1100: loss: 3939.011
iteration 1200: loss: 3939.525
iteration 1300: loss: 3938.365
iteration 1400: loss: 3943.611
iteration 1500: loss: 3935.408
iteration 1600: loss: 3938.371
iteration 1700: loss: 3933.606
iteration 1800: loss: 3938.119
====> Epoch: 025 Train loss: 3938.9612  took : 53.06039381027222
====> Test loss: 3943.0902
iteration 0000: loss: 3942.637
iteration 0100: loss: 3939.344
iteration 0200: loss: 3934.715
iteration 0300: loss: 3938.458
iteration 0400: loss: 3936.183
iteration 0500: loss: 3935.428
iteration 0600: loss: 3944.615
iteration 0700: loss: 3936.263
iteration 0800: loss: 3934.232
iteration 0900: loss: 3942.874
iteration 1000: loss: 3939.692
iteration 1100: loss: 3943.001
iteration 1200: loss: 3941.839
iteration 1300: loss: 3937.765
iteration 1400: loss: 3941.048
iteration 1500: loss: 3941.644
iteration 1600: loss: 3940.646
iteration 1700: loss: 3943.096
iteration 1800: loss: 3934.751
====> Epoch: 026 Train loss: 3939.1639  took : 53.099013805389404
====> Test loss: 3943.3437
iteration 0000: loss: 3941.291
iteration 0100: loss: 3945.236
iteration 0200: loss: 3943.397
iteration 0300: loss: 3931.933
iteration 0400: loss: 3936.104
iteration 0500: loss: 3942.776
iteration 0600: loss: 3938.597
iteration 0700: loss: 3940.544
iteration 0800: loss: 3944.178
iteration 0900: loss: 3942.149
iteration 1000: loss: 3935.975
iteration 1100: loss: 3939.204
iteration 1200: loss: 3939.274
iteration 1300: loss: 3940.698
iteration 1400: loss: 3937.747
iteration 1500: loss: 3941.909
iteration 1600: loss: 3937.903
iteration 1700: loss: 3946.531
iteration 1800: loss: 3936.717
====> Epoch: 027 Train loss: 3938.9487  took : 52.97849154472351
====> Test loss: 3942.9944
iteration 0000: loss: 3937.139
iteration 0100: loss: 3940.508
iteration 0200: loss: 3935.488
iteration 0300: loss: 3941.876
iteration 0400: loss: 3935.763
iteration 0500: loss: 3936.148
iteration 0600: loss: 3940.175
iteration 0700: loss: 3936.459
iteration 0800: loss: 3934.392
iteration 0900: loss: 3937.377
iteration 1000: loss: 3936.485
iteration 1100: loss: 3934.037
iteration 1200: loss: 3938.053
iteration 1300: loss: 3940.840
iteration 1400: loss: 3937.749
iteration 1500: loss: 3936.922
iteration 1600: loss: 3937.264
iteration 1700: loss: 3936.675
iteration 1800: loss: 3938.862
====> Epoch: 028 Train loss: 3938.9587  took : 53.2515869140625
====> Test loss: 3942.8006
iteration 0000: loss: 3933.526
iteration 0100: loss: 3940.810
iteration 0200: loss: 3940.379
iteration 0300: loss: 3939.005
iteration 0400: loss: 3942.416
iteration 0500: loss: 3939.641
iteration 0600: loss: 3936.872
iteration 0700: loss: 3933.534
iteration 0800: loss: 3940.185
iteration 0900: loss: 3943.135
iteration 1000: loss: 3939.109
iteration 1100: loss: 3941.409
iteration 1200: loss: 3948.300
iteration 1300: loss: 3938.738
iteration 1400: loss: 3937.777
iteration 1500: loss: 3932.961
iteration 1600: loss: 3935.190
iteration 1700: loss: 3944.437
iteration 1800: loss: 3938.268
====> Epoch: 029 Train loss: 3938.6531  took : 53.301125049591064
====> Test loss: 3942.7058
iteration 0000: loss: 3937.595
iteration 0100: loss: 3938.830
iteration 0200: loss: 3933.454
iteration 0300: loss: 3938.904
iteration 0400: loss: 3939.171
iteration 0500: loss: 3934.698
iteration 0600: loss: 3934.594
iteration 0700: loss: 3944.379
iteration 0800: loss: 3940.229
iteration 0900: loss: 3937.837
iteration 1000: loss: 3933.681
iteration 1100: loss: 3940.908
iteration 1200: loss: 3934.954
iteration 1300: loss: 3937.911
iteration 1400: loss: 3939.295
iteration 1500: loss: 3934.272
iteration 1600: loss: 3940.187
iteration 1700: loss: 3939.353
iteration 1800: loss: 3931.736
====> Epoch: 030 Train loss: 3938.6888  took : 53.15748858451843
====> Test loss: 3943.0496
iteration 0000: loss: 3935.566
iteration 0100: loss: 3931.935
iteration 0200: loss: 3940.405
iteration 0300: loss: 3940.006
iteration 0400: loss: 3938.642
iteration 0500: loss: 3940.039
iteration 0600: loss: 3936.290
iteration 0700: loss: 3937.694
iteration 0800: loss: 3938.403
iteration 0900: loss: 3942.711
iteration 1000: loss: 3934.354
iteration 1100: loss: 3934.139
iteration 1200: loss: 3937.632
iteration 1300: loss: 3939.869
iteration 1400: loss: 3939.087
iteration 1500: loss: 3941.823
iteration 1600: loss: 3935.392
iteration 1700: loss: 3938.695
iteration 1800: loss: 3940.625
====> Epoch: 031 Train loss: 3938.5336  took : 53.011520862579346
====> Test loss: 3942.4211
iteration 0000: loss: 3935.816
iteration 0100: loss: 3943.702
iteration 0200: loss: 3942.109
iteration 0300: loss: 3939.422
iteration 0400: loss: 3936.414
iteration 0500: loss: 3936.158
iteration 0600: loss: 3936.755
iteration 0700: loss: 3935.701
iteration 0800: loss: 3939.851
iteration 0900: loss: 3942.251
iteration 1000: loss: 3936.075
iteration 1100: loss: 3936.129
iteration 1200: loss: 3934.128
iteration 1300: loss: 3934.073
iteration 1400: loss: 3936.487
iteration 1500: loss: 3936.432
iteration 1600: loss: 3934.967
iteration 1700: loss: 3930.104
iteration 1800: loss: 3936.045
====> Epoch: 032 Train loss: 3938.5123  took : 53.564836740493774
====> Test loss: 3942.3922
iteration 0000: loss: 3935.985
iteration 0100: loss: 3937.927
iteration 0200: loss: 3939.632
iteration 0300: loss: 3936.589
iteration 0400: loss: 3943.767
iteration 0500: loss: 3925.806
iteration 0600: loss: 3938.133
iteration 0700: loss: 3940.249
iteration 0800: loss: 3931.477
iteration 0900: loss: 3938.954
iteration 1000: loss: 3942.440
iteration 1100: loss: 3938.878
iteration 1200: loss: 3939.481
iteration 1300: loss: 3947.300
iteration 1400: loss: 3940.110
iteration 1500: loss: 3941.954
iteration 1600: loss: 3939.137
iteration 1700: loss: 3935.912
iteration 1800: loss: 3939.583
====> Epoch: 033 Train loss: 3938.4558  took : 52.970386028289795
====> Test loss: 3942.7831
iteration 0000: loss: 3931.687
iteration 0100: loss: 3940.455
iteration 0200: loss: 3936.022
iteration 0300: loss: 3936.957
iteration 0400: loss: 3943.963
iteration 0500: loss: 3935.864
iteration 0600: loss: 3938.659
iteration 0700: loss: 3936.919
iteration 0800: loss: 3929.885
iteration 0900: loss: 3933.632
iteration 1000: loss: 3936.927
iteration 1100: loss: 3939.459
iteration 1200: loss: 3932.710
iteration 1300: loss: 3940.531
iteration 1400: loss: 3932.831
iteration 1500: loss: 3936.487
iteration 1600: loss: 3942.589
iteration 1700: loss: 3935.765
iteration 1800: loss: 3936.640
====> Epoch: 034 Train loss: 3938.4703  took : 52.99926805496216
====> Test loss: 3943.1511
iteration 0000: loss: 3937.708
iteration 0100: loss: 3935.622
iteration 0200: loss: 3933.895
iteration 0300: loss: 3941.863
iteration 0400: loss: 3941.673
iteration 0500: loss: 3936.375
iteration 0600: loss: 3937.331
iteration 0700: loss: 3937.892
iteration 0800: loss: 3937.410
iteration 0900: loss: 3941.209
iteration 1000: loss: 3940.063
iteration 1100: loss: 3943.324
iteration 1200: loss: 3942.749
iteration 1300: loss: 3944.725
iteration 1400: loss: 3938.769
iteration 1500: loss: 3934.037
iteration 1600: loss: 3937.936
iteration 1700: loss: 3939.482
iteration 1800: loss: 3942.480
====> Epoch: 035 Train loss: 3938.3438  took : 53.01571178436279
====> Test loss: 3942.0458
iteration 0000: loss: 3935.423
iteration 0100: loss: 3938.421
iteration 0200: loss: 3936.366
iteration 0300: loss: 3940.786
iteration 0400: loss: 3943.361
iteration 0500: loss: 3936.560
iteration 0600: loss: 3941.855
iteration 0700: loss: 3936.172
iteration 0800: loss: 3931.793
iteration 0900: loss: 3942.145
iteration 1000: loss: 3937.437
iteration 1100: loss: 3933.574
iteration 1200: loss: 3939.281
iteration 1300: loss: 3940.994
iteration 1400: loss: 3934.177
iteration 1500: loss: 3941.246
iteration 1600: loss: 3940.468
iteration 1700: loss: 3937.428
iteration 1800: loss: 3939.458
====> Epoch: 036 Train loss: 3938.4459  took : 53.1954619884491
====> Test loss: 3942.4388
iteration 0000: loss: 3937.576
iteration 0100: loss: 3935.801
iteration 0200: loss: 3941.674
iteration 0300: loss: 3940.336
iteration 0400: loss: 3938.268
iteration 0500: loss: 3935.942
iteration 0600: loss: 3942.940
iteration 0700: loss: 3942.404
iteration 0800: loss: 3940.841
iteration 0900: loss: 3941.897
iteration 1000: loss: 3934.729
iteration 1100: loss: 3934.369
iteration 1200: loss: 3935.140
iteration 1300: loss: 3936.981
iteration 1400: loss: 3934.989
iteration 1500: loss: 3944.367
iteration 1600: loss: 3932.533
iteration 1700: loss: 3940.825
iteration 1800: loss: 3940.931
====> Epoch: 037 Train loss: 3938.3192  took : 53.282198905944824
====> Test loss: 3942.5431
iteration 0000: loss: 3935.307
iteration 0100: loss: 3936.376
iteration 0200: loss: 3934.285
iteration 0300: loss: 3934.741
iteration 0400: loss: 3939.279
iteration 0500: loss: 3940.907
iteration 0600: loss: 3939.291
iteration 0700: loss: 3933.554
iteration 0800: loss: 3937.492
iteration 0900: loss: 3940.458
iteration 1000: loss: 3939.256
iteration 1100: loss: 3937.881
iteration 1200: loss: 3941.228
iteration 1300: loss: 3938.563
iteration 1400: loss: 3940.524
iteration 1500: loss: 3942.676
iteration 1600: loss: 3936.586
iteration 1700: loss: 3939.184
iteration 1800: loss: 3931.903
====> Epoch: 038 Train loss: 3938.2945  took : 52.87677502632141
====> Test loss: 3942.0326
iteration 0000: loss: 3938.890
iteration 0100: loss: 3937.839
iteration 0200: loss: 3933.781
iteration 0300: loss: 3941.024
iteration 0400: loss: 3934.304
iteration 0500: loss: 3933.663
iteration 0600: loss: 3940.908
iteration 0700: loss: 3937.099
iteration 0800: loss: 3937.572
iteration 0900: loss: 3940.928
iteration 1000: loss: 3935.001
iteration 1100: loss: 3940.033
iteration 1200: loss: 3940.683
iteration 1300: loss: 3942.704
iteration 1400: loss: 3943.612
iteration 1500: loss: 3933.644
iteration 1600: loss: 3943.508
iteration 1700: loss: 3938.082
iteration 1800: loss: 3938.140
====> Epoch: 039 Train loss: 3938.2445  took : 53.11969470977783
====> Test loss: 3942.6623
iteration 0000: loss: 3935.611
iteration 0100: loss: 3940.902
iteration 0200: loss: 3940.278
iteration 0300: loss: 3938.143
iteration 0400: loss: 3941.793
iteration 0500: loss: 3939.094
iteration 0600: loss: 3939.145
iteration 0700: loss: 3935.154
iteration 0800: loss: 3932.774
iteration 0900: loss: 3941.993
iteration 1000: loss: 3936.459
iteration 1100: loss: 3945.315
iteration 1200: loss: 3937.255
iteration 1300: loss: 3937.618
iteration 1400: loss: 3940.489
iteration 1500: loss: 3933.073
iteration 1600: loss: 3935.703
iteration 1700: loss: 3937.702
iteration 1800: loss: 3930.869
====> Epoch: 040 Train loss: 3938.2683  took : 53.13133096694946
====> Test loss: 3942.4152
iteration 0000: loss: 3947.986
iteration 0100: loss: 3939.542
iteration 0200: loss: 3942.112
iteration 0300: loss: 3940.124
iteration 0400: loss: 3936.305
iteration 0500: loss: 3933.837
iteration 0600: loss: 3939.661
iteration 0700: loss: 3934.869
iteration 0800: loss: 3937.554
iteration 0900: loss: 3934.489
iteration 1000: loss: 3941.054
iteration 1100: loss: 3939.073
iteration 1200: loss: 3936.595
iteration 1300: loss: 3937.996
iteration 1400: loss: 3934.646
iteration 1500: loss: 3932.451
iteration 1600: loss: 3937.087
iteration 1700: loss: 3933.181
iteration 1800: loss: 3935.937
====> Epoch: 041 Train loss: 3938.2139  took : 52.97388529777527
====> Test loss: 3942.1432
iteration 0000: loss: 3939.043
iteration 0100: loss: 3935.562
iteration 0200: loss: 3934.411
iteration 0300: loss: 3934.022
iteration 0400: loss: 3941.272
iteration 0500: loss: 3938.440
iteration 0600: loss: 3931.390
iteration 0700: loss: 3937.838
iteration 0800: loss: 3940.974
iteration 0900: loss: 3944.026
iteration 1000: loss: 3938.638
iteration 1100: loss: 3939.240
iteration 1200: loss: 3938.022
iteration 1300: loss: 3936.646
iteration 1400: loss: 3935.278
iteration 1500: loss: 3930.725
iteration 1600: loss: 3940.390
iteration 1700: loss: 3937.262
iteration 1800: loss: 3939.309
====> Epoch: 042 Train loss: 3938.2051  took : 52.92535972595215
====> Test loss: 3942.2973
iteration 0000: loss: 3937.812
iteration 0100: loss: 3934.420
iteration 0200: loss: 3939.731
iteration 0300: loss: 3937.869
iteration 0400: loss: 3933.425
iteration 0500: loss: 3937.135
iteration 0600: loss: 3941.590
iteration 0700: loss: 3939.446
iteration 0800: loss: 3937.127
iteration 0900: loss: 3937.749
iteration 1000: loss: 3939.753
iteration 1100: loss: 3936.820
iteration 1200: loss: 3940.920
iteration 1300: loss: 3938.474
iteration 1400: loss: 3939.947
iteration 1500: loss: 3940.312
iteration 1600: loss: 3938.494
iteration 1700: loss: 3937.893
iteration 1800: loss: 3936.216
====> Epoch: 043 Train loss: 3937.9861  took : 53.0858097076416
====> Test loss: 3942.3482
iteration 0000: loss: 3940.360
iteration 0100: loss: 3933.049
iteration 0200: loss: 3935.189
iteration 0300: loss: 3944.072
iteration 0400: loss: 3940.026
iteration 0500: loss: 3939.315
iteration 0600: loss: 3937.288
iteration 0700: loss: 3941.281
iteration 0800: loss: 3934.269
iteration 0900: loss: 3935.958
iteration 1000: loss: 3935.594
iteration 1100: loss: 3936.396
iteration 1200: loss: 3937.527
iteration 1300: loss: 3934.109
iteration 1400: loss: 3933.675
iteration 1500: loss: 3939.576
iteration 1600: loss: 3943.685
iteration 1700: loss: 3937.027
iteration 1800: loss: 3934.444
====> Epoch: 044 Train loss: 3937.9726  took : 53.149035692214966
====> Test loss: 3942.2764
iteration 0000: loss: 3936.401
iteration 0100: loss: 3939.252
iteration 0200: loss: 3933.463
iteration 0300: loss: 3938.118
iteration 0400: loss: 3937.208
iteration 0500: loss: 3937.530
iteration 0600: loss: 3936.902
iteration 0700: loss: 3943.052
iteration 0800: loss: 3934.337
iteration 0900: loss: 3936.050
iteration 1000: loss: 3940.128
iteration 1100: loss: 3936.401
iteration 1200: loss: 3941.426
iteration 1300: loss: 3936.911
iteration 1400: loss: 3939.842
iteration 1500: loss: 3939.087
iteration 1600: loss: 3936.875
iteration 1700: loss: 3946.208
iteration 1800: loss: 3938.030
====> Epoch: 045 Train loss: 3938.0439  took : 53.11414551734924
====> Test loss: 3942.3224
iteration 0000: loss: 3948.431
iteration 0100: loss: 3934.373
iteration 0200: loss: 3941.065
iteration 0300: loss: 3943.810
iteration 0400: loss: 3939.133
iteration 0500: loss: 3944.084
iteration 0600: loss: 3942.432
iteration 0700: loss: 3940.937
iteration 0800: loss: 3939.095
iteration 0900: loss: 3939.563
iteration 1000: loss: 3934.034
iteration 1100: loss: 3936.006
iteration 1200: loss: 3939.215
iteration 1300: loss: 3939.304
iteration 1400: loss: 3941.402
iteration 1500: loss: 3944.208
iteration 1600: loss: 3939.329
iteration 1700: loss: 3941.921
iteration 1800: loss: 3945.452
====> Epoch: 046 Train loss: 3937.9037  took : 52.93865776062012
====> Test loss: 3942.6619
iteration 0000: loss: 3937.471
iteration 0100: loss: 3940.196
iteration 0200: loss: 3941.831
iteration 0300: loss: 3937.500
iteration 0400: loss: 3942.832
iteration 0500: loss: 3941.723
iteration 0600: loss: 3934.473
iteration 0700: loss: 3933.364
iteration 0800: loss: 3943.330
iteration 0900: loss: 3941.146
iteration 1000: loss: 3940.999
iteration 1100: loss: 3928.750
iteration 1200: loss: 3932.141
iteration 1300: loss: 3933.862
iteration 1400: loss: 3934.044
iteration 1500: loss: 3938.405
iteration 1600: loss: 3939.938
iteration 1700: loss: 3940.505
iteration 1800: loss: 3941.290
====> Epoch: 047 Train loss: 3937.9447  took : 53.00285315513611
====> Test loss: 3942.1722
iteration 0000: loss: 3939.959
iteration 0100: loss: 3938.001
iteration 0200: loss: 3939.667
iteration 0300: loss: 3934.743
iteration 0400: loss: 3933.734
iteration 0500: loss: 3935.312
iteration 0600: loss: 3941.272
iteration 0700: loss: 3935.529
iteration 0800: loss: 3944.305
iteration 0900: loss: 3929.976
iteration 1000: loss: 3936.162
iteration 1100: loss: 3931.093
iteration 1200: loss: 3942.093
iteration 1300: loss: 3938.982
iteration 1400: loss: 3937.813
iteration 1500: loss: 3939.836
iteration 1600: loss: 3942.716
iteration 1700: loss: 3941.930
iteration 1800: loss: 3935.682
====> Epoch: 048 Train loss: 3937.9632  took : 53.129690647125244
====> Test loss: 3941.8734
iteration 0000: loss: 3938.381
iteration 0100: loss: 3936.170
iteration 0200: loss: 3938.467
iteration 0300: loss: 3943.415
iteration 0400: loss: 3935.378
iteration 0500: loss: 3940.150
iteration 0600: loss: 3933.526
iteration 0700: loss: 3942.263
iteration 0800: loss: 3937.744
iteration 0900: loss: 3937.601
iteration 1000: loss: 3937.807
iteration 1100: loss: 3937.251
iteration 1200: loss: 3933.608
iteration 1300: loss: 3934.814
iteration 1400: loss: 3932.913
iteration 1500: loss: 3937.410
iteration 1600: loss: 3934.207
iteration 1700: loss: 3944.454
iteration 1800: loss: 3936.977
====> Epoch: 049 Train loss: 3937.7680  took : 53.43936228752136
====> Test loss: 3942.2413
iteration 0000: loss: 3933.326
iteration 0100: loss: 3934.245
iteration 0200: loss: 3932.842
iteration 0300: loss: 3935.355
iteration 0400: loss: 3936.834
iteration 0500: loss: 3944.167
iteration 0600: loss: 3935.374
iteration 0700: loss: 3933.640
iteration 0800: loss: 3936.156
iteration 0900: loss: 3936.144
iteration 1000: loss: 3933.922
iteration 1100: loss: 3929.320
iteration 1200: loss: 3934.883
iteration 1300: loss: 3935.852
iteration 1400: loss: 3939.538
iteration 1500: loss: 3937.773
iteration 1600: loss: 3942.778
iteration 1700: loss: 3939.679
iteration 1800: loss: 3939.179
====> Epoch: 050 Train loss: 3937.5795  took : 53.21749496459961
====> Test loss: 3941.9219
====> [MM-VAE] Time: 3162.536s or 00:52:42
Arguments (initial):
{'pretrained_path': './rslt/parameter-search-32/VAE_CMNIST/vae_cmnist',
 'run_id': 'vae_cmnist',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'parameter-search-32', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 4, 'use_conditional': False, 'use_cnn': 'mlp', 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 50, 'K': 20, 'learn_prior': True, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_0
Run Directory:
 ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}


Model runner was constructed.
{'K': 20,
 'batch_size': 128,
 'epochs': 50,
 'experiment': 'parameter-search-32',
 'latent_dim': 20,
 'learn_prior': True,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 4,
 'obj': 'dreg',
 'print_freq': 100,
 'run_type': 'train',
 'seed': 4,
 'use_cnn': 'mlp',
 'use_conditional': False}
Encoder based on MLP was constructed.
Decoder based on MLP was constructed.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                  [-1, 400]         160,400
              ReLU-4                  [-1, 400]               0Traceback (most recent call last):
  File "/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/./main.py", line 225, in <module>
    run_all()
  File "/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/./main.py", line 214, in run_all
    main(args=args)
  File "/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/./main.py", line 145, in main
    analyse(
  File "/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py", line 999, in analyse
    runner = Runner(args=args)
  File "/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/runner.py", line 40, in __init__
    torch.load(args.pretrained_path + '/model.rar'))
  File "/home/taka/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/serialization.py", line 789, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/taka/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/serialization.py", line 1131, in _load
    result = unpickler.load()
  File "/home/taka/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/serialization.py", line 1101, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/taka/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/serialization.py", line 1083, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/home/taka/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/serialization.py", line 215, in default_restore_location
    result = fn(storage, location)
  File "/home/taka/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/serialization.py", line 182, in _cuda_deserialize
    device = validate_cuda_device(location)
  File "/home/taka/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/serialization.py", line 173, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on CUDA device '
RuntimeError: Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.

            Linear-5                  [-1, 400]         160,400
              ReLU-6                  [-1, 400]               0
            Linear-7                  [-1, 400]         160,400
              ReLU-8                  [-1, 400]               0
            Linear-9                   [-1, 20]           8,020
           Linear-10                   [-1, 20]           8,020
           EncMLP-11       [[-1, 20], [-1, 20]]               0
           Linear-12               [-1, 2, 400]           8,400
             ReLU-13               [-1, 2, 400]               0
           Linear-14               [-1, 2, 400]         160,400
             ReLU-15               [-1, 2, 400]               0
           Linear-16               [-1, 2, 400]         160,400
             ReLU-17               [-1, 2, 400]               0
           Linear-18               [-1, 2, 400]         160,400
             ReLU-19               [-1, 2, 400]               0
           Linear-20              [-1, 2, 2352]         943,152
Print of model summary was skipped because setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.
Loading model VAE_CMNIST from ./rslt/parameter-search-32/VAE_CMNIST/vae_cmnist_seed_0
2023/12/19 PM 01:30:01
start time:  1702872556
end time:  1702960201
run time:  87645
