working directory:  /home/taka/MMVAE/NumberSenseAndMultimodalLearning
omp thread num:  2
ncpus:  2
cuda visible devices:  GPU-1ce72701-f930-8001-09eb-a082cba45729
2023/12/05 PM 06:46:13
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'pretrained_path': '',
 'print_freq': 0,
 'run_id': 'test_classifier-cmnist',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 test_classifier-cmnist
Run Directory:
 ./rslt/test-pbs-1/Classifier_CMNIST/test_classifier-cmnist
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'output_dir': './rslt/test-pbs-1/Classifier_CMNIST/test_classifier-cmnist/train',
 'pretrained_path': '',
 'print_freq': 0,
 'run_dir': './rslt/test-pbs-1/Classifier_CMNIST/test_classifier-cmnist',
 'run_id': 'test_classifier-cmnist',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross
====> Epoch: 001 Train loss: 0.0026  took : 2.1955320835113525
====> Test loss: -0.0015, Test accuracy: 0.9450
====> Epoch: 002 Train loss: 0.0009  took : 2.1233038902282715
====> Test loss: -0.0010, Test accuracy: 0.9673
====> Epoch: 003 Train loss: 0.0005  took : 2.1315159797668457
====> Test loss: -0.0009, Test accuracy: 0.9710
====> Epoch: 004 Train loss: 0.0003  took : 2.129574775695801
====> Test loss: -0.0010, Test accuracy: 0.9735
====> Epoch: 005 Train loss: 0.0001  took : 2.113107919692993
====> Test loss: -0.0010, Test accuracy: 0.9740
====> Epoch: 006 Train loss: 0.0001  took : 2.1205434799194336
====> Test loss: -0.0010, Test accuracy: 0.9762
====> Epoch: 007 Train loss: 0.0000  took : 2.125656843185425
====> Test loss: -0.0011, Test accuracy: 0.9750
====> Epoch: 008 Train loss: 0.0000  took : 2.0982139110565186
====> Test loss: -0.0012, Test accuracy: 0.9760
====> Epoch: 009 Train loss: 0.0000  took : 2.105072498321533
====> Test loss: -0.0011, Test accuracy: 0.9768
====> Epoch: 010 Train loss: 0.0000  took : 2.13677716255188
====> Test loss: -0.0011, Test accuracy: 0.9758
====> Epoch: 011 Train loss: 0.0000  took : 2.090243101119995
====> Test loss: -0.0012, Test accuracy: 0.9760
====> Epoch: 012 Train loss: 0.0000  took : 2.0902044773101807
====> Test loss: -0.0012, Test accuracy: 0.9768
====> Epoch: 013 Train loss: 0.0000  took : 2.1367831230163574
====> Test loss: -0.0012, Test accuracy: 0.9762
====> Epoch: 014 Train loss: 0.0000  took : 2.1274805068969727
====> Test loss: -0.0013, Test accuracy: 0.9760
====> Epoch: 015 Train loss: 0.0000  took : 2.1398396492004395
====> Test loss: -0.0013, Test accuracy: 0.9758
====> Epoch: 016 Train loss: 0.0000  took : 2.150899648666382
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 017 Train loss: 0.0000  took : 2.1458444595336914
====> Test loss: -0.0013, Test accuracy: 0.9762
====> Epoch: 018 Train loss: 0.0000  took : 2.115299940109253
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 019 Train loss: 0.0000  took : 2.134869337081909
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 020 Train loss: 0.0000  took : 2.1065499782562256
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 021 Train loss: 0.0000  took : 2.099804401397705
====> Test loss: -0.0014, Test accuracy: 0.9760
====> Epoch: 022 Train loss: 0.0000  took : 2.1471805572509766
====> Test loss: -0.0013, Test accuracy: 0.9765
====> Epoch: 023 Train loss: 0.0000  took : 2.108018159866333
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 024 Train loss: 0.0000  took : 2.134927749633789
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 025 Train loss: 0.0000  took : 2.110966205596924
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 026 Train loss: 0.0000  took : 2.107789993286133
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 027 Train loss: 0.0000  took : 2.1294684410095215
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 028 Train loss: 0.0000  took : 2.1363306045532227
====> Test loss: -0.0015, Test accuracy: 0.9765
====> Epoch: 029 Train loss: 0.0000  took : 2.0849556922912598
====> Test loss: -0.0014, Test accuracy: 0.9765
====> Epoch: 030 Train loss: 0.0000  took : 2.0980238914489746
====> Test loss: -0.0015, Test accuracy: 0.9765
====> [MM-VAE] Time:  76.144s or 00:01:16
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'pretrained_path': '',
 'print_freq': 0,
 'run_id': 'test-classifier-oscn',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 test-classifier-oscn
Run Directory:
 ./rslt/test-pbs-1/Classifier_OSCN/test-classifier-oscn
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'Classifier_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'cross',
 'output_dir': './rslt/test-pbs-1/Classifier_OSCN/test-classifier-oscn/train',
 'pretrained_path': '',
 'print_freq': 0,
 'run_dir': './rslt/test-pbs-1/Classifier_OSCN/test-classifier-oscn',
 'run_id': 'test-classifier-oscn',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross
====> Epoch: 001 Train loss: 0.0015  took : 2.969757080078125
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 002 Train loss: 0.0000  took : 2.936030387878418
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 003 Train loss: 0.0000  took : 3.07125186920166
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 004 Train loss: 0.0000  took : 2.9616854190826416
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 005 Train loss: 0.0000  took : 2.987961530685425
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 006 Train loss: 0.0000  took : 2.9081525802612305
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 007 Train loss: 0.0000  took : 2.9141438007354736
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 008 Train loss: 0.0000  took : 2.9492692947387695
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 009 Train loss: 0.0000  took : 2.922990083694458
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 010 Train loss: 0.0000  took : 3.009974956512451
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 011 Train loss: 0.0000  took : 2.9225425720214844
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 012 Train loss: 0.0000  took : 2.9216690063476562
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 013 Train loss: 0.0000  took : 2.9260151386260986
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 014 Train loss: 0.0000  took : 2.933314323425293
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 015 Train loss: 0.0000  took : 2.995864152908325
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 016 Train loss: 0.0000  took : 2.999393939971924
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 017 Train loss: 0.0000  took : 2.933882713317871
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 018 Train loss: 0.0000  took : 3.0606563091278076
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 019 Train loss: 0.0000  took : 3.0168604850769043
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 020 Train loss: 0.0000  took : 2.976112127304077
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 021 Train loss: 0.0000  took : 2.9447414875030518
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 022 Train loss: 0.0000  took : 2.9388904571533203
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 023 Train loss: 0.0000  took : 2.96395206451416
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 024 Train loss: 0.0000  took : 2.960357427597046
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 025 Train loss: 0.0000  took : 3.0303025245666504
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 026 Train loss: 0.0000  took : 2.9385740756988525
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 027 Train loss: 0.0000  took : 2.9247443675994873
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 028 Train loss: 0.0000  took : 2.9470949172973633
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 029 Train loss: 0.0000  took : 2.9102072715759277
====> Test loss: -0.0000, Test accuracy: 1.0000
====> Epoch: 030 Train loss: 0.0000  took : 2.9358696937561035
====> Test loss: -0.0000, Test accuracy: 1.0000
====> [MM-VAE] Time:  98.184s or 00:01:38
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'print_freq': 100,
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_0
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1993.552
iteration 0100: loss: 1572.296
iteration 0200: loss: 1556.896
iteration 0300: loss: 1551.120
iteration 0400: loss: 1541.943
iteration 0500: loss: 1541.464
iteration 0600: loss: 1542.875
iteration 0700: loss: 1538.344
iteration 0800: loss: 1536.366
iteration 0900: loss: 1541.367
====> Epoch: 001 Train loss: 1551.8095  took : 3.8948538303375244
====> Test loss: 1531.4405
iteration 0000: loss: 1535.404
iteration 0100: loss: 1531.760
iteration 0200: loss: 1536.481
iteration 0300: loss: 1535.013
iteration 0400: loss: 1538.104
iteration 0500: loss: 1535.611
iteration 0600: loss: 1533.547
iteration 0700: loss: 1538.750
iteration 0800: loss: 1532.781
iteration 0900: loss: 1531.346
====> Epoch: 002 Train loss: 1534.0265  took : 3.9604103565216064
====> Test loss: 1527.1358
iteration 0000: loss: 1533.235
iteration 0100: loss: 1530.160
iteration 0200: loss: 1530.821
iteration 0300: loss: 1532.710
iteration 0400: loss: 1527.171
iteration 0500: loss: 1527.048
iteration 0600: loss: 1533.412
iteration 0700: loss: 1526.254
iteration 0800: loss: 1534.011
iteration 0900: loss: 1531.451
====> Epoch: 003 Train loss: 1531.3003  took : 3.9190986156463623
====> Test loss: 1525.5146
iteration 0000: loss: 1530.399
iteration 0100: loss: 1526.265
iteration 0200: loss: 1530.819
iteration 0300: loss: 1528.401
iteration 0400: loss: 1530.388
iteration 0500: loss: 1529.413
iteration 0600: loss: 1529.705
iteration 0700: loss: 1532.364
iteration 0800: loss: 1527.393
iteration 0900: loss: 1528.432
====> Epoch: 004 Train loss: 1530.1085  took : 3.89430832862854
====> Test loss: 1524.7097
iteration 0000: loss: 1528.126
iteration 0100: loss: 1528.626
iteration 0200: loss: 1528.685
iteration 0300: loss: 1532.624
iteration 0400: loss: 1528.808
iteration 0500: loss: 1530.698
iteration 0600: loss: 1529.271
iteration 0700: loss: 1528.573
iteration 0800: loss: 1526.950
iteration 0900: loss: 1528.520
====> Epoch: 005 Train loss: 1529.3308  took : 3.9124081134796143
====> Test loss: 1524.1514
iteration 0000: loss: 1527.825
iteration 0100: loss: 1526.419
iteration 0200: loss: 1524.677
iteration 0300: loss: 1527.880
iteration 0400: loss: 1529.652
iteration 0500: loss: 1525.282
iteration 0600: loss: 1531.178
iteration 0700: loss: 1527.467
iteration 0800: loss: 1529.472
iteration 0900: loss: 1528.282
====> Epoch: 006 Train loss: 1528.7638  took : 3.8936820030212402
====> Test loss: 1523.6905
iteration 0000: loss: 1529.046
iteration 0100: loss: 1530.664
iteration 0200: loss: 1526.075
iteration 0300: loss: 1529.714
iteration 0400: loss: 1527.443
iteration 0500: loss: 1528.655
iteration 0600: loss: 1530.573
iteration 0700: loss: 1531.424
iteration 0800: loss: 1522.908
iteration 0900: loss: 1527.313
====> Epoch: 007 Train loss: 1528.3374  took : 3.890533447265625
====> Test loss: 1523.2171
iteration 0000: loss: 1530.499
iteration 0100: loss: 1527.552
iteration 0200: loss: 1525.858
iteration 0300: loss: 1528.065
iteration 0400: loss: 1529.690
iteration 0500: loss: 1529.367
iteration 0600: loss: 1527.506
iteration 0700: loss: 1526.438
iteration 0800: loss: 1528.852
iteration 0900: loss: 1527.857
====> Epoch: 008 Train loss: 1527.9861  took : 3.904029607772827
====> Test loss: 1522.9885
iteration 0000: loss: 1527.423
iteration 0100: loss: 1524.745
iteration 0200: loss: 1525.573
iteration 0300: loss: 1525.878
iteration 0400: loss: 1530.625
iteration 0500: loss: 1523.914
iteration 0600: loss: 1525.400
iteration 0700: loss: 1529.799
iteration 0800: loss: 1526.840
iteration 0900: loss: 1529.279
====> Epoch: 009 Train loss: 1527.6945  took : 3.9412026405334473
====> Test loss: 1522.8372
iteration 0000: loss: 1528.085
iteration 0100: loss: 1526.448
iteration 0200: loss: 1525.850
iteration 0300: loss: 1525.681
iteration 0400: loss: 1527.986
iteration 0500: loss: 1528.667
iteration 0600: loss: 1526.921
iteration 0700: loss: 1527.729
iteration 0800: loss: 1527.628
iteration 0900: loss: 1531.221
====> Epoch: 010 Train loss: 1527.4252  took : 3.8861441612243652
====> Test loss: 1522.6721
iteration 0000: loss: 1525.967
iteration 0100: loss: 1526.067
iteration 0200: loss: 1527.014
iteration 0300: loss: 1529.842
iteration 0400: loss: 1524.806
iteration 0500: loss: 1524.903
iteration 0600: loss: 1527.998
iteration 0700: loss: 1527.418
iteration 0800: loss: 1528.547
iteration 0900: loss: 1528.298
====> Epoch: 011 Train loss: 1527.1920  took : 3.9192256927490234
====> Test loss: 1522.4114
iteration 0000: loss: 1526.372
iteration 0100: loss: 1526.399
iteration 0200: loss: 1526.380
iteration 0300: loss: 1528.115
iteration 0400: loss: 1528.812
iteration 0500: loss: 1527.389
iteration 0600: loss: 1528.478
iteration 0700: loss: 1527.583
iteration 0800: loss: 1530.561
iteration 0900: loss: 1528.179
====> Epoch: 012 Train loss: 1526.9717  took : 3.9100286960601807
====> Test loss: 1522.1585
iteration 0000: loss: 1527.620
iteration 0100: loss: 1525.695
iteration 0200: loss: 1527.734
iteration 0300: loss: 1527.627
iteration 0400: loss: 1527.075
iteration 0500: loss: 1528.122
iteration 0600: loss: 1528.393
iteration 0700: loss: 1529.814
iteration 0800: loss: 1525.309
iteration 0900: loss: 1526.514
====> Epoch: 013 Train loss: 1526.7716  took : 3.9168481826782227
====> Test loss: 1522.0073
iteration 0000: loss: 1525.840
iteration 0100: loss: 1524.164
iteration 0200: loss: 1526.395
iteration 0300: loss: 1528.344
iteration 0400: loss: 1525.880
iteration 0500: loss: 1526.135
iteration 0600: loss: 1526.123
iteration 0700: loss: 1525.687
iteration 0800: loss: 1526.962
iteration 0900: loss: 1524.313
====> Epoch: 014 Train loss: 1526.5985  took : 3.90334415435791
====> Test loss: 1521.9148
iteration 0000: loss: 1525.970
iteration 0100: loss: 1526.759
iteration 0200: loss: 1526.902
iteration 0300: loss: 1525.722
iteration 0400: loss: 1527.481
iteration 0500: loss: 1528.096
iteration 0600: loss: 1525.814
iteration 0700: loss: 1528.620
iteration 0800: loss: 1524.576
iteration 0900: loss: 1528.375
====> Epoch: 015 Train loss: 1526.4729  took : 3.9678244590759277
====> Test loss: 1521.9079
iteration 0000: loss: 1526.579
iteration 0100: loss: 1523.211
iteration 0200: loss: 1524.640
iteration 0300: loss: 1526.297
iteration 0400: loss: 1526.271
iteration 0500: loss: 1523.369
iteration 0600: loss: 1526.021
iteration 0700: loss: 1529.389
iteration 0800: loss: 1524.354
iteration 0900: loss: 1525.631
====> Epoch: 016 Train loss: 1526.3276  took : 3.919900894165039
====> Test loss: 1521.5109
iteration 0000: loss: 1524.877
iteration 0100: loss: 1524.371
iteration 0200: loss: 1527.488
iteration 0300: loss: 1524.069
iteration 0400: loss: 1524.042
iteration 0500: loss: 1526.159
iteration 0600: loss: 1527.608
iteration 0700: loss: 1524.737
iteration 0800: loss: 1523.150
iteration 0900: loss: 1525.395
====> Epoch: 017 Train loss: 1526.2217  took : 3.9193739891052246
====> Test loss: 1521.5284
iteration 0000: loss: 1526.298
iteration 0100: loss: 1524.151
iteration 0200: loss: 1526.770
iteration 0300: loss: 1527.175
iteration 0400: loss: 1525.093
iteration 0500: loss: 1524.105
iteration 0600: loss: 1527.430
iteration 0700: loss: 1523.782
iteration 0800: loss: 1526.835
iteration 0900: loss: 1527.781
====> Epoch: 018 Train loss: 1526.0967  took : 3.898069381713867
====> Test loss: 1521.2980
iteration 0000: loss: 1524.492
iteration 0100: loss: 1525.726
iteration 0200: loss: 1526.877
iteration 0300: loss: 1526.005
iteration 0400: loss: 1527.068
iteration 0500: loss: 1525.119
iteration 0600: loss: 1522.792
iteration 0700: loss: 1522.974
iteration 0800: loss: 1524.846
iteration 0900: loss: 1528.976
====> Epoch: 019 Train loss: 1525.9829  took : 3.9288785457611084
====> Test loss: 1521.3875
iteration 0000: loss: 1527.396
iteration 0100: loss: 1528.377
iteration 0200: loss: 1527.305
iteration 0300: loss: 1527.597
iteration 0400: loss: 1526.298
iteration 0500: loss: 1526.312
iteration 0600: loss: 1525.801
iteration 0700: loss: 1523.107
iteration 0800: loss: 1525.300
iteration 0900: loss: 1525.309
====> Epoch: 020 Train loss: 1525.9039  took : 3.939544200897217
====> Test loss: 1521.3186
iteration 0000: loss: 1524.610
iteration 0100: loss: 1525.937
iteration 0200: loss: 1527.730
iteration 0300: loss: 1524.487
iteration 0400: loss: 1524.260
iteration 0500: loss: 1530.269
iteration 0600: loss: 1527.553
iteration 0700: loss: 1527.074
iteration 0800: loss: 1524.863
iteration 0900: loss: 1524.205
====> Epoch: 021 Train loss: 1525.8287  took : 3.9456121921539307
====> Test loss: 1521.1903
iteration 0000: loss: 1525.215
iteration 0100: loss: 1527.146
iteration 0200: loss: 1527.812
iteration 0300: loss: 1527.519
iteration 0400: loss: 1525.460
iteration 0500: loss: 1523.468
iteration 0600: loss: 1527.427
iteration 0700: loss: 1527.395
iteration 0800: loss: 1523.590
iteration 0900: loss: 1524.438
====> Epoch: 022 Train loss: 1525.7392  took : 3.9571666717529297
====> Test loss: 1521.2208
iteration 0000: loss: 1526.768
iteration 0100: loss: 1527.820
iteration 0200: loss: 1523.119
iteration 0300: loss: 1524.295
iteration 0400: loss: 1522.508
iteration 0500: loss: 1526.570
iteration 0600: loss: 1525.361
iteration 0700: loss: 1525.693
iteration 0800: loss: 1524.054
iteration 0900: loss: 1523.761
====> Epoch: 023 Train loss: 1525.6782  took : 3.9241416454315186
====> Test loss: 1521.1451
iteration 0000: loss: 1526.626
iteration 0100: loss: 1525.591
iteration 0200: loss: 1526.723
iteration 0300: loss: 1524.232
iteration 0400: loss: 1527.312
iteration 0500: loss: 1526.514
iteration 0600: loss: 1527.165
iteration 0700: loss: 1527.571
iteration 0800: loss: 1526.804
iteration 0900: loss: 1522.302
====> Epoch: 024 Train loss: 1525.6041  took : 3.9130756855010986
====> Test loss: 1520.9999
iteration 0000: loss: 1529.751
iteration 0100: loss: 1526.640
iteration 0200: loss: 1524.586
iteration 0300: loss: 1529.794
iteration 0400: loss: 1522.973
iteration 0500: loss: 1525.010
iteration 0600: loss: 1524.490
iteration 0700: loss: 1523.633
iteration 0800: loss: 1526.107
iteration 0900: loss: 1525.326
====> Epoch: 025 Train loss: 1525.5230  took : 3.9013001918792725
====> Test loss: 1520.9470
iteration 0000: loss: 1527.809
iteration 0100: loss: 1527.685
iteration 0200: loss: 1523.372
iteration 0300: loss: 1525.018
iteration 0400: loss: 1522.983
iteration 0500: loss: 1525.966
iteration 0600: loss: 1523.717
iteration 0700: loss: 1525.396
iteration 0800: loss: 1526.286
iteration 0900: loss: 1529.029
====> Epoch: 026 Train loss: 1525.4906  took : 3.915778875350952
====> Test loss: 1520.9471
iteration 0000: loss: 1524.396
iteration 0100: loss: 1522.375
iteration 0200: loss: 1525.019
iteration 0300: loss: 1528.094
iteration 0400: loss: 1525.152
iteration 0500: loss: 1523.954
iteration 0600: loss: 1527.103
iteration 0700: loss: 1526.347
iteration 0800: loss: 1526.072
iteration 0900: loss: 1525.675
====> Epoch: 027 Train loss: 1525.4366  took : 3.967237949371338
====> Test loss: 1520.8545
iteration 0000: loss: 1524.857
iteration 0100: loss: 1526.035
iteration 0200: loss: 1524.331
iteration 0300: loss: 1525.351
iteration 0400: loss: 1525.203
iteration 0500: loss: 1525.218
iteration 0600: loss: 1528.328
iteration 0700: loss: 1526.615
iteration 0800: loss: 1524.719
iteration 0900: loss: 1526.214
====> Epoch: 028 Train loss: 1525.4046  took : 3.960576295852661
====> Test loss: 1520.9483
iteration 0000: loss: 1523.774
iteration 0100: loss: 1523.564
iteration 0200: loss: 1527.695
iteration 0300: loss: 1526.191
iteration 0400: loss: 1523.876
iteration 0500: loss: 1526.268
iteration 0600: loss: 1526.073
iteration 0700: loss: 1520.578
iteration 0800: loss: 1525.902
iteration 0900: loss: 1525.028
====> Epoch: 029 Train loss: 1525.3269  took : 3.932034969329834
====> Test loss: 1520.7340
iteration 0000: loss: 1525.597
iteration 0100: loss: 1526.599
iteration 0200: loss: 1528.157
iteration 0300: loss: 1523.892
iteration 0400: loss: 1525.467
iteration 0500: loss: 1525.251
iteration 0600: loss: 1525.893
iteration 0700: loss: 1524.735
iteration 0800: loss: 1524.619
iteration 0900: loss: 1524.572
====> Epoch: 030 Train loss: 1525.2555  took : 3.934797763824463
====> Test loss: 1520.7609
====> [MM-VAE] Time: 196.751s or 00:03:16
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'print_freq': 100,
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2678.374
iteration 0100: loss: 2101.544
iteration 0200: loss: 2117.659
iteration 0300: loss: 2079.970
iteration 0400: loss: 2064.823
iteration 0500: loss: 2051.722
iteration 0600: loss: 2024.514
iteration 0700: loss: 1998.531
iteration 0800: loss: 1980.224
iteration 0900: loss: 1969.693
====> Epoch: 001 Train loss: 2055.8428  took : 6.1736366748809814
====> Test loss: 1961.2696
iteration 0000: loss: 1967.917
iteration 0100: loss: 1964.225
iteration 0200: loss: 1963.908
iteration 0300: loss: 1964.030
iteration 0400: loss: 1963.210
iteration 0500: loss: 1960.647
iteration 0600: loss: 1960.930
iteration 0700: loss: 1960.847
iteration 0800: loss: 1960.594
iteration 0900: loss: 1959.506
====> Epoch: 002 Train loss: 1962.1927  took : 6.183809041976929
====> Test loss: 1954.8776
iteration 0000: loss: 1960.668
iteration 0100: loss: 1960.504
iteration 0200: loss: 1961.625
iteration 0300: loss: 1960.375
iteration 0400: loss: 1960.635
iteration 0500: loss: 1960.067
iteration 0600: loss: 1961.061
iteration 0700: loss: 1959.238
iteration 0800: loss: 1959.552
iteration 0900: loss: 1959.276
====> Epoch: 003 Train loss: 1959.7635  took : 6.122182846069336
====> Test loss: 1953.7060
iteration 0000: loss: 1958.743
iteration 0100: loss: 1959.240
iteration 0200: loss: 1959.557
iteration 0300: loss: 1958.338
iteration 0400: loss: 1958.834
iteration 0500: loss: 1959.636
iteration 0600: loss: 1959.089
iteration 0700: loss: 1959.094
iteration 0800: loss: 1960.005
iteration 0900: loss: 1960.168
====> Epoch: 004 Train loss: 1958.9701  took : 6.173460483551025
====> Test loss: 1953.3750
iteration 0000: loss: 1959.639
iteration 0100: loss: 1957.347
iteration 0200: loss: 1958.340
iteration 0300: loss: 1958.637
iteration 0400: loss: 1957.333
iteration 0500: loss: 1958.519
iteration 0600: loss: 1957.918
iteration 0700: loss: 1958.094
iteration 0800: loss: 1959.730
iteration 0900: loss: 1957.405
====> Epoch: 005 Train loss: 1958.6272  took : 6.174424648284912
====> Test loss: 1953.1167
iteration 0000: loss: 1959.662
iteration 0100: loss: 1957.627
iteration 0200: loss: 1958.006
iteration 0300: loss: 1959.901
iteration 0400: loss: 1957.958
iteration 0500: loss: 1958.311
iteration 0600: loss: 1957.922
iteration 0700: loss: 1958.246
iteration 0800: loss: 1957.624
iteration 0900: loss: 1957.853
====> Epoch: 006 Train loss: 1958.3813  took : 6.1836206912994385
====> Test loss: 1953.1619
iteration 0000: loss: 1958.321
iteration 0100: loss: 1958.971
iteration 0200: loss: 1958.269
iteration 0300: loss: 1959.405
iteration 0400: loss: 1958.706
iteration 0500: loss: 1958.239
iteration 0600: loss: 1959.106
iteration 0700: loss: 1957.623
iteration 0800: loss: 1959.531
iteration 0900: loss: 1957.695
====> Epoch: 007 Train loss: 1958.1488  took : 6.158472537994385
====> Test loss: 1953.1741
iteration 0000: loss: 1957.699
iteration 0100: loss: 1957.631
iteration 0200: loss: 1957.737
iteration 0300: loss: 1956.915
iteration 0400: loss: 1957.257
iteration 0500: loss: 1957.768
iteration 0600: loss: 1958.632
iteration 0700: loss: 1957.266
iteration 0800: loss: 1958.318
iteration 0900: loss: 1958.769
====> Epoch: 008 Train loss: 1958.0128  took : 6.142755508422852
====> Test loss: 1953.1082
iteration 0000: loss: 1957.571
iteration 0100: loss: 1957.870
iteration 0200: loss: 1957.450
iteration 0300: loss: 1957.435
iteration 0400: loss: 1957.190
iteration 0500: loss: 1956.745
iteration 0600: loss: 1956.160
iteration 0700: loss: 1957.035
iteration 0800: loss: 1958.109
iteration 0900: loss: 1956.679
====> Epoch: 009 Train loss: 1957.8102  took : 6.203418731689453
====> Test loss: 1952.6084
iteration 0000: loss: 1957.933
iteration 0100: loss: 1958.707
iteration 0200: loss: 1957.289
iteration 0300: loss: 1958.325
iteration 0400: loss: 1957.582
iteration 0500: loss: 1958.159
iteration 0600: loss: 1957.325
iteration 0700: loss: 1958.114
iteration 0800: loss: 1957.248
iteration 0900: loss: 1957.318
====> Epoch: 010 Train loss: 1957.7285  took : 6.163360357284546
====> Test loss: 1952.5372
iteration 0000: loss: 1958.958
iteration 0100: loss: 1958.417
iteration 0200: loss: 1957.345
iteration 0300: loss: 1956.336
iteration 0400: loss: 1957.566
iteration 0500: loss: 1957.932
iteration 0600: loss: 1959.830
iteration 0700: loss: 1957.339
iteration 0800: loss: 1956.622
iteration 0900: loss: 1957.893
====> Epoch: 011 Train loss: 1957.6050  took : 6.198155164718628
====> Test loss: 1952.2977
iteration 0000: loss: 1959.148
iteration 0100: loss: 1956.468
iteration 0200: loss: 1957.215
iteration 0300: loss: 1958.463
iteration 0400: loss: 1957.325
iteration 0500: loss: 1956.706
iteration 0600: loss: 1957.423
iteration 0700: loss: 1956.879
iteration 0800: loss: 1959.481
iteration 0900: loss: 1957.267
====> Epoch: 012 Train loss: 1957.4649  took : 6.170694828033447
====> Test loss: 1951.9574
iteration 0000: loss: 1956.819
iteration 0100: loss: 1957.393
iteration 0200: loss: 1956.642
iteration 0300: loss: 1957.293
iteration 0400: loss: 1958.577
iteration 0500: loss: 1957.503
iteration 0600: loss: 1956.932
iteration 0700: loss: 1956.790
iteration 0800: loss: 1956.688
iteration 0900: loss: 1956.910
====> Epoch: 013 Train loss: 1957.4016  took : 6.140226125717163
====> Test loss: 1952.2601
iteration 0000: loss: 1957.074
iteration 0100: loss: 1957.282
iteration 0200: loss: 1957.698
iteration 0300: loss: 1958.411
iteration 0400: loss: 1958.435
iteration 0500: loss: 1956.559
iteration 0600: loss: 1958.283
iteration 0700: loss: 1958.052
iteration 0800: loss: 1956.046
iteration 0900: loss: 1958.235
====> Epoch: 014 Train loss: 1957.3419  took : 6.152667284011841
====> Test loss: 1951.7909
iteration 0000: loss: 1956.566
iteration 0100: loss: 1956.656
iteration 0200: loss: 1956.529
iteration 0300: loss: 1956.743
iteration 0400: loss: 1958.261
iteration 0500: loss: 1957.288
iteration 0600: loss: 1958.283
iteration 0700: loss: 1958.442
iteration 0800: loss: 1959.324
iteration 0900: loss: 1958.309
====> Epoch: 015 Train loss: 1957.2824  took : 6.14985728263855
====> Test loss: 1952.0803
iteration 0000: loss: 1959.771
iteration 0100: loss: 1957.586
iteration 0200: loss: 1957.711
iteration 0300: loss: 1956.901
iteration 0400: loss: 1958.176
iteration 0500: loss: 1957.133
iteration 0600: loss: 1955.894
iteration 0700: loss: 1958.278
iteration 0800: loss: 1957.584
iteration 0900: loss: 1958.040
====> Epoch: 016 Train loss: 1957.2372  took : 6.155366659164429
====> Test loss: 1951.9727
iteration 0000: loss: 1956.700
iteration 0100: loss: 1957.024
iteration 0200: loss: 1955.548
iteration 0300: loss: 1957.017
iteration 0400: loss: 1956.673
iteration 0500: loss: 1956.530
iteration 0600: loss: 1956.337
iteration 0700: loss: 1955.999
iteration 0800: loss: 1957.179
iteration 0900: loss: 1959.391
====> Epoch: 017 Train loss: 1957.1657  took : 6.177213907241821
====> Test loss: 1952.5859
iteration 0000: loss: 1957.821
iteration 0100: loss: 1956.436
iteration 0200: loss: 1956.232
iteration 0300: loss: 1957.295
iteration 0400: loss: 1958.777
iteration 0500: loss: 1956.162
iteration 0600: loss: 1956.049
iteration 0700: loss: 1957.497
iteration 0800: loss: 1956.967
iteration 0900: loss: 1956.791
====> Epoch: 018 Train loss: 1957.0348  took : 6.151981830596924
====> Test loss: 1951.9704
iteration 0000: loss: 1956.907
iteration 0100: loss: 1957.056
iteration 0200: loss: 1956.214
iteration 0300: loss: 1957.318
iteration 0400: loss: 1957.019
iteration 0500: loss: 1956.736
iteration 0600: loss: 1955.401
iteration 0700: loss: 1957.826
iteration 0800: loss: 1957.046
iteration 0900: loss: 1956.527
====> Epoch: 019 Train loss: 1956.9685  took : 6.172454833984375
====> Test loss: 1952.0344
iteration 0000: loss: 1956.381
iteration 0100: loss: 1957.210
iteration 0200: loss: 1956.426
iteration 0300: loss: 1956.584
iteration 0400: loss: 1957.295
iteration 0500: loss: 1957.061
iteration 0600: loss: 1956.049
iteration 0700: loss: 1957.235
iteration 0800: loss: 1956.819
iteration 0900: loss: 1959.172
====> Epoch: 020 Train loss: 1956.9077  took : 6.18478798866272
====> Test loss: 1952.0222
iteration 0000: loss: 1957.076
iteration 0100: loss: 1956.978
iteration 0200: loss: 1957.378
iteration 0300: loss: 1955.856
iteration 0400: loss: 1957.771
iteration 0500: loss: 1956.312
iteration 0600: loss: 1955.351
iteration 0700: loss: 1956.892
iteration 0800: loss: 1957.498
iteration 0900: loss: 1957.811
====> Epoch: 021 Train loss: 1956.8875  took : 6.179140329360962
====> Test loss: 1951.9038
iteration 0000: loss: 1956.065
iteration 0100: loss: 1957.027
iteration 0200: loss: 1956.455
iteration 0300: loss: 1957.861
iteration 0400: loss: 1957.353
iteration 0500: loss: 1956.163
iteration 0600: loss: 1956.743
iteration 0700: loss: 1956.638
iteration 0800: loss: 1957.206
iteration 0900: loss: 1955.779
====> Epoch: 022 Train loss: 1956.9093  took : 6.175017595291138
====> Test loss: 1951.7054
iteration 0000: loss: 1956.644
iteration 0100: loss: 1956.047
iteration 0200: loss: 1956.599
iteration 0300: loss: 1957.257
iteration 0400: loss: 1956.927
iteration 0500: loss: 1957.059
iteration 0600: loss: 1956.123
iteration 0700: loss: 1957.507
iteration 0800: loss: 1955.218
iteration 0900: loss: 1956.623
====> Epoch: 023 Train loss: 1956.9036  took : 6.156491041183472
====> Test loss: 1951.9213
iteration 0000: loss: 1957.727
iteration 0100: loss: 1956.746
iteration 0200: loss: 1956.304
iteration 0300: loss: 1956.238
iteration 0400: loss: 1954.961
iteration 0500: loss: 1957.347
iteration 0600: loss: 1958.121
iteration 0700: loss: 1956.761
iteration 0800: loss: 1956.534
iteration 0900: loss: 1956.837
====> Epoch: 024 Train loss: 1956.8075  took : 6.207751750946045
====> Test loss: 1951.7238
iteration 0000: loss: 1956.157
iteration 0100: loss: 1956.770
iteration 0200: loss: 1955.841
iteration 0300: loss: 1956.373
iteration 0400: loss: 1956.728
iteration 0500: loss: 1955.929
iteration 0600: loss: 1956.244
iteration 0700: loss: 1956.343
iteration 0800: loss: 1956.149
iteration 0900: loss: 1957.998
====> Epoch: 025 Train loss: 1956.8200  took : 6.141724586486816
====> Test loss: 1951.8810
iteration 0000: loss: 1957.093
iteration 0100: loss: 1956.278
iteration 0200: loss: 1956.523
iteration 0300: loss: 1956.025
iteration 0400: loss: 1956.319
iteration 0500: loss: 1955.348
iteration 0600: loss: 1956.396
iteration 0700: loss: 1956.185
iteration 0800: loss: 1956.886
iteration 0900: loss: 1957.198
====> Epoch: 026 Train loss: 1956.8025  took : 6.184929847717285
====> Test loss: 1951.7303
iteration 0000: loss: 1956.779
iteration 0100: loss: 1956.785
iteration 0200: loss: 1956.691
iteration 0300: loss: 1955.306
iteration 0400: loss: 1959.649
iteration 0500: loss: 1956.894
iteration 0600: loss: 1957.129
iteration 0700: loss: 1956.141
iteration 0800: loss: 1957.013
iteration 0900: loss: 1956.570
====> Epoch: 027 Train loss: 1956.7820  took : 6.175700902938843
====> Test loss: 1951.5952
iteration 0000: loss: 1956.174
iteration 0100: loss: 1955.815
iteration 0200: loss: 1958.065
iteration 0300: loss: 1956.390
iteration 0400: loss: 1956.905
iteration 0500: loss: 1956.147
iteration 0600: loss: 1956.507
iteration 0700: loss: 1956.490
iteration 0800: loss: 1957.048
iteration 0900: loss: 1956.249
====> Epoch: 028 Train loss: 1956.7456  took : 6.186967134475708
====> Test loss: 1951.8728
iteration 0000: loss: 1956.266
iteration 0100: loss: 1956.122
iteration 0200: loss: 1956.665
iteration 0300: loss: 1956.834
iteration 0400: loss: 1957.050
iteration 0500: loss: 1956.761
iteration 0600: loss: 1956.021
iteration 0700: loss: 1956.691
iteration 0800: loss: 1956.989
iteration 0900: loss: 1957.954
====> Epoch: 029 Train loss: 1956.6918  took : 6.154213905334473
====> Test loss: 1951.5693
iteration 0000: loss: 1956.863
iteration 0100: loss: 1956.451
iteration 0200: loss: 1956.446
iteration 0300: loss: 1956.058
iteration 0400: loss: 1957.020
iteration 0500: loss: 1956.530
iteration 0600: loss: 1957.660
iteration 0700: loss: 1960.216
iteration 0800: loss: 1957.728
iteration 0900: loss: 1956.710
====> Epoch: 030 Train loss: 1956.6810  took : 6.181526184082031
====> Test loss: 1951.9314
====> [MM-VAE] Time: 252.777s or 00:04:12
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'print_freq': 100,
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5162.991
iteration 0100: loss: 4140.397
iteration 0200: loss: 4096.389
iteration 0300: loss: 4022.871
iteration 0400: loss: 4018.385
iteration 0500: loss: 4012.801
iteration 0600: loss: 4000.320
iteration 0700: loss: 4012.484
iteration 0800: loss: 3992.952
iteration 0900: loss: 3994.399
iteration 1000: loss: 3979.442
iteration 1100: loss: 3993.036
iteration 1200: loss: 3975.156
iteration 1300: loss: 3980.634
iteration 1400: loss: 3987.593
iteration 1500: loss: 3979.887
iteration 1600: loss: 3983.659
iteration 1700: loss: 3983.761
iteration 1800: loss: 3971.498
====> Epoch: 001 Train loss: 4013.0659  took : 138.1199426651001
====> Test loss: 3981.8896
iteration 0000: loss: 3969.213
iteration 0100: loss: 3977.544
iteration 0200: loss: 3971.578
iteration 0300: loss: 3967.447
iteration 0400: loss: 3974.996
iteration 0500: loss: 3974.260
iteration 0600: loss: 3972.101
iteration 0700: loss: 3966.512
iteration 0800: loss: 3966.846
iteration 0900: loss: 3964.343
iteration 1000: loss: 3962.958
iteration 1100: loss: 3967.546
iteration 1200: loss: 3959.147
iteration 1300: loss: 3953.376
iteration 1400: loss: 3961.267
iteration 1500: loss: 3956.962
iteration 1600: loss: 3958.667
iteration 1700: loss: 3958.943
iteration 1800: loss: 3963.050
====> Epoch: 002 Train loss: 3966.4815  took : 137.7462968826294
====> Test loss: 3961.1689
iteration 0000: loss: 3965.815
iteration 0100: loss: 3960.040
iteration 0200: loss: 3953.174
iteration 0300: loss: 3957.197
iteration 0400: loss: 3950.807
iteration 0500: loss: 3958.540
iteration 0600: loss: 3956.417
iteration 0700: loss: 3955.644
iteration 0800: loss: 3958.842
iteration 0900: loss: 3958.662
iteration 1000: loss: 3951.604
iteration 1100: loss: 3951.779
iteration 1200: loss: 3963.613
iteration 1300: loss: 3959.216
iteration 1400: loss: 3955.009
iteration 1500: loss: 3955.567
iteration 1600: loss: 3956.986
iteration 1700: loss: 3960.898
iteration 1800: loss: 3962.873
====> Epoch: 003 Train loss: 3957.0416  took : 138.0089249610901
====> Test loss: 3958.2023
iteration 0000: loss: 3959.567
iteration 0100: loss: 3958.666
iteration 0200: loss: 3958.012
iteration 0300: loss: 3958.156
iteration 0400: loss: 3952.417
iteration 0500: loss: 3957.414
iteration 0600: loss: 3956.970
iteration 0700: loss: 3959.029
iteration 0800: loss: 3960.569
iteration 0900: loss: 3951.570
iteration 1000: loss: 3957.667
iteration 1100: loss: 3957.041
iteration 1200: loss: 3959.185
iteration 1300: loss: 3954.165
iteration 1400: loss: 3956.273
iteration 1500: loss: 3947.149
iteration 1600: loss: 3949.744
iteration 1700: loss: 3958.477
iteration 1800: loss: 3951.905
====> Epoch: 004 Train loss: 3954.9370  took : 137.82577514648438
====> Test loss: 3957.2797
iteration 0000: loss: 3952.298
iteration 0100: loss: 3949.458
iteration 0200: loss: 3951.680
iteration 0300: loss: 3956.057
iteration 0400: loss: 3956.772
iteration 0500: loss: 3957.171
iteration 0600: loss: 3950.908
iteration 0700: loss: 3957.140
iteration 0800: loss: 3946.887
iteration 0900: loss: 3955.027
iteration 1000: loss: 3955.435
iteration 1100: loss: 3948.708
iteration 1200: loss: 3950.710
iteration 1300: loss: 3956.184
iteration 1400: loss: 3956.952
iteration 1500: loss: 3961.583
iteration 1600: loss: 3953.437
iteration 1700: loss: 3950.195
iteration 1800: loss: 3951.844
====> Epoch: 005 Train loss: 3953.6629  took : 137.93780899047852
====> Test loss: 3956.1554
iteration 0000: loss: 3949.742
iteration 0100: loss: 3958.225
iteration 0200: loss: 3948.216
iteration 0300: loss: 3952.093
iteration 0400: loss: 3948.781
iteration 0500: loss: 3958.428
iteration 0600: loss: 3948.304
iteration 0700: loss: 3948.858
iteration 0800: loss: 3945.742
iteration 0900: loss: 3953.267
iteration 1000: loss: 3953.003
iteration 1100: loss: 3949.212
iteration 1200: loss: 3953.533
iteration 1300: loss: 3952.604
iteration 1400: loss: 3963.343
iteration 1500: loss: 3953.994
iteration 1600: loss: 3950.919
iteration 1700: loss: 3949.797
iteration 1800: loss: 3945.187
====> Epoch: 006 Train loss: 3952.4196  took : 137.8170382976532
====> Test loss: 3954.9882
iteration 0000: loss: 3954.125
iteration 0100: loss: 3952.653
iteration 0200: loss: 3949.225
iteration 0300: loss: 3954.890
iteration 0400: loss: 3957.186
iteration 0500: loss: 3952.098
iteration 0600: loss: 3956.677
iteration 0700: loss: 3960.152
iteration 0800: loss: 3948.780
iteration 0900: loss: 3943.583
iteration 1000: loss: 3949.372
iteration 1100: loss: 3949.688
iteration 1200: loss: 3950.635
iteration 1300: loss: 3950.942
iteration 1400: loss: 3952.286
iteration 1500: loss: 3954.425
iteration 1600: loss: 3954.821
iteration 1700: loss: 3950.299
iteration 1800: loss: 3954.848
====> Epoch: 007 Train loss: 3951.6269  took : 137.8410313129425
====> Test loss: 3954.2366
iteration 0000: loss: 3955.057
iteration 0100: loss: 3953.717
iteration 0200: loss: 3949.987
iteration 0300: loss: 3948.755
iteration 0400: loss: 3948.754
iteration 0500: loss: 3949.206
iteration 0600: loss: 3958.751
iteration 0700: loss: 3950.587
iteration 0800: loss: 3953.651
iteration 0900: loss: 3951.599
iteration 1000: loss: 3953.357
iteration 1100: loss: 3947.663
iteration 1200: loss: 3957.000
iteration 1300: loss: 3946.219
iteration 1400: loss: 3953.444
iteration 1500: loss: 3949.445
iteration 1600: loss: 3953.113
iteration 1700: loss: 3955.395
iteration 1800: loss: 3956.439
====> Epoch: 008 Train loss: 3950.8982  took : 137.81370329856873
====> Test loss: 3953.1431
iteration 0000: loss: 3947.828
iteration 0100: loss: 3951.587
iteration 0200: loss: 3950.790
iteration 0300: loss: 3945.760
iteration 0400: loss: 3945.255
iteration 0500: loss: 3949.222
iteration 0600: loss: 3946.826
iteration 0700: loss: 3946.769
iteration 0800: loss: 3955.184
iteration 0900: loss: 3951.633
iteration 1000: loss: 3953.998
iteration 1100: loss: 3948.497
iteration 1200: loss: 3948.033
iteration 1300: loss: 3945.910
iteration 1400: loss: 3953.876
iteration 1500: loss: 3953.802
iteration 1600: loss: 3951.442
iteration 1700: loss: 3942.653
iteration 1800: loss: 3952.064
====> Epoch: 009 Train loss: 3950.4022  took : 137.81965923309326
====> Test loss: 3953.7121
iteration 0000: loss: 3952.233
iteration 0100: loss: 3946.600
iteration 0200: loss: 3959.548
iteration 0300: loss: 3945.907
iteration 0400: loss: 3958.166
iteration 0500: loss: 3956.181
iteration 0600: loss: 3948.184
iteration 0700: loss: 3951.540
iteration 0800: loss: 3952.032
iteration 0900: loss: 3954.489
iteration 1000: loss: 3957.009
iteration 1100: loss: 3944.876
iteration 1200: loss: 3948.386
iteration 1300: loss: 3951.750
iteration 1400: loss: 3953.867
iteration 1500: loss: 3950.738
iteration 1600: loss: 3949.112
iteration 1700: loss: 3944.300
iteration 1800: loss: 3941.460
====> Epoch: 010 Train loss: 3949.9725  took : 137.73851919174194
====> Test loss: 3952.5238
iteration 0000: loss: 3952.959
iteration 0100: loss: 3945.311
iteration 0200: loss: 3947.305
iteration 0300: loss: 3949.954
iteration 0400: loss: 3950.652
iteration 0500: loss: 3944.362
iteration 0600: loss: 3949.589
iteration 0700: loss: 3943.457
iteration 0800: loss: 3948.674
iteration 0900: loss: 3947.174
iteration 1000: loss: 3953.659
iteration 1100: loss: 3951.992
iteration 1200: loss: 3948.224
iteration 1300: loss: 3950.627
iteration 1400: loss: 3946.918
iteration 1500: loss: 3944.712
iteration 1600: loss: 3955.333
iteration 1700: loss: 3948.322
iteration 1800: loss: 3953.257
====> Epoch: 011 Train loss: 3949.6526  took : 137.6537823677063
====> Test loss: 3952.3284
iteration 0000: loss: 3949.937
iteration 0100: loss: 3952.683
iteration 0200: loss: 3942.778
iteration 0300: loss: 3947.826
iteration 0400: loss: 3946.902
iteration 0500: loss: 3945.253
iteration 0600: loss: 3940.951
iteration 0700: loss: 3951.538
iteration 0800: loss: 3958.044
iteration 0900: loss: 3947.402
iteration 1000: loss: 3948.471
iteration 1100: loss: 3952.785
iteration 1200: loss: 3946.371
iteration 1300: loss: 3949.159
iteration 1400: loss: 3949.726
iteration 1500: loss: 3951.316
iteration 1600: loss: 3947.415
iteration 1700: loss: 3949.048
iteration 1800: loss: 3945.983
====> Epoch: 012 Train loss: 3949.1048  took : 137.68124508857727
====> Test loss: 3951.9890
iteration 0000: loss: 3947.732
iteration 0100: loss: 3950.147
iteration 0200: loss: 3947.345
iteration 0300: loss: 3948.501
iteration 0400: loss: 3952.502
iteration 0500: loss: 3953.351
iteration 0600: loss: 3950.979
iteration 0700: loss: 3944.864
iteration 0800: loss: 3952.063
iteration 0900: loss: 3955.752
iteration 1000: loss: 3957.315
iteration 1100: loss: 3954.260
iteration 1200: loss: 3948.589
iteration 1300: loss: 3946.209
iteration 1400: loss: 3947.430
iteration 1500: loss: 3951.453
iteration 1600: loss: 3950.877
iteration 1700: loss: 3949.904
iteration 1800: loss: 3954.817
====> Epoch: 013 Train loss: 3948.8736  took : 137.6385796070099
====> Test loss: 3951.5629
iteration 0000: loss: 3947.177
iteration 0100: loss: 3951.760
iteration 0200: loss: 3952.981
iteration 0300: loss: 3948.911
iteration 0400: loss: 3946.644
iteration 0500: loss: 3949.245
iteration 0600: loss: 3943.247
iteration 0700: loss: 3943.929
iteration 0800: loss: 3945.139
iteration 0900: loss: 3950.700
iteration 1000: loss: 3948.828
iteration 1100: loss: 3947.413
iteration 1200: loss: 3945.451
iteration 1300: loss: 3953.002
iteration 1400: loss: 3951.033
iteration 1500: loss: 3953.443
iteration 1600: loss: 3953.264
iteration 1700: loss: 3949.906
iteration 1800: loss: 3948.547
====> Epoch: 014 Train loss: 3948.5937  took : 137.60003399848938
====> Test loss: 3951.4644
iteration 0000: loss: 3949.030
iteration 0100: loss: 3946.083
iteration 0200: loss: 3945.361
iteration 0300: loss: 3945.639
iteration 0400: loss: 3949.660
iteration 0500: loss: 3946.520
iteration 0600: loss: 3948.287
iteration 0700: loss: 3944.025
iteration 0800: loss: 3953.590
iteration 0900: loss: 3945.950
iteration 1000: loss: 3956.297
iteration 1100: loss: 3952.390
iteration 1200: loss: 3955.141
iteration 1300: loss: 3944.170
iteration 1400: loss: 3948.500
iteration 1500: loss: 3947.174
iteration 1600: loss: 3940.129
iteration 1700: loss: 3948.981
iteration 1800: loss: 3951.638
====> Epoch: 015 Train loss: 3948.2296  took : 137.56473469734192
====> Test loss: 3951.4864
iteration 0000: loss: 3941.919
iteration 0100: loss: 3952.482
iteration 0200: loss: 3952.041
iteration 0300: loss: 3944.567
iteration 0400: loss: 3941.192
iteration 0500: loss: 3950.642
iteration 0600: loss: 3945.783
iteration 0700: loss: 3948.148
iteration 0800: loss: 3951.119
iteration 0900: loss: 3947.027
iteration 1000: loss: 3952.016
iteration 1100: loss: 3944.996
iteration 1200: loss: 3948.815
iteration 1300: loss: 3952.387
iteration 1400: loss: 3947.842
iteration 1500: loss: 3951.461
iteration 1600: loss: 3948.794
iteration 1700: loss: 3945.443
iteration 1800: loss: 3950.170
====> Epoch: 016 Train loss: 3948.0402  took : 137.47196435928345
====> Test loss: 3951.5512
iteration 0000: loss: 3951.886
iteration 0100: loss: 3956.085
iteration 0200: loss: 3950.422
iteration 0300: loss: 3946.264
iteration 0400: loss: 3952.039
iteration 0500: loss: 3948.884
iteration 0600: loss: 3943.536
iteration 0700: loss: 3945.704
iteration 0800: loss: 3948.722
iteration 0900: loss: 3948.768
iteration 1000: loss: 3949.518
iteration 1100: loss: 3951.458
iteration 1200: loss: 3951.144
iteration 1300: loss: 3945.023
iteration 1400: loss: 3955.721
iteration 1500: loss: 3946.364
iteration 1600: loss: 3946.276
iteration 1700: loss: 3953.920
iteration 1800: loss: 3950.266
====> Epoch: 017 Train loss: 3947.7936  took : 137.5122926235199
====> Test loss: 3950.6735
iteration 0000: loss: 3944.581
iteration 0100: loss: 3948.435
iteration 0200: loss: 3939.928
iteration 0300: loss: 3952.156
iteration 0400: loss: 3948.059
iteration 0500: loss: 3947.355
iteration 0600: loss: 3938.226
iteration 0700: loss: 3946.319
iteration 0800: loss: 3944.691
iteration 0900: loss: 3947.399
iteration 1000: loss: 3942.546
iteration 1100: loss: 3952.496
iteration 1200: loss: 3943.666
iteration 1300: loss: 3944.891
iteration 1400: loss: 3947.541
iteration 1500: loss: 3946.694
iteration 1600: loss: 3944.593
iteration 1700: loss: 3952.752
iteration 1800: loss: 3942.450
====> Epoch: 018 Train loss: 3947.5740  took : 137.469486951828
====> Test loss: 3950.4273
iteration 0000: loss: 3942.153
iteration 0100: loss: 3947.050
iteration 0200: loss: 3950.697
iteration 0300: loss: 3949.350
iteration 0400: loss: 3945.184
iteration 0500: loss: 3947.652
iteration 0600: loss: 3944.491
iteration 0700: loss: 3947.318
iteration 0800: loss: 3950.823
iteration 0900: loss: 3948.372
iteration 1000: loss: 3951.423
iteration 1100: loss: 3948.943
iteration 1200: loss: 3943.653
iteration 1300: loss: 3947.002
iteration 1400: loss: 3950.332
iteration 1500: loss: 3947.571
iteration 1600: loss: 3944.617
iteration 1700: loss: 3945.838
iteration 1800: loss: 3950.377
====> Epoch: 019 Train loss: 3947.3843  took : 137.43581819534302
====> Test loss: 3950.5023
iteration 0000: loss: 3950.013
iteration 0100: loss: 3944.505
iteration 0200: loss: 3949.229
iteration 0300: loss: 3941.373
iteration 0400: loss: 3947.796
iteration 0500: loss: 3940.245
iteration 0600: loss: 3950.422
iteration 0700: loss: 3956.984
iteration 0800: loss: 3947.963
iteration 0900: loss: 3948.829
iteration 1000: loss: 3946.836
iteration 1100: loss: 3944.656
iteration 1200: loss: 3947.803
iteration 1300: loss: 3946.544
iteration 1400: loss: 3949.486
iteration 1500: loss: 3944.652
iteration 1600: loss: 3950.226
iteration 1700: loss: 3942.104
iteration 1800: loss: 3947.776
====> Epoch: 020 Train loss: 3947.1708  took : 137.2635736465454
====> Test loss: 3950.7969
iteration 0000: loss: 3949.973
iteration 0100: loss: 3949.911
iteration 0200: loss: 3952.715
iteration 0300: loss: 3955.262
iteration 0400: loss: 3945.635
iteration 0500: loss: 3946.684
iteration 0600: loss: 3945.848
iteration 0700: loss: 3944.886
iteration 0800: loss: 3949.068
iteration 0900: loss: 3944.437
iteration 1000: loss: 3950.507
iteration 1100: loss: 3949.000
iteration 1200: loss: 3946.402
iteration 1300: loss: 3948.578
iteration 1400: loss: 3942.431
iteration 1500: loss: 3944.631
iteration 1600: loss: 3942.459
iteration 1700: loss: 3951.868
iteration 1800: loss: 3947.020
====> Epoch: 021 Train loss: 3947.0027  took : 137.52731370925903
====> Test loss: 3950.4657
iteration 0000: loss: 3948.415
iteration 0100: loss: 3952.530
iteration 0200: loss: 3944.220
iteration 0300: loss: 3946.673
iteration 0400: loss: 3946.521
iteration 0500: loss: 3946.823
iteration 0600: loss: 3955.063
iteration 0700: loss: 3950.388
iteration 0800: loss: 3942.610
iteration 0900: loss: 3942.026
iteration 1000: loss: 3949.329
iteration 1100: loss: 3945.047
iteration 1200: loss: 3940.156
iteration 1300: loss: 3947.201
iteration 1400: loss: 3943.746
iteration 1500: loss: 3948.830
iteration 1600: loss: 3947.928
iteration 1700: loss: 3941.021
iteration 1800: loss: 3951.289
====> Epoch: 022 Train loss: 3946.8234  took : 137.51007103919983
====> Test loss: 3949.9130
iteration 0000: loss: 3949.833
iteration 0100: loss: 3948.271
iteration 0200: loss: 3947.016
iteration 0300: loss: 3944.976
iteration 0400: loss: 3945.393
iteration 0500: loss: 3938.139
iteration 0600: loss: 3952.087
iteration 0700: loss: 3945.111
iteration 0800: loss: 3948.431
iteration 0900: loss: 3951.133
iteration 1000: loss: 3944.195
iteration 1100: loss: 3941.021
iteration 1200: loss: 3947.798
iteration 1300: loss: 3952.305
iteration 1400: loss: 3947.855
iteration 1500: loss: 3947.134
iteration 1600: loss: 3949.520
iteration 1700: loss: 3945.401
iteration 1800: loss: 3943.637
====> Epoch: 023 Train loss: 3946.7368  took : 137.4782693386078
====> Test loss: 3949.9827
iteration 0000: loss: 3949.021
iteration 0100: loss: 3947.159
iteration 0200: loss: 3952.208
iteration 0300: loss: 3945.953
iteration 0400: loss: 3949.717
iteration 0500: loss: 3947.019
iteration 0600: loss: 3950.117
iteration 0700: loss: 3944.223
iteration 0800: loss: 3942.508
iteration 0900: loss: 3953.856
iteration 1000: loss: 3945.225
iteration 1100: loss: 3948.900
iteration 1200: loss: 3945.755
iteration 1300: loss: 3947.362
iteration 1400: loss: 3944.121
iteration 1500: loss: 3943.584
iteration 1600: loss: 3946.630
iteration 1700: loss: 3942.124
iteration 1800: loss: 3946.808
====> Epoch: 024 Train loss: 3946.7098  took : 137.4277412891388
====> Test loss: 3949.6909
iteration 0000: loss: 3946.383
iteration 0100: loss: 3956.080
iteration 0200: loss: 3955.287
iteration 0300: loss: 3941.850
iteration 0400: loss: 3944.100
iteration 0500: loss: 3946.091
iteration 0600: loss: 3941.161
iteration 0700: loss: 3947.787
iteration 0800: loss: 3945.769
iteration 0900: loss: 3947.789
iteration 1000: loss: 3945.068
iteration 1100: loss: 3950.951
iteration 1200: loss: 3949.805
iteration 1300: loss: 3941.588
iteration 1400: loss: 3950.357
iteration 1500: loss: 3947.294
iteration 1600: loss: 3943.808
iteration 1700: loss: 3951.514
iteration 1800: loss: 3941.163
====> Epoch: 025 Train loss: 3946.4879  took : 137.40795612335205
====> Test loss: 3949.9259
iteration 0000: loss: 3938.581
iteration 0100: loss: 3946.369
iteration 0200: loss: 3948.648
iteration 0300: loss: 3948.465
iteration 0400: loss: 3955.414
iteration 0500: loss: 3945.872
iteration 0600: loss: 3953.790
iteration 0700: loss: 3947.977
iteration 0800: loss: 3946.634
iteration 0900: loss: 3949.366
iteration 1000: loss: 3940.542
iteration 1100: loss: 3952.697
iteration 1200: loss: 3937.325
iteration 1300: loss: 3953.747
iteration 1400: loss: 3944.828
iteration 1500: loss: 3940.870
iteration 1600: loss: 3949.581
iteration 1700: loss: 3946.232
iteration 1800: loss: 3945.378
====> Epoch: 026 Train loss: 3946.4586  took : 137.46508717536926
====> Test loss: 3949.5295
iteration 0000: loss: 3944.415
iteration 0100: loss: 3943.953
iteration 0200: loss: 3940.775
iteration 0300: loss: 3943.847
iteration 0400: loss: 3941.542
iteration 0500: loss: 3944.855
iteration 0600: loss: 3945.529
iteration 0700: loss: 3944.070
iteration 0800: loss: 3940.344
iteration 0900: loss: 3943.626
iteration 1000: loss: 3941.251
iteration 1100: loss: 3939.399
iteration 1200: loss: 3948.143
iteration 1300: loss: 3945.428
iteration 1400: loss: 3947.986
iteration 1500: loss: 3946.922
iteration 1600: loss: 3943.370
iteration 1700: loss: 3951.730
iteration 1800: loss: 3940.053
====> Epoch: 027 Train loss: 3946.1581  took : 137.40598058700562
====> Test loss: 3949.4681
iteration 0000: loss: 3944.928
iteration 0100: loss: 3948.892
iteration 0200: loss: 3942.944
iteration 0300: loss: 3944.073
iteration 0400: loss: 3947.324
iteration 0500: loss: 3947.532
iteration 0600: loss: 3949.366
iteration 0700: loss: 3944.171
iteration 0800: loss: 3950.465
iteration 0900: loss: 3944.831
iteration 1000: loss: 3942.114
iteration 1100: loss: 3943.519
iteration 1200: loss: 3948.522
iteration 1300: loss: 3950.756
iteration 1400: loss: 3945.873
iteration 1500: loss: 3943.561
iteration 1600: loss: 3947.194
iteration 1700: loss: 3947.417
iteration 1800: loss: 3948.009
====> Epoch: 028 Train loss: 3946.0081  took : 137.46741938591003
====> Test loss: 3949.3071
iteration 0000: loss: 3942.615
iteration 0100: loss: 3945.083
iteration 0200: loss: 3943.987
iteration 0300: loss: 3947.854
iteration 0400: loss: 3953.246
iteration 0500: loss: 3944.378
iteration 0600: loss: 3946.818
iteration 0700: loss: 3956.648
iteration 0800: loss: 3945.531
iteration 0900: loss: 3951.894
iteration 1000: loss: 3944.093
iteration 1100: loss: 3942.404
iteration 1200: loss: 3946.154
iteration 1300: loss: 3944.321
iteration 1400: loss: 3951.722
iteration 1500: loss: 3942.388
iteration 1600: loss: 3943.247
iteration 1700: loss: 3941.517
iteration 1800: loss: 3948.995
====> Epoch: 029 Train loss: 3945.9285  took : 137.42905020713806
====> Test loss: 3949.0831
iteration 0000: loss: 3945.055
iteration 0100: loss: 3943.005
iteration 0200: loss: 3949.726
iteration 0300: loss: 3946.554
iteration 0400: loss: 3939.602
iteration 0500: loss: 3937.521
iteration 0600: loss: 3947.681
iteration 0700: loss: 3947.338
iteration 0800: loss: 3943.578
iteration 0900: loss: 3949.848
iteration 1000: loss: 3947.512
iteration 1100: loss: 3941.759
iteration 1200: loss: 3945.609
iteration 1300: loss: 3943.987
iteration 1400: loss: 3945.802
iteration 1500: loss: 3947.385
iteration 1600: loss: 3944.962
iteration 1700: loss: 3948.490
iteration 1800: loss: 3948.037
====> Epoch: 030 Train loss: 3945.8162  took : 137.47017002105713
====> Test loss: 3949.1549
====> [MM-VAE] Time: 4631.644s or 01:17:11
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_1
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1996.208
iteration 0100: loss: 1568.251
iteration 0200: loss: 1550.165
iteration 0300: loss: 1555.022
iteration 0400: loss: 1546.504
iteration 0500: loss: 1543.819
iteration 0600: loss: 1543.932
iteration 0700: loss: 1546.568
iteration 0800: loss: 1537.953
iteration 0900: loss: 1540.136
====> Epoch: 001 Train loss: 1551.8442  took : 3.8625669479370117
====> Test loss: 1531.9003
iteration 0000: loss: 1534.289
iteration 0100: loss: 1539.691
iteration 0200: loss: 1536.526
iteration 0300: loss: 1535.751
iteration 0400: loss: 1533.717
iteration 0500: loss: 1535.919
iteration 0600: loss: 1537.655
iteration 0700: loss: 1531.822
iteration 0800: loss: 1533.344
iteration 0900: loss: 1536.217
====> Epoch: 002 Train loss: 1534.3722  took : 3.861759662628174
====> Test loss: 1527.6368
iteration 0000: loss: 1532.847
iteration 0100: loss: 1532.011
iteration 0200: loss: 1533.666
iteration 0300: loss: 1537.073
iteration 0400: loss: 1532.333
iteration 0500: loss: 1532.605
iteration 0600: loss: 1537.469
iteration 0700: loss: 1527.462
iteration 0800: loss: 1532.112
iteration 0900: loss: 1529.732
====> Epoch: 003 Train loss: 1531.4808  took : 3.859903573989868
====> Test loss: 1525.9625
iteration 0000: loss: 1533.837
iteration 0100: loss: 1530.211
iteration 0200: loss: 1528.345
iteration 0300: loss: 1530.689
iteration 0400: loss: 1528.763
iteration 0500: loss: 1529.892
iteration 0600: loss: 1530.272
iteration 0700: loss: 1533.199
iteration 0800: loss: 1533.453
iteration 0900: loss: 1526.457
====> Epoch: 004 Train loss: 1530.1742  took : 3.837315082550049
====> Test loss: 1524.8232
iteration 0000: loss: 1534.794
iteration 0100: loss: 1527.719
iteration 0200: loss: 1526.417
iteration 0300: loss: 1530.442
iteration 0400: loss: 1526.987
iteration 0500: loss: 1528.144
iteration 0600: loss: 1528.765
iteration 0700: loss: 1528.994
iteration 0800: loss: 1531.010
iteration 0900: loss: 1531.248
====> Epoch: 005 Train loss: 1529.3527  took : 3.8487720489501953
====> Test loss: 1524.2582
iteration 0000: loss: 1531.881
iteration 0100: loss: 1529.194
iteration 0200: loss: 1528.059
iteration 0300: loss: 1528.684
iteration 0400: loss: 1530.046
iteration 0500: loss: 1528.810
iteration 0600: loss: 1531.610
iteration 0700: loss: 1528.652
iteration 0800: loss: 1526.687
iteration 0900: loss: 1527.634
====> Epoch: 006 Train loss: 1528.7667  took : 3.834331750869751
====> Test loss: 1523.6574
iteration 0000: loss: 1528.615
iteration 0100: loss: 1525.734
iteration 0200: loss: 1528.623
iteration 0300: loss: 1530.125
iteration 0400: loss: 1530.943
iteration 0500: loss: 1525.443
iteration 0600: loss: 1527.034
iteration 0700: loss: 1524.748
iteration 0800: loss: 1528.962
iteration 0900: loss: 1526.665
====> Epoch: 007 Train loss: 1528.3626  took : 3.8446409702301025
====> Test loss: 1523.5594
iteration 0000: loss: 1528.713
iteration 0100: loss: 1525.529
iteration 0200: loss: 1527.119
iteration 0300: loss: 1523.780
iteration 0400: loss: 1528.708
iteration 0500: loss: 1529.654
iteration 0600: loss: 1526.956
iteration 0700: loss: 1527.693
iteration 0800: loss: 1529.870
iteration 0900: loss: 1526.563
====> Epoch: 008 Train loss: 1528.0140  took : 3.851900339126587
====> Test loss: 1523.0943
iteration 0000: loss: 1527.840
iteration 0100: loss: 1529.431
iteration 0200: loss: 1526.190
iteration 0300: loss: 1526.829
iteration 0400: loss: 1530.798
iteration 0500: loss: 1528.222
iteration 0600: loss: 1529.400
iteration 0700: loss: 1529.234
iteration 0800: loss: 1525.871
iteration 0900: loss: 1527.155
====> Epoch: 009 Train loss: 1527.7266  took : 3.8625376224517822
====> Test loss: 1522.8639
iteration 0000: loss: 1527.969
iteration 0100: loss: 1526.771
iteration 0200: loss: 1526.912
iteration 0300: loss: 1525.583
iteration 0400: loss: 1526.719
iteration 0500: loss: 1528.931
iteration 0600: loss: 1527.490
iteration 0700: loss: 1532.087
iteration 0800: loss: 1528.365
iteration 0900: loss: 1529.288
====> Epoch: 010 Train loss: 1527.4730  took : 3.839215040206909
====> Test loss: 1522.6411
iteration 0000: loss: 1526.270
iteration 0100: loss: 1528.439
iteration 0200: loss: 1527.963
iteration 0300: loss: 1529.352
iteration 0400: loss: 1527.026
iteration 0500: loss: 1525.235
iteration 0600: loss: 1526.610
iteration 0700: loss: 1527.028
iteration 0800: loss: 1526.481
iteration 0900: loss: 1527.490
====> Epoch: 011 Train loss: 1527.2420  took : 3.849473237991333
====> Test loss: 1522.4936
iteration 0000: loss: 1527.770
iteration 0100: loss: 1524.928
iteration 0200: loss: 1527.624
iteration 0300: loss: 1527.391
iteration 0400: loss: 1526.855
iteration 0500: loss: 1525.838
iteration 0600: loss: 1524.887
iteration 0700: loss: 1528.299
iteration 0800: loss: 1524.776
iteration 0900: loss: 1528.088
====> Epoch: 012 Train loss: 1527.0913  took : 3.869103193283081
====> Test loss: 1522.2867
iteration 0000: loss: 1526.704
iteration 0100: loss: 1524.518
iteration 0200: loss: 1526.278
iteration 0300: loss: 1524.971
iteration 0400: loss: 1530.319
iteration 0500: loss: 1527.443
iteration 0600: loss: 1525.496
iteration 0700: loss: 1526.496
iteration 0800: loss: 1526.930
iteration 0900: loss: 1528.898
====> Epoch: 013 Train loss: 1526.9395  took : 3.8649802207946777
====> Test loss: 1522.2101
iteration 0000: loss: 1528.864
iteration 0100: loss: 1528.718
iteration 0200: loss: 1527.057
iteration 0300: loss: 1524.026
iteration 0400: loss: 1525.409
iteration 0500: loss: 1525.975
iteration 0600: loss: 1525.449
iteration 0700: loss: 1528.449
iteration 0800: loss: 1525.766
iteration 0900: loss: 1530.112
====> Epoch: 014 Train loss: 1526.7732  took : 3.8496367931365967
====> Test loss: 1522.0883
iteration 0000: loss: 1525.787
iteration 0100: loss: 1527.703
iteration 0200: loss: 1529.510
iteration 0300: loss: 1527.983
iteration 0400: loss: 1529.303
iteration 0500: loss: 1528.835
iteration 0600: loss: 1527.046
iteration 0700: loss: 1530.602
iteration 0800: loss: 1528.516
iteration 0900: loss: 1527.494
====> Epoch: 015 Train loss: 1526.6675  took : 3.843751907348633
====> Test loss: 1522.0414
iteration 0000: loss: 1528.429
iteration 0100: loss: 1523.963
iteration 0200: loss: 1527.606
iteration 0300: loss: 1524.262
iteration 0400: loss: 1528.049
iteration 0500: loss: 1524.564
iteration 0600: loss: 1528.179
iteration 0700: loss: 1528.046
iteration 0800: loss: 1527.712
iteration 0900: loss: 1526.224
====> Epoch: 016 Train loss: 1526.5500  took : 3.8421831130981445
====> Test loss: 1521.8553
iteration 0000: loss: 1526.460
iteration 0100: loss: 1527.070
iteration 0200: loss: 1525.990
iteration 0300: loss: 1523.317
iteration 0400: loss: 1526.898
iteration 0500: loss: 1526.805
iteration 0600: loss: 1528.559
iteration 0700: loss: 1527.735
iteration 0800: loss: 1528.226
iteration 0900: loss: 1523.706
====> Epoch: 017 Train loss: 1526.4500  took : 3.8441505432128906
====> Test loss: 1521.9183
iteration 0000: loss: 1529.797
iteration 0100: loss: 1523.839
iteration 0200: loss: 1525.102
iteration 0300: loss: 1525.935
iteration 0400: loss: 1524.384
iteration 0500: loss: 1526.543
iteration 0600: loss: 1528.218
iteration 0700: loss: 1528.660
iteration 0800: loss: 1526.922
iteration 0900: loss: 1527.124
====> Epoch: 018 Train loss: 1526.3315  took : 3.8530168533325195
====> Test loss: 1521.8365
iteration 0000: loss: 1525.705
iteration 0100: loss: 1523.863
iteration 0200: loss: 1526.170
iteration 0300: loss: 1526.014
iteration 0400: loss: 1527.360
iteration 0500: loss: 1526.688
iteration 0600: loss: 1523.752
iteration 0700: loss: 1526.296
iteration 0800: loss: 1525.502
iteration 0900: loss: 1525.824
====> Epoch: 019 Train loss: 1526.2498  took : 3.859976291656494
====> Test loss: 1521.6684
iteration 0000: loss: 1526.775
iteration 0100: loss: 1530.472
iteration 0200: loss: 1527.572
iteration 0300: loss: 1526.390
iteration 0400: loss: 1525.283
iteration 0500: loss: 1524.866
iteration 0600: loss: 1527.559
iteration 0700: loss: 1527.489
iteration 0800: loss: 1526.044
iteration 0900: loss: 1524.698
====> Epoch: 020 Train loss: 1526.1733  took : 3.8500454425811768
====> Test loss: 1521.6283
iteration 0000: loss: 1522.254
iteration 0100: loss: 1530.115
iteration 0200: loss: 1527.022
iteration 0300: loss: 1527.608
iteration 0400: loss: 1522.320
iteration 0500: loss: 1525.666
iteration 0600: loss: 1527.751
iteration 0700: loss: 1527.573
iteration 0800: loss: 1529.411
iteration 0900: loss: 1526.461
====> Epoch: 021 Train loss: 1526.1198  took : 3.855527400970459
====> Test loss: 1521.5886
iteration 0000: loss: 1525.074
iteration 0100: loss: 1524.050
iteration 0200: loss: 1524.385
iteration 0300: loss: 1524.084
iteration 0400: loss: 1525.701
iteration 0500: loss: 1529.584
iteration 0600: loss: 1528.490
iteration 0700: loss: 1528.488
iteration 0800: loss: 1525.652
iteration 0900: loss: 1525.464
====> Epoch: 022 Train loss: 1525.9818  took : 3.87675404548645
====> Test loss: 1521.4975
iteration 0000: loss: 1521.926
iteration 0100: loss: 1524.286
iteration 0200: loss: 1528.091
iteration 0300: loss: 1525.323
iteration 0400: loss: 1525.122
iteration 0500: loss: 1526.819
iteration 0600: loss: 1525.026
iteration 0700: loss: 1527.362
iteration 0800: loss: 1527.520
iteration 0900: loss: 1528.212
====> Epoch: 023 Train loss: 1525.9496  took : 3.846219062805176
====> Test loss: 1521.4905
iteration 0000: loss: 1523.401
iteration 0100: loss: 1528.414
iteration 0200: loss: 1524.394
iteration 0300: loss: 1528.753
iteration 0400: loss: 1528.050
iteration 0500: loss: 1526.412
iteration 0600: loss: 1522.734
iteration 0700: loss: 1524.920
iteration 0800: loss: 1525.183
iteration 0900: loss: 1524.730
====> Epoch: 024 Train loss: 1525.8615  took : 3.860809803009033
====> Test loss: 1521.3547
iteration 0000: loss: 1526.340
iteration 0100: loss: 1523.526
iteration 0200: loss: 1525.883
iteration 0300: loss: 1529.292
iteration 0400: loss: 1528.426
iteration 0500: loss: 1523.786
iteration 0600: loss: 1528.450
iteration 0700: loss: 1522.724
iteration 0800: loss: 1527.226
iteration 0900: loss: 1523.834
====> Epoch: 025 Train loss: 1525.8229  took : 3.8576276302337646
====> Test loss: 1521.2257
iteration 0000: loss: 1524.516
iteration 0100: loss: 1525.036
iteration 0200: loss: 1524.983
iteration 0300: loss: 1526.706
iteration 0400: loss: 1525.720
iteration 0500: loss: 1523.328
iteration 0600: loss: 1528.815
iteration 0700: loss: 1526.523
iteration 0800: loss: 1524.510
iteration 0900: loss: 1528.360
====> Epoch: 026 Train loss: 1525.7314  took : 3.855093240737915
====> Test loss: 1521.3415
iteration 0000: loss: 1527.711
iteration 0100: loss: 1525.489
iteration 0200: loss: 1524.434
iteration 0300: loss: 1530.587
iteration 0400: loss: 1525.755
iteration 0500: loss: 1527.715
iteration 0600: loss: 1526.726
iteration 0700: loss: 1525.224
iteration 0800: loss: 1524.219
iteration 0900: loss: 1526.566
====> Epoch: 027 Train loss: 1525.6813  took : 3.8634986877441406
====> Test loss: 1521.1502
iteration 0000: loss: 1524.065
iteration 0100: loss: 1527.304
iteration 0200: loss: 1524.854
iteration 0300: loss: 1527.354
iteration 0400: loss: 1524.286
iteration 0500: loss: 1525.385
iteration 0600: loss: 1527.938
iteration 0700: loss: 1527.093
iteration 0800: loss: 1523.489
iteration 0900: loss: 1527.366
====> Epoch: 028 Train loss: 1525.6359  took : 3.8515048027038574
====> Test loss: 1521.2806
iteration 0000: loss: 1522.660
iteration 0100: loss: 1524.569
iteration 0200: loss: 1524.146
iteration 0300: loss: 1527.488
iteration 0400: loss: 1526.772
iteration 0500: loss: 1525.317
iteration 0600: loss: 1522.303
iteration 0700: loss: 1528.084
iteration 0800: loss: 1522.746
iteration 0900: loss: 1524.511
====> Epoch: 029 Train loss: 1525.5754  took : 3.877842903137207
====> Test loss: 1521.1899
iteration 0000: loss: 1525.424
iteration 0100: loss: 1525.093
iteration 0200: loss: 1524.430
iteration 0300: loss: 1527.798
iteration 0400: loss: 1524.025
iteration 0500: loss: 1525.242
iteration 0600: loss: 1524.598
iteration 0700: loss: 1526.719
iteration 0800: loss: 1527.188
iteration 0900: loss: 1528.867
====> Epoch: 030 Train loss: 1525.5630  took : 3.8602893352508545
====> Test loss: 1521.2546
====> [MM-VAE] Time: 192.404s or 00:03:12
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2668.104
iteration 0100: loss: 2114.261
iteration 0200: loss: 2098.857
iteration 0300: loss: 2100.760
iteration 0400: loss: 2063.276
iteration 0500: loss: 2043.829
iteration 0600: loss: 2021.921
iteration 0700: loss: 2002.945
iteration 0800: loss: 1987.606
iteration 0900: loss: 1970.252
====> Epoch: 001 Train loss: 2059.9898  took : 6.0961785316467285
====> Test loss: 1961.0966
iteration 0000: loss: 1968.829
iteration 0100: loss: 1964.341
iteration 0200: loss: 1962.358
iteration 0300: loss: 1962.494
iteration 0400: loss: 1963.115
iteration 0500: loss: 1960.922
iteration 0600: loss: 1961.449
iteration 0700: loss: 1960.576
iteration 0800: loss: 1961.149
iteration 0900: loss: 1959.999
====> Epoch: 002 Train loss: 1962.0720  took : 6.086493730545044
====> Test loss: 1955.1753
iteration 0000: loss: 1960.430
iteration 0100: loss: 1959.778
iteration 0200: loss: 1958.499
iteration 0300: loss: 1959.705
iteration 0400: loss: 1960.105
iteration 0500: loss: 1960.086
iteration 0600: loss: 1958.751
iteration 0700: loss: 1960.546
iteration 0800: loss: 1958.448
iteration 0900: loss: 1958.615
====> Epoch: 003 Train loss: 1959.4790  took : 6.0872180461883545
====> Test loss: 1953.9895
iteration 0000: loss: 1959.739
iteration 0100: loss: 1958.578
iteration 0200: loss: 1960.112
iteration 0300: loss: 1957.412
iteration 0400: loss: 1959.997
iteration 0500: loss: 1959.130
iteration 0600: loss: 1958.927
iteration 0700: loss: 1958.234
iteration 0800: loss: 1958.516
iteration 0900: loss: 1958.720
====> Epoch: 004 Train loss: 1958.8290  took : 6.103006362915039
====> Test loss: 1953.3415
iteration 0000: loss: 1957.995
iteration 0100: loss: 1958.903
iteration 0200: loss: 1958.592
iteration 0300: loss: 1957.393
iteration 0400: loss: 1957.632
iteration 0500: loss: 1957.887
iteration 0600: loss: 1957.727
iteration 0700: loss: 1958.408
iteration 0800: loss: 1957.797
iteration 0900: loss: 1956.619
====> Epoch: 005 Train loss: 1958.2611  took : 6.094911098480225
====> Test loss: 1953.2549
iteration 0000: loss: 1957.567
iteration 0100: loss: 1957.722
iteration 0200: loss: 1958.874
iteration 0300: loss: 1958.252
iteration 0400: loss: 1957.268
iteration 0500: loss: 1958.266
iteration 0600: loss: 1958.098
iteration 0700: loss: 1958.581
iteration 0800: loss: 1958.399
iteration 0900: loss: 1959.017
====> Epoch: 006 Train loss: 1958.0228  took : 6.09109354019165
====> Test loss: 1954.1280
iteration 0000: loss: 1959.319
iteration 0100: loss: 1957.969
iteration 0200: loss: 1956.260
iteration 0300: loss: 1957.099
iteration 0400: loss: 1957.331
iteration 0500: loss: 1958.296
iteration 0600: loss: 1957.533
iteration 0700: loss: 1957.396
iteration 0800: loss: 1956.861
iteration 0900: loss: 1956.061
====> Epoch: 007 Train loss: 1957.8734  took : 6.089946031570435
====> Test loss: 1952.2402
iteration 0000: loss: 1956.973
iteration 0100: loss: 1957.042
iteration 0200: loss: 1958.455
iteration 0300: loss: 1957.123
iteration 0400: loss: 1957.349
iteration 0500: loss: 1957.479
iteration 0600: loss: 1958.477
iteration 0700: loss: 1956.767
iteration 0800: loss: 1957.143
iteration 0900: loss: 1957.072
====> Epoch: 008 Train loss: 1957.7602  took : 6.0928661823272705
====> Test loss: 1952.1549
iteration 0000: loss: 1956.254
iteration 0100: loss: 1957.372
iteration 0200: loss: 1957.323
iteration 0300: loss: 1958.298
iteration 0400: loss: 1957.071
iteration 0500: loss: 1957.754
iteration 0600: loss: 1956.557
iteration 0700: loss: 1956.234
iteration 0800: loss: 1957.238
iteration 0900: loss: 1957.075
====> Epoch: 009 Train loss: 1957.6117  took : 6.091957092285156
====> Test loss: 1952.5365
iteration 0000: loss: 1957.016
iteration 0100: loss: 1957.717
iteration 0200: loss: 1955.952
iteration 0300: loss: 1959.450
iteration 0400: loss: 1958.370
iteration 0500: loss: 1958.735
iteration 0600: loss: 1957.620
iteration 0700: loss: 1956.675
iteration 0800: loss: 1956.251
iteration 0900: loss: 1957.772
====> Epoch: 010 Train loss: 1957.5495  took : 6.09949254989624
====> Test loss: 1952.3158
iteration 0000: loss: 1956.750
iteration 0100: loss: 1958.513
iteration 0200: loss: 1957.167
iteration 0300: loss: 1957.133
iteration 0400: loss: 1957.979
iteration 0500: loss: 1958.988
iteration 0600: loss: 1957.142
iteration 0700: loss: 1957.730
iteration 0800: loss: 1957.634
iteration 0900: loss: 1956.527
====> Epoch: 011 Train loss: 1957.4973  took : 6.088690757751465
====> Test loss: 1952.0747
iteration 0000: loss: 1956.432
iteration 0100: loss: 1958.281
iteration 0200: loss: 1957.556
iteration 0300: loss: 1956.885
iteration 0400: loss: 1957.119
iteration 0500: loss: 1958.128
iteration 0600: loss: 1955.949
iteration 0700: loss: 1957.575
iteration 0800: loss: 1959.528
iteration 0900: loss: 1956.598
====> Epoch: 012 Train loss: 1957.3587  took : 6.105870008468628
====> Test loss: 1951.9195
iteration 0000: loss: 1957.744
iteration 0100: loss: 1956.334
iteration 0200: loss: 1958.006
iteration 0300: loss: 1956.303
iteration 0400: loss: 1957.087
iteration 0500: loss: 1956.452
iteration 0600: loss: 1957.017
iteration 0700: loss: 1956.725
iteration 0800: loss: 1957.292
iteration 0900: loss: 1956.525
====> Epoch: 013 Train loss: 1957.2794  took : 6.096182584762573
====> Test loss: 1952.3486
iteration 0000: loss: 1959.028
iteration 0100: loss: 1958.793
iteration 0200: loss: 1957.107
iteration 0300: loss: 1957.969
iteration 0400: loss: 1956.948
iteration 0500: loss: 1956.307
iteration 0600: loss: 1956.997
iteration 0700: loss: 1957.482
iteration 0800: loss: 1957.135
iteration 0900: loss: 1958.540
====> Epoch: 014 Train loss: 1957.1759  took : 6.098736524581909
====> Test loss: 1952.0173
iteration 0000: loss: 1957.757
iteration 0100: loss: 1957.223
iteration 0200: loss: 1956.875
iteration 0300: loss: 1956.924
iteration 0400: loss: 1958.919
iteration 0500: loss: 1957.328
iteration 0600: loss: 1957.735
iteration 0700: loss: 1956.505
iteration 0800: loss: 1956.401
iteration 0900: loss: 1956.455
====> Epoch: 015 Train loss: 1957.1772  took : 6.11141037940979
====> Test loss: 1951.7948
iteration 0000: loss: 1957.147
iteration 0100: loss: 1957.244
iteration 0200: loss: 1959.220
iteration 0300: loss: 1957.411
iteration 0400: loss: 1958.015
iteration 0500: loss: 1957.520
iteration 0600: loss: 1958.912
iteration 0700: loss: 1957.159
iteration 0800: loss: 1956.671
iteration 0900: loss: 1956.417
====> Epoch: 016 Train loss: 1957.0690  took : 6.116122245788574
====> Test loss: 1952.4147
iteration 0000: loss: 1958.169
iteration 0100: loss: 1957.630
iteration 0200: loss: 1957.470
iteration 0300: loss: 1956.286
iteration 0400: loss: 1956.909
iteration 0500: loss: 1958.234
iteration 0600: loss: 1956.972
iteration 0700: loss: 1958.127
iteration 0800: loss: 1956.268
iteration 0900: loss: 1956.612
====> Epoch: 017 Train loss: 1956.9842  took : 6.1277031898498535
====> Test loss: 1951.7272
iteration 0000: loss: 1958.274
iteration 0100: loss: 1956.632
iteration 0200: loss: 1957.177
iteration 0300: loss: 1955.586
iteration 0400: loss: 1956.763
iteration 0500: loss: 1957.581
iteration 0600: loss: 1958.128
iteration 0700: loss: 1956.004
iteration 0800: loss: 1956.518
iteration 0900: loss: 1957.512
====> Epoch: 018 Train loss: 1956.9356  took : 6.13005256652832
====> Test loss: 1951.8487
iteration 0000: loss: 1956.106
iteration 0100: loss: 1957.620
iteration 0200: loss: 1955.941
iteration 0300: loss: 1956.941
iteration 0400: loss: 1957.374
iteration 0500: loss: 1956.973
iteration 0600: loss: 1955.949
iteration 0700: loss: 1957.949
iteration 0800: loss: 1956.302
iteration 0900: loss: 1957.619
====> Epoch: 019 Train loss: 1956.9615  took : 6.1075661182403564
====> Test loss: 1952.2446
iteration 0000: loss: 1956.000
iteration 0100: loss: 1957.586
iteration 0200: loss: 1957.143
iteration 0300: loss: 1956.664
iteration 0400: loss: 1956.684
iteration 0500: loss: 1957.858
iteration 0600: loss: 1956.370
iteration 0700: loss: 1956.496
iteration 0800: loss: 1957.002
iteration 0900: loss: 1958.550
====> Epoch: 020 Train loss: 1956.9218  took : 6.110682725906372
====> Test loss: 1951.5773
iteration 0000: loss: 1955.520
iteration 0100: loss: 1957.468
iteration 0200: loss: 1958.162
iteration 0300: loss: 1956.857
iteration 0400: loss: 1957.593
iteration 0500: loss: 1957.601
iteration 0600: loss: 1957.009
iteration 0700: loss: 1956.173
iteration 0800: loss: 1955.197
iteration 0900: loss: 1955.499
====> Epoch: 021 Train loss: 1956.8832  took : 6.124166250228882
====> Test loss: 1951.5354
iteration 0000: loss: 1956.268
iteration 0100: loss: 1956.906
iteration 0200: loss: 1956.394
iteration 0300: loss: 1956.408
iteration 0400: loss: 1955.923
iteration 0500: loss: 1956.829
iteration 0600: loss: 1956.283
iteration 0700: loss: 1957.346
iteration 0800: loss: 1955.590
iteration 0900: loss: 1955.985
====> Epoch: 022 Train loss: 1956.8313  took : 6.115288019180298
====> Test loss: 1952.0685
iteration 0000: loss: 1956.397
iteration 0100: loss: 1957.855
iteration 0200: loss: 1956.203
iteration 0300: loss: 1956.273
iteration 0400: loss: 1956.608
iteration 0500: loss: 1956.360
iteration 0600: loss: 1956.520
iteration 0700: loss: 1956.033
iteration 0800: loss: 1956.705
iteration 0900: loss: 1955.659
====> Epoch: 023 Train loss: 1956.7870  took : 6.113575458526611
====> Test loss: 1951.9783
iteration 0000: loss: 1957.206
iteration 0100: loss: 1956.957
iteration 0200: loss: 1956.037
iteration 0300: loss: 1955.888
iteration 0400: loss: 1956.287
iteration 0500: loss: 1955.964
iteration 0600: loss: 1956.035
iteration 0700: loss: 1958.160
iteration 0800: loss: 1957.013
iteration 0900: loss: 1956.743
====> Epoch: 024 Train loss: 1956.7908  took : 6.113487005233765
====> Test loss: 1951.8800
iteration 0000: loss: 1956.794
iteration 0100: loss: 1956.573
iteration 0200: loss: 1957.023
iteration 0300: loss: 1956.517
iteration 0400: loss: 1956.222
iteration 0500: loss: 1956.692
iteration 0600: loss: 1957.670
iteration 0700: loss: 1957.921
iteration 0800: loss: 1957.008
iteration 0900: loss: 1957.062
====> Epoch: 025 Train loss: 1956.7190  took : 6.130829095840454
====> Test loss: 1951.5476
iteration 0000: loss: 1957.081
iteration 0100: loss: 1956.263
iteration 0200: loss: 1957.663
iteration 0300: loss: 1956.262
iteration 0400: loss: 1955.918
iteration 0500: loss: 1955.645
iteration 0600: loss: 1956.600
iteration 0700: loss: 1956.494
iteration 0800: loss: 1955.961
iteration 0900: loss: 1955.330
====> Epoch: 026 Train loss: 1956.6593  took : 6.126849174499512
====> Test loss: 1951.4730
iteration 0000: loss: 1956.330
iteration 0100: loss: 1957.042
iteration 0200: loss: 1956.712
iteration 0300: loss: 1957.318
iteration 0400: loss: 1956.452
iteration 0500: loss: 1956.479
iteration 0600: loss: 1958.394
iteration 0700: loss: 1956.867
iteration 0800: loss: 1958.075
iteration 0900: loss: 1956.476
====> Epoch: 027 Train loss: 1956.7348  took : 6.118331432342529
====> Test loss: 1951.7068
iteration 0000: loss: 1956.499
iteration 0100: loss: 1955.222
iteration 0200: loss: 1956.546
iteration 0300: loss: 1955.560
iteration 0400: loss: 1960.076
iteration 0500: loss: 1955.985
iteration 0600: loss: 1958.743
iteration 0700: loss: 1956.986
iteration 0800: loss: 1957.471
iteration 0900: loss: 1956.874
====> Epoch: 028 Train loss: 1956.6603  took : 6.117718935012817
====> Test loss: 1951.6670
iteration 0000: loss: 1956.902
iteration 0100: loss: 1956.127
iteration 0200: loss: 1956.905
iteration 0300: loss: 1956.188
iteration 0400: loss: 1956.061
iteration 0500: loss: 1956.778
iteration 0600: loss: 1956.688
iteration 0700: loss: 1955.913
iteration 0800: loss: 1956.270
iteration 0900: loss: 1955.312
====> Epoch: 029 Train loss: 1956.6579  took : 6.119693756103516
====> Test loss: 1951.5449
iteration 0000: loss: 1955.908
iteration 0100: loss: 1957.293
iteration 0200: loss: 1956.512
iteration 0300: loss: 1956.396
iteration 0400: loss: 1956.672
iteration 0500: loss: 1956.594
iteration 0600: loss: 1956.171
iteration 0700: loss: 1956.311
iteration 0800: loss: 1956.641
iteration 0900: loss: 1956.889
====> Epoch: 030 Train loss: 1956.6397  took : 6.1069042682647705
====> Test loss: 1951.3712
====> [MM-VAE] Time: 248.533s or 00:04:08
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5262.674
iteration 0100: loss: 4133.714
iteration 0200: loss: 4068.015
iteration 0300: loss: 4032.242
iteration 0400: loss: 4021.135
iteration 0500: loss: 4002.501
iteration 0600: loss: 3999.450
iteration 0700: loss: 3991.381
iteration 0800: loss: 3973.944
iteration 0900: loss: 3973.002
iteration 1000: loss: 3968.419
iteration 1100: loss: 3963.388
iteration 1200: loss: 3970.460
iteration 1300: loss: 3963.875
iteration 1400: loss: 3963.842
iteration 1500: loss: 3960.193
iteration 1600: loss: 3963.322
iteration 1700: loss: 3957.496
iteration 1800: loss: 3960.766
====> Epoch: 001 Train loss: 4000.5164  took : 138.39214873313904
====> Test loss: 3962.8080
iteration 0000: loss: 3963.320
iteration 0100: loss: 3953.801
iteration 0200: loss: 3960.350
iteration 0300: loss: 3968.069
iteration 0400: loss: 3967.147
iteration 0500: loss: 3962.961
iteration 0600: loss: 3958.501
iteration 0700: loss: 3956.759
iteration 0800: loss: 3951.240
iteration 0900: loss: 3957.619
iteration 1000: loss: 3954.084
iteration 1100: loss: 3955.277
iteration 1200: loss: 3954.551
iteration 1300: loss: 3955.478
iteration 1400: loss: 3956.331
iteration 1500: loss: 3958.917
iteration 1600: loss: 3958.463
iteration 1700: loss: 3956.184
iteration 1800: loss: 3946.516
====> Epoch: 002 Train loss: 3957.7961  took : 138.1770942211151
====> Test loss: 3957.8112
iteration 0000: loss: 3957.529
iteration 0100: loss: 3956.312
iteration 0200: loss: 3960.372
iteration 0300: loss: 3954.030
iteration 0400: loss: 3955.089
iteration 0500: loss: 3955.887
iteration 0600: loss: 3954.354
iteration 0700: loss: 3958.000
iteration 0800: loss: 3951.885
iteration 0900: loss: 3957.667
iteration 1000: loss: 3953.167
iteration 1100: loss: 3950.624
iteration 1200: loss: 3953.192
iteration 1300: loss: 3957.166
iteration 1400: loss: 3952.765
iteration 1500: loss: 3955.071
iteration 1600: loss: 3955.037
iteration 1700: loss: 3955.025
iteration 1800: loss: 3961.721
====> Epoch: 003 Train loss: 3954.8949  took : 138.13961672782898
====> Test loss: 3956.6633
iteration 0000: loss: 3959.411
iteration 0100: loss: 3952.098
iteration 0200: loss: 3955.223
iteration 0300: loss: 3953.700
iteration 0400: loss: 3952.163
iteration 0500: loss: 3952.048
iteration 0600: loss: 3957.262
iteration 0700: loss: 3961.785
iteration 0800: loss: 3954.862
iteration 0900: loss: 3948.756
iteration 1000: loss: 3957.879
iteration 1100: loss: 3947.521
iteration 1200: loss: 3951.512
iteration 1300: loss: 3949.182
iteration 1400: loss: 3951.043
iteration 1500: loss: 3948.382
iteration 1600: loss: 3956.908
iteration 1700: loss: 3954.671
iteration 1800: loss: 3957.433
====> Epoch: 004 Train loss: 3953.3326  took : 137.9178647994995
====> Test loss: 3955.1821
iteration 0000: loss: 3956.255
iteration 0100: loss: 3951.119
iteration 0200: loss: 3955.550
iteration 0300: loss: 3956.484
iteration 0400: loss: 3947.017
iteration 0500: loss: 3947.658
iteration 0600: loss: 3956.585
iteration 0700: loss: 3954.889
iteration 0800: loss: 3956.586
iteration 0900: loss: 3946.171
iteration 1000: loss: 3951.780
iteration 1100: loss: 3949.121
iteration 1200: loss: 3950.008
iteration 1300: loss: 3947.830
iteration 1400: loss: 3948.033
iteration 1500: loss: 3957.969
iteration 1600: loss: 3944.797
iteration 1700: loss: 3950.239
iteration 1800: loss: 3953.569
====> Epoch: 005 Train loss: 3952.2246  took : 137.85154962539673
====> Test loss: 3954.2762
iteration 0000: loss: 3952.008
iteration 0100: loss: 3953.426
iteration 0200: loss: 3957.340
iteration 0300: loss: 3949.477
iteration 0400: loss: 3949.800
iteration 0500: loss: 3949.606
iteration 0600: loss: 3952.777
iteration 0700: loss: 3961.075
iteration 0800: loss: 3954.059
iteration 0900: loss: 3958.333
iteration 1000: loss: 3960.167
iteration 1100: loss: 3948.471
iteration 1200: loss: 3946.559
iteration 1300: loss: 3950.797
iteration 1400: loss: 3948.418
iteration 1500: loss: 3944.939
iteration 1600: loss: 3958.142
iteration 1700: loss: 3945.689
iteration 1800: loss: 3950.019
====> Epoch: 006 Train loss: 3951.3877  took : 137.86198329925537
====> Test loss: 3953.8845
iteration 0000: loss: 3957.726
iteration 0100: loss: 3950.653
iteration 0200: loss: 3944.277
iteration 0300: loss: 3947.861
iteration 0400: loss: 3948.816
iteration 0500: loss: 3946.779
iteration 0600: loss: 3954.548
iteration 0700: loss: 3946.627
iteration 0800: loss: 3953.160
iteration 0900: loss: 3949.992
iteration 1000: loss: 3951.006
iteration 1100: loss: 3956.754
iteration 1200: loss: 3953.053
iteration 1300: loss: 3949.639
iteration 1400: loss: 3948.981
iteration 1500: loss: 3947.969
iteration 1600: loss: 3947.613
iteration 1700: loss: 3950.896
iteration 1800: loss: 3947.441
====> Epoch: 007 Train loss: 3950.7550  took : 137.82018423080444
====> Test loss: 3953.0611
iteration 0000: loss: 3945.789
iteration 0100: loss: 3944.670
iteration 0200: loss: 3954.152
iteration 0300: loss: 3958.265
iteration 0400: loss: 3949.553
iteration 0500: loss: 3947.442
iteration 0600: loss: 3945.479
iteration 0700: loss: 3951.654
iteration 0800: loss: 3950.825
iteration 0900: loss: 3951.046
iteration 1000: loss: 3944.310
iteration 1100: loss: 3940.522
iteration 1200: loss: 3952.297
iteration 1300: loss: 3953.270
iteration 1400: loss: 3945.628
iteration 1500: loss: 3943.701
iteration 1600: loss: 3946.680
iteration 1700: loss: 3955.337
iteration 1800: loss: 3953.027
====> Epoch: 008 Train loss: 3950.2937  took : 137.90815210342407
====> Test loss: 3953.0854
iteration 0000: loss: 3947.018
iteration 0100: loss: 3953.388
iteration 0200: loss: 3952.855
iteration 0300: loss: 3948.383
iteration 0400: loss: 3943.424
iteration 0500: loss: 3948.110
iteration 0600: loss: 3949.299
iteration 0700: loss: 3952.212
iteration 0800: loss: 3945.633
iteration 0900: loss: 3949.854
iteration 1000: loss: 3946.913
iteration 1100: loss: 3947.333
iteration 1200: loss: 3950.603
iteration 1300: loss: 3950.536
iteration 1400: loss: 3950.957
iteration 1500: loss: 3949.320
iteration 1600: loss: 3948.631
iteration 1700: loss: 3951.878
iteration 1800: loss: 3955.738
====> Epoch: 009 Train loss: 3949.8032  took : 137.8398334980011
====> Test loss: 3952.4039
iteration 0000: loss: 3953.144
iteration 0100: loss: 3949.028
iteration 0200: loss: 3941.759
iteration 0300: loss: 3944.476
iteration 0400: loss: 3955.081
iteration 0500: loss: 3947.764
iteration 0600: loss: 3950.329
iteration 0700: loss: 3947.002
iteration 0800: loss: 3952.607
iteration 0900: loss: 3953.524
iteration 1000: loss: 3954.774
iteration 1100: loss: 3946.580
iteration 1200: loss: 3950.522
iteration 1300: loss: 3952.854
iteration 1400: loss: 3946.493
iteration 1500: loss: 3945.435
iteration 1600: loss: 3947.731
iteration 1700: loss: 3951.035
iteration 1800: loss: 3951.225
====> Epoch: 010 Train loss: 3949.3695  took : 137.77483558654785
====> Test loss: 3952.4027
iteration 0000: loss: 3948.590
iteration 0100: loss: 3943.552
iteration 0200: loss: 3947.031
iteration 0300: loss: 3953.010
iteration 0400: loss: 3957.054
iteration 0500: loss: 3949.071
iteration 0600: loss: 3953.352
iteration 0700: loss: 3950.825
iteration 0800: loss: 3949.169
iteration 0900: loss: 3952.433
iteration 1000: loss: 3950.508
iteration 1100: loss: 3944.181
iteration 1200: loss: 3949.085
iteration 1300: loss: 3955.521
iteration 1400: loss: 3945.341
iteration 1500: loss: 3950.202
iteration 1600: loss: 3943.319
iteration 1700: loss: 3950.088
iteration 1800: loss: 3949.712
====> Epoch: 011 Train loss: 3949.0532  took : 137.82450199127197
====> Test loss: 3951.5350
iteration 0000: loss: 3947.052
iteration 0100: loss: 3946.977
iteration 0200: loss: 3944.482
iteration 0300: loss: 3951.097
iteration 0400: loss: 3947.293
iteration 0500: loss: 3946.421
iteration 0600: loss: 3947.603
iteration 0700: loss: 3947.437
iteration 0800: loss: 3948.850
iteration 0900: loss: 3956.542
iteration 1000: loss: 3952.295
iteration 1100: loss: 3946.994
iteration 1200: loss: 3940.750
iteration 1300: loss: 3946.351
iteration 1400: loss: 3948.094
iteration 1500: loss: 3951.247
iteration 1600: loss: 3947.548
iteration 1700: loss: 3947.267
iteration 1800: loss: 3949.526
====> Epoch: 012 Train loss: 3948.7451  took : 137.8346016407013
====> Test loss: 3951.6922
iteration 0000: loss: 3952.949
iteration 0100: loss: 3947.683
iteration 0200: loss: 3950.218
iteration 0300: loss: 3946.030
iteration 0400: loss: 3945.970
iteration 0500: loss: 3949.666
iteration 0600: loss: 3952.167
iteration 0700: loss: 3950.277
iteration 0800: loss: 3946.484
iteration 0900: loss: 3946.720
iteration 1000: loss: 3953.730
iteration 1100: loss: 3943.533
iteration 1200: loss: 3953.569
iteration 1300: loss: 3946.064
iteration 1400: loss: 3946.159
iteration 1500: loss: 3947.489
iteration 1600: loss: 3956.409
iteration 1700: loss: 3949.974
iteration 1800: loss: 3949.880
====> Epoch: 013 Train loss: 3948.4307  took : 137.7092924118042
====> Test loss: 3951.2207
iteration 0000: loss: 3948.900
iteration 0100: loss: 3948.589
iteration 0200: loss: 3950.144
iteration 0300: loss: 3952.308
iteration 0400: loss: 3947.608
iteration 0500: loss: 3948.873
iteration 0600: loss: 3948.094
iteration 0700: loss: 3947.451
iteration 0800: loss: 3946.277
iteration 0900: loss: 3954.167
iteration 1000: loss: 3947.100
iteration 1100: loss: 3948.656
iteration 1200: loss: 3943.015
iteration 1300: loss: 3951.626
iteration 1400: loss: 3945.960
iteration 1500: loss: 3950.711
iteration 1600: loss: 3942.036
iteration 1700: loss: 3948.305
iteration 1800: loss: 3947.093
====> Epoch: 014 Train loss: 3948.1929  took : 137.81703877449036
====> Test loss: 3951.0907
iteration 0000: loss: 3941.488
iteration 0100: loss: 3948.109
iteration 0200: loss: 3952.828
iteration 0300: loss: 3945.474
iteration 0400: loss: 3953.279
iteration 0500: loss: 3947.192
iteration 0600: loss: 3941.435
iteration 0700: loss: 3944.577
iteration 0800: loss: 3953.199
iteration 0900: loss: 3947.532
iteration 1000: loss: 3946.478
iteration 1100: loss: 3949.722
iteration 1200: loss: 3949.742
iteration 1300: loss: 3944.408
iteration 1400: loss: 3946.424
iteration 1500: loss: 3944.153
iteration 1600: loss: 3953.055
iteration 1700: loss: 3948.633
iteration 1800: loss: 3943.119
====> Epoch: 015 Train loss: 3947.8899  took : 137.72075200080872
====> Test loss: 3950.8532
iteration 0000: loss: 3949.061
iteration 0100: loss: 3951.287
iteration 0200: loss: 3945.352
iteration 0300: loss: 3948.647
iteration 0400: loss: 3953.080
iteration 0500: loss: 3946.979
iteration 0600: loss: 3948.904
iteration 0700: loss: 3951.163
iteration 0800: loss: 3949.960
iteration 0900: loss: 3946.137
iteration 1000: loss: 3948.451
iteration 1100: loss: 3944.480
iteration 1200: loss: 3947.417
iteration 1300: loss: 3946.848
iteration 1400: loss: 3947.842
iteration 1500: loss: 3947.754
iteration 1600: loss: 3943.057
iteration 1700: loss: 3948.958
iteration 1800: loss: 3947.830
====> Epoch: 016 Train loss: 3947.7035  took : 137.64826250076294
====> Test loss: 3950.7104
iteration 0000: loss: 3948.040
iteration 0100: loss: 3952.800
iteration 0200: loss: 3947.892
iteration 0300: loss: 3939.021
iteration 0400: loss: 3950.472
iteration 0500: loss: 3944.147
iteration 0600: loss: 3945.889
iteration 0700: loss: 3946.841
iteration 0800: loss: 3941.695
iteration 0900: loss: 3941.695
iteration 1000: loss: 3947.093
iteration 1100: loss: 3949.026
iteration 1200: loss: 3944.559
iteration 1300: loss: 3948.764
iteration 1400: loss: 3947.015
iteration 1500: loss: 3944.104
iteration 1600: loss: 3943.072
iteration 1700: loss: 3949.164
iteration 1800: loss: 3945.003
====> Epoch: 017 Train loss: 3947.5365  took : 137.70988726615906
====> Test loss: 3950.5330
iteration 0000: loss: 3946.775
iteration 0100: loss: 3951.449
iteration 0200: loss: 3954.635
iteration 0300: loss: 3952.039
iteration 0400: loss: 3944.314
iteration 0500: loss: 3947.321
iteration 0600: loss: 3950.477
iteration 0700: loss: 3943.344
iteration 0800: loss: 3944.666
iteration 0900: loss: 3946.268
iteration 1000: loss: 3944.861
iteration 1100: loss: 3941.928
iteration 1200: loss: 3947.575
iteration 1300: loss: 3952.993
iteration 1400: loss: 3947.958
iteration 1500: loss: 3944.158
iteration 1600: loss: 3948.673
iteration 1700: loss: 3946.052
iteration 1800: loss: 3946.548
====> Epoch: 018 Train loss: 3947.3686  took : 137.61820244789124
====> Test loss: 3950.4298
iteration 0000: loss: 3940.473
iteration 0100: loss: 3956.580
iteration 0200: loss: 3953.646
iteration 0300: loss: 3945.555
iteration 0400: loss: 3945.633
iteration 0500: loss: 3947.231
iteration 0600: loss: 3949.065
iteration 0700: loss: 3947.981
iteration 0800: loss: 3946.276
iteration 0900: loss: 3951.903
iteration 1000: loss: 3949.357
iteration 1100: loss: 3946.214
iteration 1200: loss: 3942.603
iteration 1300: loss: 3947.927
iteration 1400: loss: 3950.103
iteration 1500: loss: 3948.271
iteration 1600: loss: 3951.722
iteration 1700: loss: 3948.887
iteration 1800: loss: 3948.692
====> Epoch: 019 Train loss: 3947.2587  took : 137.74611973762512
====> Test loss: 3950.2127
iteration 0000: loss: 3939.026
iteration 0100: loss: 3941.787
iteration 0200: loss: 3945.114
iteration 0300: loss: 3945.562
iteration 0400: loss: 3945.951
iteration 0500: loss: 3943.743
iteration 0600: loss: 3949.187
iteration 0700: loss: 3943.600
iteration 0800: loss: 3946.487
iteration 0900: loss: 3944.335
iteration 1000: loss: 3941.662
iteration 1100: loss: 3947.036
iteration 1200: loss: 3943.745
iteration 1300: loss: 3946.918
iteration 1400: loss: 3945.495
iteration 1500: loss: 3947.617
iteration 1600: loss: 3948.190
iteration 1700: loss: 3945.733
iteration 1800: loss: 3945.514
====> Epoch: 020 Train loss: 3947.0152  took : 137.65681910514832
====> Test loss: 3949.8273
iteration 0000: loss: 3944.958
iteration 0100: loss: 3946.500
iteration 0200: loss: 3947.055
iteration 0300: loss: 3946.121
iteration 0400: loss: 3946.683
iteration 0500: loss: 3946.806
iteration 0600: loss: 3948.828
iteration 0700: loss: 3946.107
iteration 0800: loss: 3943.896
iteration 0900: loss: 3954.761
iteration 1000: loss: 3945.971
iteration 1100: loss: 3948.612
iteration 1200: loss: 3943.884
iteration 1300: loss: 3943.876
iteration 1400: loss: 3943.826
iteration 1500: loss: 3945.738
iteration 1600: loss: 3950.822
iteration 1700: loss: 3946.743
iteration 1800: loss: 3948.064
====> Epoch: 021 Train loss: 3946.7571  took : 137.62165641784668
====> Test loss: 3949.7560
iteration 0000: loss: 3942.047
iteration 0100: loss: 3947.315
iteration 0200: loss: 3943.557
iteration 0300: loss: 3947.281
iteration 0400: loss: 3948.165
iteration 0500: loss: 3946.602
iteration 0600: loss: 3943.655
iteration 0700: loss: 3949.811
iteration 0800: loss: 3945.471
iteration 0900: loss: 3945.653
iteration 1000: loss: 3943.122
iteration 1100: loss: 3954.986
iteration 1200: loss: 3951.030
iteration 1300: loss: 3943.687
iteration 1400: loss: 3947.689
iteration 1500: loss: 3941.901
iteration 1600: loss: 3951.524
iteration 1700: loss: 3948.306
iteration 1800: loss: 3949.344
====> Epoch: 022 Train loss: 3946.6256  took : 137.6508686542511
====> Test loss: 3949.8956
iteration 0000: loss: 3945.916
iteration 0100: loss: 3946.149
iteration 0200: loss: 3946.568
iteration 0300: loss: 3950.375
iteration 0400: loss: 3944.071
iteration 0500: loss: 3946.073
iteration 0600: loss: 3943.148
iteration 0700: loss: 3941.582
iteration 0800: loss: 3945.731
iteration 0900: loss: 3948.192
iteration 1000: loss: 3949.072
iteration 1100: loss: 3948.669
iteration 1200: loss: 3947.202
iteration 1300: loss: 3948.872
iteration 1400: loss: 3950.769
iteration 1500: loss: 3942.398
iteration 1600: loss: 3945.501
iteration 1700: loss: 3942.875
iteration 1800: loss: 3950.561
====> Epoch: 023 Train loss: 3946.5784  took : 137.67301845550537
====> Test loss: 3949.6283
iteration 0000: loss: 3950.683
iteration 0100: loss: 3946.657
iteration 0200: loss: 3946.226
iteration 0300: loss: 3950.033
iteration 0400: loss: 3945.316
iteration 0500: loss: 3945.837
iteration 0600: loss: 3944.283
iteration 0700: loss: 3947.850
iteration 0800: loss: 3951.266
iteration 0900: loss: 3950.530
iteration 1000: loss: 3950.524
iteration 1100: loss: 3947.363
iteration 1200: loss: 3939.971
iteration 1300: loss: 3953.363
iteration 1400: loss: 3941.760
iteration 1500: loss: 3942.262
iteration 1600: loss: 3951.027
iteration 1700: loss: 3948.850
iteration 1800: loss: 3950.016
====> Epoch: 024 Train loss: 3946.2729  took : 137.61011600494385
====> Test loss: 3949.3847
iteration 0000: loss: 3948.920
iteration 0100: loss: 3943.134
iteration 0200: loss: 3945.760
iteration 0300: loss: 3947.966
iteration 0400: loss: 3946.414
iteration 0500: loss: 3945.503
iteration 0600: loss: 3945.187
iteration 0700: loss: 3952.225
iteration 0800: loss: 3944.070
iteration 0900: loss: 3947.226
iteration 1000: loss: 3955.905
iteration 1100: loss: 3942.952
iteration 1200: loss: 3942.060
iteration 1300: loss: 3944.470
iteration 1400: loss: 3949.729
iteration 1500: loss: 3946.006
iteration 1600: loss: 3948.833
iteration 1700: loss: 3944.293
iteration 1800: loss: 3944.466
====> Epoch: 025 Train loss: 3946.0797  took : 137.66936087608337
====> Test loss: 3949.3248
iteration 0000: loss: 3950.052
iteration 0100: loss: 3947.632
iteration 0200: loss: 3951.510
iteration 0300: loss: 3943.917
iteration 0400: loss: 3947.035
iteration 0500: loss: 3945.448
iteration 0600: loss: 3945.258
iteration 0700: loss: 3942.476
iteration 0800: loss: 3946.421
iteration 0900: loss: 3944.512
iteration 1000: loss: 3943.762
iteration 1100: loss: 3945.080
iteration 1200: loss: 3950.281
iteration 1300: loss: 3946.041
iteration 1400: loss: 3944.760
iteration 1500: loss: 3947.628
iteration 1600: loss: 3947.252
iteration 1700: loss: 3944.645
iteration 1800: loss: 3945.491
====> Epoch: 026 Train loss: 3945.9098  took : 137.6392319202423
====> Test loss: 3949.1132
iteration 0000: loss: 3947.549
iteration 0100: loss: 3946.433
iteration 0200: loss: 3938.003
iteration 0300: loss: 3946.216
iteration 0400: loss: 3942.993
iteration 0500: loss: 3943.601
iteration 0600: loss: 3944.817
iteration 0700: loss: 3945.800
iteration 0800: loss: 3945.018
iteration 0900: loss: 3953.682
iteration 1000: loss: 3950.849
iteration 1100: loss: 3947.666
iteration 1200: loss: 3940.100
iteration 1300: loss: 3952.304
iteration 1400: loss: 3945.759
iteration 1500: loss: 3939.572
iteration 1600: loss: 3945.842
iteration 1700: loss: 3943.990
iteration 1800: loss: 3941.757
====> Epoch: 027 Train loss: 3945.6881  took : 137.5872950553894
====> Test loss: 3948.7676
iteration 0000: loss: 3950.042
iteration 0100: loss: 3945.890
iteration 0200: loss: 3949.061
iteration 0300: loss: 3950.125
iteration 0400: loss: 3951.769
iteration 0500: loss: 3948.214
iteration 0600: loss: 3942.903
iteration 0700: loss: 3945.357
iteration 0800: loss: 3946.163
iteration 0900: loss: 3953.250
iteration 1000: loss: 3944.258
iteration 1100: loss: 3943.668
iteration 1200: loss: 3949.141
iteration 1300: loss: 3947.508
iteration 1400: loss: 3950.807
iteration 1500: loss: 3940.052
iteration 1600: loss: 3944.003
iteration 1700: loss: 3939.804
iteration 1800: loss: 3939.759
====> Epoch: 028 Train loss: 3945.6224  took : 137.58952832221985
====> Test loss: 3948.8369
iteration 0000: loss: 3947.516
iteration 0100: loss: 3944.136
iteration 0200: loss: 3940.047
iteration 0300: loss: 3946.664
iteration 0400: loss: 3943.545
iteration 0500: loss: 3941.871
iteration 0600: loss: 3944.569
iteration 0700: loss: 3945.494
iteration 0800: loss: 3944.200
iteration 0900: loss: 3944.173
iteration 1000: loss: 3946.996
iteration 1100: loss: 3940.340
iteration 1200: loss: 3943.358
iteration 1300: loss: 3946.025
iteration 1400: loss: 3943.460
iteration 1500: loss: 3952.917
iteration 1600: loss: 3940.979
iteration 1700: loss: 3943.176
iteration 1800: loss: 3941.180
====> Epoch: 029 Train loss: 3945.3953  took : 137.57302832603455
====> Test loss: 3948.3813
iteration 0000: loss: 3948.396
iteration 0100: loss: 3943.460
iteration 0200: loss: 3941.221
iteration 0300: loss: 3942.697
iteration 0400: loss: 3943.480
iteration 0500: loss: 3941.713
iteration 0600: loss: 3944.571
iteration 0700: loss: 3939.955
iteration 0800: loss: 3947.593
iteration 0900: loss: 3940.628
iteration 1000: loss: 3946.203
iteration 1100: loss: 3944.722
iteration 1200: loss: 3938.335
iteration 1300: loss: 3941.832
iteration 1400: loss: 3942.338
iteration 1500: loss: 3946.755
iteration 1600: loss: 3947.769
iteration 1700: loss: 3942.611
iteration 1800: loss: 3950.852
====> Epoch: 030 Train loss: 3945.2821  took : 137.57338547706604
====> Test loss: 3948.2576
====> [MM-VAE] Time: 4626.100s or 01:17:06
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_2
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1995.261
iteration 0100: loss: 1561.582
iteration 0200: loss: 1555.360
iteration 0300: loss: 1547.929
iteration 0400: loss: 1546.933
iteration 0500: loss: 1544.300
iteration 0600: loss: 1546.830
iteration 0700: loss: 1543.147
iteration 0800: loss: 1542.679
iteration 0900: loss: 1537.413
====> Epoch: 001 Train loss: 1551.5146  took : 3.877941131591797
====> Test loss: 1531.5376
iteration 0000: loss: 1536.984
iteration 0100: loss: 1533.424
iteration 0200: loss: 1538.505
iteration 0300: loss: 1534.057
iteration 0400: loss: 1533.755
iteration 0500: loss: 1533.859
iteration 0600: loss: 1529.446
iteration 0700: loss: 1532.674
iteration 0800: loss: 1532.564
iteration 0900: loss: 1536.499
====> Epoch: 002 Train loss: 1534.0595  took : 3.8681747913360596
====> Test loss: 1527.1381
iteration 0000: loss: 1534.025
iteration 0100: loss: 1532.746
iteration 0200: loss: 1530.480
iteration 0300: loss: 1531.439
iteration 0400: loss: 1532.633
iteration 0500: loss: 1536.518
iteration 0600: loss: 1531.385
iteration 0700: loss: 1532.378
iteration 0800: loss: 1529.288
iteration 0900: loss: 1529.651
====> Epoch: 003 Train loss: 1531.3214  took : 3.857128143310547
====> Test loss: 1525.6601
iteration 0000: loss: 1531.185
iteration 0100: loss: 1525.843
iteration 0200: loss: 1533.682
iteration 0300: loss: 1530.353
iteration 0400: loss: 1528.484
iteration 0500: loss: 1529.863
iteration 0600: loss: 1531.332
iteration 0700: loss: 1529.370
iteration 0800: loss: 1528.428
iteration 0900: loss: 1530.985
====> Epoch: 004 Train loss: 1530.1931  took : 3.8537588119506836
====> Test loss: 1524.8260
iteration 0000: loss: 1531.341
iteration 0100: loss: 1529.877
iteration 0200: loss: 1534.062
iteration 0300: loss: 1528.354
iteration 0400: loss: 1528.020
iteration 0500: loss: 1528.578
iteration 0600: loss: 1530.813
iteration 0700: loss: 1528.656
iteration 0800: loss: 1529.122
iteration 0900: loss: 1529.669
====> Epoch: 005 Train loss: 1529.4867  took : 3.854351758956909
====> Test loss: 1524.2457
iteration 0000: loss: 1528.257
iteration 0100: loss: 1525.655
iteration 0200: loss: 1527.063
iteration 0300: loss: 1525.760
iteration 0400: loss: 1530.147
iteration 0500: loss: 1526.587
iteration 0600: loss: 1525.980
iteration 0700: loss: 1527.330
iteration 0800: loss: 1526.654
iteration 0900: loss: 1525.916
====> Epoch: 006 Train loss: 1528.9566  took : 3.869912624359131
====> Test loss: 1523.7602
iteration 0000: loss: 1528.127
iteration 0100: loss: 1527.309
iteration 0200: loss: 1526.537
iteration 0300: loss: 1528.073
iteration 0400: loss: 1531.468
iteration 0500: loss: 1529.817
iteration 0600: loss: 1529.310
iteration 0700: loss: 1528.600
iteration 0800: loss: 1532.002
iteration 0900: loss: 1528.462
====> Epoch: 007 Train loss: 1528.4865  took : 3.8526151180267334
====> Test loss: 1523.3613
iteration 0000: loss: 1525.862
iteration 0100: loss: 1525.649
iteration 0200: loss: 1525.571
iteration 0300: loss: 1528.601
iteration 0400: loss: 1525.572
iteration 0500: loss: 1529.462
iteration 0600: loss: 1527.656
iteration 0700: loss: 1526.797
iteration 0800: loss: 1526.407
iteration 0900: loss: 1526.211
====> Epoch: 008 Train loss: 1528.0825  took : 3.8690130710601807
====> Test loss: 1523.0661
iteration 0000: loss: 1529.823
iteration 0100: loss: 1525.038
iteration 0200: loss: 1527.241
iteration 0300: loss: 1526.251
iteration 0400: loss: 1525.934
iteration 0500: loss: 1526.480
iteration 0600: loss: 1531.438
iteration 0700: loss: 1526.803
iteration 0800: loss: 1529.989
iteration 0900: loss: 1524.557
====> Epoch: 009 Train loss: 1527.7736  took : 3.859478712081909
====> Test loss: 1522.8179
iteration 0000: loss: 1530.829
iteration 0100: loss: 1526.858
iteration 0200: loss: 1524.616
iteration 0300: loss: 1529.392
iteration 0400: loss: 1526.309
iteration 0500: loss: 1527.047
iteration 0600: loss: 1527.075
iteration 0700: loss: 1524.334
iteration 0800: loss: 1526.251
iteration 0900: loss: 1529.571
====> Epoch: 010 Train loss: 1527.4725  took : 3.869462251663208
====> Test loss: 1522.6092
iteration 0000: loss: 1527.175
iteration 0100: loss: 1528.506
iteration 0200: loss: 1525.783
iteration 0300: loss: 1530.581
iteration 0400: loss: 1528.720
iteration 0500: loss: 1530.281
iteration 0600: loss: 1526.449
iteration 0700: loss: 1528.259
iteration 0800: loss: 1526.526
iteration 0900: loss: 1524.872
====> Epoch: 011 Train loss: 1527.2532  took : 3.8796944618225098
====> Test loss: 1522.3722
iteration 0000: loss: 1526.890
iteration 0100: loss: 1526.108
iteration 0200: loss: 1526.002
iteration 0300: loss: 1526.328
iteration 0400: loss: 1530.517
iteration 0500: loss: 1525.418
iteration 0600: loss: 1526.845
iteration 0700: loss: 1528.002
iteration 0800: loss: 1529.534
iteration 0900: loss: 1522.814
====> Epoch: 012 Train loss: 1527.0589  took : 3.8717846870422363
====> Test loss: 1522.1545
iteration 0000: loss: 1527.570
iteration 0100: loss: 1529.062
iteration 0200: loss: 1524.942
iteration 0300: loss: 1527.937
iteration 0400: loss: 1526.173
iteration 0500: loss: 1525.759
iteration 0600: loss: 1524.932
iteration 0700: loss: 1529.542
iteration 0800: loss: 1527.030
iteration 0900: loss: 1523.446
====> Epoch: 013 Train loss: 1526.8609  took : 3.8653292655944824
====> Test loss: 1522.0832
iteration 0000: loss: 1527.864
iteration 0100: loss: 1529.804
iteration 0200: loss: 1526.266
iteration 0300: loss: 1528.396
iteration 0400: loss: 1525.580
iteration 0500: loss: 1525.521
iteration 0600: loss: 1523.299
iteration 0700: loss: 1529.545
iteration 0800: loss: 1525.072
iteration 0900: loss: 1528.828
====> Epoch: 014 Train loss: 1526.6965  took : 3.870739698410034
====> Test loss: 1521.9111
iteration 0000: loss: 1525.603
iteration 0100: loss: 1526.677
iteration 0200: loss: 1527.857
iteration 0300: loss: 1524.680
iteration 0400: loss: 1527.229
iteration 0500: loss: 1525.745
iteration 0600: loss: 1523.278
iteration 0700: loss: 1530.783
iteration 0800: loss: 1526.943
iteration 0900: loss: 1523.413
====> Epoch: 015 Train loss: 1526.5407  took : 3.8548362255096436
====> Test loss: 1521.7476
iteration 0000: loss: 1527.351
iteration 0100: loss: 1527.358
iteration 0200: loss: 1525.476
iteration 0300: loss: 1525.986
iteration 0400: loss: 1529.595
iteration 0500: loss: 1531.175
iteration 0600: loss: 1525.326
iteration 0700: loss: 1526.475
iteration 0800: loss: 1525.123
iteration 0900: loss: 1526.036
====> Epoch: 016 Train loss: 1526.4044  took : 3.8486554622650146
====> Test loss: 1521.6431
iteration 0000: loss: 1526.716
iteration 0100: loss: 1525.340
iteration 0200: loss: 1528.762
iteration 0300: loss: 1526.230
iteration 0400: loss: 1526.899
iteration 0500: loss: 1525.964
iteration 0600: loss: 1522.328
iteration 0700: loss: 1525.349
iteration 0800: loss: 1529.012
iteration 0900: loss: 1527.362
====> Epoch: 017 Train loss: 1526.3157  took : 3.875678539276123
====> Test loss: 1521.7474
iteration 0000: loss: 1526.664
iteration 0100: loss: 1526.355
iteration 0200: loss: 1527.968
iteration 0300: loss: 1528.991
iteration 0400: loss: 1528.249
iteration 0500: loss: 1526.248
iteration 0600: loss: 1527.786
iteration 0700: loss: 1527.531
iteration 0800: loss: 1524.031
iteration 0900: loss: 1527.317
====> Epoch: 018 Train loss: 1526.1918  took : 3.8612735271453857
====> Test loss: 1521.6148
iteration 0000: loss: 1530.988
iteration 0100: loss: 1524.258
iteration 0200: loss: 1526.155
iteration 0300: loss: 1526.460
iteration 0400: loss: 1531.019
iteration 0500: loss: 1526.747
iteration 0600: loss: 1524.449
iteration 0700: loss: 1527.265
iteration 0800: loss: 1526.068
iteration 0900: loss: 1528.971
====> Epoch: 019 Train loss: 1526.0865  took : 3.8630869388580322
====> Test loss: 1521.5838
iteration 0000: loss: 1527.105
iteration 0100: loss: 1527.696
iteration 0200: loss: 1526.720
iteration 0300: loss: 1525.573
iteration 0400: loss: 1526.720
iteration 0500: loss: 1525.986
iteration 0600: loss: 1525.957
iteration 0700: loss: 1525.156
iteration 0800: loss: 1526.575
iteration 0900: loss: 1525.101
====> Epoch: 020 Train loss: 1526.0035  took : 3.867197036743164
====> Test loss: 1521.3361
iteration 0000: loss: 1524.650
iteration 0100: loss: 1529.522
iteration 0200: loss: 1524.045
iteration 0300: loss: 1526.770
iteration 0400: loss: 1519.738
iteration 0500: loss: 1526.579
iteration 0600: loss: 1526.348
iteration 0700: loss: 1524.193
iteration 0800: loss: 1527.825
iteration 0900: loss: 1524.061
====> Epoch: 021 Train loss: 1525.8612  took : 3.8710532188415527
====> Test loss: 1521.4281
iteration 0000: loss: 1524.741
iteration 0100: loss: 1524.549
iteration 0200: loss: 1526.890
iteration 0300: loss: 1525.917
iteration 0400: loss: 1526.669
iteration 0500: loss: 1526.235
iteration 0600: loss: 1529.396
iteration 0700: loss: 1524.283
iteration 0800: loss: 1525.827
iteration 0900: loss: 1527.062
====> Epoch: 022 Train loss: 1525.8275  took : 3.8846189975738525
====> Test loss: 1521.4001
iteration 0000: loss: 1521.338
iteration 0100: loss: 1528.094
iteration 0200: loss: 1527.510
iteration 0300: loss: 1527.378
iteration 0400: loss: 1526.641
iteration 0500: loss: 1528.075
iteration 0600: loss: 1526.414
iteration 0700: loss: 1525.323
iteration 0800: loss: 1527.601
iteration 0900: loss: 1524.305
====> Epoch: 023 Train loss: 1525.7768  took : 3.881225109100342
====> Test loss: 1521.1767
iteration 0000: loss: 1525.863
iteration 0100: loss: 1527.197
iteration 0200: loss: 1522.437
iteration 0300: loss: 1523.191
iteration 0400: loss: 1524.030
iteration 0500: loss: 1527.216
iteration 0600: loss: 1526.648
iteration 0700: loss: 1526.681
iteration 0800: loss: 1525.188
iteration 0900: loss: 1526.383
====> Epoch: 024 Train loss: 1525.6867  took : 3.892308235168457
====> Test loss: 1521.0821
iteration 0000: loss: 1523.325
iteration 0100: loss: 1525.917
iteration 0200: loss: 1525.862
iteration 0300: loss: 1527.032
iteration 0400: loss: 1524.494
iteration 0500: loss: 1526.175
iteration 0600: loss: 1526.526
iteration 0700: loss: 1525.269
iteration 0800: loss: 1526.398
iteration 0900: loss: 1523.532
====> Epoch: 025 Train loss: 1525.6149  took : 3.875654697418213
====> Test loss: 1521.1845
iteration 0000: loss: 1526.449
iteration 0100: loss: 1523.129
iteration 0200: loss: 1522.669
iteration 0300: loss: 1525.271
iteration 0400: loss: 1525.738
iteration 0500: loss: 1525.568
iteration 0600: loss: 1525.198
iteration 0700: loss: 1527.115
iteration 0800: loss: 1526.482
iteration 0900: loss: 1524.189
====> Epoch: 026 Train loss: 1525.5538  took : 3.8600170612335205
====> Test loss: 1521.0921
iteration 0000: loss: 1526.514
iteration 0100: loss: 1524.735
iteration 0200: loss: 1526.069
iteration 0300: loss: 1528.219
iteration 0400: loss: 1526.730
iteration 0500: loss: 1526.392
iteration 0600: loss: 1526.328
iteration 0700: loss: 1527.179
iteration 0800: loss: 1522.805
iteration 0900: loss: 1524.921
====> Epoch: 027 Train loss: 1525.5139  took : 3.8693654537200928
====> Test loss: 1521.0301
iteration 0000: loss: 1526.993
iteration 0100: loss: 1528.323
iteration 0200: loss: 1526.450
iteration 0300: loss: 1525.682
iteration 0400: loss: 1524.312
iteration 0500: loss: 1522.651
iteration 0600: loss: 1525.932
iteration 0700: loss: 1523.535
iteration 0800: loss: 1525.114
iteration 0900: loss: 1525.210
====> Epoch: 028 Train loss: 1525.4319  took : 3.856206178665161
====> Test loss: 1521.0291
iteration 0000: loss: 1525.797
iteration 0100: loss: 1524.541
iteration 0200: loss: 1525.891
iteration 0300: loss: 1526.789
iteration 0400: loss: 1523.796
iteration 0500: loss: 1524.652
iteration 0600: loss: 1523.107
iteration 0700: loss: 1521.353
iteration 0800: loss: 1526.796
iteration 0900: loss: 1524.279
====> Epoch: 029 Train loss: 1525.4069  took : 3.8551132678985596
====> Test loss: 1520.9192
iteration 0000: loss: 1524.697
iteration 0100: loss: 1524.422
iteration 0200: loss: 1525.414
iteration 0300: loss: 1526.737
iteration 0400: loss: 1529.257
iteration 0500: loss: 1525.105
iteration 0600: loss: 1527.608
iteration 0700: loss: 1524.458
iteration 0800: loss: 1523.678
iteration 0900: loss: 1527.951
====> Epoch: 030 Train loss: 1525.3315  took : 3.8652846813201904
====> Test loss: 1520.8895
====> [MM-VAE] Time: 191.690s or 00:03:11
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2610.270
iteration 0100: loss: 2095.733
iteration 0200: loss: 2100.085
iteration 0300: loss: 2085.385
iteration 0400: loss: 2068.661
iteration 0500: loss: 2034.503
iteration 0600: loss: 2001.011
iteration 0700: loss: 1987.996
iteration 0800: loss: 1969.711
iteration 0900: loss: 1966.044
====> Epoch: 001 Train loss: 2047.8475  took : 6.082321405410767
====> Test loss: 1958.6435
iteration 0000: loss: 1964.446
iteration 0100: loss: 1963.215
iteration 0200: loss: 1963.150
iteration 0300: loss: 1962.778
iteration 0400: loss: 1960.967
iteration 0500: loss: 1961.767
iteration 0600: loss: 1961.291
iteration 0700: loss: 1961.064
iteration 0800: loss: 1960.235
iteration 0900: loss: 1960.441
====> Epoch: 002 Train loss: 1961.6444  took : 6.1538403034210205
====> Test loss: 1954.7372
iteration 0000: loss: 1958.899
iteration 0100: loss: 1960.304
iteration 0200: loss: 1960.429
iteration 0300: loss: 1961.252
iteration 0400: loss: 1959.264
iteration 0500: loss: 1959.331
iteration 0600: loss: 1961.094
iteration 0700: loss: 1960.647
iteration 0800: loss: 1960.217
iteration 0900: loss: 1958.392
====> Epoch: 003 Train loss: 1959.9358  took : 6.119543552398682
====> Test loss: 1954.0326
iteration 0000: loss: 1959.411
iteration 0100: loss: 1959.984
iteration 0200: loss: 1959.406
iteration 0300: loss: 1958.844
iteration 0400: loss: 1958.773
iteration 0500: loss: 1958.058
iteration 0600: loss: 1959.253
iteration 0700: loss: 1958.699
iteration 0800: loss: 1959.880
iteration 0900: loss: 1958.507
====> Epoch: 004 Train loss: 1959.2776  took : 6.0935962200164795
====> Test loss: 1953.4987
iteration 0000: loss: 1959.037
iteration 0100: loss: 1958.904
iteration 0200: loss: 1959.581
iteration 0300: loss: 1959.197
iteration 0400: loss: 1959.388
iteration 0500: loss: 1958.483
iteration 0600: loss: 1958.995
iteration 0700: loss: 1959.058
iteration 0800: loss: 1958.458
iteration 0900: loss: 1960.122
====> Epoch: 005 Train loss: 1958.9264  took : 6.092766523361206
====> Test loss: 1953.6420
iteration 0000: loss: 1959.688
iteration 0100: loss: 1958.030
iteration 0200: loss: 1957.701
iteration 0300: loss: 1959.220
iteration 0400: loss: 1959.074
iteration 0500: loss: 1956.977
iteration 0600: loss: 1958.529
iteration 0700: loss: 1959.194
iteration 0800: loss: 1958.494
iteration 0900: loss: 1958.898
====> Epoch: 006 Train loss: 1958.6300  took : 6.107191801071167
====> Test loss: 1953.1446
iteration 0000: loss: 1958.124
iteration 0100: loss: 1957.579
iteration 0200: loss: 1958.097
iteration 0300: loss: 1958.402
iteration 0400: loss: 1959.825
iteration 0500: loss: 1957.626
iteration 0600: loss: 1958.774
iteration 0700: loss: 1957.396
iteration 0800: loss: 1957.546
iteration 0900: loss: 1958.288
====> Epoch: 007 Train loss: 1958.3231  took : 6.09493613243103
====> Test loss: 1953.5073
iteration 0000: loss: 1958.757
iteration 0100: loss: 1957.264
iteration 0200: loss: 1957.411
iteration 0300: loss: 1958.639
iteration 0400: loss: 1958.325
iteration 0500: loss: 1957.773
iteration 0600: loss: 1957.942
iteration 0700: loss: 1958.394
iteration 0800: loss: 1958.127
iteration 0900: loss: 1958.692
====> Epoch: 008 Train loss: 1958.2265  took : 6.100281715393066
====> Test loss: 1952.6631
iteration 0000: loss: 1956.721
iteration 0100: loss: 1957.906
iteration 0200: loss: 1958.734
iteration 0300: loss: 1958.293
iteration 0400: loss: 1956.882
iteration 0500: loss: 1957.675
iteration 0600: loss: 1958.593
iteration 0700: loss: 1957.333
iteration 0800: loss: 1956.604
iteration 0900: loss: 1957.942
====> Epoch: 009 Train loss: 1958.0466  took : 6.095873832702637
====> Test loss: 1952.6791
iteration 0000: loss: 1958.279
iteration 0100: loss: 1957.028
iteration 0200: loss: 1958.529
iteration 0300: loss: 1956.921
iteration 0400: loss: 1957.791
iteration 0500: loss: 1957.047
iteration 0600: loss: 1957.787
iteration 0700: loss: 1956.889
iteration 0800: loss: 1957.655
iteration 0900: loss: 1956.500
====> Epoch: 010 Train loss: 1957.9333  took : 6.094887018203735
====> Test loss: 1952.7000
iteration 0000: loss: 1958.003
iteration 0100: loss: 1958.379
iteration 0200: loss: 1955.808
iteration 0300: loss: 1962.564
iteration 0400: loss: 1957.673
iteration 0500: loss: 1957.887
iteration 0600: loss: 1957.316
iteration 0700: loss: 1957.475
iteration 0800: loss: 1957.761
iteration 0900: loss: 1957.145
====> Epoch: 011 Train loss: 1957.8366  took : 6.107553243637085
====> Test loss: 1952.5022
iteration 0000: loss: 1958.072
iteration 0100: loss: 1957.624
iteration 0200: loss: 1958.057
iteration 0300: loss: 1957.288
iteration 0400: loss: 1957.915
iteration 0500: loss: 1957.673
iteration 0600: loss: 1957.557
iteration 0700: loss: 1957.352
iteration 0800: loss: 1957.316
iteration 0900: loss: 1959.401
====> Epoch: 012 Train loss: 1957.7973  took : 6.1130077838897705
====> Test loss: 1952.6411
iteration 0000: loss: 1957.851
iteration 0100: loss: 1957.736
iteration 0200: loss: 1959.094
iteration 0300: loss: 1956.978
iteration 0400: loss: 1958.715
iteration 0500: loss: 1956.212
iteration 0600: loss: 1958.308
iteration 0700: loss: 1956.431
iteration 0800: loss: 1957.588
iteration 0900: loss: 1957.356
====> Epoch: 013 Train loss: 1957.6758  took : 6.113543272018433
====> Test loss: 1952.3225
iteration 0000: loss: 1957.193
iteration 0100: loss: 1957.904
iteration 0200: loss: 1957.175
iteration 0300: loss: 1957.651
iteration 0400: loss: 1957.724
iteration 0500: loss: 1957.268
iteration 0600: loss: 1956.915
iteration 0700: loss: 1956.568
iteration 0800: loss: 1956.286
iteration 0900: loss: 1958.285
====> Epoch: 014 Train loss: 1957.6430  took : 6.110026121139526
====> Test loss: 1952.3946
iteration 0000: loss: 1957.542
iteration 0100: loss: 1957.194
iteration 0200: loss: 1956.502
iteration 0300: loss: 1957.356
iteration 0400: loss: 1957.458
iteration 0500: loss: 1957.691
iteration 0600: loss: 1959.060
iteration 0700: loss: 1957.542
iteration 0800: loss: 1956.974
iteration 0900: loss: 1959.243
====> Epoch: 015 Train loss: 1957.6086  took : 6.1191136837005615
====> Test loss: 1952.8490
iteration 0000: loss: 1957.806
iteration 0100: loss: 1957.583
iteration 0200: loss: 1957.846
iteration 0300: loss: 1957.268
iteration 0400: loss: 1958.101
iteration 0500: loss: 1956.781
iteration 0600: loss: 1956.365
iteration 0700: loss: 1958.071
iteration 0800: loss: 1956.790
iteration 0900: loss: 1957.670
====> Epoch: 016 Train loss: 1957.5263  took : 6.1061155796051025
====> Test loss: 1952.1400
iteration 0000: loss: 1956.305
iteration 0100: loss: 1957.867
iteration 0200: loss: 1957.383
iteration 0300: loss: 1957.758
iteration 0400: loss: 1958.211
iteration 0500: loss: 1957.473
iteration 0600: loss: 1956.871
iteration 0700: loss: 1957.251
iteration 0800: loss: 1959.183
iteration 0900: loss: 1956.621
====> Epoch: 017 Train loss: 1957.5562  took : 6.11264443397522
====> Test loss: 1952.0871
iteration 0000: loss: 1957.891
iteration 0100: loss: 1956.214
iteration 0200: loss: 1958.882
iteration 0300: loss: 1957.158
iteration 0400: loss: 1956.023
iteration 0500: loss: 1956.938
iteration 0600: loss: 1956.024
iteration 0700: loss: 1956.973
iteration 0800: loss: 1956.533
iteration 0900: loss: 1958.453
====> Epoch: 018 Train loss: 1957.4739  took : 6.0976972579956055
====> Test loss: 1951.8963
iteration 0000: loss: 1957.327
iteration 0100: loss: 1956.800
iteration 0200: loss: 1957.801
iteration 0300: loss: 1957.058
iteration 0400: loss: 1957.690
iteration 0500: loss: 1957.489
iteration 0600: loss: 1957.422
iteration 0700: loss: 1957.718
iteration 0800: loss: 1956.939
iteration 0900: loss: 1958.616
====> Epoch: 019 Train loss: 1957.4400  took : 6.101936340332031
====> Test loss: 1952.6521
iteration 0000: loss: 1957.398
iteration 0100: loss: 1956.874
iteration 0200: loss: 1956.461
iteration 0300: loss: 1956.745
iteration 0400: loss: 1958.028
iteration 0500: loss: 1957.144
iteration 0600: loss: 1956.413
iteration 0700: loss: 1956.975
iteration 0800: loss: 1956.700
iteration 0900: loss: 1956.426
====> Epoch: 020 Train loss: 1957.3844  took : 6.095383644104004
====> Test loss: 1951.8984
iteration 0000: loss: 1957.724
iteration 0100: loss: 1957.668
iteration 0200: loss: 1957.855
iteration 0300: loss: 1957.337
iteration 0400: loss: 1956.748
iteration 0500: loss: 1958.016
iteration 0600: loss: 1960.309
iteration 0700: loss: 1956.559
iteration 0800: loss: 1956.570
iteration 0900: loss: 1957.213
====> Epoch: 021 Train loss: 1957.2983  took : 6.118767499923706
====> Test loss: 1952.1906
iteration 0000: loss: 1957.319
iteration 0100: loss: 1957.194
iteration 0200: loss: 1956.406
iteration 0300: loss: 1956.317
iteration 0400: loss: 1957.472
iteration 0500: loss: 1956.439
iteration 0600: loss: 1957.483
iteration 0700: loss: 1958.575
iteration 0800: loss: 1957.041
iteration 0900: loss: 1956.102
====> Epoch: 022 Train loss: 1957.2823  took : 6.112244606018066
====> Test loss: 1951.8598
iteration 0000: loss: 1956.745
iteration 0100: loss: 1956.725
iteration 0200: loss: 1957.232
iteration 0300: loss: 1956.806
iteration 0400: loss: 1956.310
iteration 0500: loss: 1957.137
iteration 0600: loss: 1957.069
iteration 0700: loss: 1957.039
iteration 0800: loss: 1956.100
iteration 0900: loss: 1956.539
====> Epoch: 023 Train loss: 1957.0732  took : 6.114821434020996
====> Test loss: 1952.0533
iteration 0000: loss: 1956.947
iteration 0100: loss: 1957.385
iteration 0200: loss: 1956.585
iteration 0300: loss: 1957.590
iteration 0400: loss: 1956.337
iteration 0500: loss: 1956.767
iteration 0600: loss: 1957.025
iteration 0700: loss: 1957.255
iteration 0800: loss: 1956.838
iteration 0900: loss: 1958.198
====> Epoch: 024 Train loss: 1957.0017  took : 6.113573312759399
====> Test loss: 1951.7015
iteration 0000: loss: 1957.383
iteration 0100: loss: 1957.029
iteration 0200: loss: 1956.816
iteration 0300: loss: 1956.461
iteration 0400: loss: 1960.297
iteration 0500: loss: 1957.286
iteration 0600: loss: 1957.457
iteration 0700: loss: 1957.029
iteration 0800: loss: 1956.991
iteration 0900: loss: 1956.812
====> Epoch: 025 Train loss: 1956.9826  took : 6.101748466491699
====> Test loss: 1951.8242
iteration 0000: loss: 1956.154
iteration 0100: loss: 1957.660
iteration 0200: loss: 1955.359
iteration 0300: loss: 1958.229
iteration 0400: loss: 1957.801
iteration 0500: loss: 1957.030
iteration 0600: loss: 1958.663
iteration 0700: loss: 1957.217
iteration 0800: loss: 1956.584
iteration 0900: loss: 1957.517
====> Epoch: 026 Train loss: 1956.9501  took : 6.097453832626343
====> Test loss: 1951.6568
iteration 0000: loss: 1956.930
iteration 0100: loss: 1957.238
iteration 0200: loss: 1956.673
iteration 0300: loss: 1958.553
iteration 0400: loss: 1956.341
iteration 0500: loss: 1957.215
iteration 0600: loss: 1956.910
iteration 0700: loss: 1957.198
iteration 0800: loss: 1957.911
iteration 0900: loss: 1956.781
====> Epoch: 027 Train loss: 1956.9650  took : 6.118908405303955
====> Test loss: 1952.0479
iteration 0000: loss: 1956.112
iteration 0100: loss: 1956.403
iteration 0200: loss: 1957.934
iteration 0300: loss: 1955.827
iteration 0400: loss: 1956.612
iteration 0500: loss: 1956.602
iteration 0600: loss: 1955.759
iteration 0700: loss: 1956.626
iteration 0800: loss: 1956.451
iteration 0900: loss: 1956.989
====> Epoch: 028 Train loss: 1956.9351  took : 6.1264729499816895
====> Test loss: 1951.7331
iteration 0000: loss: 1956.427
iteration 0100: loss: 1957.372
iteration 0200: loss: 1955.720
iteration 0300: loss: 1956.864
iteration 0400: loss: 1956.804
iteration 0500: loss: 1957.057
iteration 0600: loss: 1956.859
iteration 0700: loss: 1955.539
iteration 0800: loss: 1955.958
iteration 0900: loss: 1956.804
====> Epoch: 029 Train loss: 1956.9062  took : 6.106475114822388
====> Test loss: 1951.6638
iteration 0000: loss: 1955.566
iteration 0100: loss: 1955.823
iteration 0200: loss: 1956.709
iteration 0300: loss: 1956.272
iteration 0400: loss: 1956.964
iteration 0500: loss: 1957.612
iteration 0600: loss: 1959.346
iteration 0700: loss: 1957.260
iteration 0800: loss: 1957.742
iteration 0900: loss: 1956.306
====> Epoch: 030 Train loss: 1956.8874  took : 6.112037658691406
====> Test loss: 1951.6374
====> [MM-VAE] Time: 249.069s or 00:04:09
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5261.143
iteration 0100: loss: 4106.468
iteration 0200: loss: 4076.870
iteration 0300: loss: 4026.311
iteration 0400: loss: 4006.128
iteration 0500: loss: 4002.673
iteration 0600: loss: 3995.795
iteration 0700: loss: 3984.342
iteration 0800: loss: 3988.861
iteration 0900: loss: 3977.486
iteration 1000: loss: 3975.200
iteration 1100: loss: 3974.480
iteration 1200: loss: 3970.561
iteration 1300: loss: 3968.521
iteration 1400: loss: 3966.871
iteration 1500: loss: 3967.753
iteration 1600: loss: 3964.994
iteration 1700: loss: 3959.973
iteration 1800: loss: 3965.259
====> Epoch: 001 Train loss: 4003.6038  took : 138.5590877532959
====> Test loss: 3964.9185
iteration 0000: loss: 3956.838
iteration 0100: loss: 3961.123
iteration 0200: loss: 3969.047
iteration 0300: loss: 3963.972
iteration 0400: loss: 3956.757
iteration 0500: loss: 3963.824
iteration 0600: loss: 3964.341
iteration 0700: loss: 3957.549
iteration 0800: loss: 3960.524
iteration 0900: loss: 3950.633
iteration 1000: loss: 3958.365
iteration 1100: loss: 3954.622
iteration 1200: loss: 3950.951
iteration 1300: loss: 3964.687
iteration 1400: loss: 3951.081
iteration 1500: loss: 3960.207
iteration 1600: loss: 3965.915
iteration 1700: loss: 3965.566
iteration 1800: loss: 3955.637
====> Epoch: 002 Train loss: 3959.3783  took : 138.5111734867096
====> Test loss: 3959.3375
iteration 0000: loss: 3959.941
iteration 0100: loss: 3957.277
iteration 0200: loss: 3957.258
iteration 0300: loss: 3958.493
iteration 0400: loss: 3959.198
iteration 0500: loss: 3957.270
iteration 0600: loss: 3966.704
iteration 0700: loss: 3962.911
iteration 0800: loss: 3953.159
iteration 0900: loss: 3950.014
iteration 1000: loss: 3952.383
iteration 1100: loss: 3958.069
iteration 1200: loss: 3955.953
iteration 1300: loss: 3956.629
iteration 1400: loss: 3952.350
iteration 1500: loss: 3956.019
iteration 1600: loss: 3953.486
iteration 1700: loss: 3954.657
iteration 1800: loss: 3951.915
====> Epoch: 003 Train loss: 3955.5821  took : 138.37764811515808
====> Test loss: 3956.7296
iteration 0000: loss: 3955.081
iteration 0100: loss: 3956.993
iteration 0200: loss: 3954.643
iteration 0300: loss: 3959.591
iteration 0400: loss: 3955.182
iteration 0500: loss: 3960.911
iteration 0600: loss: 3953.553
iteration 0700: loss: 3953.022
iteration 0800: loss: 3953.769
iteration 0900: loss: 3947.737
iteration 1000: loss: 3950.886
iteration 1100: loss: 3945.728
iteration 1200: loss: 3956.167
iteration 1300: loss: 3955.647
iteration 1400: loss: 3952.185
iteration 1500: loss: 3957.497
iteration 1600: loss: 3949.199
iteration 1700: loss: 3947.936
iteration 1800: loss: 3947.323
====> Epoch: 004 Train loss: 3953.7761  took : 138.31964135169983
====> Test loss: 3955.8044
iteration 0000: loss: 3948.794
iteration 0100: loss: 3949.721
iteration 0200: loss: 3959.961
iteration 0300: loss: 3950.584
iteration 0400: loss: 3952.958
iteration 0500: loss: 3947.621
iteration 0600: loss: 3952.165
iteration 0700: loss: 3948.918
iteration 0800: loss: 3951.542
iteration 0900: loss: 3951.473
iteration 1000: loss: 3950.858
iteration 1100: loss: 3952.691
iteration 1200: loss: 3954.643
iteration 1300: loss: 3948.910
iteration 1400: loss: 3954.694
iteration 1500: loss: 3944.620
iteration 1600: loss: 3948.942
iteration 1700: loss: 3951.271
iteration 1800: loss: 3951.140
====> Epoch: 005 Train loss: 3952.1101  took : 138.21632862091064
====> Test loss: 3954.1125
iteration 0000: loss: 3953.457
iteration 0100: loss: 3950.381
iteration 0200: loss: 3953.600
iteration 0300: loss: 3953.685
iteration 0400: loss: 3958.543
iteration 0500: loss: 3950.957
iteration 0600: loss: 3956.403
iteration 0700: loss: 3947.559
iteration 0800: loss: 3952.026
iteration 0900: loss: 3950.419
iteration 1000: loss: 3952.462
iteration 1100: loss: 3953.734
iteration 1200: loss: 3952.111
iteration 1300: loss: 3945.113
iteration 1400: loss: 3945.062
iteration 1500: loss: 3950.588
iteration 1600: loss: 3944.623
iteration 1700: loss: 3950.862
iteration 1800: loss: 3949.934
====> Epoch: 006 Train loss: 3951.0702  took : 138.20400834083557
====> Test loss: 3953.0522
iteration 0000: loss: 3950.767
iteration 0100: loss: 3949.126
iteration 0200: loss: 3956.026
iteration 0300: loss: 3959.668
iteration 0400: loss: 3949.382
iteration 0500: loss: 3955.518
iteration 0600: loss: 3947.544
iteration 0700: loss: 3956.707
iteration 0800: loss: 3949.006
iteration 0900: loss: 3955.130
iteration 1000: loss: 3946.908
iteration 1100: loss: 3953.222
iteration 1200: loss: 3950.927
iteration 1300: loss: 3950.576
iteration 1400: loss: 3952.235
iteration 1500: loss: 3949.915
iteration 1600: loss: 3952.801
iteration 1700: loss: 3947.261
iteration 1800: loss: 3955.828
====> Epoch: 007 Train loss: 3950.3599  took : 138.09905672073364
====> Test loss: 3952.5701
iteration 0000: loss: 3951.427
iteration 0100: loss: 3955.485
iteration 0200: loss: 3952.796
iteration 0300: loss: 3945.904
iteration 0400: loss: 3951.788
iteration 0500: loss: 3950.889
iteration 0600: loss: 3949.677
iteration 0700: loss: 3945.163
iteration 0800: loss: 3954.848
iteration 0900: loss: 3956.658
iteration 1000: loss: 3944.826
iteration 1100: loss: 3956.604
iteration 1200: loss: 3952.620
iteration 1300: loss: 3946.716
iteration 1400: loss: 3958.877
iteration 1500: loss: 3948.048
iteration 1600: loss: 3946.210
iteration 1700: loss: 3947.245
iteration 1800: loss: 3951.725
====> Epoch: 008 Train loss: 3949.7311  took : 138.05738401412964
====> Test loss: 3952.2331
iteration 0000: loss: 3951.459
iteration 0100: loss: 3950.255
iteration 0200: loss: 3957.588
iteration 0300: loss: 3945.817
iteration 0400: loss: 3945.185
iteration 0500: loss: 3940.892
iteration 0600: loss: 3953.181
iteration 0700: loss: 3948.142
iteration 0800: loss: 3951.625
iteration 0900: loss: 3949.593
iteration 1000: loss: 3945.939
iteration 1100: loss: 3949.712
iteration 1200: loss: 3950.597
iteration 1300: loss: 3950.025
iteration 1400: loss: 3947.304
iteration 1500: loss: 3950.348
iteration 1600: loss: 3940.043
iteration 1700: loss: 3947.490
iteration 1800: loss: 3945.881
====> Epoch: 009 Train loss: 3949.3062  took : 138.12534832954407
====> Test loss: 3951.9017
iteration 0000: loss: 3951.081
iteration 0100: loss: 3951.540
iteration 0200: loss: 3953.538
iteration 0300: loss: 3951.071
iteration 0400: loss: 3951.589
iteration 0500: loss: 3950.262
iteration 0600: loss: 3948.078
iteration 0700: loss: 3948.580
iteration 0800: loss: 3952.783
iteration 0900: loss: 3949.410
iteration 1000: loss: 3959.702
iteration 1100: loss: 3947.416
iteration 1200: loss: 3948.485
iteration 1300: loss: 3944.757
iteration 1400: loss: 3945.573
iteration 1500: loss: 3954.202
iteration 1600: loss: 3953.286
iteration 1700: loss: 3958.771
iteration 1800: loss: 3940.368
====> Epoch: 010 Train loss: 3948.9538  took : 138.07710647583008
====> Test loss: 3952.2408
iteration 0000: loss: 3941.722
iteration 0100: loss: 3941.719
iteration 0200: loss: 3950.169
iteration 0300: loss: 3950.038
iteration 0400: loss: 3956.647
iteration 0500: loss: 3949.997
iteration 0600: loss: 3951.012
iteration 0700: loss: 3950.979
iteration 0800: loss: 3954.618
iteration 0900: loss: 3951.655
iteration 1000: loss: 3951.364
iteration 1100: loss: 3959.828
iteration 1200: loss: 3949.204
iteration 1300: loss: 3951.920
iteration 1400: loss: 3944.599
iteration 1500: loss: 3946.711
iteration 1600: loss: 3949.178
iteration 1700: loss: 3947.640
iteration 1800: loss: 3952.029
====> Epoch: 011 Train loss: 3948.6150  took : 138.10391116142273
====> Test loss: 3951.4794
iteration 0000: loss: 3954.593
iteration 0100: loss: 3944.569
iteration 0200: loss: 3951.504
iteration 0300: loss: 3947.820
iteration 0400: loss: 3941.325
iteration 0500: loss: 3944.544
iteration 0600: loss: 3944.832
iteration 0700: loss: 3944.673
iteration 0800: loss: 3943.679
iteration 0900: loss: 3949.827
iteration 1000: loss: 3947.446
iteration 1100: loss: 3943.319
iteration 1200: loss: 3945.875
iteration 1300: loss: 3945.640
iteration 1400: loss: 3942.894
iteration 1500: loss: 3949.877
iteration 1600: loss: 3948.165
iteration 1700: loss: 3943.518
iteration 1800: loss: 3944.956
====> Epoch: 012 Train loss: 3948.2820  took : 137.95033621788025
====> Test loss: 3951.1337
iteration 0000: loss: 3945.578
iteration 0100: loss: 3954.483
iteration 0200: loss: 3948.938
iteration 0300: loss: 3949.896
iteration 0400: loss: 3945.254
iteration 0500: loss: 3949.457
iteration 0600: loss: 3945.759
iteration 0700: loss: 3948.688
iteration 0800: loss: 3942.819
iteration 0900: loss: 3945.499
iteration 1000: loss: 3943.339
iteration 1100: loss: 3950.932
iteration 1200: loss: 3940.038
iteration 1300: loss: 3950.076
iteration 1400: loss: 3945.022
iteration 1500: loss: 3947.752
iteration 1600: loss: 3947.355
iteration 1700: loss: 3942.009
iteration 1800: loss: 3946.231
====> Epoch: 013 Train loss: 3948.0310  took : 137.9682812690735
====> Test loss: 3950.8660
iteration 0000: loss: 3944.584
iteration 0100: loss: 3950.171
iteration 0200: loss: 3941.289
iteration 0300: loss: 3946.331
iteration 0400: loss: 3951.692
iteration 0500: loss: 3948.316
iteration 0600: loss: 3949.563
iteration 0700: loss: 3952.888
iteration 0800: loss: 3950.046
iteration 0900: loss: 3942.632
iteration 1000: loss: 3945.210
iteration 1100: loss: 3950.916
iteration 1200: loss: 3945.548
iteration 1300: loss: 3943.641
iteration 1400: loss: 3948.258
iteration 1500: loss: 3938.565
iteration 1600: loss: 3953.488
iteration 1700: loss: 3951.312
iteration 1800: loss: 3942.481
====> Epoch: 014 Train loss: 3947.7153  took : 138.0267460346222
====> Test loss: 3950.5132
iteration 0000: loss: 3951.530
iteration 0100: loss: 3946.985
iteration 0200: loss: 3949.157
iteration 0300: loss: 3944.550
iteration 0400: loss: 3948.282
iteration 0500: loss: 3942.428
iteration 0600: loss: 3954.033
iteration 0700: loss: 3949.412
iteration 0800: loss: 3944.741
iteration 0900: loss: 3949.167
iteration 1000: loss: 3948.364
iteration 1100: loss: 3943.850
iteration 1200: loss: 3943.892
iteration 1300: loss: 3945.613
iteration 1400: loss: 3952.392
iteration 1500: loss: 3947.876
iteration 1600: loss: 3951.835
iteration 1700: loss: 3953.225
iteration 1800: loss: 3948.044
====> Epoch: 015 Train loss: 3947.5106  took : 137.95932459831238
====> Test loss: 3950.3700
iteration 0000: loss: 3947.583
iteration 0100: loss: 3954.444
iteration 0200: loss: 3951.046
iteration 0300: loss: 3949.866
iteration 0400: loss: 3944.843
iteration 0500: loss: 3952.132
iteration 0600: loss: 3947.628
iteration 0700: loss: 3942.483
iteration 0800: loss: 3949.162
iteration 0900: loss: 3942.612
iteration 1000: loss: 3953.935
iteration 1100: loss: 3952.283
iteration 1200: loss: 3948.783
iteration 1300: loss: 3947.603
iteration 1400: loss: 3947.532
iteration 1500: loss: 3950.076
iteration 1600: loss: 3943.777
iteration 1700: loss: 3946.383
iteration 1800: loss: 3943.744
====> Epoch: 016 Train loss: 3947.2553  took : 137.96469020843506
====> Test loss: 3950.3239
iteration 0000: loss: 3945.872
iteration 0100: loss: 3948.297
iteration 0200: loss: 3948.332
iteration 0300: loss: 3942.398
iteration 0400: loss: 3942.962
iteration 0500: loss: 3948.482
iteration 0600: loss: 3944.594
iteration 0700: loss: 3941.524
iteration 0800: loss: 3943.649
iteration 0900: loss: 3945.784
iteration 1000: loss: 3944.323
iteration 1100: loss: 3943.546
iteration 1200: loss: 3944.118
iteration 1300: loss: 3939.507
iteration 1400: loss: 3951.264
iteration 1500: loss: 3949.142
iteration 1600: loss: 3945.175
iteration 1700: loss: 3940.907
iteration 1800: loss: 3946.368
====> Epoch: 017 Train loss: 3947.0791  took : 137.9879720211029
====> Test loss: 3949.9529
iteration 0000: loss: 3948.333
iteration 0100: loss: 3947.722
iteration 0200: loss: 3949.363
iteration 0300: loss: 3943.776
iteration 0400: loss: 3945.467
iteration 0500: loss: 3943.260
iteration 0600: loss: 3942.573
iteration 0700: loss: 3946.932
iteration 0800: loss: 3944.982
iteration 0900: loss: 3949.271
iteration 1000: loss: 3939.679
iteration 1100: loss: 3947.190
iteration 1200: loss: 3945.188
iteration 1300: loss: 3947.338
iteration 1400: loss: 3951.115
iteration 1500: loss: 3949.602
iteration 1600: loss: 3941.867
iteration 1700: loss: 3938.483
iteration 1800: loss: 3948.890
====> Epoch: 018 Train loss: 3946.9379  took : 137.99463438987732
====> Test loss: 3950.1098
iteration 0000: loss: 3943.003
iteration 0100: loss: 3954.606
iteration 0200: loss: 3944.489
iteration 0300: loss: 3943.839
iteration 0400: loss: 3945.918
iteration 0500: loss: 3939.782
iteration 0600: loss: 3948.046
iteration 0700: loss: 3951.469
iteration 0800: loss: 3955.057
iteration 0900: loss: 3952.081
iteration 1000: loss: 3943.930
iteration 1100: loss: 3946.014
iteration 1200: loss: 3945.896
iteration 1300: loss: 3947.344
iteration 1400: loss: 3950.991
iteration 1500: loss: 3944.504
iteration 1600: loss: 3946.589
iteration 1700: loss: 3949.581
iteration 1800: loss: 3947.162
====> Epoch: 019 Train loss: 3946.7645  took : 137.9478793144226
====> Test loss: 3950.1398
iteration 0000: loss: 3948.095
iteration 0100: loss: 3941.284
iteration 0200: loss: 3946.650
iteration 0300: loss: 3946.916
iteration 0400: loss: 3949.479
iteration 0500: loss: 3948.861
iteration 0600: loss: 3949.764
iteration 0700: loss: 3942.771
iteration 0800: loss: 3950.032
iteration 0900: loss: 3947.843
iteration 1000: loss: 3945.571
iteration 1100: loss: 3951.138
iteration 1200: loss: 3953.224
iteration 1300: loss: 3942.595
iteration 1400: loss: 3940.373
iteration 1500: loss: 3943.375
iteration 1600: loss: 3942.945
iteration 1700: loss: 3947.574
iteration 1800: loss: 3944.765
====> Epoch: 020 Train loss: 3946.6034  took : 137.86930131912231
====> Test loss: 3949.5975
iteration 0000: loss: 3948.737
iteration 0100: loss: 3948.247
iteration 0200: loss: 3953.403
iteration 0300: loss: 3948.010
iteration 0400: loss: 3952.319
iteration 0500: loss: 3947.500
iteration 0600: loss: 3942.778
iteration 0700: loss: 3952.957
iteration 0800: loss: 3945.797
iteration 0900: loss: 3945.684
iteration 1000: loss: 3943.918
iteration 1100: loss: 3951.670
iteration 1200: loss: 3944.302
iteration 1300: loss: 3948.460
iteration 1400: loss: 3952.988
iteration 1500: loss: 3945.259
iteration 1600: loss: 3949.364
iteration 1700: loss: 3944.011
iteration 1800: loss: 3951.723
====> Epoch: 021 Train loss: 3946.4779  took : 137.81817507743835
====> Test loss: 3949.4597
iteration 0000: loss: 3945.900
iteration 0100: loss: 3940.896
iteration 0200: loss: 3944.320
iteration 0300: loss: 3946.194
iteration 0400: loss: 3947.026
iteration 0500: loss: 3953.111
iteration 0600: loss: 3945.987
iteration 0700: loss: 3942.343
iteration 0800: loss: 3947.264
iteration 0900: loss: 3942.113
iteration 1000: loss: 3947.448
iteration 1100: loss: 3945.875
iteration 1200: loss: 3945.792
iteration 1300: loss: 3945.316
iteration 1400: loss: 3948.778
iteration 1500: loss: 3945.737
iteration 1600: loss: 3942.891
iteration 1700: loss: 3951.672
iteration 1800: loss: 3946.745
====> Epoch: 022 Train loss: 3946.1484  took : 137.834379196167
====> Test loss: 3949.3415
iteration 0000: loss: 3942.905
iteration 0100: loss: 3944.572
iteration 0200: loss: 3946.089
iteration 0300: loss: 3947.896
iteration 0400: loss: 3947.508
iteration 0500: loss: 3944.432
iteration 0600: loss: 3947.738
iteration 0700: loss: 3944.622
iteration 0800: loss: 3946.275
iteration 0900: loss: 3942.958
iteration 1000: loss: 3940.438
iteration 1100: loss: 3947.624
iteration 1200: loss: 3946.354
iteration 1300: loss: 3946.665
iteration 1400: loss: 3943.921
iteration 1500: loss: 3940.991
iteration 1600: loss: 3944.843
iteration 1700: loss: 3946.766
iteration 1800: loss: 3941.805
====> Epoch: 023 Train loss: 3945.9929  took : 137.85188364982605
====> Test loss: 3949.9277
iteration 0000: loss: 3946.455
iteration 0100: loss: 3948.783
iteration 0200: loss: 3946.639
iteration 0300: loss: 3937.226
iteration 0400: loss: 3944.233
iteration 0500: loss: 3951.926
iteration 0600: loss: 3947.860
iteration 0700: loss: 3950.438
iteration 0800: loss: 3944.556
iteration 0900: loss: 3946.123
iteration 1000: loss: 3946.152
iteration 1100: loss: 3945.845
iteration 1200: loss: 3941.327
iteration 1300: loss: 3943.909
iteration 1400: loss: 3942.348
iteration 1500: loss: 3941.144
iteration 1600: loss: 3942.569
iteration 1700: loss: 3941.541
iteration 1800: loss: 3949.739
====> Epoch: 024 Train loss: 3945.8547  took : 137.82644844055176
====> Test loss: 3949.0697
iteration 0000: loss: 3946.874
iteration 0100: loss: 3944.703
iteration 0200: loss: 3946.656
iteration 0300: loss: 3947.597
iteration 0400: loss: 3940.866
iteration 0500: loss: 3939.528
iteration 0600: loss: 3945.798
iteration 0700: loss: 3949.327
iteration 0800: loss: 3944.027
iteration 0900: loss: 3945.612
iteration 1000: loss: 3946.209
iteration 1100: loss: 3949.958
iteration 1200: loss: 3954.305
iteration 1300: loss: 3947.813
iteration 1400: loss: 3941.425
iteration 1500: loss: 3938.497
iteration 1600: loss: 3951.021
iteration 1700: loss: 3947.804
iteration 1800: loss: 3940.780
====> Epoch: 025 Train loss: 3945.7528  took : 137.80415081977844
====> Test loss: 3948.6631
iteration 0000: loss: 3945.240
iteration 0100: loss: 3938.879
iteration 0200: loss: 3941.625
iteration 0300: loss: 3941.867
iteration 0400: loss: 3948.611
iteration 0500: loss: 3952.290
iteration 0600: loss: 3941.576
iteration 0700: loss: 3939.547
iteration 0800: loss: 3944.945
iteration 0900: loss: 3945.957
iteration 1000: loss: 3948.918
iteration 1100: loss: 3939.545
iteration 1200: loss: 3950.738
iteration 1300: loss: 3944.758
iteration 1400: loss: 3946.868
iteration 1500: loss: 3943.401
iteration 1600: loss: 3947.023
iteration 1700: loss: 3949.282
iteration 1800: loss: 3947.878
====> Epoch: 026 Train loss: 3945.6105  took : 137.81013584136963
====> Test loss: 3948.8624
iteration 0000: loss: 3949.741
iteration 0100: loss: 3941.958
iteration 0200: loss: 3942.142
iteration 0300: loss: 3946.003
iteration 0400: loss: 3949.710
iteration 0500: loss: 3945.569
iteration 0600: loss: 3945.196
iteration 0700: loss: 3952.160
iteration 0800: loss: 3949.927
iteration 0900: loss: 3941.684
iteration 1000: loss: 3943.656
iteration 1100: loss: 3941.426
iteration 1200: loss: 3947.408
iteration 1300: loss: 3950.998
iteration 1400: loss: 3939.344
iteration 1500: loss: 3945.375
iteration 1600: loss: 3942.252
iteration 1700: loss: 3944.318
iteration 1800: loss: 3949.138
====> Epoch: 027 Train loss: 3945.4405  took : 137.89361357688904
====> Test loss: 3948.8436
iteration 0000: loss: 3948.656
iteration 0100: loss: 3944.388
iteration 0200: loss: 3945.172
iteration 0300: loss: 3949.579
iteration 0400: loss: 3945.052
iteration 0500: loss: 3948.918
iteration 0600: loss: 3943.741
iteration 0700: loss: 3943.258
iteration 0800: loss: 3950.157
iteration 0900: loss: 3947.635
iteration 1000: loss: 3946.243
iteration 1100: loss: 3940.273
iteration 1200: loss: 3946.484
iteration 1300: loss: 3941.953
iteration 1400: loss: 3943.261
iteration 1500: loss: 3946.222
iteration 1600: loss: 3948.083
iteration 1700: loss: 3944.062
iteration 1800: loss: 3940.703
====> Epoch: 028 Train loss: 3945.3257  took : 137.8950572013855
====> Test loss: 3948.3829
iteration 0000: loss: 3945.117
iteration 0100: loss: 3940.991
iteration 0200: loss: 3948.170
iteration 0300: loss: 3951.480
iteration 0400: loss: 3943.004
iteration 0500: loss: 3945.290
iteration 0600: loss: 3948.687
iteration 0700: loss: 3945.968
iteration 0800: loss: 3944.284
iteration 0900: loss: 3950.479
iteration 1000: loss: 3950.128
iteration 1100: loss: 3943.969
iteration 1200: loss: 3946.790
iteration 1300: loss: 3942.968
iteration 1400: loss: 3940.168
iteration 1500: loss: 3943.381
iteration 1600: loss: 3947.867
iteration 1700: loss: 3949.796
iteration 1800: loss: 3950.457
====> Epoch: 029 Train loss: 3945.2129  took : 137.85282588005066
====> Test loss: 3948.5589
iteration 0000: loss: 3944.034
iteration 0100: loss: 3942.000
iteration 0200: loss: 3940.757
iteration 0300: loss: 3952.533
iteration 0400: loss: 3947.083
iteration 0500: loss: 3953.556
iteration 0600: loss: 3945.961
iteration 0700: loss: 3952.320
iteration 0800: loss: 3944.048
iteration 0900: loss: 3945.629
iteration 1000: loss: 3946.730
iteration 1100: loss: 3946.707
iteration 1200: loss: 3950.106
iteration 1300: loss: 3948.957
iteration 1400: loss: 3941.201
iteration 1500: loss: 3942.402
iteration 1600: loss: 3946.278
iteration 1700: loss: 3947.289
iteration 1800: loss: 3953.105
====> Epoch: 030 Train loss: 3945.0498  took : 137.92727088928223
====> Test loss: 3948.6170
====> [MM-VAE] Time: 4633.536s or 01:17:13
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_3
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1994.771
iteration 0100: loss: 1570.413
iteration 0200: loss: 1550.970
iteration 0300: loss: 1551.916
iteration 0400: loss: 1548.214
iteration 0500: loss: 1545.278
iteration 0600: loss: 1545.524
iteration 0700: loss: 1538.689
iteration 0800: loss: 1538.140
iteration 0900: loss: 1538.337
====> Epoch: 001 Train loss: 1551.9740  took : 3.8856658935546875
====> Test loss: 1531.6960
iteration 0000: loss: 1536.637
iteration 0100: loss: 1530.525
iteration 0200: loss: 1535.387
iteration 0300: loss: 1534.885
iteration 0400: loss: 1533.961
iteration 0500: loss: 1534.845
iteration 0600: loss: 1533.419
iteration 0700: loss: 1532.774
iteration 0800: loss: 1529.126
iteration 0900: loss: 1534.966
====> Epoch: 002 Train loss: 1534.2397  took : 3.8844149112701416
====> Test loss: 1527.6137
iteration 0000: loss: 1531.979
iteration 0100: loss: 1532.389
iteration 0200: loss: 1532.771
iteration 0300: loss: 1532.626
iteration 0400: loss: 1531.602
iteration 0500: loss: 1533.244
iteration 0600: loss: 1531.850
iteration 0700: loss: 1532.734
iteration 0800: loss: 1531.214
iteration 0900: loss: 1531.485
====> Epoch: 003 Train loss: 1531.5924  took : 3.853793144226074
====> Test loss: 1525.9342
iteration 0000: loss: 1528.474
iteration 0100: loss: 1531.778
iteration 0200: loss: 1530.680
iteration 0300: loss: 1526.925
iteration 0400: loss: 1529.867
iteration 0500: loss: 1529.866
iteration 0600: loss: 1528.250
iteration 0700: loss: 1529.610
iteration 0800: loss: 1529.957
iteration 0900: loss: 1527.829
====> Epoch: 004 Train loss: 1530.3446  took : 3.8598430156707764
====> Test loss: 1524.9801
iteration 0000: loss: 1531.275
iteration 0100: loss: 1528.609
iteration 0200: loss: 1531.479
iteration 0300: loss: 1533.745
iteration 0400: loss: 1527.099
iteration 0500: loss: 1531.224
iteration 0600: loss: 1527.458
iteration 0700: loss: 1530.981
iteration 0800: loss: 1535.822
iteration 0900: loss: 1527.186
====> Epoch: 005 Train loss: 1529.5763  took : 3.868217945098877
====> Test loss: 1524.4080
iteration 0000: loss: 1530.407
iteration 0100: loss: 1530.137
iteration 0200: loss: 1533.800
iteration 0300: loss: 1529.381
iteration 0400: loss: 1528.605
iteration 0500: loss: 1531.415
iteration 0600: loss: 1528.477
iteration 0700: loss: 1529.422
iteration 0800: loss: 1527.281
iteration 0900: loss: 1530.661
====> Epoch: 006 Train loss: 1528.9768  took : 3.8762004375457764
====> Test loss: 1523.8161
iteration 0000: loss: 1528.509
iteration 0100: loss: 1530.167
iteration 0200: loss: 1530.681
iteration 0300: loss: 1526.106
iteration 0400: loss: 1525.591
iteration 0500: loss: 1524.770
iteration 0600: loss: 1531.354
iteration 0700: loss: 1526.901
iteration 0800: loss: 1527.490
iteration 0900: loss: 1526.958
====> Epoch: 007 Train loss: 1528.4561  took : 3.8601508140563965
====> Test loss: 1523.3821
iteration 0000: loss: 1527.294
iteration 0100: loss: 1530.222
iteration 0200: loss: 1529.118
iteration 0300: loss: 1527.909
iteration 0400: loss: 1527.298
iteration 0500: loss: 1527.953
iteration 0600: loss: 1526.391
iteration 0700: loss: 1529.048
iteration 0800: loss: 1531.221
iteration 0900: loss: 1528.021
====> Epoch: 008 Train loss: 1528.0468  took : 3.8577282428741455
====> Test loss: 1523.0467
iteration 0000: loss: 1524.673
iteration 0100: loss: 1529.397
iteration 0200: loss: 1528.245
iteration 0300: loss: 1528.057
iteration 0400: loss: 1528.033
iteration 0500: loss: 1526.196
iteration 0600: loss: 1528.049
iteration 0700: loss: 1526.158
iteration 0800: loss: 1527.893
iteration 0900: loss: 1526.549
====> Epoch: 009 Train loss: 1527.7621  took : 3.8558661937713623
====> Test loss: 1522.9104
iteration 0000: loss: 1527.222
iteration 0100: loss: 1528.410
iteration 0200: loss: 1529.543
iteration 0300: loss: 1530.557
iteration 0400: loss: 1528.816
iteration 0500: loss: 1527.163
iteration 0600: loss: 1527.587
iteration 0700: loss: 1526.005
iteration 0800: loss: 1527.853
iteration 0900: loss: 1528.203
====> Epoch: 010 Train loss: 1527.4794  took : 3.8791940212249756
====> Test loss: 1522.5883
iteration 0000: loss: 1528.126
iteration 0100: loss: 1523.231
iteration 0200: loss: 1528.294
iteration 0300: loss: 1528.996
iteration 0400: loss: 1525.859
iteration 0500: loss: 1527.394
iteration 0600: loss: 1529.538
iteration 0700: loss: 1528.703
iteration 0800: loss: 1531.782
iteration 0900: loss: 1525.793
====> Epoch: 011 Train loss: 1527.2380  took : 3.8797733783721924
====> Test loss: 1522.4358
iteration 0000: loss: 1528.439
iteration 0100: loss: 1527.324
iteration 0200: loss: 1526.761
iteration 0300: loss: 1527.946
iteration 0400: loss: 1529.864
iteration 0500: loss: 1525.145
iteration 0600: loss: 1529.017
iteration 0700: loss: 1524.036
iteration 0800: loss: 1527.146
iteration 0900: loss: 1527.503
====> Epoch: 012 Train loss: 1527.0824  took : 3.886204719543457
====> Test loss: 1522.2937
iteration 0000: loss: 1528.532
iteration 0100: loss: 1530.204
iteration 0200: loss: 1526.234
iteration 0300: loss: 1527.026
iteration 0400: loss: 1524.954
iteration 0500: loss: 1525.886
iteration 0600: loss: 1521.693
iteration 0700: loss: 1533.180
iteration 0800: loss: 1527.384
iteration 0900: loss: 1528.663
====> Epoch: 013 Train loss: 1526.9170  took : 3.893221139907837
====> Test loss: 1522.0684
iteration 0000: loss: 1526.592
iteration 0100: loss: 1523.692
iteration 0200: loss: 1524.052
iteration 0300: loss: 1523.648
iteration 0400: loss: 1528.339
iteration 0500: loss: 1523.601
iteration 0600: loss: 1528.474
iteration 0700: loss: 1526.135
iteration 0800: loss: 1527.319
iteration 0900: loss: 1523.149
====> Epoch: 014 Train loss: 1526.7565  took : 3.871264696121216
====> Test loss: 1522.0313
iteration 0000: loss: 1526.985
iteration 0100: loss: 1525.745
iteration 0200: loss: 1526.485
iteration 0300: loss: 1526.382
iteration 0400: loss: 1525.958
iteration 0500: loss: 1523.546
iteration 0600: loss: 1525.140
iteration 0700: loss: 1525.137
iteration 0800: loss: 1526.610
iteration 0900: loss: 1524.344
====> Epoch: 015 Train loss: 1526.6060  took : 3.8778955936431885
====> Test loss: 1521.8473
iteration 0000: loss: 1527.210
iteration 0100: loss: 1528.493
iteration 0200: loss: 1522.692
iteration 0300: loss: 1527.298
iteration 0400: loss: 1525.602
iteration 0500: loss: 1525.477
iteration 0600: loss: 1527.754
iteration 0700: loss: 1527.000
iteration 0800: loss: 1527.054
iteration 0900: loss: 1524.791
====> Epoch: 016 Train loss: 1526.4864  took : 3.883162260055542
====> Test loss: 1521.7298
iteration 0000: loss: 1525.217
iteration 0100: loss: 1523.772
iteration 0200: loss: 1524.215
iteration 0300: loss: 1527.767
iteration 0400: loss: 1526.392
iteration 0500: loss: 1527.422
iteration 0600: loss: 1527.717
iteration 0700: loss: 1526.865
iteration 0800: loss: 1526.627
iteration 0900: loss: 1526.517
====> Epoch: 017 Train loss: 1526.3962  took : 3.8775217533111572
====> Test loss: 1521.7427
iteration 0000: loss: 1529.843
iteration 0100: loss: 1522.500
iteration 0200: loss: 1526.644
iteration 0300: loss: 1528.790
iteration 0400: loss: 1528.495
iteration 0500: loss: 1527.511
iteration 0600: loss: 1527.057
iteration 0700: loss: 1522.000
iteration 0800: loss: 1524.515
iteration 0900: loss: 1527.681
====> Epoch: 018 Train loss: 1526.2537  took : 3.8621273040771484
====> Test loss: 1521.6497
iteration 0000: loss: 1524.964
iteration 0100: loss: 1526.530
iteration 0200: loss: 1526.048
iteration 0300: loss: 1525.331
iteration 0400: loss: 1529.957
iteration 0500: loss: 1531.048
iteration 0600: loss: 1527.316
iteration 0700: loss: 1527.124
iteration 0800: loss: 1530.406
iteration 0900: loss: 1528.733
====> Epoch: 019 Train loss: 1526.1902  took : 3.8566346168518066
====> Test loss: 1521.5339
iteration 0000: loss: 1527.298
iteration 0100: loss: 1526.603
iteration 0200: loss: 1527.619
iteration 0300: loss: 1525.452
iteration 0400: loss: 1524.018
iteration 0500: loss: 1529.249
iteration 0600: loss: 1525.423
iteration 0700: loss: 1525.033
iteration 0800: loss: 1528.577
iteration 0900: loss: 1528.435
====> Epoch: 020 Train loss: 1526.0831  took : 3.8582656383514404
====> Test loss: 1521.5869
iteration 0000: loss: 1525.781
iteration 0100: loss: 1526.872
iteration 0200: loss: 1525.818
iteration 0300: loss: 1524.229
iteration 0400: loss: 1523.552
iteration 0500: loss: 1525.004
iteration 0600: loss: 1528.135
iteration 0700: loss: 1527.394
iteration 0800: loss: 1526.680
iteration 0900: loss: 1528.221
====> Epoch: 021 Train loss: 1525.9957  took : 3.859318256378174
====> Test loss: 1521.4078
iteration 0000: loss: 1528.143
iteration 0100: loss: 1522.478
iteration 0200: loss: 1522.232
iteration 0300: loss: 1524.995
iteration 0400: loss: 1526.266
iteration 0500: loss: 1528.615
iteration 0600: loss: 1526.353
iteration 0700: loss: 1525.581
iteration 0800: loss: 1526.033
iteration 0900: loss: 1524.045
====> Epoch: 022 Train loss: 1525.9277  took : 3.8724365234375
====> Test loss: 1521.4348
iteration 0000: loss: 1524.899
iteration 0100: loss: 1524.346
iteration 0200: loss: 1524.829
iteration 0300: loss: 1524.089
iteration 0400: loss: 1523.902
iteration 0500: loss: 1526.687
iteration 0600: loss: 1526.847
iteration 0700: loss: 1524.742
iteration 0800: loss: 1527.370
iteration 0900: loss: 1528.941
====> Epoch: 023 Train loss: 1525.8588  took : 3.8738250732421875
====> Test loss: 1521.2730
iteration 0000: loss: 1526.830
iteration 0100: loss: 1527.093
iteration 0200: loss: 1524.465
iteration 0300: loss: 1526.993
iteration 0400: loss: 1526.332
iteration 0500: loss: 1525.063
iteration 0600: loss: 1523.652
iteration 0700: loss: 1526.450
iteration 0800: loss: 1526.775
iteration 0900: loss: 1523.953
====> Epoch: 024 Train loss: 1525.7700  took : 3.8802757263183594
====> Test loss: 1521.2519
iteration 0000: loss: 1528.298
iteration 0100: loss: 1525.386
iteration 0200: loss: 1527.295
iteration 0300: loss: 1524.860
iteration 0400: loss: 1527.304
iteration 0500: loss: 1523.956
iteration 0600: loss: 1522.229
iteration 0700: loss: 1525.041
iteration 0800: loss: 1527.140
iteration 0900: loss: 1527.709
====> Epoch: 025 Train loss: 1525.7173  took : 3.8723249435424805
====> Test loss: 1521.1327
iteration 0000: loss: 1523.505
iteration 0100: loss: 1525.278
iteration 0200: loss: 1526.135
iteration 0300: loss: 1525.559
iteration 0400: loss: 1526.814
iteration 0500: loss: 1525.171
iteration 0600: loss: 1528.234
iteration 0700: loss: 1523.815
iteration 0800: loss: 1525.124
iteration 0900: loss: 1524.750
====> Epoch: 026 Train loss: 1525.6587  took : 3.862205982208252
====> Test loss: 1521.1261
iteration 0000: loss: 1522.207
iteration 0100: loss: 1526.499
iteration 0200: loss: 1526.179
iteration 0300: loss: 1528.015
iteration 0400: loss: 1525.276
iteration 0500: loss: 1525.520
iteration 0600: loss: 1524.703
iteration 0700: loss: 1526.782
iteration 0800: loss: 1524.034
iteration 0900: loss: 1525.654
====> Epoch: 027 Train loss: 1525.6121  took : 3.870307445526123
====> Test loss: 1521.0866
iteration 0000: loss: 1523.783
iteration 0100: loss: 1525.752
iteration 0200: loss: 1524.044
iteration 0300: loss: 1526.961
iteration 0400: loss: 1526.661
iteration 0500: loss: 1522.406
iteration 0600: loss: 1524.699
iteration 0700: loss: 1525.676
iteration 0800: loss: 1523.545
iteration 0900: loss: 1525.639
====> Epoch: 028 Train loss: 1525.5423  took : 3.859049081802368
====> Test loss: 1521.0393
iteration 0000: loss: 1524.998
iteration 0100: loss: 1527.679
iteration 0200: loss: 1525.796
iteration 0300: loss: 1527.007
iteration 0400: loss: 1527.623
iteration 0500: loss: 1524.556
iteration 0600: loss: 1526.902
iteration 0700: loss: 1525.694
iteration 0800: loss: 1524.303
iteration 0900: loss: 1526.098
====> Epoch: 029 Train loss: 1525.5223  took : 3.851409435272217
====> Test loss: 1520.9725
iteration 0000: loss: 1523.829
iteration 0100: loss: 1526.272
iteration 0200: loss: 1525.107
iteration 0300: loss: 1525.823
iteration 0400: loss: 1524.285
iteration 0500: loss: 1523.105
iteration 0600: loss: 1526.557
iteration 0700: loss: 1525.980
iteration 0800: loss: 1522.151
iteration 0900: loss: 1523.220
====> Epoch: 030 Train loss: 1525.4994  took : 3.870779275894165
====> Test loss: 1521.0319
====> [MM-VAE] Time: 193.551s or 00:03:13
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2565.039
iteration 0100: loss: 2104.684
iteration 0200: loss: 2080.218
iteration 0300: loss: 2039.763
iteration 0400: loss: 2021.707
iteration 0500: loss: 2020.630
iteration 0600: loss: 2000.453
iteration 0700: loss: 1977.212
iteration 0800: loss: 1969.304
iteration 0900: loss: 1964.882
====> Epoch: 001 Train loss: 2031.7324  took : 6.114850997924805
====> Test loss: 1958.9085
iteration 0000: loss: 1965.556
iteration 0100: loss: 1963.156
iteration 0200: loss: 1962.771
iteration 0300: loss: 1962.063
iteration 0400: loss: 1962.077
iteration 0500: loss: 1961.088
iteration 0600: loss: 1961.207
iteration 0700: loss: 1961.260
iteration 0800: loss: 1961.652
iteration 0900: loss: 1960.251
====> Epoch: 002 Train loss: 1961.7898  took : 6.096958637237549
====> Test loss: 1955.0814
iteration 0000: loss: 1961.395
iteration 0100: loss: 1959.724
iteration 0200: loss: 1959.320
iteration 0300: loss: 1959.081
iteration 0400: loss: 1958.517
iteration 0500: loss: 1959.347
iteration 0600: loss: 1959.396
iteration 0700: loss: 1958.476
iteration 0800: loss: 1958.675
iteration 0900: loss: 1959.863
====> Epoch: 003 Train loss: 1959.3743  took : 6.112682104110718
====> Test loss: 1953.6289
iteration 0000: loss: 1959.462
iteration 0100: loss: 1958.330
iteration 0200: loss: 1959.023
iteration 0300: loss: 1959.353
iteration 0400: loss: 1956.696
iteration 0500: loss: 1958.553
iteration 0600: loss: 1958.022
iteration 0700: loss: 1959.018
iteration 0800: loss: 1959.133
iteration 0900: loss: 1958.304
====> Epoch: 004 Train loss: 1958.5699  took : 6.127809762954712
====> Test loss: 1953.2566
iteration 0000: loss: 1958.041
iteration 0100: loss: 1957.390
iteration 0200: loss: 1958.035
iteration 0300: loss: 1957.950
iteration 0400: loss: 1958.028
iteration 0500: loss: 1959.429
iteration 0600: loss: 1958.155
iteration 0700: loss: 1958.545
iteration 0800: loss: 1958.182
iteration 0900: loss: 1957.700
====> Epoch: 005 Train loss: 1958.2471  took : 6.112276792526245
====> Test loss: 1952.7439
iteration 0000: loss: 1958.077
iteration 0100: loss: 1957.327
iteration 0200: loss: 1957.812
iteration 0300: loss: 1958.411
iteration 0400: loss: 1958.958
iteration 0500: loss: 1959.109
iteration 0600: loss: 1958.393
iteration 0700: loss: 1956.944
iteration 0800: loss: 1957.526
iteration 0900: loss: 1957.850
====> Epoch: 006 Train loss: 1958.0414  took : 6.10677695274353
====> Test loss: 1952.2592
iteration 0000: loss: 1957.138
iteration 0100: loss: 1957.632
iteration 0200: loss: 1958.394
iteration 0300: loss: 1957.371
iteration 0400: loss: 1957.602
iteration 0500: loss: 1957.027
iteration 0600: loss: 1958.031
iteration 0700: loss: 1957.431
iteration 0800: loss: 1957.389
iteration 0900: loss: 1957.595
====> Epoch: 007 Train loss: 1957.8228  took : 6.114180088043213
====> Test loss: 1952.5134
iteration 0000: loss: 1958.652
iteration 0100: loss: 1956.531
iteration 0200: loss: 1957.667
iteration 0300: loss: 1957.733
iteration 0400: loss: 1957.701
iteration 0500: loss: 1957.562
iteration 0600: loss: 1957.244
iteration 0700: loss: 1957.017
iteration 0800: loss: 1957.812
iteration 0900: loss: 1958.645
====> Epoch: 008 Train loss: 1957.7025  took : 6.117124795913696
====> Test loss: 1952.5991
iteration 0000: loss: 1958.246
iteration 0100: loss: 1956.954
iteration 0200: loss: 1958.003
iteration 0300: loss: 1957.905
iteration 0400: loss: 1958.282
iteration 0500: loss: 1957.357
iteration 0600: loss: 1956.979
iteration 0700: loss: 1957.540
iteration 0800: loss: 1956.593
iteration 0900: loss: 1957.540
====> Epoch: 009 Train loss: 1957.4887  took : 6.108187675476074
====> Test loss: 1952.4690
iteration 0000: loss: 1958.769
iteration 0100: loss: 1958.003
iteration 0200: loss: 1957.985
iteration 0300: loss: 1958.096
iteration 0400: loss: 1957.551
iteration 0500: loss: 1959.456
iteration 0600: loss: 1955.979
iteration 0700: loss: 1956.677
iteration 0800: loss: 1957.216
iteration 0900: loss: 1957.151
====> Epoch: 010 Train loss: 1957.4906  took : 6.126981973648071
====> Test loss: 1952.4537
iteration 0000: loss: 1958.106
iteration 0100: loss: 1957.264
iteration 0200: loss: 1957.024
iteration 0300: loss: 1958.365
iteration 0400: loss: 1958.762
iteration 0500: loss: 1957.808
iteration 0600: loss: 1957.956
iteration 0700: loss: 1957.653
iteration 0800: loss: 1957.436
iteration 0900: loss: 1957.381
====> Epoch: 011 Train loss: 1957.3138  took : 6.115228652954102
====> Test loss: 1952.0784
iteration 0000: loss: 1956.916
iteration 0100: loss: 1958.466
iteration 0200: loss: 1956.890
iteration 0300: loss: 1956.906
iteration 0400: loss: 1956.816
iteration 0500: loss: 1957.014
iteration 0600: loss: 1958.345
iteration 0700: loss: 1957.032
iteration 0800: loss: 1956.647
iteration 0900: loss: 1956.345
====> Epoch: 012 Train loss: 1957.2168  took : 6.122328281402588
====> Test loss: 1952.1566
iteration 0000: loss: 1957.001
iteration 0100: loss: 1958.439
iteration 0200: loss: 1957.455
iteration 0300: loss: 1957.321
iteration 0400: loss: 1955.712
iteration 0500: loss: 1956.738
iteration 0600: loss: 1959.045
iteration 0700: loss: 1957.092
iteration 0800: loss: 1957.620
iteration 0900: loss: 1956.978
====> Epoch: 013 Train loss: 1957.1759  took : 6.128047227859497
====> Test loss: 1952.0705
iteration 0000: loss: 1956.953
iteration 0100: loss: 1956.473
iteration 0200: loss: 1957.219
iteration 0300: loss: 1956.979
iteration 0400: loss: 1957.956
iteration 0500: loss: 1958.237
iteration 0600: loss: 1956.104
iteration 0700: loss: 1957.208
iteration 0800: loss: 1957.063
iteration 0900: loss: 1957.781
====> Epoch: 014 Train loss: 1957.1222  took : 6.114149808883667
====> Test loss: 1952.2107
iteration 0000: loss: 1957.565
iteration 0100: loss: 1957.920
iteration 0200: loss: 1956.195
iteration 0300: loss: 1956.600
iteration 0400: loss: 1957.908
iteration 0500: loss: 1956.998
iteration 0600: loss: 1957.399
iteration 0700: loss: 1957.599
iteration 0800: loss: 1956.982
iteration 0900: loss: 1956.313
====> Epoch: 015 Train loss: 1957.0685  took : 6.106171131134033
====> Test loss: 1952.0289
iteration 0000: loss: 1958.276
iteration 0100: loss: 1958.082
iteration 0200: loss: 1958.671
iteration 0300: loss: 1957.197
iteration 0400: loss: 1957.570
iteration 0500: loss: 1956.494
iteration 0600: loss: 1958.182
iteration 0700: loss: 1956.113
iteration 0800: loss: 1956.174
iteration 0900: loss: 1956.756
====> Epoch: 016 Train loss: 1956.9546  took : 6.1135313510894775
====> Test loss: 1951.9919
iteration 0000: loss: 1955.490
iteration 0100: loss: 1957.190
iteration 0200: loss: 1957.105
iteration 0300: loss: 1958.370
iteration 0400: loss: 1957.767
iteration 0500: loss: 1956.909
iteration 0600: loss: 1956.829
iteration 0700: loss: 1956.217
iteration 0800: loss: 1955.396
iteration 0900: loss: 1956.371
====> Epoch: 017 Train loss: 1956.9218  took : 6.117392301559448
====> Test loss: 1952.0689
iteration 0000: loss: 1957.085
iteration 0100: loss: 1956.189
iteration 0200: loss: 1957.516
iteration 0300: loss: 1958.051
iteration 0400: loss: 1956.766
iteration 0500: loss: 1957.800
iteration 0600: loss: 1958.236
iteration 0700: loss: 1957.410
iteration 0800: loss: 1960.915
iteration 0900: loss: 1957.170
====> Epoch: 018 Train loss: 1956.9050  took : 6.0996387004852295
====> Test loss: 1951.8660
iteration 0000: loss: 1957.480
iteration 0100: loss: 1955.700
iteration 0200: loss: 1956.723
iteration 0300: loss: 1956.689
iteration 0400: loss: 1956.787
iteration 0500: loss: 1957.173
iteration 0600: loss: 1956.024
iteration 0700: loss: 1956.852
iteration 0800: loss: 1958.556
iteration 0900: loss: 1956.996
====> Epoch: 019 Train loss: 1956.8420  took : 6.117823362350464
====> Test loss: 1951.7339
iteration 0000: loss: 1956.127
iteration 0100: loss: 1956.393
iteration 0200: loss: 1957.248
iteration 0300: loss: 1956.635
iteration 0400: loss: 1957.068
iteration 0500: loss: 1958.820
iteration 0600: loss: 1956.779
iteration 0700: loss: 1956.374
iteration 0800: loss: 1956.160
iteration 0900: loss: 1957.361
====> Epoch: 020 Train loss: 1956.8365  took : 6.113543748855591
====> Test loss: 1951.6027
iteration 0000: loss: 1957.011
iteration 0100: loss: 1957.256
iteration 0200: loss: 1957.060
iteration 0300: loss: 1957.370
iteration 0400: loss: 1956.337
iteration 0500: loss: 1956.815
iteration 0600: loss: 1957.308
iteration 0700: loss: 1956.346
iteration 0800: loss: 1955.946
iteration 0900: loss: 1957.923
====> Epoch: 021 Train loss: 1956.8428  took : 6.117569446563721
====> Test loss: 1951.5245
iteration 0000: loss: 1956.638
iteration 0100: loss: 1955.836
iteration 0200: loss: 1957.536
iteration 0300: loss: 1956.173
iteration 0400: loss: 1955.359
iteration 0500: loss: 1957.914
iteration 0600: loss: 1955.800
iteration 0700: loss: 1956.005
iteration 0800: loss: 1958.586
iteration 0900: loss: 1957.242
====> Epoch: 022 Train loss: 1956.8405  took : 6.1536524295806885
====> Test loss: 1951.6328
iteration 0000: loss: 1956.062
iteration 0100: loss: 1956.525
iteration 0200: loss: 1956.891
iteration 0300: loss: 1957.066
iteration 0400: loss: 1956.705
iteration 0500: loss: 1958.483
iteration 0600: loss: 1955.411
iteration 0700: loss: 1955.927
iteration 0800: loss: 1956.044
iteration 0900: loss: 1957.302
====> Epoch: 023 Train loss: 1956.7251  took : 6.121071100234985
====> Test loss: 1951.6655
iteration 0000: loss: 1956.107
iteration 0100: loss: 1956.588
iteration 0200: loss: 1956.008
iteration 0300: loss: 1956.775
iteration 0400: loss: 1956.764
iteration 0500: loss: 1956.596
iteration 0600: loss: 1956.183
iteration 0700: loss: 1957.141
iteration 0800: loss: 1957.564
iteration 0900: loss: 1956.313
====> Epoch: 024 Train loss: 1956.7382  took : 6.1007843017578125
====> Test loss: 1951.6705
iteration 0000: loss: 1955.987
iteration 0100: loss: 1955.830
iteration 0200: loss: 1956.109
iteration 0300: loss: 1956.955
iteration 0400: loss: 1956.919
iteration 0500: loss: 1956.293
iteration 0600: loss: 1957.475
iteration 0700: loss: 1957.557
iteration 0800: loss: 1955.271
iteration 0900: loss: 1956.474
====> Epoch: 025 Train loss: 1956.7622  took : 6.120526313781738
====> Test loss: 1951.9221
iteration 0000: loss: 1955.951
iteration 0100: loss: 1957.170
iteration 0200: loss: 1956.443
iteration 0300: loss: 1956.948
iteration 0400: loss: 1958.012
iteration 0500: loss: 1956.267
iteration 0600: loss: 1956.314
iteration 0700: loss: 1956.128
iteration 0800: loss: 1956.985
iteration 0900: loss: 1955.758
====> Epoch: 026 Train loss: 1956.6801  took : 6.12069034576416
====> Test loss: 1951.8058
iteration 0000: loss: 1956.155
iteration 0100: loss: 1956.173
iteration 0200: loss: 1956.236
iteration 0300: loss: 1955.913
iteration 0400: loss: 1956.678
iteration 0500: loss: 1956.035
iteration 0600: loss: 1956.523
iteration 0700: loss: 1956.054
iteration 0800: loss: 1956.433
iteration 0900: loss: 1955.774
====> Epoch: 027 Train loss: 1956.6316  took : 6.10401725769043
====> Test loss: 1951.4516
iteration 0000: loss: 1956.620
iteration 0100: loss: 1958.778
iteration 0200: loss: 1956.781
iteration 0300: loss: 1956.955
iteration 0400: loss: 1958.152
iteration 0500: loss: 1956.887
iteration 0600: loss: 1956.512
iteration 0700: loss: 1955.513
iteration 0800: loss: 1956.873
iteration 0900: loss: 1955.538
====> Epoch: 028 Train loss: 1956.6095  took : 6.102260112762451
====> Test loss: 1951.6413
iteration 0000: loss: 1957.111
iteration 0100: loss: 1956.756
iteration 0200: loss: 1956.116
iteration 0300: loss: 1955.745
iteration 0400: loss: 1955.790
iteration 0500: loss: 1955.730
iteration 0600: loss: 1955.993
iteration 0700: loss: 1956.188
iteration 0800: loss: 1956.945
iteration 0900: loss: 1958.663
====> Epoch: 029 Train loss: 1956.5509  took : 6.120363473892212
====> Test loss: 1951.7982
iteration 0000: loss: 1956.186
iteration 0100: loss: 1956.291
iteration 0200: loss: 1956.508
iteration 0300: loss: 1956.492
iteration 0400: loss: 1955.873
iteration 0500: loss: 1956.588
iteration 0600: loss: 1955.673
iteration 0700: loss: 1957.567
iteration 0800: loss: 1955.836
iteration 0900: loss: 1956.502
====> Epoch: 030 Train loss: 1956.5879  took : 6.1176605224609375
====> Test loss: 1951.5458
====> [MM-VAE] Time: 249.690s or 00:04:09
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5264.734
iteration 0100: loss: 4131.432
iteration 0200: loss: 4076.684
iteration 0300: loss: 4041.317
iteration 0400: loss: 4008.671
iteration 0500: loss: 4013.985
iteration 0600: loss: 4003.243
iteration 0700: loss: 3988.575
iteration 0800: loss: 3983.944
iteration 0900: loss: 3980.987
iteration 1000: loss: 3986.754
iteration 1100: loss: 3982.867
iteration 1200: loss: 3983.234
iteration 1300: loss: 3995.002
iteration 1400: loss: 3982.481
iteration 1500: loss: 3990.191
iteration 1600: loss: 3970.735
iteration 1700: loss: 3969.842
iteration 1800: loss: 3975.970
====> Epoch: 001 Train loss: 4013.2297  took : 137.93062281608582
====> Test loss: 3981.8350
iteration 0000: loss: 3974.680
iteration 0100: loss: 3982.596
iteration 0200: loss: 3970.674
iteration 0300: loss: 3974.191
iteration 0400: loss: 3964.565
iteration 0500: loss: 3972.436
iteration 0600: loss: 3978.099
iteration 0700: loss: 3987.133
iteration 0800: loss: 3972.381
iteration 0900: loss: 3975.317
iteration 1000: loss: 3976.193
iteration 1100: loss: 3969.071
iteration 1200: loss: 3971.581
iteration 1300: loss: 3974.318
iteration 1400: loss: 3979.881
iteration 1500: loss: 3969.463
iteration 1600: loss: 3968.293
iteration 1700: loss: 3973.639
iteration 1800: loss: 3968.810
====> Epoch: 002 Train loss: 3974.5044  took : 137.59577822685242
====> Test loss: 3975.6796
iteration 0000: loss: 3967.131
iteration 0100: loss: 3968.604
iteration 0200: loss: 3971.747
iteration 0300: loss: 3972.802
iteration 0400: loss: 3965.711
iteration 0500: loss: 3965.493
iteration 0600: loss: 3977.239
iteration 0700: loss: 3969.943
iteration 0800: loss: 3970.610
iteration 0900: loss: 3968.820
iteration 1000: loss: 3965.949
iteration 1100: loss: 3966.013
iteration 1200: loss: 3963.782
iteration 1300: loss: 3965.544
iteration 1400: loss: 3971.919
iteration 1500: loss: 3973.635
iteration 1600: loss: 3964.650
iteration 1700: loss: 3966.085
iteration 1800: loss: 3957.159
====> Epoch: 003 Train loss: 3970.3825  took : 137.60895657539368
====> Test loss: 3973.6424
iteration 0000: loss: 3976.219
iteration 0100: loss: 3968.783
iteration 0200: loss: 3967.563
iteration 0300: loss: 3966.116
iteration 0400: loss: 3965.587
iteration 0500: loss: 3961.874
iteration 0600: loss: 3976.048
iteration 0700: loss: 3966.201
iteration 0800: loss: 3976.302
iteration 0900: loss: 3975.663
iteration 1000: loss: 3972.179
iteration 1100: loss: 3970.019
iteration 1200: loss: 3975.530
iteration 1300: loss: 3967.346
iteration 1400: loss: 3964.387
iteration 1500: loss: 3968.212
iteration 1600: loss: 3972.758
iteration 1700: loss: 3972.032
iteration 1800: loss: 3974.062
====> Epoch: 004 Train loss: 3968.4457  took : 137.5593273639679
====> Test loss: 3972.2074
iteration 0000: loss: 3965.446
iteration 0100: loss: 3969.024
iteration 0200: loss: 3963.938
iteration 0300: loss: 3975.243
iteration 0400: loss: 3970.359
iteration 0500: loss: 3967.107
iteration 0600: loss: 3959.215
iteration 0700: loss: 3968.538
iteration 0800: loss: 3968.119
iteration 0900: loss: 3965.739
iteration 1000: loss: 3982.979
iteration 1100: loss: 3970.841
iteration 1200: loss: 3967.528
iteration 1300: loss: 3971.312
iteration 1400: loss: 3970.457
iteration 1500: loss: 3964.496
iteration 1600: loss: 3980.311
iteration 1700: loss: 3976.759
iteration 1800: loss: 3960.085
====> Epoch: 005 Train loss: 3967.3036  took : 137.4970667362213
====> Test loss: 3971.7959
iteration 0000: loss: 3969.837
iteration 0100: loss: 3966.479
iteration 0200: loss: 3969.394
iteration 0300: loss: 3968.843
iteration 0400: loss: 3971.068
iteration 0500: loss: 3969.552
iteration 0600: loss: 3957.407
iteration 0700: loss: 3969.401
iteration 0800: loss: 3977.902
iteration 0900: loss: 3964.178
iteration 1000: loss: 3964.158
iteration 1100: loss: 3962.976
iteration 1200: loss: 3968.962
iteration 1300: loss: 3969.490
iteration 1400: loss: 3967.215
iteration 1500: loss: 3965.846
iteration 1600: loss: 3974.588
iteration 1700: loss: 3974.812
iteration 1800: loss: 3974.561
====> Epoch: 006 Train loss: 3966.2560  took : 137.38729286193848
====> Test loss: 3970.7662
iteration 0000: loss: 3961.129
iteration 0100: loss: 3970.705
iteration 0200: loss: 3973.002
iteration 0300: loss: 3974.102
iteration 0400: loss: 3967.746
iteration 0500: loss: 3966.878
iteration 0600: loss: 3966.455
iteration 0700: loss: 3961.929
iteration 0800: loss: 3965.344
iteration 0900: loss: 3969.440
iteration 1000: loss: 3960.729
iteration 1100: loss: 3960.651
iteration 1200: loss: 3970.591
iteration 1300: loss: 3965.667
iteration 1400: loss: 3966.853
iteration 1500: loss: 3961.158
iteration 1600: loss: 3968.848
iteration 1700: loss: 3963.043
iteration 1800: loss: 3965.020
====> Epoch: 007 Train loss: 3965.5541  took : 137.34799313545227
====> Test loss: 3970.3751
iteration 0000: loss: 3966.495
iteration 0100: loss: 3960.205
iteration 0200: loss: 3964.080
iteration 0300: loss: 3971.643
iteration 0400: loss: 3964.613
iteration 0500: loss: 3959.026
iteration 0600: loss: 3971.271
iteration 0700: loss: 3966.928
iteration 0800: loss: 3970.375
iteration 0900: loss: 3960.053
iteration 1000: loss: 3968.630
iteration 1100: loss: 3967.563
iteration 1200: loss: 3967.158
iteration 1300: loss: 3960.681
iteration 1400: loss: 3969.610
iteration 1500: loss: 3965.863
iteration 1600: loss: 3963.938
iteration 1700: loss: 3962.211
iteration 1800: loss: 3963.651
====> Epoch: 008 Train loss: 3964.9517  took : 137.32762384414673
====> Test loss: 3970.3778
iteration 0000: loss: 3968.098
iteration 0100: loss: 3958.602
iteration 0200: loss: 3966.127
iteration 0300: loss: 3966.830
iteration 0400: loss: 3968.459
iteration 0500: loss: 3963.206
iteration 0600: loss: 3970.332
iteration 0700: loss: 3960.385
iteration 0800: loss: 3963.583
iteration 0900: loss: 3961.442
iteration 1000: loss: 3965.821
iteration 1100: loss: 3962.327
iteration 1200: loss: 3963.719
iteration 1300: loss: 3957.954
iteration 1400: loss: 3964.320
iteration 1500: loss: 3961.421
iteration 1600: loss: 3961.101
iteration 1700: loss: 3969.536
iteration 1800: loss: 3961.883
====> Epoch: 009 Train loss: 3964.4687  took : 137.3029990196228
====> Test loss: 3969.7253
iteration 0000: loss: 3971.210
iteration 0100: loss: 3968.746
iteration 0200: loss: 3963.180
iteration 0300: loss: 3964.402
iteration 0400: loss: 3962.704
iteration 0500: loss: 3968.918
iteration 0600: loss: 3965.936
iteration 0700: loss: 3968.835
iteration 0800: loss: 3965.293
iteration 0900: loss: 3965.693
iteration 1000: loss: 3962.733
iteration 1100: loss: 3966.795
iteration 1200: loss: 3972.835
iteration 1300: loss: 3967.129
iteration 1400: loss: 3960.280
iteration 1500: loss: 3966.031
iteration 1600: loss: 3963.324
iteration 1700: loss: 3965.414
iteration 1800: loss: 3962.879
====> Epoch: 010 Train loss: 3964.1743  took : 137.20905447006226
====> Test loss: 3968.9574
iteration 0000: loss: 3960.059
iteration 0100: loss: 3966.571
iteration 0200: loss: 3957.489
iteration 0300: loss: 3968.644
iteration 0400: loss: 3964.003
iteration 0500: loss: 3971.504
iteration 0600: loss: 3960.825
iteration 0700: loss: 3972.961
iteration 0800: loss: 3957.828
iteration 0900: loss: 3966.791
iteration 1000: loss: 3963.153
iteration 1100: loss: 3960.927
iteration 1200: loss: 3957.918
iteration 1300: loss: 3964.844
iteration 1400: loss: 3959.846
iteration 1500: loss: 3968.787
iteration 1600: loss: 3959.754
iteration 1700: loss: 3972.630
iteration 1800: loss: 3971.589
====> Epoch: 011 Train loss: 3963.6430  took : 137.29470682144165
====> Test loss: 3969.3071
iteration 0000: loss: 3957.622
iteration 0100: loss: 3958.682
iteration 0200: loss: 3959.080
iteration 0300: loss: 3969.858
iteration 0400: loss: 3957.071
iteration 0500: loss: 3959.446
iteration 0600: loss: 3963.605
iteration 0700: loss: 3963.926
iteration 0800: loss: 3964.386
iteration 0900: loss: 3957.732
iteration 1000: loss: 3960.678
iteration 1100: loss: 3948.304
iteration 1200: loss: 3959.912
iteration 1300: loss: 3962.626
iteration 1400: loss: 3965.448
iteration 1500: loss: 3960.101
iteration 1600: loss: 3964.060
iteration 1700: loss: 3969.227
iteration 1800: loss: 3963.222
====> Epoch: 012 Train loss: 3963.3247  took : 137.33173894882202
====> Test loss: 3968.6290
iteration 0000: loss: 3958.113
iteration 0100: loss: 3959.881
iteration 0200: loss: 3963.135
iteration 0300: loss: 3963.392
iteration 0400: loss: 3950.358
iteration 0500: loss: 3955.362
iteration 0600: loss: 3954.207
iteration 0700: loss: 3954.739
iteration 0800: loss: 3962.644
iteration 0900: loss: 3956.449
iteration 1000: loss: 3957.470
iteration 1100: loss: 3972.041
iteration 1200: loss: 3969.805
iteration 1300: loss: 3960.618
iteration 1400: loss: 3964.570
iteration 1500: loss: 3959.874
iteration 1600: loss: 3959.242
iteration 1700: loss: 3955.913
iteration 1800: loss: 3963.647
====> Epoch: 013 Train loss: 3962.8288  took : 137.24873733520508
====> Test loss: 3968.5960
iteration 0000: loss: 3972.748
iteration 0100: loss: 3963.845
iteration 0200: loss: 3970.276
iteration 0300: loss: 3958.764
iteration 0400: loss: 3964.154
iteration 0500: loss: 3956.777
iteration 0600: loss: 3962.978
iteration 0700: loss: 3962.629
iteration 0800: loss: 3959.945
iteration 0900: loss: 3956.516
iteration 1000: loss: 3956.182
iteration 1100: loss: 3956.682
iteration 1200: loss: 3957.378
iteration 1300: loss: 3959.277
iteration 1400: loss: 3959.749
iteration 1500: loss: 3964.613
iteration 1600: loss: 3963.479
iteration 1700: loss: 3953.653
iteration 1800: loss: 3966.822
====> Epoch: 014 Train loss: 3962.7061  took : 137.26040816307068
====> Test loss: 3967.6882
iteration 0000: loss: 3955.345
iteration 0100: loss: 3960.156
iteration 0200: loss: 3963.639
iteration 0300: loss: 3959.846
iteration 0400: loss: 3960.812
iteration 0500: loss: 3957.431
iteration 0600: loss: 3964.589
iteration 0700: loss: 3974.174
iteration 0800: loss: 3967.375
iteration 0900: loss: 3954.670
iteration 1000: loss: 3958.189
iteration 1100: loss: 3958.957
iteration 1200: loss: 3964.506
iteration 1300: loss: 3966.285
iteration 1400: loss: 3964.745
iteration 1500: loss: 3964.554
iteration 1600: loss: 3973.339
iteration 1700: loss: 3953.527
iteration 1800: loss: 3956.962
====> Epoch: 015 Train loss: 3962.3661  took : 137.23715925216675
====> Test loss: 3967.9840
iteration 0000: loss: 3961.798
iteration 0100: loss: 3963.022
iteration 0200: loss: 3957.047
iteration 0300: loss: 3967.228
iteration 0400: loss: 3960.466
iteration 0500: loss: 3966.324
iteration 0600: loss: 3968.744
iteration 0700: loss: 3959.012
iteration 0800: loss: 3955.878
iteration 0900: loss: 3960.091
iteration 1000: loss: 3957.029
iteration 1100: loss: 3965.519
iteration 1200: loss: 3953.672
iteration 1300: loss: 3962.907
iteration 1400: loss: 3967.177
iteration 1500: loss: 3960.516
iteration 1600: loss: 3972.900
iteration 1700: loss: 3962.891
iteration 1800: loss: 3969.590
====> Epoch: 016 Train loss: 3962.1786  took : 137.23243951797485
====> Test loss: 3967.6563
iteration 0000: loss: 3971.225
iteration 0100: loss: 3959.245
iteration 0200: loss: 3961.714
iteration 0300: loss: 3964.886
iteration 0400: loss: 3952.454
iteration 0500: loss: 3950.816
iteration 0600: loss: 3960.750
iteration 0700: loss: 3966.188
iteration 0800: loss: 3963.308
iteration 0900: loss: 3969.622
iteration 1000: loss: 3959.768
iteration 1100: loss: 3959.661
iteration 1200: loss: 3951.581
iteration 1300: loss: 3956.361
iteration 1400: loss: 3966.853
iteration 1500: loss: 3959.834
iteration 1600: loss: 3962.863
iteration 1700: loss: 3970.529
iteration 1800: loss: 3960.778
====> Epoch: 017 Train loss: 3962.1205  took : 137.21685075759888
====> Test loss: 3967.8434
iteration 0000: loss: 3964.002
iteration 0100: loss: 3965.650
iteration 0200: loss: 3965.058
iteration 0300: loss: 3959.672
iteration 0400: loss: 3960.135
iteration 0500: loss: 3962.068
iteration 0600: loss: 3964.986
iteration 0700: loss: 3965.727
iteration 0800: loss: 3962.557
iteration 0900: loss: 3959.663
iteration 1000: loss: 3969.209
iteration 1100: loss: 3966.657
iteration 1200: loss: 3958.596
iteration 1300: loss: 3960.157
iteration 1400: loss: 3955.842
iteration 1500: loss: 3960.889
iteration 1600: loss: 3959.899
iteration 1700: loss: 3962.021
iteration 1800: loss: 3962.352
====> Epoch: 018 Train loss: 3961.6833  took : 137.21525931358337
====> Test loss: 3967.0244
iteration 0000: loss: 3965.358
iteration 0100: loss: 3962.030
iteration 0200: loss: 3964.197
iteration 0300: loss: 3969.189
iteration 0400: loss: 3950.288
iteration 0500: loss: 3962.596
iteration 0600: loss: 3964.501
iteration 0700: loss: 3962.264
iteration 0800: loss: 3961.050
iteration 0900: loss: 3966.574
iteration 1000: loss: 3972.410
iteration 1100: loss: 3959.218
iteration 1200: loss: 3959.259
iteration 1300: loss: 3963.252
iteration 1400: loss: 3957.443
iteration 1500: loss: 3955.001
iteration 1600: loss: 3966.371
iteration 1700: loss: 3957.331
iteration 1800: loss: 3955.138
====> Epoch: 019 Train loss: 3961.4360  took : 137.24098324775696
====> Test loss: 3967.3443
iteration 0000: loss: 3960.655
iteration 0100: loss: 3958.549
iteration 0200: loss: 3968.236
iteration 0300: loss: 3966.035
iteration 0400: loss: 3959.246
iteration 0500: loss: 3961.148
iteration 0600: loss: 3958.782
iteration 0700: loss: 3962.360
iteration 0800: loss: 3960.345
iteration 0900: loss: 3963.774
iteration 1000: loss: 3964.242
iteration 1100: loss: 3965.171
iteration 1200: loss: 3967.178
iteration 1300: loss: 3969.748
iteration 1400: loss: 3965.664
iteration 1500: loss: 3964.640
iteration 1600: loss: 3959.704
iteration 1700: loss: 3960.748
iteration 1800: loss: 3964.655
====> Epoch: 020 Train loss: 3961.4610  took : 137.23326897621155
====> Test loss: 3966.4808
iteration 0000: loss: 3955.612
iteration 0100: loss: 3960.407
iteration 0200: loss: 3959.162
iteration 0300: loss: 3966.585
iteration 0400: loss: 3962.168
iteration 0500: loss: 3952.202
iteration 0600: loss: 3953.237
iteration 0700: loss: 3964.981
iteration 0800: loss: 3961.240
iteration 0900: loss: 3954.112
iteration 1000: loss: 3961.905
iteration 1100: loss: 3977.482
iteration 1200: loss: 3953.760
iteration 1300: loss: 3965.436
iteration 1400: loss: 3970.009
iteration 1500: loss: 3958.590
iteration 1600: loss: 3967.062
iteration 1700: loss: 3958.951
iteration 1800: loss: 3962.420
====> Epoch: 021 Train loss: 3961.1554  took : 137.28483080863953
====> Test loss: 3966.8493
iteration 0000: loss: 3955.427
iteration 0100: loss: 3957.483
iteration 0200: loss: 3969.293
iteration 0300: loss: 3957.234
iteration 0400: loss: 3960.799
iteration 0500: loss: 3967.813
iteration 0600: loss: 3964.674
iteration 0700: loss: 3967.521
iteration 0800: loss: 3961.882
iteration 0900: loss: 3960.614
iteration 1000: loss: 3961.060
iteration 1100: loss: 3965.732
iteration 1200: loss: 3961.134
iteration 1300: loss: 3952.852
iteration 1400: loss: 3960.423
iteration 1500: loss: 3959.865
iteration 1600: loss: 3973.486
iteration 1700: loss: 3964.961
iteration 1800: loss: 3960.093
====> Epoch: 022 Train loss: 3961.2613  took : 137.20228362083435
====> Test loss: 3966.7122
iteration 0000: loss: 3964.465
iteration 0100: loss: 3957.633
iteration 0200: loss: 3963.523
iteration 0300: loss: 3961.467
iteration 0400: loss: 3959.293
iteration 0500: loss: 3962.871
iteration 0600: loss: 3959.026
iteration 0700: loss: 3956.654
iteration 0800: loss: 3957.344
iteration 0900: loss: 3954.490
iteration 1000: loss: 3962.554
iteration 1100: loss: 3954.146
iteration 1200: loss: 3956.125
iteration 1300: loss: 3964.757
iteration 1400: loss: 3956.838
iteration 1500: loss: 3962.955
iteration 1600: loss: 3964.270
iteration 1700: loss: 3966.249
iteration 1800: loss: 3956.104
====> Epoch: 023 Train loss: 3960.9568  took : 137.2507677078247
====> Test loss: 3966.4671
iteration 0000: loss: 3959.423
iteration 0100: loss: 3960.249
iteration 0200: loss: 3956.759
iteration 0300: loss: 3953.465
iteration 0400: loss: 3957.820
iteration 0500: loss: 3962.920
iteration 0600: loss: 3968.280
iteration 0700: loss: 3963.969
iteration 0800: loss: 3968.495
iteration 0900: loss: 3962.916
iteration 1000: loss: 3966.934
iteration 1100: loss: 3961.605
iteration 1200: loss: 3960.186
iteration 1300: loss: 3959.287
iteration 1400: loss: 3957.981
iteration 1500: loss: 3954.905
iteration 1600: loss: 3955.846
iteration 1700: loss: 3955.659
iteration 1800: loss: 3957.147
====> Epoch: 024 Train loss: 3960.9459  took : 137.15784859657288
====> Test loss: 3967.3411
iteration 0000: loss: 3967.278
iteration 0100: loss: 3961.636
iteration 0200: loss: 3963.292
iteration 0300: loss: 3962.347
iteration 0400: loss: 3959.980
iteration 0500: loss: 3962.541
iteration 0600: loss: 3956.464
iteration 0700: loss: 3957.643
iteration 0800: loss: 3958.545
iteration 0900: loss: 3962.896
iteration 1000: loss: 3960.913
iteration 1100: loss: 3965.078
iteration 1200: loss: 3959.619
iteration 1300: loss: 3965.738
iteration 1400: loss: 3961.574
iteration 1500: loss: 3962.611
iteration 1600: loss: 3959.684
iteration 1700: loss: 3961.374
iteration 1800: loss: 3953.483
====> Epoch: 025 Train loss: 3961.0067  took : 137.18342757225037
====> Test loss: 3966.7248
iteration 0000: loss: 3962.930
iteration 0100: loss: 3957.102
iteration 0200: loss: 3965.397
iteration 0300: loss: 3964.199
iteration 0400: loss: 3958.753
iteration 0500: loss: 3962.138
iteration 0600: loss: 3959.966
iteration 0700: loss: 3957.671
iteration 0800: loss: 3961.484
iteration 0900: loss: 3964.720
iteration 1000: loss: 3959.681
iteration 1100: loss: 3961.191
iteration 1200: loss: 3960.997
iteration 1300: loss: 3956.607
iteration 1400: loss: 3959.288
iteration 1500: loss: 3961.969
iteration 1600: loss: 3958.326
iteration 1700: loss: 3963.039
iteration 1800: loss: 3963.378
====> Epoch: 026 Train loss: 3960.7909  took : 137.12044763565063
====> Test loss: 3966.2348
iteration 0000: loss: 3962.091
iteration 0100: loss: 3967.615
iteration 0200: loss: 3955.469
iteration 0300: loss: 3954.277
iteration 0400: loss: 3957.202
iteration 0500: loss: 3953.897
iteration 0600: loss: 3956.096
iteration 0700: loss: 3960.081
iteration 0800: loss: 3955.764
iteration 0900: loss: 3962.956
iteration 1000: loss: 3954.713
iteration 1100: loss: 3968.286
iteration 1200: loss: 3948.618
iteration 1300: loss: 3955.510
iteration 1400: loss: 3956.944
iteration 1500: loss: 3962.734
iteration 1600: loss: 3954.068
iteration 1700: loss: 3966.022
iteration 1800: loss: 3951.832
====> Epoch: 027 Train loss: 3960.4823  took : 137.18263363838196
====> Test loss: 3966.4980
iteration 0000: loss: 3963.186
iteration 0100: loss: 3965.543
iteration 0200: loss: 3966.440
iteration 0300: loss: 3955.633
iteration 0400: loss: 3962.421
iteration 0500: loss: 3966.373
iteration 0600: loss: 3956.009
iteration 0700: loss: 3961.735
iteration 0800: loss: 3957.163
iteration 0900: loss: 3948.254
iteration 1000: loss: 3960.953
iteration 1100: loss: 3958.203
iteration 1200: loss: 3961.257
iteration 1300: loss: 3959.401
iteration 1400: loss: 3967.728
iteration 1500: loss: 3961.448
iteration 1600: loss: 3960.955
iteration 1700: loss: 3955.495
iteration 1800: loss: 3960.549
====> Epoch: 028 Train loss: 3960.3829  took : 137.15984225273132
====> Test loss: 3966.0289
iteration 0000: loss: 3962.338
iteration 0100: loss: 3952.198
iteration 0200: loss: 3965.953
iteration 0300: loss: 3969.576
iteration 0400: loss: 3955.817
iteration 0500: loss: 3963.101
iteration 0600: loss: 3953.682
iteration 0700: loss: 3960.104
iteration 0800: loss: 3961.844
iteration 0900: loss: 3958.655
iteration 1000: loss: 3961.987
iteration 1100: loss: 3963.791
iteration 1200: loss: 3960.647
iteration 1300: loss: 3960.877
iteration 1400: loss: 3950.438
iteration 1500: loss: 3953.590
iteration 1600: loss: 3965.906
iteration 1700: loss: 3969.958
iteration 1800: loss: 3962.257
====> Epoch: 029 Train loss: 3960.3404  took : 137.13483595848083
====> Test loss: 3965.6333
iteration 0000: loss: 3972.870
iteration 0100: loss: 3953.420
iteration 0200: loss: 3960.668
iteration 0300: loss: 3962.617
iteration 0400: loss: 3954.709
iteration 0500: loss: 3956.350
iteration 0600: loss: 3964.273
iteration 0700: loss: 3959.847
iteration 0800: loss: 3971.761
iteration 0900: loss: 3956.810
iteration 1000: loss: 3960.520
iteration 1100: loss: 3958.156
iteration 1200: loss: 3962.462
iteration 1300: loss: 3953.134
iteration 1400: loss: 3956.112
iteration 1500: loss: 3956.433
iteration 1600: loss: 3958.587
iteration 1700: loss: 3959.726
iteration 1800: loss: 3958.251
====> Epoch: 030 Train loss: 3959.9451  took : 137.11829781532288
====> Test loss: 3965.8518
====> [MM-VAE] Time: 4602.727s or 01:16:42
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_4
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 1993.524
iteration 0100: loss: 1561.219
iteration 0200: loss: 1555.871
iteration 0300: loss: 1545.247
iteration 0400: loss: 1552.786
iteration 0500: loss: 1542.813
iteration 0600: loss: 1542.755
iteration 0700: loss: 1537.459
iteration 0800: loss: 1537.816
iteration 0900: loss: 1533.737
====> Epoch: 001 Train loss: 1551.0419  took : 3.8630731105804443
====> Test loss: 1531.6262
iteration 0000: loss: 1538.727
iteration 0100: loss: 1535.260
iteration 0200: loss: 1534.888
iteration 0300: loss: 1536.174
iteration 0400: loss: 1535.584
iteration 0500: loss: 1539.712
iteration 0600: loss: 1535.173
iteration 0700: loss: 1534.192
iteration 0800: loss: 1531.385
iteration 0900: loss: 1531.758
====> Epoch: 002 Train loss: 1533.8789  took : 3.8739020824432373
====> Test loss: 1527.3146
iteration 0000: loss: 1534.255
iteration 0100: loss: 1532.072
iteration 0200: loss: 1529.984
iteration 0300: loss: 1530.233
iteration 0400: loss: 1530.343
iteration 0500: loss: 1534.356
iteration 0600: loss: 1530.954
iteration 0700: loss: 1533.132
iteration 0800: loss: 1530.312
iteration 0900: loss: 1531.686
====> Epoch: 003 Train loss: 1531.0845  took : 3.8754677772521973
====> Test loss: 1525.5936
iteration 0000: loss: 1532.433
iteration 0100: loss: 1529.847
iteration 0200: loss: 1529.054
iteration 0300: loss: 1532.446
iteration 0400: loss: 1530.473
iteration 0500: loss: 1528.316
iteration 0600: loss: 1527.724
iteration 0700: loss: 1531.099
iteration 0800: loss: 1529.666
iteration 0900: loss: 1533.167
====> Epoch: 004 Train loss: 1529.7895  took : 3.875298500061035
====> Test loss: 1524.6685
iteration 0000: loss: 1531.394
iteration 0100: loss: 1527.473
iteration 0200: loss: 1526.405
iteration 0300: loss: 1527.901
iteration 0400: loss: 1527.484
iteration 0500: loss: 1528.402
iteration 0600: loss: 1528.382
iteration 0700: loss: 1529.626
iteration 0800: loss: 1531.657
iteration 0900: loss: 1524.184
====> Epoch: 005 Train loss: 1529.0319  took : 3.8608574867248535
====> Test loss: 1524.0629
iteration 0000: loss: 1527.070
iteration 0100: loss: 1530.497
iteration 0200: loss: 1527.774
iteration 0300: loss: 1527.588
iteration 0400: loss: 1530.200
iteration 0500: loss: 1529.116
iteration 0600: loss: 1527.540
iteration 0700: loss: 1529.819
iteration 0800: loss: 1527.568
iteration 0900: loss: 1529.036
====> Epoch: 006 Train loss: 1528.4901  took : 3.879486083984375
====> Test loss: 1523.6574
iteration 0000: loss: 1529.661
iteration 0100: loss: 1527.890
iteration 0200: loss: 1531.822
iteration 0300: loss: 1528.797
iteration 0400: loss: 1529.377
iteration 0500: loss: 1529.789
iteration 0600: loss: 1525.537
iteration 0700: loss: 1528.568
iteration 0800: loss: 1526.405
iteration 0900: loss: 1527.448
====> Epoch: 007 Train loss: 1528.1312  took : 3.88283634185791
====> Test loss: 1523.2528
iteration 0000: loss: 1525.876
iteration 0100: loss: 1528.989
iteration 0200: loss: 1531.146
iteration 0300: loss: 1524.760
iteration 0400: loss: 1529.403
iteration 0500: loss: 1528.641
iteration 0600: loss: 1528.183
iteration 0700: loss: 1526.702
iteration 0800: loss: 1526.214
iteration 0900: loss: 1526.055
====> Epoch: 008 Train loss: 1527.7841  took : 3.880103349685669
====> Test loss: 1522.9715
iteration 0000: loss: 1525.697
iteration 0100: loss: 1523.869
iteration 0200: loss: 1522.793
iteration 0300: loss: 1529.669
iteration 0400: loss: 1526.809
iteration 0500: loss: 1526.178
iteration 0600: loss: 1527.951
iteration 0700: loss: 1527.433
iteration 0800: loss: 1527.104
iteration 0900: loss: 1525.573
====> Epoch: 009 Train loss: 1527.5650  took : 3.862283706665039
====> Test loss: 1522.7224
iteration 0000: loss: 1524.024
iteration 0100: loss: 1527.555
iteration 0200: loss: 1528.332
iteration 0300: loss: 1526.711
iteration 0400: loss: 1528.666
iteration 0500: loss: 1528.950
iteration 0600: loss: 1527.894
iteration 0700: loss: 1526.277
iteration 0800: loss: 1525.441
iteration 0900: loss: 1527.483
====> Epoch: 010 Train loss: 1527.3028  took : 3.863579273223877
====> Test loss: 1522.6089
iteration 0000: loss: 1527.535
iteration 0100: loss: 1526.753
iteration 0200: loss: 1528.485
iteration 0300: loss: 1527.236
iteration 0400: loss: 1528.280
iteration 0500: loss: 1527.884
iteration 0600: loss: 1527.435
iteration 0700: loss: 1526.981
iteration 0800: loss: 1526.980
iteration 0900: loss: 1526.800
====> Epoch: 011 Train loss: 1527.1153  took : 3.8618035316467285
====> Test loss: 1522.3425
iteration 0000: loss: 1525.551
iteration 0100: loss: 1529.112
iteration 0200: loss: 1525.838
iteration 0300: loss: 1526.426
iteration 0400: loss: 1528.076
iteration 0500: loss: 1527.533
iteration 0600: loss: 1525.386
iteration 0700: loss: 1527.148
iteration 0800: loss: 1525.521
iteration 0900: loss: 1528.251
====> Epoch: 012 Train loss: 1526.9222  took : 3.8807923793792725
====> Test loss: 1522.2967
iteration 0000: loss: 1528.280
iteration 0100: loss: 1528.743
iteration 0200: loss: 1528.369
iteration 0300: loss: 1526.101
iteration 0400: loss: 1526.322
iteration 0500: loss: 1524.041
iteration 0600: loss: 1523.412
iteration 0700: loss: 1523.396
iteration 0800: loss: 1527.455
iteration 0900: loss: 1529.717
====> Epoch: 013 Train loss: 1526.7957  took : 3.8798484802246094
====> Test loss: 1522.0773
iteration 0000: loss: 1526.890
iteration 0100: loss: 1525.958
iteration 0200: loss: 1527.220
iteration 0300: loss: 1523.353
iteration 0400: loss: 1528.884
iteration 0500: loss: 1525.395
iteration 0600: loss: 1528.636
iteration 0700: loss: 1529.112
iteration 0800: loss: 1528.871
iteration 0900: loss: 1524.101
====> Epoch: 014 Train loss: 1526.6336  took : 3.866453170776367
====> Test loss: 1521.9248
iteration 0000: loss: 1529.999
iteration 0100: loss: 1530.639
iteration 0200: loss: 1524.920
iteration 0300: loss: 1525.670
iteration 0400: loss: 1524.038
iteration 0500: loss: 1527.850
iteration 0600: loss: 1526.659
iteration 0700: loss: 1527.906
iteration 0800: loss: 1523.800
iteration 0900: loss: 1529.749
====> Epoch: 015 Train loss: 1526.5148  took : 3.8692140579223633
====> Test loss: 1521.7752
iteration 0000: loss: 1525.512
iteration 0100: loss: 1526.514
iteration 0200: loss: 1525.124
iteration 0300: loss: 1527.515
iteration 0400: loss: 1527.546
iteration 0500: loss: 1524.700
iteration 0600: loss: 1524.141
iteration 0700: loss: 1524.007
iteration 0800: loss: 1527.150
iteration 0900: loss: 1527.977
====> Epoch: 016 Train loss: 1526.3805  took : 3.8612353801727295
====> Test loss: 1521.7548
iteration 0000: loss: 1526.947
iteration 0100: loss: 1527.743
iteration 0200: loss: 1528.170
iteration 0300: loss: 1526.180
iteration 0400: loss: 1526.468
iteration 0500: loss: 1525.020
iteration 0600: loss: 1523.752
iteration 0700: loss: 1526.540
iteration 0800: loss: 1528.439
iteration 0900: loss: 1529.934
====> Epoch: 017 Train loss: 1526.2840  took : 3.8686983585357666
====> Test loss: 1521.6946
iteration 0000: loss: 1523.339
iteration 0100: loss: 1524.644
iteration 0200: loss: 1527.022
iteration 0300: loss: 1526.682
iteration 0400: loss: 1526.531
iteration 0500: loss: 1525.174
iteration 0600: loss: 1524.016
iteration 0700: loss: 1527.308
iteration 0800: loss: 1528.117
iteration 0900: loss: 1533.945
====> Epoch: 018 Train loss: 1526.1987  took : 3.8670291900634766
====> Test loss: 1521.6650
iteration 0000: loss: 1526.942
iteration 0100: loss: 1525.851
iteration 0200: loss: 1526.684
iteration 0300: loss: 1525.696
iteration 0400: loss: 1528.173
iteration 0500: loss: 1526.615
iteration 0600: loss: 1526.992
iteration 0700: loss: 1525.903
iteration 0800: loss: 1526.539
iteration 0900: loss: 1524.266
====> Epoch: 019 Train loss: 1526.0677  took : 3.865727663040161
====> Test loss: 1521.5915
iteration 0000: loss: 1523.993
iteration 0100: loss: 1525.235
iteration 0200: loss: 1526.629
iteration 0300: loss: 1524.570
iteration 0400: loss: 1525.133
iteration 0500: loss: 1525.998
iteration 0600: loss: 1525.391
iteration 0700: loss: 1527.067
iteration 0800: loss: 1527.035
iteration 0900: loss: 1525.787
====> Epoch: 020 Train loss: 1526.0165  took : 3.8660197257995605
====> Test loss: 1521.5266
iteration 0000: loss: 1526.587
iteration 0100: loss: 1530.799
iteration 0200: loss: 1524.630
iteration 0300: loss: 1525.279
iteration 0400: loss: 1524.663
iteration 0500: loss: 1524.353
iteration 0600: loss: 1526.107
iteration 0700: loss: 1525.055
iteration 0800: loss: 1520.108
iteration 0900: loss: 1527.030
====> Epoch: 021 Train loss: 1525.9189  took : 3.8796565532684326
====> Test loss: 1521.4417
iteration 0000: loss: 1526.174
iteration 0100: loss: 1525.080
iteration 0200: loss: 1527.588
iteration 0300: loss: 1523.552
iteration 0400: loss: 1528.928
iteration 0500: loss: 1523.138
iteration 0600: loss: 1525.678
iteration 0700: loss: 1525.509
iteration 0800: loss: 1523.860
iteration 0900: loss: 1522.859
====> Epoch: 022 Train loss: 1525.8349  took : 3.874250650405884
====> Test loss: 1521.3920
iteration 0000: loss: 1526.808
iteration 0100: loss: 1527.685
iteration 0200: loss: 1526.328
iteration 0300: loss: 1524.067
iteration 0400: loss: 1525.458
iteration 0500: loss: 1524.844
iteration 0600: loss: 1524.523
iteration 0700: loss: 1526.055
iteration 0800: loss: 1524.823
iteration 0900: loss: 1523.628
====> Epoch: 023 Train loss: 1525.7463  took : 3.869555950164795
====> Test loss: 1521.2082
iteration 0000: loss: 1523.915
iteration 0100: loss: 1523.087
iteration 0200: loss: 1527.485
iteration 0300: loss: 1524.108
iteration 0400: loss: 1522.532
iteration 0500: loss: 1525.112
iteration 0600: loss: 1525.792
iteration 0700: loss: 1523.126
iteration 0800: loss: 1525.634
iteration 0900: loss: 1525.416
====> Epoch: 024 Train loss: 1525.7193  took : 3.862041473388672
====> Test loss: 1521.3959
iteration 0000: loss: 1524.566
iteration 0100: loss: 1523.903
iteration 0200: loss: 1524.899
iteration 0300: loss: 1524.742
iteration 0400: loss: 1523.040
iteration 0500: loss: 1523.921
iteration 0600: loss: 1526.338
iteration 0700: loss: 1524.190
iteration 0800: loss: 1524.417
iteration 0900: loss: 1524.470
====> Epoch: 025 Train loss: 1525.6104  took : 3.8741884231567383
====> Test loss: 1521.2674
iteration 0000: loss: 1523.157
iteration 0100: loss: 1526.558
iteration 0200: loss: 1527.630
iteration 0300: loss: 1523.071
iteration 0400: loss: 1526.780
iteration 0500: loss: 1526.641
iteration 0600: loss: 1525.544
iteration 0700: loss: 1524.849
iteration 0800: loss: 1524.659
iteration 0900: loss: 1528.054
====> Epoch: 026 Train loss: 1525.5803  took : 3.888784408569336
====> Test loss: 1521.1438
iteration 0000: loss: 1524.790
iteration 0100: loss: 1524.409
iteration 0200: loss: 1524.879
iteration 0300: loss: 1524.631
iteration 0400: loss: 1525.966
iteration 0500: loss: 1525.255
iteration 0600: loss: 1525.732
iteration 0700: loss: 1524.496
iteration 0800: loss: 1523.781
iteration 0900: loss: 1525.339
====> Epoch: 027 Train loss: 1525.5239  took : 3.876272201538086
====> Test loss: 1521.0743
iteration 0000: loss: 1526.687
iteration 0100: loss: 1522.782
iteration 0200: loss: 1524.592
iteration 0300: loss: 1524.739
iteration 0400: loss: 1524.667
iteration 0500: loss: 1524.580
iteration 0600: loss: 1527.555
iteration 0700: loss: 1526.623
iteration 0800: loss: 1527.020
iteration 0900: loss: 1527.868
====> Epoch: 028 Train loss: 1525.4394  took : 3.879763603210449
====> Test loss: 1521.0456
iteration 0000: loss: 1526.019
iteration 0100: loss: 1525.109
iteration 0200: loss: 1524.521
iteration 0300: loss: 1525.233
iteration 0400: loss: 1524.499
iteration 0500: loss: 1526.607
iteration 0600: loss: 1526.029
iteration 0700: loss: 1525.712
iteration 0800: loss: 1527.874
iteration 0900: loss: 1523.653
====> Epoch: 029 Train loss: 1525.4180  took : 3.866469144821167
====> Test loss: 1521.0510
iteration 0000: loss: 1526.814
iteration 0100: loss: 1524.025
iteration 0200: loss: 1525.171
iteration 0300: loss: 1525.367
iteration 0400: loss: 1526.319
iteration 0500: loss: 1523.297
iteration 0600: loss: 1524.868
iteration 0700: loss: 1523.386
iteration 0800: loss: 1526.418
iteration 0900: loss: 1524.224
====> Epoch: 030 Train loss: 1525.3633  took : 3.893211841583252
====> Test loss: 1521.0712
====> [MM-VAE] Time: 193.891s or 00:03:13
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae
iteration 0000: loss: 2619.250
iteration 0100: loss: 2097.327
iteration 0200: loss: 2068.743
iteration 0300: loss: 2063.432
iteration 0400: loss: 2034.128
iteration 0500: loss: 2024.156
iteration 0600: loss: 2015.428
iteration 0700: loss: 1996.707
iteration 0800: loss: 1978.119
iteration 0900: loss: 1969.995
====> Epoch: 001 Train loss: 2041.6066  took : 6.1011364459991455
====> Test loss: 1961.5919
iteration 0000: loss: 1969.054
iteration 0100: loss: 1965.342
iteration 0200: loss: 1961.937
iteration 0300: loss: 1961.428
iteration 0400: loss: 1962.086
iteration 0500: loss: 1961.548
iteration 0600: loss: 1961.710
iteration 0700: loss: 1959.558
iteration 0800: loss: 1959.416
iteration 0900: loss: 1959.000
====> Epoch: 002 Train loss: 1961.6712  took : 6.122194766998291
====> Test loss: 1954.5770
iteration 0000: loss: 1959.162
iteration 0100: loss: 1959.705
iteration 0200: loss: 1958.607
iteration 0300: loss: 1960.320
iteration 0400: loss: 1959.366
iteration 0500: loss: 1958.022
iteration 0600: loss: 1959.136
iteration 0700: loss: 1957.505
iteration 0800: loss: 1959.375
iteration 0900: loss: 1958.103
====> Epoch: 003 Train loss: 1959.0180  took : 6.105191946029663
====> Test loss: 1953.7811
iteration 0000: loss: 1957.944
iteration 0100: loss: 1958.997
iteration 0200: loss: 1957.953
iteration 0300: loss: 1958.533
iteration 0400: loss: 1957.680
iteration 0500: loss: 1957.608
iteration 0600: loss: 1958.024
iteration 0700: loss: 1957.720
iteration 0800: loss: 1959.538
iteration 0900: loss: 1957.776
====> Epoch: 004 Train loss: 1958.4715  took : 6.124593496322632
====> Test loss: 1953.2656
iteration 0000: loss: 1958.086
iteration 0100: loss: 1957.109
iteration 0200: loss: 1959.947
iteration 0300: loss: 1957.823
iteration 0400: loss: 1957.044
iteration 0500: loss: 1956.736
iteration 0600: loss: 1958.599
iteration 0700: loss: 1956.476
iteration 0800: loss: 1957.787
iteration 0900: loss: 1958.123
====> Epoch: 005 Train loss: 1957.9923  took : 6.1037187576293945
====> Test loss: 1952.6982
iteration 0000: loss: 1958.015
iteration 0100: loss: 1955.954
iteration 0200: loss: 1958.421
iteration 0300: loss: 1957.097
iteration 0400: loss: 1957.098
iteration 0500: loss: 1957.301
iteration 0600: loss: 1957.385
iteration 0700: loss: 1958.448
iteration 0800: loss: 1957.916
iteration 0900: loss: 1958.822
====> Epoch: 006 Train loss: 1957.7048  took : 6.11519193649292
====> Test loss: 1952.9218
iteration 0000: loss: 1958.035
iteration 0100: loss: 1957.957
iteration 0200: loss: 1956.958
iteration 0300: loss: 1957.744
iteration 0400: loss: 1956.862
iteration 0500: loss: 1957.942
iteration 0600: loss: 1959.218
iteration 0700: loss: 1957.196
iteration 0800: loss: 1960.496
iteration 0900: loss: 1957.777
====> Epoch: 007 Train loss: 1957.6150  took : 6.138531446456909
====> Test loss: 1952.4392
iteration 0000: loss: 1957.433
iteration 0100: loss: 1957.449
iteration 0200: loss: 1957.680
iteration 0300: loss: 1957.512
iteration 0400: loss: 1957.964
iteration 0500: loss: 1957.867
iteration 0600: loss: 1957.013
iteration 0700: loss: 1957.245
iteration 0800: loss: 1958.122
iteration 0900: loss: 1957.176
====> Epoch: 008 Train loss: 1957.4906  took : 6.112675189971924
====> Test loss: 1952.3759
iteration 0000: loss: 1957.131
iteration 0100: loss: 1958.243
iteration 0200: loss: 1958.434
iteration 0300: loss: 1958.274
iteration 0400: loss: 1956.588
iteration 0500: loss: 1957.612
iteration 0600: loss: 1956.544
iteration 0700: loss: 1956.789
iteration 0800: loss: 1956.356
iteration 0900: loss: 1957.845
====> Epoch: 009 Train loss: 1957.4227  took : 6.126123905181885
====> Test loss: 1952.4810
iteration 0000: loss: 1956.560
iteration 0100: loss: 1958.538
iteration 0200: loss: 1956.133
iteration 0300: loss: 1955.799
iteration 0400: loss: 1958.242
iteration 0500: loss: 1957.978
iteration 0600: loss: 1958.263
iteration 0700: loss: 1956.735
iteration 0800: loss: 1958.147
iteration 0900: loss: 1956.708
====> Epoch: 010 Train loss: 1957.2773  took : 6.125599384307861
====> Test loss: 1952.6185
iteration 0000: loss: 1957.936
iteration 0100: loss: 1955.716
iteration 0200: loss: 1958.265
iteration 0300: loss: 1956.892
iteration 0400: loss: 1956.699
iteration 0500: loss: 1956.424
iteration 0600: loss: 1958.445
iteration 0700: loss: 1956.722
iteration 0800: loss: 1957.037
iteration 0900: loss: 1959.718
====> Epoch: 011 Train loss: 1957.2606  took : 6.119122743606567
====> Test loss: 1952.0837
iteration 0000: loss: 1956.734
iteration 0100: loss: 1956.657
iteration 0200: loss: 1957.361
iteration 0300: loss: 1956.843
iteration 0400: loss: 1957.167
iteration 0500: loss: 1957.013
iteration 0600: loss: 1956.730
iteration 0700: loss: 1956.812
iteration 0800: loss: 1956.763
iteration 0900: loss: 1958.042
====> Epoch: 012 Train loss: 1957.2096  took : 6.125565767288208
====> Test loss: 1951.9876
iteration 0000: loss: 1956.613
iteration 0100: loss: 1956.152
iteration 0200: loss: 1956.988
iteration 0300: loss: 1958.168
iteration 0400: loss: 1958.025
iteration 0500: loss: 1956.319
iteration 0600: loss: 1956.149
iteration 0700: loss: 1957.736
iteration 0800: loss: 1957.343
iteration 0900: loss: 1956.619
====> Epoch: 013 Train loss: 1957.0870  took : 6.12156081199646
====> Test loss: 1951.6237
iteration 0000: loss: 1956.772
iteration 0100: loss: 1956.690
iteration 0200: loss: 1957.524
iteration 0300: loss: 1958.070
iteration 0400: loss: 1956.805
iteration 0500: loss: 1956.798
iteration 0600: loss: 1957.645
iteration 0700: loss: 1956.740
iteration 0800: loss: 1956.544
iteration 0900: loss: 1957.107
====> Epoch: 014 Train loss: 1957.0014  took : 6.116128206253052
====> Test loss: 1951.9625
iteration 0000: loss: 1957.970
iteration 0100: loss: 1957.958
iteration 0200: loss: 1956.731
iteration 0300: loss: 1957.340
iteration 0400: loss: 1955.810
iteration 0500: loss: 1957.875
iteration 0600: loss: 1956.515
iteration 0700: loss: 1956.177
iteration 0800: loss: 1957.802
iteration 0900: loss: 1956.396
====> Epoch: 015 Train loss: 1956.9830  took : 6.112506866455078
====> Test loss: 1951.8243
iteration 0000: loss: 1957.017
iteration 0100: loss: 1956.913
iteration 0200: loss: 1957.442
iteration 0300: loss: 1957.912
iteration 0400: loss: 1956.422
iteration 0500: loss: 1957.107
iteration 0600: loss: 1957.115
iteration 0700: loss: 1955.495
iteration 0800: loss: 1957.485
iteration 0900: loss: 1957.167
====> Epoch: 016 Train loss: 1956.9353  took : 6.111423969268799
====> Test loss: 1951.9735
iteration 0000: loss: 1957.454
iteration 0100: loss: 1958.958
iteration 0200: loss: 1956.400
iteration 0300: loss: 1958.497
iteration 0400: loss: 1958.115
iteration 0500: loss: 1957.495
iteration 0600: loss: 1956.168
iteration 0700: loss: 1956.068
iteration 0800: loss: 1957.407
iteration 0900: loss: 1956.867
====> Epoch: 017 Train loss: 1956.9457  took : 6.127439737319946
====> Test loss: 1951.7536
iteration 0000: loss: 1957.846
iteration 0100: loss: 1955.670
iteration 0200: loss: 1956.679
iteration 0300: loss: 1956.603
iteration 0400: loss: 1956.119
iteration 0500: loss: 1956.519
iteration 0600: loss: 1956.215
iteration 0700: loss: 1957.190
iteration 0800: loss: 1956.481
iteration 0900: loss: 1956.975
====> Epoch: 018 Train loss: 1956.8284  took : 6.122714519500732
====> Test loss: 1951.7230
iteration 0000: loss: 1956.670
iteration 0100: loss: 1956.302
iteration 0200: loss: 1956.764
iteration 0300: loss: 1955.115
iteration 0400: loss: 1955.871
iteration 0500: loss: 1956.211
iteration 0600: loss: 1956.626
iteration 0700: loss: 1956.806
iteration 0800: loss: 1956.512
iteration 0900: loss: 1959.827
====> Epoch: 019 Train loss: 1956.8718  took : 6.120329141616821
====> Test loss: 1951.9650
iteration 0000: loss: 1956.042
iteration 0100: loss: 1956.206
iteration 0200: loss: 1957.833
iteration 0300: loss: 1956.201
iteration 0400: loss: 1956.535
iteration 0500: loss: 1956.634
iteration 0600: loss: 1957.344
iteration 0700: loss: 1957.724
iteration 0800: loss: 1956.032
iteration 0900: loss: 1956.809
====> Epoch: 020 Train loss: 1956.7653  took : 6.108038425445557
====> Test loss: 1951.8556
iteration 0000: loss: 1956.564
iteration 0100: loss: 1957.234
iteration 0200: loss: 1956.003
iteration 0300: loss: 1957.863
iteration 0400: loss: 1956.104
iteration 0500: loss: 1956.340
iteration 0600: loss: 1955.770
iteration 0700: loss: 1957.842
iteration 0800: loss: 1956.968
iteration 0900: loss: 1958.296
====> Epoch: 021 Train loss: 1956.7827  took : 6.136913537979126
====> Test loss: 1951.8569
iteration 0000: loss: 1956.175
iteration 0100: loss: 1956.099
iteration 0200: loss: 1955.564
iteration 0300: loss: 1955.460
iteration 0400: loss: 1957.453
iteration 0500: loss: 1958.442
iteration 0600: loss: 1957.614
iteration 0700: loss: 1958.067
iteration 0800: loss: 1956.250
iteration 0900: loss: 1955.389
====> Epoch: 022 Train loss: 1956.7118  took : 6.127444505691528
====> Test loss: 1951.5911
iteration 0000: loss: 1955.607
iteration 0100: loss: 1956.842
iteration 0200: loss: 1958.632
iteration 0300: loss: 1955.823
iteration 0400: loss: 1957.716
iteration 0500: loss: 1956.814
iteration 0600: loss: 1955.689
iteration 0700: loss: 1957.340
iteration 0800: loss: 1956.932
iteration 0900: loss: 1956.183
====> Epoch: 023 Train loss: 1956.6621  took : 6.131016254425049
====> Test loss: 1951.9756
iteration 0000: loss: 1956.439
iteration 0100: loss: 1955.925
iteration 0200: loss: 1956.986
iteration 0300: loss: 1957.385
iteration 0400: loss: 1957.405
iteration 0500: loss: 1956.121
iteration 0600: loss: 1956.651
iteration 0700: loss: 1957.061
iteration 0800: loss: 1955.927
iteration 0900: loss: 1956.276
====> Epoch: 024 Train loss: 1956.7042  took : 6.1290202140808105
====> Test loss: 1951.9075
iteration 0000: loss: 1956.722
iteration 0100: loss: 1956.963
iteration 0200: loss: 1955.739
iteration 0300: loss: 1956.152
iteration 0400: loss: 1955.776
iteration 0500: loss: 1957.308
iteration 0600: loss: 1957.399
iteration 0700: loss: 1956.668
iteration 0800: loss: 1957.818
iteration 0900: loss: 1955.593
====> Epoch: 025 Train loss: 1956.6484  took : 6.128363847732544
====> Test loss: 1951.5908
iteration 0000: loss: 1957.040
iteration 0100: loss: 1958.394
iteration 0200: loss: 1956.017
iteration 0300: loss: 1956.067
iteration 0400: loss: 1956.560
iteration 0500: loss: 1956.316
iteration 0600: loss: 1956.545
iteration 0700: loss: 1955.793
iteration 0800: loss: 1956.981
iteration 0900: loss: 1956.340
====> Epoch: 026 Train loss: 1956.6465  took : 6.129620552062988
====> Test loss: 1951.7075
iteration 0000: loss: 1955.915
iteration 0100: loss: 1957.268
iteration 0200: loss: 1957.259
iteration 0300: loss: 1956.163
iteration 0400: loss: 1956.005
iteration 0500: loss: 1955.487
iteration 0600: loss: 1956.605
iteration 0700: loss: 1955.917
iteration 0800: loss: 1956.349
iteration 0900: loss: 1955.741
====> Epoch: 027 Train loss: 1956.6083  took : 6.132713317871094
====> Test loss: 1951.5015
iteration 0000: loss: 1956.848
iteration 0100: loss: 1956.240
iteration 0200: loss: 1956.561
iteration 0300: loss: 1957.973
iteration 0400: loss: 1956.180
iteration 0500: loss: 1955.398
iteration 0600: loss: 1956.037
iteration 0700: loss: 1957.545
iteration 0800: loss: 1957.993
iteration 0900: loss: 1956.207
====> Epoch: 028 Train loss: 1956.5881  took : 6.116533517837524
====> Test loss: 1951.4345
iteration 0000: loss: 1957.385
iteration 0100: loss: 1959.062
iteration 0200: loss: 1957.602
iteration 0300: loss: 1954.990
iteration 0400: loss: 1955.795
iteration 0500: loss: 1956.402
iteration 0600: loss: 1954.927
iteration 0700: loss: 1956.740
iteration 0800: loss: 1956.705
iteration 0900: loss: 1956.319
====> Epoch: 029 Train loss: 1956.5538  took : 6.112722396850586
====> Test loss: 1951.7418
iteration 0000: loss: 1957.630
iteration 0100: loss: 1956.932
iteration 0200: loss: 1956.405
iteration 0300: loss: 1956.247
iteration 0400: loss: 1955.439
iteration 0500: loss: 1956.206
iteration 0600: loss: 1956.916
iteration 0700: loss: 1956.369
iteration 0800: loss: 1956.957
iteration 0900: loss: 1956.471
====> Epoch: 030 Train loss: 1956.5777  took : 6.110375642776489
====> Test loss: 1951.7044
====> [MM-VAE] Time: 249.828s or 00:04:09
Arguments (initial):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae
iteration 0000: loss: 5220.323
iteration 0100: loss: 4144.352
iteration 0200: loss: 4107.292
iteration 0300: loss: 4064.151
iteration 0400: loss: 4029.247
iteration 0500: loss: 4016.890
iteration 0600: loss: 4009.627
iteration 0700: loss: 4003.607
iteration 0800: loss: 3990.520
iteration 0900: loss: 3996.226
iteration 1000: loss: 3987.522
iteration 1100: loss: 3986.988
iteration 1200: loss: 4002.943
iteration 1300: loss: 3993.568
iteration 1400: loss: 3985.792
iteration 1500: loss: 3984.983
iteration 1600: loss: 3972.411
iteration 1700: loss: 3974.000
iteration 1800: loss: 3983.063
====> Epoch: 001 Train loss: 4016.7844  took : 137.95198488235474
====> Test loss: 3983.0415
iteration 0000: loss: 3993.637
iteration 0100: loss: 3982.592
iteration 0200: loss: 3981.124
iteration 0300: loss: 3975.128
iteration 0400: loss: 3973.645
iteration 0500: loss: 3973.107
iteration 0600: loss: 3981.333
iteration 0700: loss: 3984.566
iteration 0800: loss: 3977.390
iteration 0900: loss: 3967.232
iteration 1000: loss: 3968.737
iteration 1100: loss: 3974.697
iteration 1200: loss: 3975.803
iteration 1300: loss: 3968.750
iteration 1400: loss: 3975.884
iteration 1500: loss: 3968.792
iteration 1600: loss: 3967.047
iteration 1700: loss: 3980.519
iteration 1800: loss: 3972.165
====> Epoch: 002 Train loss: 3975.3182  took : 137.8208134174347
====> Test loss: 3976.6038
iteration 0000: loss: 3973.843
iteration 0100: loss: 3977.058
iteration 0200: loss: 3978.452
iteration 0300: loss: 3972.676
iteration 0400: loss: 3975.516
iteration 0500: loss: 3964.202
iteration 0600: loss: 3964.372
iteration 0700: loss: 3961.622
iteration 0800: loss: 3972.442
iteration 0900: loss: 3970.870
iteration 1000: loss: 3963.830
iteration 1100: loss: 3974.471
iteration 1200: loss: 3974.033
iteration 1300: loss: 3964.626
iteration 1400: loss: 3968.109
iteration 1500: loss: 3965.464
iteration 1600: loss: 3974.015
iteration 1700: loss: 3967.318
iteration 1800: loss: 3965.028
====> Epoch: 003 Train loss: 3971.0228  took : 137.70038437843323
====> Test loss: 3973.9932
iteration 0000: loss: 3969.054
iteration 0100: loss: 3959.414
iteration 0200: loss: 3964.353
iteration 0300: loss: 3973.156
iteration 0400: loss: 3961.659
iteration 0500: loss: 3968.563
iteration 0600: loss: 3966.716
iteration 0700: loss: 3970.967
iteration 0800: loss: 3964.198
iteration 0900: loss: 3966.803
iteration 1000: loss: 3966.910
iteration 1100: loss: 3967.913
iteration 1200: loss: 3963.084
iteration 1300: loss: 3962.537
iteration 1400: loss: 3972.108
iteration 1500: loss: 3961.851
iteration 1600: loss: 3960.812
iteration 1700: loss: 3974.916
iteration 1800: loss: 3973.605
====> Epoch: 004 Train loss: 3968.8055  took : 137.58984303474426
====> Test loss: 3972.6276
iteration 0000: loss: 3968.392
iteration 0100: loss: 3963.668
iteration 0200: loss: 3968.479
iteration 0300: loss: 3969.765
iteration 0400: loss: 3964.246
iteration 0500: loss: 3969.570
iteration 0600: loss: 3966.880
iteration 0700: loss: 3962.180
iteration 0800: loss: 3968.656
iteration 0900: loss: 3972.093
iteration 1000: loss: 3962.936
iteration 1100: loss: 3973.235
iteration 1200: loss: 3972.438
iteration 1300: loss: 3969.513
iteration 1400: loss: 3976.822
iteration 1500: loss: 3957.450
iteration 1600: loss: 3964.877
iteration 1700: loss: 3965.957
iteration 1800: loss: 3964.112
====> Epoch: 005 Train loss: 3967.2592  took : 137.5798089504242
====> Test loss: 3971.3246
iteration 0000: loss: 3966.320
iteration 0100: loss: 3968.354
iteration 0200: loss: 3968.395
iteration 0300: loss: 3964.382
iteration 0400: loss: 3964.076
iteration 0500: loss: 3963.894
iteration 0600: loss: 3959.506
iteration 0700: loss: 3969.558
iteration 0800: loss: 3967.735
iteration 0900: loss: 3968.580
iteration 1000: loss: 3961.257
iteration 1100: loss: 3971.991
iteration 1200: loss: 3965.907
iteration 1300: loss: 3971.393
iteration 1400: loss: 3963.712
iteration 1500: loss: 3960.984
iteration 1600: loss: 3960.467
iteration 1700: loss: 3964.010
iteration 1800: loss: 3964.445
====> Epoch: 006 Train loss: 3966.1321  took : 137.53627610206604
====> Test loss: 3970.5918
iteration 0000: loss: 3968.240
iteration 0100: loss: 3964.547
iteration 0200: loss: 3968.159
iteration 0300: loss: 3969.141
iteration 0400: loss: 3963.925
iteration 0500: loss: 3958.195
iteration 0600: loss: 3959.686
iteration 0700: loss: 3966.462
iteration 0800: loss: 3968.817
iteration 0900: loss: 3971.750
iteration 1000: loss: 3954.127
iteration 1100: loss: 3963.970
iteration 1200: loss: 3958.380
iteration 1300: loss: 3963.106
iteration 1400: loss: 3969.440
iteration 1500: loss: 3970.998
iteration 1600: loss: 3974.889
iteration 1700: loss: 3962.691
iteration 1800: loss: 3956.316
====> Epoch: 007 Train loss: 3965.4417  took : 137.54976201057434
====> Test loss: 3970.0789
iteration 0000: loss: 3970.159
iteration 0100: loss: 3959.520
iteration 0200: loss: 3970.719
iteration 0300: loss: 3961.353
iteration 0400: loss: 3974.958
iteration 0500: loss: 3958.178
iteration 0600: loss: 3970.015
iteration 0700: loss: 3966.359
iteration 0800: loss: 3971.250
iteration 0900: loss: 3971.060
iteration 1000: loss: 3967.958
iteration 1100: loss: 3972.634
iteration 1200: loss: 3966.724
iteration 1300: loss: 3967.168
iteration 1400: loss: 3966.889
iteration 1500: loss: 3962.312
iteration 1600: loss: 3966.327
iteration 1700: loss: 3959.177
iteration 1800: loss: 3958.281
====> Epoch: 008 Train loss: 3964.7724  took : 137.57616758346558
====> Test loss: 3969.5121
iteration 0000: loss: 3971.329
iteration 0100: loss: 3971.160
iteration 0200: loss: 3961.712
iteration 0300: loss: 3959.400
iteration 0400: loss: 3973.705
iteration 0500: loss: 3965.766
iteration 0600: loss: 3966.053
iteration 0700: loss: 3960.209
iteration 0800: loss: 3954.911
iteration 0900: loss: 3968.110
iteration 1000: loss: 3965.202
iteration 1100: loss: 3967.594
iteration 1200: loss: 3963.916
iteration 1300: loss: 3959.285
iteration 1400: loss: 3960.882
iteration 1500: loss: 3956.392
iteration 1600: loss: 3959.000
iteration 1700: loss: 3967.252
iteration 1800: loss: 3960.189
====> Epoch: 009 Train loss: 3964.0215  took : 137.5343189239502
====> Test loss: 3968.9305
iteration 0000: loss: 3960.258
iteration 0100: loss: 3965.613
iteration 0200: loss: 3961.366
iteration 0300: loss: 3957.098
iteration 0400: loss: 3971.539
iteration 0500: loss: 3963.923
iteration 0600: loss: 3963.844
iteration 0700: loss: 3961.493
iteration 0800: loss: 3962.565
iteration 0900: loss: 3966.366
iteration 1000: loss: 3965.056
iteration 1100: loss: 3961.743
iteration 1200: loss: 3963.639
iteration 1300: loss: 3969.750
iteration 1400: loss: 3962.776
iteration 1500: loss: 3956.991
iteration 1600: loss: 3964.323
iteration 1700: loss: 3977.209
iteration 1800: loss: 3959.814
====> Epoch: 010 Train loss: 3963.6268  took : 137.5140414237976
====> Test loss: 3968.6863
iteration 0000: loss: 3956.638
iteration 0100: loss: 3966.530
iteration 0200: loss: 3956.421
iteration 0300: loss: 3953.189
iteration 0400: loss: 3961.784
iteration 0500: loss: 3973.539
iteration 0600: loss: 3955.802
iteration 0700: loss: 3966.643
iteration 0800: loss: 3963.489
iteration 0900: loss: 3951.931
iteration 1000: loss: 3961.604
iteration 1100: loss: 3965.149
iteration 1200: loss: 3963.833
iteration 1300: loss: 3961.507
iteration 1400: loss: 3959.181
iteration 1500: loss: 3960.195
iteration 1600: loss: 3961.244
iteration 1700: loss: 3968.725
iteration 1800: loss: 3962.945
====> Epoch: 011 Train loss: 3963.3527  took : 137.40715265274048
====> Test loss: 3968.2936
iteration 0000: loss: 3960.563
iteration 0100: loss: 3961.052
iteration 0200: loss: 3960.004
iteration 0300: loss: 3960.657
iteration 0400: loss: 3959.110
iteration 0500: loss: 3962.719
iteration 0600: loss: 3959.094
iteration 0700: loss: 3972.953
iteration 0800: loss: 3964.839
iteration 0900: loss: 3965.122
iteration 1000: loss: 3959.451
iteration 1100: loss: 3961.172
iteration 1200: loss: 3964.421
iteration 1300: loss: 3957.945
iteration 1400: loss: 3958.087
iteration 1500: loss: 3963.796
iteration 1600: loss: 3960.527
iteration 1700: loss: 3958.168
iteration 1800: loss: 3969.505
====> Epoch: 012 Train loss: 3963.1113  took : 137.51634192466736
====> Test loss: 3968.8702
iteration 0000: loss: 3963.810
iteration 0100: loss: 3964.930
iteration 0200: loss: 3970.714
iteration 0300: loss: 3959.562
iteration 0400: loss: 3963.910
iteration 0500: loss: 3960.176
iteration 0600: loss: 3965.797
iteration 0700: loss: 3968.526
iteration 0800: loss: 3961.011
iteration 0900: loss: 3963.348
iteration 1000: loss: 3973.045
iteration 1100: loss: 3968.459
iteration 1200: loss: 3967.848
iteration 1300: loss: 3960.982
iteration 1400: loss: 3970.777
iteration 1500: loss: 3963.944
iteration 1600: loss: 3966.465
iteration 1700: loss: 3959.751
iteration 1800: loss: 3960.659
====> Epoch: 013 Train loss: 3962.7137  took : 137.458589553833
====> Test loss: 3967.4638
iteration 0000: loss: 3964.600
iteration 0100: loss: 3956.203
iteration 0200: loss: 3954.418
iteration 0300: loss: 3960.504
iteration 0400: loss: 3958.565
iteration 0500: loss: 3961.616
iteration 0600: loss: 3962.060
iteration 0700: loss: 3956.010
iteration 0800: loss: 3962.734
iteration 0900: loss: 3964.004
iteration 1000: loss: 3956.869
iteration 1100: loss: 3966.661
iteration 1200: loss: 3960.529
iteration 1300: loss: 3961.373
iteration 1400: loss: 3968.361
iteration 1500: loss: 3964.554
iteration 1600: loss: 3964.456
iteration 1700: loss: 3956.980
iteration 1800: loss: 3966.693
====> Epoch: 014 Train loss: 3962.4046  took : 137.4668173789978
====> Test loss: 3967.9895
iteration 0000: loss: 3961.089
iteration 0100: loss: 3961.562
iteration 0200: loss: 3958.225
iteration 0300: loss: 3966.294
iteration 0400: loss: 3958.512
iteration 0500: loss: 3958.426
iteration 0600: loss: 3966.045
iteration 0700: loss: 3955.271
iteration 0800: loss: 3949.941
iteration 0900: loss: 3963.320
iteration 1000: loss: 3955.950
iteration 1100: loss: 3963.396
iteration 1200: loss: 3969.153
iteration 1300: loss: 3965.445
iteration 1400: loss: 3962.560
iteration 1500: loss: 3968.716
iteration 1600: loss: 3961.749
iteration 1700: loss: 3962.982
iteration 1800: loss: 3966.448
====> Epoch: 015 Train loss: 3962.0803  took : 137.47095584869385
====> Test loss: 3969.0098
iteration 0000: loss: 3963.103
iteration 0100: loss: 3953.930
iteration 0200: loss: 3966.787
iteration 0300: loss: 3963.780
iteration 0400: loss: 3972.903
iteration 0500: loss: 3960.960
iteration 0600: loss: 3971.625
iteration 0700: loss: 3962.723
iteration 0800: loss: 3965.263
iteration 0900: loss: 3963.968
iteration 1000: loss: 3963.332
iteration 1100: loss: 3963.333
iteration 1200: loss: 3958.737
iteration 1300: loss: 3966.399
iteration 1400: loss: 3967.307
iteration 1500: loss: 3961.717
iteration 1600: loss: 3967.086
iteration 1700: loss: 3968.240
iteration 1800: loss: 3964.377
====> Epoch: 016 Train loss: 3961.8929  took : 137.3702893257141
====> Test loss: 3967.3321
iteration 0000: loss: 3961.078
iteration 0100: loss: 3955.740
iteration 0200: loss: 3960.429
iteration 0300: loss: 3957.890
iteration 0400: loss: 3964.195
iteration 0500: loss: 3968.247
iteration 0600: loss: 3963.758
iteration 0700: loss: 3963.034
iteration 0800: loss: 3964.589
iteration 0900: loss: 3964.522
iteration 1000: loss: 3961.193
iteration 1100: loss: 3956.156
iteration 1200: loss: 3951.635
iteration 1300: loss: 3959.616
iteration 1400: loss: 3957.511
iteration 1500: loss: 3966.635
iteration 1600: loss: 3962.397
iteration 1700: loss: 3963.626
iteration 1800: loss: 3967.236
====> Epoch: 017 Train loss: 3961.6649  took : 137.4384229183197
====> Test loss: 3967.5380
iteration 0000: loss: 3962.013
iteration 0100: loss: 3961.827
iteration 0200: loss: 3961.144
iteration 0300: loss: 3955.495
iteration 0400: loss: 3963.482
iteration 0500: loss: 3955.759
iteration 0600: loss: 3960.767
iteration 0700: loss: 3956.709
iteration 0800: loss: 3960.528
iteration 0900: loss: 3963.807
iteration 1000: loss: 3964.739
iteration 1100: loss: 3961.096
iteration 1200: loss: 3956.437
iteration 1300: loss: 3963.496
iteration 1400: loss: 3963.254
iteration 1500: loss: 3967.776
iteration 1600: loss: 3963.521
iteration 1700: loss: 3962.379
iteration 1800: loss: 3960.323
====> Epoch: 018 Train loss: 3961.5679  took : 137.45419597625732
====> Test loss: 3966.9597
iteration 0000: loss: 3962.998
iteration 0100: loss: 3958.348
iteration 0200: loss: 3969.193
iteration 0300: loss: 3964.011
iteration 0400: loss: 3957.957
iteration 0500: loss: 3965.744
iteration 0600: loss: 3962.940
iteration 0700: loss: 3955.945
iteration 0800: loss: 3965.897
iteration 0900: loss: 3962.232
iteration 1000: loss: 3961.663
iteration 1100: loss: 3960.902
iteration 1200: loss: 3959.804
iteration 1300: loss: 3962.589
iteration 1400: loss: 3966.752
iteration 1500: loss: 3963.169
iteration 1600: loss: 3959.692
iteration 1700: loss: 3959.432
iteration 1800: loss: 3957.627
====> Epoch: 019 Train loss: 3961.2804  took : 137.35255312919617
====> Test loss: 3966.8881
iteration 0000: loss: 3967.881
iteration 0100: loss: 3959.181
iteration 0200: loss: 3969.695
iteration 0300: loss: 3967.642
iteration 0400: loss: 3967.791
iteration 0500: loss: 3967.002
iteration 0600: loss: 3959.365
iteration 0700: loss: 3966.006
iteration 0800: loss: 3961.001
iteration 0900: loss: 3961.671
iteration 1000: loss: 3962.821
iteration 1100: loss: 3956.708
iteration 1200: loss: 3962.805
iteration 1300: loss: 3955.820
iteration 1400: loss: 3960.937
iteration 1500: loss: 3964.435
iteration 1600: loss: 3957.591
iteration 1700: loss: 3965.425
iteration 1800: loss: 3957.168
====> Epoch: 020 Train loss: 3961.2062  took : 137.34088397026062
====> Test loss: 3966.9574
iteration 0000: loss: 3960.213
iteration 0100: loss: 3962.317
iteration 0200: loss: 3961.577
iteration 0300: loss: 3962.673
iteration 0400: loss: 3962.781
iteration 0500: loss: 3963.845
iteration 0600: loss: 3962.978
iteration 0700: loss: 3961.703
iteration 0800: loss: 3961.740
iteration 0900: loss: 3966.821
iteration 1000: loss: 3966.665
iteration 1100: loss: 3960.185
iteration 1200: loss: 3951.119
iteration 1300: loss: 3965.236
iteration 1400: loss: 3964.347
iteration 1500: loss: 3962.978
iteration 1600: loss: 3962.420
iteration 1700: loss: 3967.119
iteration 1800: loss: 3956.445
====> Epoch: 021 Train loss: 3961.1183  took : 137.43773436546326
====> Test loss: 3966.2786
iteration 0000: loss: 3964.312
iteration 0100: loss: 3960.419
iteration 0200: loss: 3963.435
iteration 0300: loss: 3965.521
iteration 0400: loss: 3961.734
iteration 0500: loss: 3958.152
iteration 0600: loss: 3967.748
iteration 0700: loss: 3967.629
iteration 0800: loss: 3964.669
iteration 0900: loss: 3960.766
iteration 1000: loss: 3965.017
iteration 1100: loss: 3953.410
iteration 1200: loss: 3957.894
iteration 1300: loss: 3957.606
iteration 1400: loss: 3959.097
iteration 1500: loss: 3954.106
iteration 1600: loss: 3957.418
iteration 1700: loss: 3958.283
iteration 1800: loss: 3958.991
====> Epoch: 022 Train loss: 3960.8108  took : 137.42441749572754
====> Test loss: 3966.6136
iteration 0000: loss: 3960.236
iteration 0100: loss: 3958.804
iteration 0200: loss: 3962.056
iteration 0300: loss: 3966.157
iteration 0400: loss: 3969.647
iteration 0500: loss: 3962.615
iteration 0600: loss: 3954.413
iteration 0700: loss: 3955.328
iteration 0800: loss: 3964.756
iteration 0900: loss: 3960.903
iteration 1000: loss: 3958.711
iteration 1100: loss: 3953.672
iteration 1200: loss: 3955.028
iteration 1300: loss: 3961.913
iteration 1400: loss: 3950.652
iteration 1500: loss: 3964.818
iteration 1600: loss: 3959.767
iteration 1700: loss: 3967.336
iteration 1800: loss: 3959.448
====> Epoch: 023 Train loss: 3960.8200  took : 137.46020936965942
====> Test loss: 3966.2564
iteration 0000: loss: 3958.825
iteration 0100: loss: 3967.437
iteration 0200: loss: 3966.119
iteration 0300: loss: 3957.345
iteration 0400: loss: 3962.404
iteration 0500: loss: 3953.321
iteration 0600: loss: 3960.260
iteration 0700: loss: 3957.943
iteration 0800: loss: 3960.886
iteration 0900: loss: 3959.958
iteration 1000: loss: 3956.803
iteration 1100: loss: 3957.916
iteration 1200: loss: 3962.593
iteration 1300: loss: 3957.370
iteration 1400: loss: 3955.470
iteration 1500: loss: 3967.214
iteration 1600: loss: 3965.542
iteration 1700: loss: 3959.839
iteration 1800: loss: 3962.030
====> Epoch: 024 Train loss: 3960.5393  took : 137.34103560447693
====> Test loss: 3966.2820
iteration 0000: loss: 3958.941
iteration 0100: loss: 3953.943
iteration 0200: loss: 3961.145
iteration 0300: loss: 3954.918
iteration 0400: loss: 3958.166
iteration 0500: loss: 3955.168
iteration 0600: loss: 3964.241
iteration 0700: loss: 3959.199
iteration 0800: loss: 3964.581
iteration 0900: loss: 3955.898
iteration 1000: loss: 3961.738
iteration 1100: loss: 3964.126
iteration 1200: loss: 3960.421
iteration 1300: loss: 3965.514
iteration 1400: loss: 3957.021
iteration 1500: loss: 3963.390
iteration 1600: loss: 3951.742
iteration 1700: loss: 3959.398
iteration 1800: loss: 3964.143
====> Epoch: 025 Train loss: 3960.3137  took : 137.35179448127747
====> Test loss: 3966.0898
iteration 0000: loss: 3954.805
iteration 0100: loss: 3967.977
iteration 0200: loss: 3963.620
iteration 0300: loss: 3958.243
iteration 0400: loss: 3958.809
iteration 0500: loss: 3964.688
iteration 0600: loss: 3957.994
iteration 0700: loss: 3959.688
iteration 0800: loss: 3961.654
iteration 0900: loss: 3957.855
iteration 1000: loss: 3967.463
iteration 1100: loss: 3951.844
iteration 1200: loss: 3958.666
iteration 1300: loss: 3960.669
iteration 1400: loss: 3956.522
iteration 1500: loss: 3958.239
iteration 1600: loss: 3955.905
iteration 1700: loss: 3968.589
iteration 1800: loss: 3966.221
====> Epoch: 026 Train loss: 3960.2317  took : 137.40352845191956
====> Test loss: 3965.4889
iteration 0000: loss: 3961.804
iteration 0100: loss: 3966.525
iteration 0200: loss: 3962.221
iteration 0300: loss: 3952.739
iteration 0400: loss: 3964.961
iteration 0500: loss: 3961.318
iteration 0600: loss: 3961.562
iteration 0700: loss: 3954.837
iteration 0800: loss: 3967.403
iteration 0900: loss: 3961.339
iteration 1000: loss: 3958.491
iteration 1100: loss: 3961.215
iteration 1200: loss: 3965.711
iteration 1300: loss: 3955.240
iteration 1400: loss: 3967.380
iteration 1500: loss: 3961.330
iteration 1600: loss: 3961.729
iteration 1700: loss: 3946.375
iteration 1800: loss: 3960.222
====> Epoch: 027 Train loss: 3960.2638  took : 137.51712036132812
====> Test loss: 3966.4785
iteration 0000: loss: 3956.867
iteration 0100: loss: 3963.646
iteration 0200: loss: 3958.544
iteration 0300: loss: 3958.776
iteration 0400: loss: 3956.987
iteration 0500: loss: 3954.883
iteration 0600: loss: 3959.167
iteration 0700: loss: 3961.073
iteration 0800: loss: 3960.118
iteration 0900: loss: 3957.429
iteration 1000: loss: 3960.925
iteration 1100: loss: 3964.445
iteration 1200: loss: 3957.761
iteration 1300: loss: 3961.110
iteration 1400: loss: 3958.549
iteration 1500: loss: 3963.747
iteration 1600: loss: 3964.075
iteration 1700: loss: 3961.845
iteration 1800: loss: 3959.048
====> Epoch: 028 Train loss: 3960.0805  took : 137.40721035003662
====> Test loss: 3965.7866
iteration 0000: loss: 3952.959
iteration 0100: loss: 3962.553
iteration 0200: loss: 3954.550
iteration 0300: loss: 3961.414
iteration 0400: loss: 3950.622
iteration 0500: loss: 3968.312
iteration 0600: loss: 3960.783
iteration 0700: loss: 3959.291
iteration 0800: loss: 3958.790
iteration 0900: loss: 3961.785
iteration 1000: loss: 3960.390
iteration 1100: loss: 3963.394
iteration 1200: loss: 3959.573
iteration 1300: loss: 3963.361
iteration 1400: loss: 3961.000
iteration 1500: loss: 3961.126
iteration 1600: loss: 3963.005
iteration 1700: loss: 3952.083
iteration 1800: loss: 3957.978
====> Epoch: 029 Train loss: 3960.3088  took : 137.3859827518463
====> Test loss: 3966.1850
iteration 0000: loss: 3959.837
iteration 0100: loss: 3959.505
iteration 0200: loss: 3958.595
iteration 0300: loss: 3961.938
iteration 0400: loss: 3952.855
iteration 0500: loss: 3956.999
iteration 0600: loss: 3955.004
iteration 0700: loss: 3959.496
iteration 0800: loss: 3961.524
iteration 0900: loss: 3956.645
iteration 1000: loss: 3961.198
iteration 1100: loss: 3965.750
iteration 1200: loss: 3965.107
iteration 1300: loss: 3966.999
iteration 1400: loss: 3954.681
iteration 1500: loss: 3954.258
iteration 1600: loss: 3952.879
iteration 1700: loss: 3966.602
iteration 1800: loss: 3968.761
====> Epoch: 030 Train loss: 3960.0086  took : 137.25874948501587
====> Test loss: 3965.6926
====> [MM-VAE] Time: 4609.023s or 01:16:49
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 0, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_0', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0', 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/train'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_0
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0',
 'run_id': 'vae_cmnist_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([2, 1, 3, 2, 6, 3, 6, 4, 5, 9, 2, 9, 5, 4, 6, 1, 9, 9, 2, 7, 7, 5, 5, 6,
        6, 7, 1, 7, 5, 4, 3, 7, 9, 1, 9, 8, 6, 6, 7, 1, 9, 9, 2, 6, 3, 1, 9, 1,
        6, 8, 8, 2, 3, 2, 5, 6, 3, 9, 2, 9, 9, 1, 2, 8, 9, 5, 9, 8, 9, 4, 3, 6,
        2, 3, 3, 1, 2, 7, 7, 4, 8, 2, 2, 6, 6, 4, 5, 5, 3, 2, 4, 8, 2, 6, 2, 9,
        8, 5, 7, 3, 5, 1, 8, 3, 1, 9, 1, 9, 4, 7, 8, 3, 1, 9, 4, 7, 2, 2, 6, 1,
        6, 1, 8, 6, 5, 5, 5, 2], device='cuda:0') 
label[0]: tensor(2, device='cuda:0') 
label[1]: tensor(1, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.00955222  0.16960028  0.02084704 ...  0.1733405   0.17617634
  0.14877322]
Average of silhouette coef: 0.10958265
---
  0   1   2   3
0 0.0 2.8 2.9 2.1 
1 2.8 0.0 2.9 1.9 
2 2.9 2.9 0.0 2.1 
3 2.1 1.9 2.1 0.0 
correlation [[ 1.         -0.42810417]
 [-0.42810417  1.        ]]
---
[[], [], [], []] [3 0 3 ... 0 0 1] [[-31.856983 -16.342798]
 [ 58.766285  33.68635 ]
 [-23.913645   6.99727 ]
 ...
 [ -5.236058  48.600906]
 [-16.578114  40.32022 ]
 [ 69.772675   4.113416]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.10958265, 'cluster_all': array([-0.00955222,  0.16960028,  0.02084704, ...,  0.1733405 ,
        0.17617634,  0.14877322], dtype=float32), 'magnitude_avg': -0.4281041651849867, 'magnitude_all': -0.4281041651849867, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_0', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.10958265, 'magnitude_avg': -0.4281041651849867, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([1, 2, 4, 9, 6, 4, 9, 5, 4, 9, 6, 9, 7, 9, 9, 2, 2, 4, 2, 4, 7, 6, 8, 7,
        8, 1, 2, 5, 6, 8, 7, 4, 8, 1, 7, 7, 9, 7, 3, 2, 3, 6, 9, 4, 6, 7, 9, 4,
        1, 8, 3, 1, 6, 3, 2, 1, 1, 7, 5, 6, 5, 9, 6, 6, 3, 1, 8, 9, 3, 8, 9, 3,
        9, 5, 9, 7, 2, 6, 3, 2, 5, 5, 7, 3, 4, 8, 9, 7, 8, 1, 9, 9, 9, 9, 5, 2,
        4, 7, 8, 3, 7, 4, 5, 6, 9, 2, 8, 9, 3, 1, 4, 5, 2, 3, 8, 5, 2, 8, 2, 1,
        5, 1, 4, 7, 4, 2, 3, 7], device='cuda:0') 
label[0]: tensor(1, device='cuda:0') 
label[1]: tensor(2, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([9, 1, 8, 5, 7, 2, 7, 8, 6, 1, 1, 6, 2, 9, 6, 2, 5, 9, 2, 5, 6, 1, 1, 7,
        4, 2, 6, 3, 3, 6, 5, 9, 2, 4, 4, 9, 4, 9, 9, 2, 1, 8, 2, 2, 3, 9, 2, 1,
        2, 4, 3, 6, 3, 1, 8, 3, 2, 6, 5, 3, 9, 9, 9, 5, 2, 9, 4, 2, 5, 6, 6, 2,
        4, 2, 8, 9, 3, 1, 8, 6, 6, 7, 7, 6, 9, 4, 2, 7, 2, 4, 3, 4, 8, 8, 7, 1,
        3, 2, 3, 7, 3, 9, 9, 6, 7, 2, 2, 4, 6, 6, 6, 8, 5, 8, 9, 2, 6, 5, 2, 2,
        3, 4, 8, 4, 7, 3, 2, 7])
Accuracy (count): tensor(10) 
Accuracy (ratio) tensor(0.0781)
Accuracy:
 [[10  0  0  0  0  0  0  0  0]
 [25  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [13  0  0  0  0  0  0  0  0]
 [ 9  0  0  0  0  0  0  0  0]
 [18  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]
 [17  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [ 0.10996814  0.00361464  0.00974927 ...  0.08432662 -0.04831238
 -0.02742257]
Average of silhouette coef: 0.072924435
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.6 3.0 3.0 3.0 3.0 3.2 2.6 2.7 
2 2.6 0.0 2.4 2.7 2.8 2.6 3.2 2.4 3.0 
3 3.0 2.4 0.0 3.6 2.2 3.3 3.1 2.3 2.8 
4 3.0 2.7 3.6 0.0 2.9 2.5 2.7 2.5 1.7 
5 3.0 2.8 2.2 2.9 0.0 2.8 3.0 2.1 2.3 
6 3.0 2.6 3.3 2.5 2.8 0.0 3.8 3.1 2.9 
7 3.2 3.2 3.1 2.7 3.0 3.8 0.0 3.1 1.8 
8 2.6 2.4 2.3 2.5 2.1 3.1 3.1 0.0 1.9 
9 2.7 3.0 2.8 1.7 2.3 2.9 1.8 1.9 0.0 
correlation [[ 1.         -0.06761165]
 [-0.06761165  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [1 2 4 ... 8 4 2] [[ 16.209557  -55.45132  ]
 [-25.481262  -33.532883 ]
 [ 11.965747  -28.50031  ]
 ...
 [ -3.7553444 -47.396744 ]
 [ 25.300516   55.518692 ]
 [  5.491386   27.925737 ]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[45  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [61  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [45  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.1 -0.2  0.2 -0.0  1.5  0.2  0.1 -0.1  0.4  1.0 -0.0 -0.1  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0  0.4 -1.2  0.3 -0.0  0.0  0.2  0.6 -0.5 -0.0 -0.4 -0.0 -1.0  0.0 
 0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.6 -0.6 -0.2  0.0 -0.4  1.3  0.4  1.1 -0.0 -0.2 -0.0 -0.3  0.0 
 0.0  0.0  0.0  0.0  0.0 -0.0  0.0  1.0  0.2 -0.2  0.0 -0.4 -1.0 -0.1 -0.8  0.0 -0.1 -0.0  0.7  0.0 
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  1.0  0.7 -0.0 -0.5  0.5  0.2  0.7 -0.3 -0.3  0.0 -0.4  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.0  0.8 -0.2 -0.0 -0.1  0.3  1.1 -1.7 -0.2 -0.2 -0.0 -0.2  0.0 
-0.0 -0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.9 -0.4  0.1 -0.0 -0.2 -1.6 -0.2  0.9 -0.2 -0.1 -0.0  0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0  0.5  0.0 -0.1 -0.0 -0.2  0.7 -1.1  0.1  0.2 -0.3 -0.0 -0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.2  0.3 -0.2  0.0 -0.1 -0.6 -0.8  0.1  0.0 -0.2  0.0  0.6  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.9035825
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.1 -0.2  0.2 -0.0  1.5  0.2  0.1 -0.1  0.4  1.0 -0.0 -0.1  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0  0.4 -1.2  0.3 -0.0  0.0  0.2  0.6 -0.5 -0.0 -0.4 -0.0 -1.0  0.0 
 0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.6 -0.6 -0.2  0.0 -0.4  1.3  0.4  1.1 -0.0 -0.2 -0.0 -0.3  0.0 
 0.0  0.0  0.0  0.0  0.0 -0.0  0.0  1.0  0.2 -0.2  0.0 -0.4 -1.0 -0.1 -0.8  0.0 -0.1 -0.0  0.7  0.0 
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  1.0  0.7 -0.0 -0.5  0.5  0.2  0.7 -0.3 -0.3  0.0 -0.4  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.0  0.8 -0.2 -0.0 -0.1  0.3  1.1 -1.7 -0.2 -0.2 -0.0 -0.2  0.0 
-0.0 -0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.9 -0.4  0.1 -0.0 -0.2 -1.6 -0.2  0.9 -0.2 -0.1 -0.0  0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0  0.5  0.0 -0.1 -0.0 -0.2  0.7 -1.1  0.1  0.2 -0.3 -0.0 -0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.2  0.3 -0.2  0.0 -0.1 -0.6 -0.8  0.1  0.0 -0.2  0.0  0.6  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.5133529
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.1 -0.2  0.2 -0.0  1.5  0.2  0.1 -0.1  0.4  1.0 -0.0 -0.1  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0  0.4 -1.2  0.3 -0.0  0.0  0.2  0.6 -0.5 -0.0 -0.4 -0.0 -1.0  0.0 
 0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.6 -0.6 -0.2  0.0 -0.4  1.3  0.4  1.1 -0.0 -0.2 -0.0 -0.3  0.0 
 0.0  0.0  0.0  0.0  0.0 -0.0/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
  0.0  1.0  0.2 -0.2  0.0 -0.4 -1.0 -0.1 -0.8  0.0 -0.1 -0.0  0.7  0.0 
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  1.0  0.7 -0.0 -0.5  0.5  0.2  0.7 -0.3 -0.3  0.0 -0.4  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.0  0.8 -0.2 -0.0 -0.1  0.3  1.1 -1.7 -0.2 -0.2 -0.0 -0.2  0.0 
-0.0 -0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.9 -0.4  0.1 -0.0 -0.2 -1.6 -0.2  0.9 -0.2 -0.1 -0.0  0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0  0.5  0.0 -0.1 -0.0 -0.2  0.7 -1.1  0.1  0.2 -0.3 -0.0 -0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.2  0.3 -0.2  0.0 -0.1 -0.6 -0.8  0.1  0.0 -0.2  0.0  0.6  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.634674
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.1 -0.2  0.2 -0.0  1.5  0.2  0.1 -0.1  0.4  1.0 -0.0 -0.1  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0  0.4 -1.2  0.3 -0.0  0.0  0.2  0.6 -0.5 -0.0 -0.4 -0.0 -1.0  0.0 
 0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.6 -0.6 -0.2  0.0 -0.4  1.3  0.4  1.1 -0.0 -0.2 -0.0 -0.3  0.0 
 0.0  0.0  0.0  0.0  0.0 -0.0  0.0  1.0  0.2 -0.2  0.0 -0.4 -1.0 -0.1 -0.8  0.0 -0.1 -0.0  0.7  0.0 
-0.0  0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0  1.0  0.7 -0.0 -0.5  0.5  0.2  0.7 -0.3 -0.3  0.0 -0.4  0.0 
 0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.0  0.8 -0.2 -0.0 -0.1  0.3  1.1 -1.7 -0.2 -0.2 -0.0 -0.2  0.0 
-0.0 -0.0  0.0 -0.0 -0.0 -0.0  0.0 -0.9 -0.4  0.1 -0.0 -0.2 -1.6 -0.2  0.9 -0.2 -0.1 -0.0  0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0  0.5  0.0 -0.1 -0.0 -0.2  0.7 -1.1  0.1  0.2 -0.3 -0.0 -0.5  0.0 
 0.0  0.0 -0.0  0.0  0.0 -0.0  0.0 -0.2  0.3 -0.2  0.0 -0.1 -0.6 -0.8  0.1  0.0 -0.2  0.0  0.6  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.4463484
results (all): {'reconst_0x0_avg': 0.078125, 'reconst_0x0_all': nan, 'cluster_avg': 0.072924435, 'cluster_all': array([ 0.10996814,  0.00361464,  0.00974927, ...,  0.08432662,
       -0.04831238, -0.02742257], dtype=float32), 'magnitude_avg': -0.06761164719318356, 'magnitude_all': -0.06761164719318356, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_0', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.078125, 'cluster_avg': 0.072924435, 'magnitude_avg': -0.06761164719318356, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 1, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_1', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1', 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_1
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1',
 'run_id': 'vae_cmnist_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([1, 1, 9, 5, 1, 6, 7, 8, 4, 8, 3, 3, 6, 1, 6, 2, 9, 6, 9, 9, 2, 7, 6, 7,
        1, 2, 5, 2, 2, 8, 9, 7, 5, 1, 6, 6, 8, 2, 1, 1, 9, 3, 7, 1, 2, 9, 5, 3,
        4, 2, 7, 7, 9, 7, 5, 7, 6, 9, 2, 9, 5, 4, 5, 3, 5, 8, 9, 7, 3, 1, 7, 9,
        5, 6, 2, 7, 3, 3, 2, 8, 3, 4, 5, 5, 5, 1, 4, 7, 3, 4, 8, 9, 3, 2, 7, 3,
        2, 5, 7, 6, 2, 7, 2, 4, 1, 8, 6, 8, 6, 4, 2, 7, 9, 9, 5, 2, 3, 1, 2, 3,
        8, 1, 1, 4, 3, 3, 2, 2], device='cuda:0') 
label[0]: tensor(1, device='cuda:0') 
label[1]: tensor(1, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.22141413  0.19520015  0.21461317 ...  0.16409384 -0.10117966
  0.13491714]
Average of silhouette coef: 0.12996711
---
  0   1   2   3
0 0.0 3.0 3.0 2.2 
1 3.0 0.0 3.1 2.2 
2 3.0 3.1 0.0 2.2 
3 2.2 2.2 2.2 0.0 
correlation [[ 1.         -0.48705052]
 [-0.48705052  1.        ]]
---
[[], [], [], []] [1 0 2 ... 0 3 2] [[ 44.156036   37.964077 ]
 [  2.5048144  67.12149  ]
 [ -5.655044  -61.585228 ]
 ...
 [-26.256329   28.916618 ]
 [ 33.743702   30.486244 ]
 [-15.320864  -31.757206 ]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.12996711, 'cluster_all': array([ 0.22141413,  0.19520015,  0.21461317, ...,  0.16409384,
       -0.10117966,  0.13491714], dtype=float32), 'magnitude_avg': -0.4870505200376358, 'magnitude_all': -0.4870505200376358, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_1', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.12996711, 'magnitude_avg': -0.4870505200376358, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([5, 8, 3, 5, 5, 7, 6, 3, 9, 5, 5, 9, 6, 7, 3, 7, 2, 3, 6, 1, 4, 4, 3, 5,
        5, 8, 2, 2, 2, 3, 8, 6, 2, 4, 1, 5, 1, 7, 9, 1, 4, 2, 1, 6, 4, 9, 2, 4,
        6, 1, 5, 5, 3, 2, 3, 6, 2, 1, 7, 2, 3, 3, 4, 4, 7, 6, 4, 8, 4, 1, 8, 1,
        1, 1, 8, 3, 9, 6, 9, 5, 1, 4, 4, 4, 7, 9, 6, 9, 3, 8, 6, 5, 7, 5, 6, 6,
        2, 8, 1, 5, 3, 9, 5, 2, 4, 5, 2, 2, 8, 5, 7, 5, 5, 9, 4, 9, 1, 9, 2, 2,
        7, 1, 6, 2, 3, 8, 9, 1], device='cuda:0') 
label[0]: tensor(5, device='cuda:0') 
label[1]: tensor(8, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([5, 5, 5, 6, 2, 4, 7, 8, 2, 1, 9, 1, 8, 8, 8, 2, 2, 6, 7, 1, 6, 6, 3, 4,
        3, 6, 2, 7, 4, 1, 5, 9, 2, 1, 3, 6, 1, 8, 2, 2, 5, 2, 2, 1, 9, 3, 1, 1,
        3, 8, 4, 2, 2, 5, 1, 8, 7, 4, 6, 7, 8, 8, 8, 1, 7, 4, 9, 1, 4, 5, 1, 2,
        1, 9, 5, 5, 7, 2, 4, 6, 8, 4, 7, 2, 1, 6, 4, 2, 4, 1, 3, 7, 3, 9, 9, 9,
        5, 3, 6, 2, 6, 5, 7, 3, 8, 3, 6, 5, 7, 8, 4, 2, 7, 9, 1, 5, 1, 3, 4, 1,
        3, 1, 9, 8, 4, 1, 8, 1])
Accuracy (count): tensor(12) 
Accuracy (ratio) tensor(0.0938)
Accuracy:
 [[ 0  0  0  0  0  0 22  0  0]
 [ 0  0  0  0  0  0 18  0  0]
 [ 0  0  0  0  0  0 12  0  0]
 [ 0  0  0  0  0  0 14  0  0]
 [ 0  0  0  0  0  0 13  0  0]
 [ 0  0  0  0  0  0 12  0  0]
 [ 0  0  0  0  0  0 12  0  0]
 [ 0  0  0  0  0  0 15  0  0]
 [ 0  0  0  0  0  0 10  0  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]]
---
Silhouette values: [ 0.04458445  0.05938691  0.08086712 ...  0.0060354   0.04039713
 -0.0407277 ]
Average of silhouette coef: 0.060168363
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.7 3.1 3.3 3.1 3.0 3.0 2.7 2.8 
2 2.7 0.0 2.3 2.8 2.9 2.7 2.9 2.4 2.8 
3 3.1 2.3 0.0 3.6 2.1 3.3 2.9 2.2 2.9 
4 3.3 2.8 3.6 0.0 3.0 2.6 2.5 2.5 1.5 
5 3.1 2.9 2.1 3.0 0.0 2.5 2.6 1.8 2.2 
6 3.0 2.7 3.3 2.6 2.5 0.0 3.8 2.5 2.8 
7 3.0 2.9 2.9 2.5 2.6 3.8 0.0 2.8 1.7 
8 2.7 2.4 2.2 2.5 1.8 2.5 2.8 0.0 1.9 
9 2.8 2.8 2.9 1.5 2.2 2.8 1.7 1.9 0.0 
correlation [[ 1.        -0.0455566]
 [-0.0455566  1.       ]]
---
[[], [], [], [], [], [], [], [], [], []] [5 8 3 ... 8 2 5] [[-41.5814     -8.731257 ]
 [ 30.83013    40.86766  ]
 [-13.060424   15.4520235]
 ...
 [ 43.23853   -18.349688 ]
 [ 25.585375  -23.455517 ]
 [ 31.8333      2.1559353]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(57) 
Accuracy (ratio) tensor(0.1166)
Accuracy:
 [[ 0  0  0  0  0  0 45  0  0]
 [ 0  0  0  0  0  0 52  0  0]
 [ 0  0  0  0  0  0 57  0  0]
 [ 0  0  0  0  0  0 60  0  0]
 [ 0  0  0  0  0  0 61  0  0]
 [ 0  0  0  0  0  0 60  0  0]
 [ 0  0  0  0  0  0 57  0  0]
 [ 0  0  0  0  0  0 52  0  0]
 [ 0  0  0  0  0  0 45  0  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.9 -0.4  0.0  0.0 -0.0 -0.9  0.0  0.3 -0.3  0.0  0.8  0.0 -0.0  0.0  1.2 -0.0  0.0 -0.0  0.0 -0.0 
-0.3  0.3  0.5 -0.0 -0.5  0.1  0.0 -1.0 -0.8 -0.0  0.8 -0.0 -0.0  0.1 -0.3  0.1 -0.0 -0.0  0.0 -0.0 
 0.2  0.7  0.0 -0.0 -1.1  0.7  0.0  0.8 -0.5  0.0  0.5  0.0 -0.0  0.0 -0.6 -0.2 -0.0 -0.0  0.0 -0.0 
-0.0 -0.5 -0.4 -0.0  0.7  0.1 -0.0 -1.3  0.9  0.0 -0.6  0.0 -0.0 -0.0 -0.3 -0.1 -0.0 -0.0 -0.0  0.0 
-0.2  0.6  0.1 -0.0  0.2  0.4  0.0  1.1 -0.3  0.0 -0.9  0.0 -0.0 -0.0 -0.3  0.2  0.0 -0.0 -0.0 -0.0 
 0.3  0.6  0.0 -0.0  1.6 -0.6 -0.0 -0.4 -0.7  0.0 -0.1  0.0 -0.0 -0.1 -0.7 -0.2  0.0 -0.0 -0.0  0.0 
-0.4 -0.7  0.3 -0.0 -1.1  0.3  0.0 -0.1  0.7 -0.0 -0.9  0.0 -0.0 -0.0  0.5 -0.2 -0.0 -0.0  0.0  0.0 
-0.4 -0.0 -0.7 -0.0  0.5  0.8  0.0  0.4 -0.4  0.0  0.3 -0.0 -0.0 -0.2  0.0  0.1 -0.0 -0.0  0.0  0.0 
-0.1 -0.7  0.1  0.0  0.4  0.5  0.0 -0.0  0.9 -0.0 -0.5  0.0  0.0  0.1 -0.1 -0.2 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.9322664
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.9 -0.4  0.0  0.0 -0.0 -0.9  0.0  0.3 -0.3  0.0  0.8  0.0 -0.0  0.0  1.2 -0.0  0.0 -0.0  0.0 -0.0 
-0.3  0.3  0.5 -0.0 -0.5  0.1  0.0 -1.0 -0.8 -0.0  0.8 -0.0 -0.0  0.1 -0.3  0.1 -0.0 -0.0  0.0 -0.0 
 0.2  0.7  0.0 -0.0 -1.1  0.7  0.0  0.8 -0.5  0.0  0.5  0.0 -0.0  0.0 -0.6 -0.2 -0.0 -0.0  0.0 -0.0 
-0.0 -0.5 -0.4 -0.0  0.7  0.1 -0.0 -1.3  0.9  0.0 -0.6  0.0 -0.0 -0.0 -0.3 -0.1 -0.0 -0.0 -0.0  0.0 
-0.2  0.6  0.1 -0.0  0.2  0.4  0.0  1.1 -0.3  0.0 -0.9  0.0 -0.0 -0.0 -0.3  0.2  0.0 -0.0 -0.0 -0.0 
 0.3  0.6  0.0 -0.0  1.6 -0.6 -0.0 -0.4 -0.7  0.0 -0.1  0.0 -0.0 -0.1 -0.7 -0.2  0.0 -0.0 -0.0  0.0 
-0.4 -0.7  0.3 -0.0 -1.1  0.3  0.0 -0.1  0.7 -0.0 -0.9  0.0 -0.0 -0.0  0.5 -0.2 -0.0 -0.0  0.0  0.0 
-0.4 -0.0 -0.7 -0.0  0.5  0.8  0.0  0.4 -0.4  0.0  0.3 -0.0 -0.0 -0.2  0.0  0.1 -0.0 -0.0  0.0  0.0 
-0.1 -0.7  0.1  0.0  0.4  0.5  0.0 -0.0  0.9 -0.0 -0.5  0.0  0.0  0.1 -0.1 -0.2 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.5191758
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.9 -0.4  0.0  0.0 -0.0 -0.9  0.0  0.3 -0.3  0.0  0.8  0.0 -0.0  0.0  1.2 -0.0  0.0 -0.0  0.0 -0.0 
-0.3  0.3  0.5 -0.0 -0.5  0.1  0.0 -1.0 -0.8 -0.0  0.8 -0.0 -0.0  0.1 -0.3  0.1 -0.0 -0.0  0.0 -0.0 
 0.2  0.7  0.0 -0.0 -1.1  0.7  0.0  0.8 -0.5  0.0  0.5  0.0 -0.0  0.0 -0.6 -0.2 -0.0 -0.0  0.0 -0.0 
-0.0 -0.5 -0.4 -0.0  0.7  0.1 -0.0 -1.3  0.9  0.0 -0.6  0.0 -0.0 -0.0 -0.3 -0.1 -0.0 -0.0 -0.0  0.0 
-0.2  0.6  0.1 -0.0  0.2  0.4  0.0  1.1 -0.3  0.0 -0.9  0.0 -0.0 -0.0 -0.3  0.2  0.0 -0.0 -0.0 -0.0 
 0.3  0.6  0.0 -0.0  1.6 -0.6 -0.0 -0.4 -0.7  0.0 -0.1  0.0 -0.0 -0.1 -0.7 -0.2  0.0 -0.0 -0.0  0.0 
-0.4 -0.7  0.3 -0.0 -1.1  0.3  0.0 -0.1  0.7 -0.0 -0.9  0.0 -0.0 -0.0  0.5 -0.2 -0.0 -0.0  0.0  0.0 
-0.4 -0.0 -0.7 -0.0  0.5  0.8  0.0  0.4 -0.4  0.0  0.3 -0.0 -0.0 -0.2  0.0  0.1 -0.0 -0.0  0.0  0.0 
-0.1 -0.7  0.1  0.0  0.4  0.5  0.0 -0.0  0.9 -0.0 -0.5  0.0  0.0  0.1 -0.1 -0.2 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.7390344
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.9 -0.4  0.0  0.0 -0.0 -0.9  0.0  0.3 -0.3  0.0  0.8  0.0 -0.0  0.0  1.2 -0.0  0.0 -0.0  0.0 -0.0 
-0.3  0.3  0.5 -0.0 -0.5  0.1  0.0 -1.0 -0.8 -0.0  0.8 -0.0 -0.0  0.1 -0.3  0.1 -0.0 -0.0  0.0 -0.0 
 0.2  0.7  0.0 -0.0 -1.1  0.7  0.0  0.8 -0.5  0.0  0.5  0.0 -0.0  0.0 -0.6 -0.2 -0.0 -0.0  0.0 -0.0 
-0.0 -0.5 -0.4 -0.0  0.7  0.1 -0.0 -1.3  0.9  0.0 -0.6  0.0 -0.0 -0.0 -0.3 -0.1 -0.0 -0.0 -0.0  0.0 
-0.2  0.6  0.1 -0.0  0.2  0.4  0.0  1.1 -0.3  0.0 -0.9  0.0 -0.0 -0.0 -0.3  0.2  0.0 -0.0 -0.0 -0.0 
 0.3  0.6  0.0 -0.0  1.6 -0.6 -0.0 -0.4 -0.7  0.0 -0.1  0.0 -0.0 -0.1 -0.7 -0.2  0.0 -0.0 -0.0  0.0 
-0.4 -0.7  0.3 -0.0 -1.1  0.3  0.0 -0.1  0.7 -0.0 -0.9  0.0 -0.0 -0.0  0.5 -0.2 -0.0 -0.0  0.0  0.0 
-0.4 -0.0 -0.7 -0.0  0.5  0.8  0.0  0.4 -0.4  0.0  0.3 -0.0 -0.0 -0.2  0.0  0.1 -0.0 -0.0  0.0  0.0 
-0.1 -0.7  0.1  0.0  0.4  0.5  0.0 -0.0  0.9 -0.0 -0.5  0.0  0.0  0.1 -0.1 -0.2 -0.0 -0.0  0.0 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.2784312
results (all): {'reconst_0x0_avg': 0.09375, 'reconst_0x0_all': nan, 'cluster_avg': 0.060168363, 'cluster_all': array([ 0.04458445,  0.05938691,  0.08086712, ...,  0.0060354 ,
        0.04039713, -0.0407277 ], dtype=float32), 'magnitude_avg': -0.04555659719307587, 'magnitude_all': -0.04555659719307587, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_1', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.09375, 'cluster_avg': 0.060168363, 'magnitude_avg': -0.04555659719307587, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 2, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_2', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2', 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/train', 'device': 'cuda'}/home/taka/.pyenv/versions/3.6.9/lib/python3.6/site-packages/sklearn/metrics/_plot/confusion_matrix.py:81: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_2
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2',
 'run_id': 'vae_cmnist_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([3, 8, 1, 7, 5, 1, 1, 8, 7, 2, 8, 2, 9, 7, 8, 6, 7, 4, 1, 5, 4, 8, 7, 5,
        9, 6, 7, 4, 5, 8, 6, 9, 1, 5, 1, 7, 2, 2, 1, 7, 3, 3, 9, 2, 9, 3, 1, 6,
        5, 1, 9, 6, 7, 6, 7, 3, 4, 6, 6, 2, 8, 8, 3, 9, 6, 1, 2, 4, 7, 3, 7, 2,
        1, 5, 7, 1, 3, 9, 9, 5, 6, 8, 3, 5, 2, 3, 5, 4, 6, 4, 5, 1, 5, 3, 1, 8,
        9, 1, 7, 2, 2, 9, 2, 3, 8, 7, 2, 8, 7, 6, 4, 6, 1, 3, 4, 1, 9, 9, 9, 4,
        8, 8, 6, 2, 3, 1, 4, 6], device='cuda:0') 
label[0]: tensor(3, device='cuda:0') 
label[1]: tensor(8, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.01472503  0.1428537   0.00154484 ...  0.17961738  0.15383755
  0.13438945]
Average of silhouette coef: 0.12235793
---
  0   1   2   3
0 0.0 3.0 3.0 1.9 
1 3.0 0.0 3.1 2.2 
2 3.0 3.1 0.0 2.0 
3 1.9 2.2 2.0 0.0 
correlation [[ 1.         -0.49957479]
 [-0.49957479  1.        ]]
---
[[], [], [], []] [3 0 3 ... 2 2 2] [[ -6.46338     14.189786  ]
 [ 33.294613    18.706923  ]
 [  9.642021    52.08143   ]
 ...
 [ 14.506778   -65.88056   ]
 [-28.181517   -27.559278  ]
 [ -0.16399159 -66.7326    ]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.12235793, 'cluster_all': array([-0.01472503,  0.1428537 ,  0.00154484, ...,  0.17961738,
        0.15383755,  0.13438945], dtype=float32), 'magnitude_avg': -0.499574790616416, 'magnitude_all': -0.499574790616416, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_2', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.12235793, 'magnitude_avg': -0.499574790616416, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([6, 9, 1, 4, 8, 9, 5, 2, 7, 1, 4, 9, 9, 6, 7, 5, 1, 6, 1, 1, 6, 7, 2, 1,
        7, 1, 3, 5, 7, 8, 3, 1, 7, 9, 8, 5, 4, 4, 6, 6, 4, 4, 8, 6, 7, 2, 2, 4,
        6, 5, 9, 3, 8, 7, 6, 1, 2, 6, 1, 9, 8, 9, 8, 5, 3, 3, 2, 4, 9, 8, 6, 6,
        5, 6, 2, 9, 4, 7, 1, 4, 2, 9, 1, 9, 6, 4, 2, 2, 8, 9, 5, 7, 5, 6, 5, 8,
        4, 7, 1, 2, 7, 3, 3, 1, 4, 5, 4, 6, 5, 1, 1, 7, 9, 2, 6, 4, 4, 2, 3, 8,
        1, 5, 2, 4, 9, 4, 1, 5], device='cuda:0') 
label[0]: tensor(6, device='cuda:0') 
label[1]: tensor(9, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([1, 1, 1, 1, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1,
        1, 1, 1, 1, 3, 3, 3, 8, 3, 1, 3, 1, 3, 8, 1, 1, 3, 1, 3, 1, 3, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 3, 8, 3, 3, 8, 3, 1, 8, 1, 8, 3, 3, 3,
        1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1,
        1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3,
        3, 1, 1, 1, 1, 3, 8, 3]) 
tar: tensor([2, 1, 7, 9, 9, 9, 7, 3, 3, 8, 8, 8, 3, 3, 7, 5, 3, 9, 1, 1, 7, 2, 9, 8,
        4, 1, 9, 4, 2, 6, 5, 6, 7, 8, 2, 6, 7, 4, 4, 8, 6, 6, 8, 1, 7, 7, 1, 2,
        1, 5, 4, 7, 9, 8, 7, 1, 1, 6, 4, 2, 4, 2, 9, 4, 8, 9, 4, 8, 2, 9, 2, 8,
        9, 3, 9, 5, 7, 1, 5, 7, 8, 4, 5, 8, 1, 7, 3, 2, 3, 1, 2, 1, 8, 1, 1, 8,
        1, 1, 3, 7, 9, 3, 2, 2, 6, 4, 1, 1, 7, 7, 3, 9, 2, 9, 1, 1, 9, 1, 9, 2,
        5, 1, 1, 3, 6, 3, 2, 2])
Accuracy (count): tensor(28) 
Accuracy (ratio) tensor(0.2188)
Accuracy:
 [[20  0  4  0  0  0  0  0  0]
 [ 8  0  7  0  0  0  0  2  0]
 [ 5  0  8  0  0  0  0  0  0]
 [ 6  0  0  0  0  0  0  5  0]
 [ 5  0  2  0  0  0  0  0  0]
 [ 5  0  2  0  0  0  0  1  0]
 [11  0  5  0  0  0  0  0  0]
 [11  0  4  0  0  0  0  0  0]
 [14  0  3  0  0  0  0  0  0]]
Accuracy:
 [[0.833 0.    0.167 0.    0.    0.    0.    0.    0.   ]
 [0.471 0.    0.412 0.    0.    0.    0.    0.118 0.   ]
 [0.385 0.    0.615 0.    0.    0.    0.    0.    0.   ]
 [0.545 0.    0.    0.    0.    0.    0.    0.455 0.   ]
 [0.714 0.    0.286 0.    0.    0.    0.    0.    0.   ]
 [0.625 0.    0.25  0.    0.    0.    0.    0.125 0.   ]
 [0.688 0.    0.312 0.    0.    0.    0.    0.    0.   ]
 [0.733 0.    0.267 0.    0.    0.    0.    0.    0.   ]
 [0.824 0.    0.176 0.    0.    0.    0.    0.    0.   ]]
---
Silhouette values: [0.22121905 0.12303829 0.322871   ... 0.2216655  0.02668661 0.07663569]
Average of silhouette coef: 0.07014498
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.9 3.1 3.1 3.0 3.0 3.0 2.7 2.7 
2 2.9 0.0 2.5 2.9 3.0 2.8 3.0 2.3 2.9 
3 3.1 2.5 0.0 3.5 1.9 3.1 3.0 2.3 2.8 
4 3.1 2.9 3.5 0.0 3.2 2.7 2.7 2.5 1.6 
5 3.0 3.0 1.9 3.2 0.0 2.7 2.8 2.1 2.1 
6 3.0 2.8 3.1 2.7 2.7 0.0 3.6 2.8 2.8 
7 3.0 3.0 3.0 2.7 2.8 3.6 0.0 3.0 1.8 
8 2.7 2.3 2.3 2.5 2.1 2.8 3.0 0.0 1.9 
9 2.7 2.9 2.8 1.6 2.1 2.8 1.8 1.9 0.0 
correlation [[ 1.         -0.12643091]
 [-0.12643091  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [6 9 1 ... 1 6 5] [[ 25.227945 -27.433289]
 [-10.152334  64.78426 ]
 [  3.855831 -67.31515 ]
 ...
 [ 57.782135 -33.993694]
 [ 14.798735 -37.885654]
 [ 40.708946  -9.890179]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([1, 3, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 3, 1,
        1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 8, 1, 3,
        3, 8, 3, 3, 3, 3, 8, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 3, 1, 8, 3, 3, 1, 1,
        1, 1, 1, 8, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,
        3, 1, 1, 1, 3, 1, 3, 3, 3, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1,
        8, 1, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 8, 3, 3, 1, 1, 1, 1, 3, 1, 3,
        3, 8, 1, 1, 1, 3, 3, 8, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 3, 1, 3, 1,
        1, 8, 8, 3, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 8, 3, 1, 8, 8, 1, 1, 3, 1,
        8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 8, 1, 1, 1, 1, 3, 1, 8, 1, 1,
        1, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 1, 8, 1, 3, 1, 3, 1,
        1, 1, 1, 3, 1, 1, 1, 1, 3, 8, 1, 3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 3,
        3, 1, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 1, 1, 8, 1, 1, 3, 1, 8, 1, 3, 1, 1,
        3, 3, 3, 3, 3, 1, 1, 3, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1,
        1, 8, 1, 1, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 8, 3,
        1, 1, 1, 1, 1, 1, 3, 3, 8, 8, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 8, 1, 3, 1,
        1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 8, 1, 1, 1, 1, 1, 1,
        3, 3, 1, 1, 1, 8, 3, 3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 1, 1, 3, 8,
        1, 3, 1, 1, 3, 3, 1, 3, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 8, 1, 3, 1, 1,
        3, 1, 3, 3, 1, 3, 3, 1, 1, 3, 8, 3, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1,
        1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3,
        8, 3, 1, 1, 3, 1, 3, 3, 1]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(48) 
Accuracy (ratio) tensor(0.0982)
Accuracy:
 [[28  0 13  0  0  0  0  4  0]
 [36  0 13  0  0  0  0  3  0]
 [35  0 19  0  0  0  0  3  0]
 [36  0 21  0  0  0  0  3  0]
 [29  0 24  0  0  0  0  8  0]
 [27  0 25  0  0  0  0  8  0]
 [32  0 23  0  0  0  0  2  0]
 [32  0 19  0  0  0  0  1  0]
 [30  0 11  0  0  0  0  4  0]]
Accuracy:
 [[0.622 0.    0.289 0.    0.    0.    0.    0.089 0.   ]
 [0.692 0.    0.25  0.    0.    0.    0.    0.058 0.   ]
 [0.614 0.    0.333 0.    0.    0.    0.    0.053 0.   ]
 [0.6   0.    0.35  0.    0.    0.    0.    0.05  0.   ]
 [0.475 0.    0.393 0.    0.    0.    0.    0.131 0.   ]
 [0.45  0.    0.417 0.    0.    0.    0.    0.133 0.   ]
 [0.561 0.    0.404 0.    0.    0.    0.    0.035 0.   ]
 [0.615 0.    0.365 0.    0.    0.    0.    0.019 0.   ]
 [0.667 0.    0.244 0.    0.    0.    0.    0.089 0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1,
        1, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0 -0.0  0.5 -0.5 -0.0 -0.3 -1.8 -0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0 -0.5 -0.0  0.0  0.0  0.1 
 0.0  0.5 -0.6  1.0  0.0  0.7 -0.2  0.0  0.0  0.0  0.9  0.0  0.2 -0.0  0.0 -0.2 -0.0  0.6  0.1 -0.0 
 0.0 -1.0 -0.4  0.7  0.0  1.1 -0.0 -0.0  0.0  0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.1  0.0  0.2  0.0  0.0 
-0.0  1.0 -0.2 -0.8 -0.0 -0.3  0.6 -0.0  0.0  0.0  0.4 -0.0  0.9 -0.0  0.0  0.3  0.0 -0.6  0.0 -0.2 
 0.0 -0.6  0.1  0.5  0.0  0.1  0.5 -0.0  0.0  0.0 -0.6 -0.0 -1.2  0.0  0.0 -0.1 -0.0 -0.7  0.2  0.1 
 0.0  0.4  1.7  0.8  0.0 -0.0  0.3  0.0  0.0  0.0  0.0 -0.0  0.4 -0.0  0.0  0.2 -0.0 -0.1  0.0  0.0 
-0.0  0.0 -0.8 -1.0  0.0 -0.6  0.4  0.0  0.0  0.0 -0.4  0.0 -0.5 -0.0 -0.0 -0.3 -0.0  1.1  0.0 -0.1 
-0.0 -0.8 -0.5  0.5  0.0 -0.1 -0.1 -0.0  0.0 -0.0  0.5 -0.0  0.3 -0.0 -0.0  0.1 -0.0 -0.9 -0.1 -0.1 
 0.0 -0.2 -0.2 -0.8  0.0 -0.6  0.6  0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0  0.2  0.0 -0.3 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.8668436
pred: tensor([3, 8, 1, 8, 3, 1, 3, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 1, 1, 3, 3]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0 -0.0  0.5 -0.5 -0.0 -0.3 -1.8 -0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0 -0.5 -0.0  0.0  0.0  0.1 
 0.0  0.5 -0.6  1.0  0.0  0.7 -0.2  0.0  0.0  0.0  0.9  0.0  0.2 -0.0  0.0 -0.2 -0.0  0.6  0.1 -0.0 
 0.0 -1.0 -0.4  0.7  0.0  1.1 -0.0 -0.0  0.0  0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.1  0.0  0.2  0.0  0.0 
-0.0  1.0 -0.2 -0.8 -0.0 -0.3  0.6 -0.0  0.0  0.0  0.4 -0.0  0.9 -0.0  0.0  0.3  0.0 -0.6  0.0 -0.2 
 0.0 -0.6  0.1  0.5  0.0  0.1  0.5 -0.0  0.0  0.0 -0.6 -0.0 -1.2  0.0  0.0 -0.1 -0.0 -0.7  0.2  0.1 
 0.0  0.4  1.7  0.8  0.0 -0.0  0.3  0.0  0.0  0.0  0.0 -0.0  0.4 -0.0  0.0  0.2 -0.0 -0.1  0.0  0.0 
-0.0  0.0 -0.8 -1.0  0.0 -0.6  0.4  0.0  0.0  0.0 -0.4  0.0 -0.5 -0.0 -0.0 -0.3 -0.0  1.1  0.0 -0.1 
-0.0 -0.8 -0.5  0.5  0.0 -0.1 -0.1 -0.0  0.0 -0.0  0.5 -0.0  0.3 -0.0 -0.0  0.1 -0.0 -0.9 -0.1 -0.1 
 0.0 -0.2 -0.2 -0.8  0.0 -0.6  0.6  0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0  0.2  0.0 -0.3 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.472314
pred: tensor([8, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 8, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 1,
        3, 3, 8, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 3,
        1, 1, 8, 8, 3, 8, 1, 8, 3, 1, 1, 1, 1, 3, 8, 1]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(9) 
Accuracy (ratio) tensor(0.1406)
Accuracy:
 [[9]]
Accuracy:
 [[1.]]
 0.0 -0.0  0.5 -0.5 -0.0 -0.3 -1.8 -0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0 -0.5 -0.0  0.0  0.0  0.1 
 0.0  0.5 -0.6  1.0  0.0  0.7 -0.2  0.0  0.0  0.0  0.9  0.0  0.2 -0.0  0.0 -0.2 -0.0  0.6  0.1 -0.0 
 0.0 -1.0 -0.4  0.7  0.0  1.1 -0.0 -0.0  0.0  0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.1  0.0  0.2  0.0  0.0 
-0.0  1.0 -0.2 -0.8 -0.0 -0.3  0.6 -0.0  0.0  0.0  0.4 -0.0  0.9 -0.0  0.0  0.3  0.0 -0.6  0.0 -0.2 
 0.0 -0.6  0.1  0.5  0.0  0.1  0.5 -0.0  0.0  0.0 -0.6 -0.0 -1.2  0.0  0.0 -0.1 -0.0 -0.7  0.2  0.1 
 0.0  0.4  1.7  0.8  0.0 -0.0  0.3  0.0  0.0  0.0  0.0 -0.0  0.4 -0.0  0.0  0.2 -0.0 -0.1  0.0  0.0 
-0.0  0.0 -0.8 -1.0  0.0 -0.6  0.4  0.0  0.0  0.0 -0.4  0.0 -0.5 -0.0 -0.0 -0.3 -0.0  1.1  0.0 -0.1 
-0.0 -0.8 -0.5  0.5  0.0 -0.1 -0.1 -0.0  0.0 -0.0  0.5 -0.0  0.3 -0.0 -0.0  0.1 -0.0 -0.9 -0.1 -0.1 
 0.0 -0.2 -0.2 -0.8  0.0 -0.6  0.6  0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0  0.2  0.0 -0.3 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.9268239
pred: tensor([3, 3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 1, 3, 1, 3, 1,
        1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 8, 1, 3, 3, 1, 3, 3,
        3, 1, 1, 3, 1, 3, 1, 3, 3, 8, 3, 3, 1, 3, 3, 1]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0 -0.0  0.5 -0.5 -0.0 -0.3 -1.8 -0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0 -0.5 -0.0  0.0  0.0  0.1 
 0.0  0.5 -0.6  1.0  0.0  0.7 -0.2  0.0  0.0  0.0  0.9  0.0  0.2 -0.0  0.0 -0.2 -0.0  0.6  0.1 -0.0 
 0.0 -1.0 -0.4  0.7  0.0  1.1 -0.0 -0.0  0.0  0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.1  0.0  0.2  0.0  0.0 
-0.0  1.0 -0.2 -0.8 -0.0 -0.3  0.6 -0.0  0.0  0.0  0.4 -0.0  0.9 -0.0  0.0  0.3  0.0 -0.6  0.0 -0.2 
 0.0 -0.6  0.1  0.5  0.0  0.1  0.5 -0.0  0.0  0.0 -0.6 -0.0 -1.2  0.0  0.0 -0.1 -0.0 -0.7  0.2  0.1 
 0.0  0.4  1.7  0.8  0.0 -0.0  0.3  0.0  0.0  0.0  0.0 -0.0  0.4 -0.0  0.0  0.2 -0.0 -0.1  0.0  0.0 
-0.0  0.0 -0.8 -1.0  0.0 -0.6  0.4  0.0  0.0  0.0 -0.4  0.0 -0.5 -0.0 -0.0 -0.3 -0.0  1.1  0.0 -0.1 
-0.0 -0.8 -0.5  0.5  0.0 -0.1 -0.1 -0.0  0.0 -0.0  0.5 -0.0  0.3 -0.0 -0.0  0.1 -0.0 -0.9 -0.1 -0.1 
 0.0 -0.2 -0.2 -0.8  0.0 -0.6  0.6  0.0  0.0  0.0  0.1 -0.0 -0.0 -0.0 -0.0  0.2  0.0 -0.3 -0.0 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.5388322
results (all): {'reconst_0x0_avg': 0.21875, 'reconst_0x0_all': nan, 'cluster_avg': 0.07014498, 'cluster_all': array([0.22121905, 0.12303829, 0.322871  , ..., 0.2216655 , 0.02668661,
       0.07663569], dtype=float32), 'magnitude_avg': -0.12643090634150192, 'magnitude_all': -0.12643090634150192, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.09815950691699982, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.140625, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_2', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.21875, 'cluster_avg': 0.07014498, 'magnitude_avg': -0.12643090634150192, 'tsne-2d_avg': nan, 'mathematics_avg': 0.09815950691699982, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.140625, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 3, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_3', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3', 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_3
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3',
 'run_id': 'vae_cmnist_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([7, 2, 6, 4, 6, 8, 1, 2, 8, 4, 7, 3, 4, 5, 8, 5, 4, 8, 5, 5, 4, 7, 5, 9,
        5, 1, 5, 3, 6, 3, 5, 2, 6, 1, 3, 7, 5, 9, 8, 1, 8, 9, 9, 3, 8, 3, 3, 6,
        7, 5, 3, 1, 5, 2, 7, 6, 1, 5, 2, 7, 5, 8, 8, 2, 7, 8, 1, 4, 3, 1, 3, 4,
        9, 2, 3, 6, 7, 6, 7, 6, 6, 7, 6, 5, 7, 5, 2, 4, 3, 1, 6, 8, 8, 7, 1, 5,
        4, 5, 8, 4, 5, 3, 1, 1, 5, 9, 9, 4, 9, 9, 2, 1, 1, 5, 9, 1, 6, 1, 3, 1,
        2, 7, 3, 6, 5, 7, 4, 5], device='cuda:0') 
label[0]: tensor(7, device='cuda:0') 
label[1]: tensor(2, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.02784052 0.17686573 0.14377996 ... 0.11846527 0.13877013 0.16319501]
Average of silhouette coef: 0.120281786
---
  0   1   2   3
0 0.0 2.8 3.2 2.2 
1 2.8 0.0 3.1 2.2 
2 3.2 3.1 0.0 2.3 
3 2.2 2.2 2.3 0.0 
correlation [[ 1.         -0.39484522]
 [-0.39484522  1.        ]]
---
[[], [], [], []] [3 1 1 ... 1 2 0] [[  7.9767365    4.3178935 ]
 [  6.0710416   28.895638  ]
 [ 35.665745    22.083525  ]
 ...
 [  0.50382495  23.927696  ]
 [ 22.301802   -19.930178  ]
 [-49.24608      4.825696  ]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.120281786, 'cluster_all': array([0.02784052, 0.17686573, 0.14377996, ..., 0.11846527, 0.13877013,
       0.16319501], dtype=float32), 'magnitude_avg': -0.39484522018962337, 'magnitude_all': -0.39484522018962337, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_3', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.120281786, 'magnitude_avg': -0.39484522018962337, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([1, 9, 7, 5, 2, 2, 1, 5, 9, 2, 4, 7, 3, 9, 2, 4, 9, 6, 5, 6, 4, 5, 3, 7,
        9, 6, 4, 2, 8, 1, 6, 2, 2, 4, 6, 9, 1, 7, 8, 4, 4, 5, 2, 3, 6, 4, 9, 9,
        4, 8, 9, 3, 9, 9, 8, 4, 2, 7, 6, 3, 1, 9, 2, 5, 7, 7, 6, 7, 6, 3, 8, 7,
        6, 6, 3, 1, 9, 6, 7, 3, 9, 5, 9, 1, 5, 3, 1, 9, 1, 6, 3, 6, 7, 6, 6, 2,
        2, 9, 8, 6, 8, 1, 8, 7, 6, 4, 6, 9, 4, 9, 3, 1, 2, 2, 9, 9, 8, 8, 9, 9,
        7, 3, 7, 4, 9, 1, 8, 3], device='cuda:0') 
label[0]: tensor(1, device='cuda:0') 
label[1]: tensor(9, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([1, 3, 3, 7, 6, 5, 6, 6, 3, 4, 6, 3, 3, 8, 9, 4, 3, 9, 5, 8, 2, 7, 9, 1,
        4, 1, 8, 1, 1, 4, 3, 8, 9, 6, 1, 1, 9, 3, 2, 8, 3, 4, 5, 5, 1, 6, 5, 8,
        9, 2, 1, 1, 2, 5, 4, 1, 1, 6, 5, 3, 7, 4, 4, 7, 8, 3, 7, 5, 2, 5, 5, 8,
        2, 7, 3, 2, 4, 4, 7, 8, 2, 9, 9, 8, 3, 2, 1, 8, 2, 6, 1, 7, 4, 9, 1, 1,
        5, 1, 5, 1, 7, 2, 2, 1, 4, 6, 1, 3, 1, 6, 8, 5, 6, 2, 7, 7, 4, 1, 8, 1,
        3, 8, 4, 8, 2, 1, 7, 3])
Accuracy (count): tensor(16) 
Accuracy (ratio) tensor(0.1250)
Accuracy:
 [[ 0  0 24  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]
 [ 0  0 13  0  0  0  0  0  0]
 [ 0  0 11  0  0  0  0  0  0]
 [ 0  0 12  0  0  0  0  0  0]
 [ 0  0 15  0  0  0  0  0  0]
 [ 0  0  9  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [ 0.11076126 -0.01403998  0.10179232 ...  0.15443128  0.10090178
 -0.05485919]
Average of silhouette coef: 0.06871143
---
  1   2   3   4   5   6   7   8   9
1 0.0 3.0 3.1 3.2 3.2 3.1 3.1 2.8 2.8 
2 3.0 0.0 2.4 2.7 3.0 2.7 3.1 2.3 2.9 
3 3.1 2.4 0.0 3.5 2.0 3.1 2.9 2.3 2.9 
4 3.2 2.7 3.5 0.0 3.1 2.7 2.8 2.5 1.6 
5 3.2 3.0 2.0 3.1 0.0 2.3 2.8 2.2 2.2 
6 3.1 2.7 3.1 2.7 2.3 0.0 3.6 2.7 2.8 
7 3.1 3.1 2.9 2.8 2.8 3.6 0.0 3.2 1.9 
8 2.8 2.3 2.3 2.5 2.2 2.7 3.2 0.0 1.9 
9 2.8 2.9 2.9 1.6 2.2 2.8 1.9 1.9 0.0 
correlation [[ 1.         -0.04215157]
 [-0.04215157  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [1 9 7 ... 5 9 5] [[-48.097363   30.478989 ]
 [ 16.751514  -22.28529  ]
 [-36.898037   48.2345   ]
 ...
 [  5.0819907  25.601126 ]
 [-52.990955  -14.137242 ]
 [ 11.205172   17.173801 ]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(56) 
Accuracy (ratio) tensor(0.1145)
Accuracy:
 [[ 0  0 45  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 56  0  1  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 61  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 45  0  0  0  0  0  0]]
Accuracy:
 [[0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.982 0.    0.018 0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.9  0.0 -1.1 -0.0  0.0  0.4 -0.0 -0.0  0.0 -0.5  0.1  0.2  0.0  0.1 -0.2 -0.0 -1.3  0.0  0.7 -0.0 
 0.0 -0.2  0.6  0.0 -0.0 -0.5 -0.0  0.0  0.0 -0.7  1.3  0.2  0.0 -0.5 -0.7 -0.0 -0.1  0.0 -0.4 -0.0 
 1.2  0.0  0.6 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.0  0.4 -0.0 -0.0  0.5  0.3  0.0 -1.1 -0.0 -0.5 -0.0 
-0.6  0.0 -0.2 -0.0 -0.0  0.1 -0.0  0.0 -0.0  0.6  0.1 -0.3  0.0 -1.2 -0.1  0.0  1.1 -0.0  0.1 -0.0 
 0.6  0.2  0.5 -0.0  0.0 -0.3 -0.0  0.0 -0.0 -0.1 -0.9  0.7 -0.0  0.6  0.7  0.0  0.0 -0.0 -0.2  0.0 
-0.8 -0.0  0.3 -0.0 -0.0 -1.7 -0.0 -0.0 -0.0 -0.1 -0.6 -0.3 -0.0  0.2  0.1 -0.0  0.1  0.0 -0.2  0.0 
-0.0 -0.0 -0.3 -0.0 -0.0  0.9 -0.0  0.0  0.0  0.5  0.9  0.2  0.0  1.1  0.3  0.0  0.9  0.0  0.5  0.0 
 0.5 -0.1  0.4  0.0 -0.0  0.3 -0.0  0.0  0.0 -0.1 -0.7 -0.0  0.0 -0.6 -0.9 -0.0 -0.3  0.0 -0.3 -0.0 
-0.1 -0.0 -0.2 -0.0 -0.0  0.7  0.0  0.0 -0.0  0.6 -0.5  0.1  0.0  0.1 -0.2  0.0  0.8  0.0  0.2  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.9413645
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.9  0.0 -1.1 -0.0  0.0  0.4 -0.0 -0.0  0.0 -0.5  0.1  0.2  0.0  0.1 -0.2 -0.0 -1.3  0.0  0.7 -0.0 
 0.0 -0.2  0.6  0.0 -0.0 -0.5 -0.0  0.0  0.0 -0.7  1.3  0.2  0.0 -0.5 -0.7 -0.0 -0.1  0.0 -0.4 -0.0 
 1.2  0.0  0.6 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.0  0.4 -0.0 -0.0  0.5  0.3  0.0 -1.1 -0.0 -0.5 -0.0 
-0.6  0.0 -0.2 -0.0 -0.0  0.1 -0.0  0.0 -0.0  0.6  0.1 -0.3  0.0 -1.2 -0.1  0.0  1.1 -0.0  0.1 -0.0 
 0.6  0.2  0.5 -0.0  0.0 -0.3 -0.0  0.0 -0.0 -0.1 -0.9  0.7 -0.0  0.6  0.7  0.0  0.0 -0.0 -0.2  0.0 
-0.8 -0.0  0.3 -0.0 -0.0 -1.7 -0.0 -0.0 -0.0 -0.1 -0.6 -0.3 -0.0  0.2  0.1 -0.0  0.1  0.0 -0.2  0.0 
-0.0 -0.0 -0.3 -0.0 -0.0  0.9 -0.0  0.0  0.0  0.5  0.9  0.2  0.0  1.1  0.3  0.0  0.9  0.0  0.5  0.0 
 0.5 -0.1  0.4  0.0 -0.0  0.3 -0.0  0.0  0.0 -0.1 -0.7 -0.0  0.0 -0.6 -0.9 -0.0 -0.3  0.0 -0.3 -0.0 
-0.1 -0.0 -0.2 -0.0 -0.0  0.7  0.0  0.0 -0.0  0.6 -0.5  0.1  0.0  0.1 -0.2  0.0  0.8  0.0  0.2  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.45654
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.9  0.0 -1.1 -0.0  0.0  0.4 -0.0 -0.0  0.0 -0.5  0.1  0.2  0.0  0.1 -0.2 -0.0 -1.3  0.0  0.7 -0.0 
 0.0 -0.2  0.6  0.0 -0.0 -0.5 -0.0  0.0  0.0 -0.7  1.3  0.2  0.0 -0.5 -0.7 -0.0 -0.1  0.0 -0.4 -0.0 
 1.2  0.0  0.6 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.0  0.4 -0.0 -0.0  0.5  0.3  0.0 -1.1 -0.0 -0.5 -0.0 
-0.6  0.0 -0.2 -0.0 -0.0  0.1 -0.0  0.0 -0.0  0.6  0.1 -0.3  0.0 -1.2 -0.1  0.0  1.1 -0.0  0.1 -0.0 
 0.6  0.2  0.5 -0.0  0.0 -0.3 -0.0  0.0 -0.0 -0.1 -0.9  0.7 -0.0  0.6  0.7  0.0  0.0 -0.0 -0.2  0.0 
-0.8 -0.0  0.3 -0.0 -0.0 -1.7 -0.0 -0.0 -0.0 -0.1 -0.6 -0.3 -0.0  0.2  0.1 -0.0  0.1  0.0 -0.2  0.0 
-0.0 -0.0 -0.3 -0.0 -0.0  0.9 -0.0  0.0  0.0  0.5  0.9  0.2  0.0  1.1  0.3  0.0  0.9  0.0  0.5  0.0 
 0.5 -0.1  0.4  0.0 -0.0  0.3 -0.0  0.0  0.0 -0.1 -0.7 -0.0  0.0 -0.6 -0.9 -0.0 -0.3  0.0 -0.3 -0.0 
-0.1 -0.0 -0.2 -0.0 -0.0  0.7  0.0  0.0 -0.0  0.6 -0.5  0.1  0.0  0.1 -0.2  0.0  0.8  0.0  0.2  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.9875278
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.9  0.0 -1.1 -0.0  0.0  0.4 -0.0 -0.0  0.0 -0.5  0.1  0.2  0.0  0.1 -0.2 -0.0 -1.3  0.0  0.7 -0.0 
 0.0 -0.2  0.6  0.0 -0.0 -0.5 -0.0  0.0  0.0 -0.7  1.3  0.2  0.0 -0.5 -0.7 -0.0 -0.1  0.0 -0.4 -0.0 
 1.2  0.0  0.6 -0.0 -0.0 -0.1  0.0  0.0  0.0  0.0  0.4 -0.0 -0.0  0.5  0.3  0.0 -1.1 -0.0 -0.5 -0.0 
-0.6  0.0 -0.2 -0.0 -0.0  0.1 -0.0  0.0 -0.0  0.6  0.1 -0.3  0.0 -1.2 -0.1  0.0  1.1 -0.0  0.1 -0.0 
 0.6  0.2  0.5 -0.0  0.0 -0.3 -0.0  0.0/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
 -0.0 -0.1 -0.9  0.7 -0.0  0.6  0.7  0.0  0.0 -0.0 -0.2  0.0 
-0.8 -0.0  0.3 -0.0 -0.0 -1.7 -0.0 -0.0 -0.0 -0.1 -0.6 -0.3 -0.0  0.2  0.1 -0.0  0.1  0.0 -0.2  0.0 
-0.0 -0.0 -0.3 -0.0 -0.0  0.9 -0.0  0.0  0.0  0.5  0.9  0.2  0.0  1.1  0.3  0.0  0.9  0.0  0.5  0.0 
 0.5 -0.1  0.4  0.0 -0.0  0.3 -0.0  0.0  0.0 -0.1 -0.7 -0.0  0.0 -0.6 -0.9 -0.0 -0.3  0.0 -0.3 -0.0 
-0.1 -0.0 -0.2 -0.0 -0.0  0.7  0.0  0.0 -0.0  0.6 -0.5  0.1  0.0  0.1 -0.2  0.0  0.8  0.0  0.2  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.4351525
results (all): {'reconst_0x0_avg': 0.125, 'reconst_0x0_all': nan, 'cluster_avg': 0.06871143, 'cluster_all': array([ 0.11076126, -0.01403998,  0.10179232, ...,  0.15443128,
        0.10090178, -0.05485919], dtype=float32), 'magnitude_avg': -0.04215156869205944, 'magnitude_all': -0.04215156869205944, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11451942473649979, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_3', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.125, 'cluster_avg': 0.06871143, 'magnitude_avg': -0.04215156869205944, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11451942473649979, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_CMNIST', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_cmnist_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4', 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_cmnist_seed_4
Run Directory:
 ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_CMNIST',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4',
 'run_id': 'vae_cmnist_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                   [-1, 20]           8,020
            Linear-4                   [-1, 20]           8,020
               Enc-5       [[-1, 20], [-1, 20]]               0
            Linear-6               [-1, 2, 400]           8,400
              ReLU-7               [-1, 2, 400]               0
            Linear-8              [-1, 2, 2352]         943,152
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_CMNIST from ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([2, 1, 3, 3, 9, 9, 8, 1, 7, 3, 4, 4, 9, 6, 1, 7, 2, 2, 7, 4, 2, 3, 6, 8,
        1, 1, 9, 9, 2, 4, 7, 3, 4, 9, 7, 4, 9, 6, 8, 1, 5, 9, 8, 1, 8, 4, 9, 3,
        8, 5, 6, 8, 8, 7, 8, 7, 3, 6, 8, 8, 7, 2, 8, 7, 6, 4, 2, 5, 8, 4, 3, 2,
        1, 7, 8, 1, 8, 2, 8, 6, 2, 1, 8, 8, 3, 1, 6, 9, 1, 5, 1, 6, 2, 7, 3, 8,
        1, 2, 7, 7, 2, 3, 8, 3, 1, 1, 1, 5, 4, 3, 3, 3, 4, 1, 9, 5, 1, 3, 2, 2,
        4, 8, 5, 8, 8, 6, 7, 1], device='cuda:0') 
label[0]: tensor(2, device='cuda:0') 
label[1]: tensor(1, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.17090064 0.14217371 0.12468472 ... 0.1898802  0.15447421 0.1998414 ]
Average of silhouette coef: 0.12015319
---
  0   1   2   3
0 0.0 2.9 2.9 2.1 
1 2.9 0.0 3.0 2.2 
2 2.9 3.0 0.0 2.2 
3 2.1 2.2 2.2 0.0 
correlation [[ 1.         -0.51859035]
 [-0.51859035  1.        ]]
---
[[], [], [], []] [2 2 2 ... 1 0 2] [[ -4.1191106  46.523354 ]
 [ 68.33551    17.740889 ]
 [ 28.95538    11.975154 ]
 ...
 [-43.766586   -8.468187 ]
 [ 32.665813  -51.18677  ]
 [ 33.593273    5.8043885]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.12015319, 'cluster_all': array([0.17090064, 0.14217371, 0.12468472, ..., 0.1898802 , 0.15447421,
       0.1998414 ], dtype=float32), 'magnitude_avg': -0.5185903500960783, 'magnitude_all': -0.5185903500960783, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_4', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.12015319, 'magnitude_avg': -0.5185903500960783, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'torch.Tensor'> 
label: tensor([5, 9, 3, 7, 1, 8, 7, 1, 9, 9, 8, 9, 9, 7, 3, 1, 2, 9, 5, 2, 5, 9, 9, 7,
        1, 4, 9, 1, 6, 9, 1, 8, 6, 7, 3, 2, 8, 2, 2, 1, 4, 4, 9, 6, 4, 3, 3, 4,
        2, 1, 1, 7, 1, 5, 8, 9, 2, 9, 8, 4, 8, 7, 4, 4, 7, 9, 3, 3, 7, 8, 9, 5,
        6, 4, 8, 6, 6, 3, 1, 2, 7, 7, 5, 9, 9, 9, 8, 8, 4, 2, 2, 7, 3, 1, 3, 2,
        7, 2, 9, 9, 8, 4, 4, 4, 3, 6, 2, 2, 7, 7, 3, 3, 5, 4, 6, 4, 1, 6, 3, 8,
        3, 3, 2, 6, 2, 6, 3, 5], device='cuda:0') 
label[0]: tensor(5, device='cuda:0') 
label[1]: tensor(9, device='cuda:0') 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 1, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 3, 6, 6]) 
tar: tensor([4, 1, 5, 6, 2, 1, 6, 3, 4, 3, 7, 5, 2, 3, 2, 2, 1, 1, 8, 1, 3, 6, 1, 3,
        1, 6, 4, 9, 2, 5, 9, 2, 4, 3, 4, 3, 7, 4, 3, 1, 5, 4, 2, 8, 2, 8, 8, 7,
        2, 1, 9, 3, 9, 6, 6, 7, 9, 9, 6, 9, 1, 7, 2, 4, 1, 4, 9, 2, 4, 1, 9, 1,
        9, 5, 5, 3, 2, 9, 4, 5, 1, 3, 7, 2, 1, 9, 2, 7, 9, 8, 1, 3, 3, 9, 3, 3,
        1, 8, 1, 1, 3, 3, 8, 2, 4, 3, 9, 3, 7, 2, 7, 5, 9, 6, 2, 2, 3, 7, 4, 7,
        8, 2, 4, 8, 7, 6, 9, 2])
Accuracy (count): tensor(8) 
Accuracy (ratio) tensor(0.0625)
Accuracy:
 [[ 0  0  0  0  0 19  0  0  0]
 [ 0  0  0  0  0 20  0  0  0]
 [ 0  0  2  0  0 18  0  0  0]
 [ 0  0  1  0  0 13  0  0  0]
 [ 0  0  1  0  0  7  0  0  0]
 [ 1  0  2  0  0  6  0  0  0]
 [ 0  0  0  0  0 12  0  0  0]
 [ 0  0  0  0  0  9  0  0  0]
 [ 0  0  1  0  0 16  0  0  0]]
Accuracy:
 [[0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.1   0.    0.    0.9   0.    0.    0.   ]
 [0.    0.    0.071 0.    0.    0.929 0.    0.    0.   ]
 [0.    0.    0.125 0.    0.    0.875 0.    0.    0.   ]
 [0.111 0.    0.222 0.    0.    0.667 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    1.    0.    0.    0.   ]
 [0.    0.    0.059 0.    0.    0.941 0.    0.    0.   ]]
---
Silhouette values: [-0.11438243  0.10709383 -0.04138448 ...  0.19109997  0.28172952
  0.12805593]
Average of silhouette coef: 0.07190026
---
  1   2   3   4   5   6   7   8   9
1 0.0 2.9 3.0 3.0 3.0 3.3 3.0 2.5 2.7 
2 2.9 0.0 2.4 2.9 3.0 2.8 3.1 2.4 3.1 
3 3.0 2.4 0.0 3.4 2.0 3.3 2.7 2.0 2.8 
4 3.0 2.9 3.4 0.0 2.8 2.6 2.6 2.3 1.6 
5 3.0 3.0 2.0 2.8 0.0 2.6 2.9 1.6 2.3 
6 3.3 2.8 3.3 2.6 2.6 0.0 3.8 2.7 3.2 
7 3.0 3.1 2.7 2.6 2.9 3.8 0.0 2.8 1.9 
8 2.5 2.4 2.0 2.3 1.6 2.7 2.8 0.0 1.8 
9 2.7 3.1 2.8 1.6 2.3 3.2 1.9 1.8 0.0 
correlation [[ 1.         -0.06074473]
 [-0.06074473  1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [5 9 3 ... 6 1 2] [[ 31.15951   -13.516655 ]
 [ 47.69011    24.394714 ]
 [ 37.67493    -4.6046286]
 ...
 [  6.7368345   2.0575304]
 [ 27.55296   -56.169106 ]
 [-22.115215   50.763542 ]]
saved ./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 3, 6, 6, 3, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6,
        6, 6, 6, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6,
        6, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 3, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 1, 3, 6, 6, 6, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(58) 
Accuracy (ratio) tensor(0.1186)
Accuracy:
 [[ 1  0  1  0  0 43  0  0  0]
 [ 0  0  1  0  0 51  0  0  0]
 [ 0  0  1  0  0 56  0  0  0]
 [ 1  0  4  0  0 55  0  0  0]
 [ 1  0  6  0  0 54  0  0  0]
 [ 2  0  2  0  0 56  0  0  0]
 [ 5  0  2  0  0 50  0  0  0]
 [ 1  0  2  0  0 49  0  0  0]
 [ 1  0  1  0  0 43  0  0  0]]
Accuracy:
 [[0.022 0.    0.022 0.    0.    0.956 0.    0.    0.   ]
 [0.    0.    0.019 0.    0.    0.981 0.    0.    0.   ]
 [0.    0.    0.018 0.    0.    0.982 0.    0.    0.   ]
 [0.017 0.    0.067 0.    0.    0.917 0.    0.    0.   ]
 [0.016 0.    0.098 0.    0.    0.885 0.    0.    0.   ]
 [0.033 0.    0.033 0.    0.    0.933 0.    0.    0.   ]
 [0.088 0.    0.035 0.    0.    0.877 0.    0.    0.   ]
 [0.019 0.    0.038 0.    0.    0.942 0.    0.    0.   ]
 [0.022 0.    0.022 0.    0.    0.956 0.    0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.4 -0.0  0.5 -0.0  0.0 -1.7  0.2 -0.0 -0.7  0.0 -0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.0 -0.0 -0.0 
-0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.2  1.3 -0.4  0.6  0.0 -0.0  0.0 -0.4  0.0 -0.1 -0.3 -0.0  0.0  0.5 
-0.0  0.2  0.0 -0.1 -0.0  0.0  0.4 -0.3 -0.3 -0.0  0.0  0.0 -0.0 -1.1  0.1 -0.1 -0.1 -0.0 -0.0  1.3 
-0.0 -0.2  0.0  0.1 -0.0  0.0  0.7  0.4  0.1 -0.0 -0.0  0.0  0.0  1.0  0.4  0.4  0.2  0.0 -0.0 -1.1 
-0.0  0.2 -0.0 -0.1 -0.0  0.0  0.4 -1.1 -0.2  0.5  0.0 -0.0  0.0  0.3 -0.8  0.1  0.1 -0.0  0.0  0.6 
 0.0 -1.4  0.0  0.1 -0.0  0.0  0.0 -0.2  1.2  0.4  0.0  0.0  0.0  1.1 -0.1  0.0  0.2  0.0  0.0  0.5 
-0.0  0.6 -0.0 -0.1 -0.0  0.0  0.4  0.1  0.2 -0.2  0.0  0.0 -0.0 -1.2 -0.3 -0.2  0.2  0.0  0.0 -1.3 
-0.0  0.7 -0.0 -0.3 -0.0  0.0  0.2  0.0 -0.1  0.3  0.0 -0.0  0.0  0.5  0.0  0.2 -0.5  0.0 -0.0  0.5 
-0.0  0.9 -0.0 -0.0 -0.0  0.0  0.7 -0.0 -0.1 -0.2  0.0 -0.0  0.0  0.5  0.2 -0.3  0.1  0.0 -0.0 -0.8 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 1.783346
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.4 -0.0  0.5 -0.0  0.0 -1.7  0.2 -0.0 -0.7  0.0 -0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.0 -0.0 -0.0 
-0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.2  1.3 -0.4  0.6  0.0 -0.0  0.0 -0.4  0.0 -0.1 -0.3 -0.0  0.0  0.5 
-0.0  0.2  0.0 -0.1 -0.0  0.0  0.4 -0.3 -0.3 -0.0  0.0  0.0 -0.0 -1.1  0.1 -0.1 -0.1 -0.0 -0.0  1.3 
-0.0 -0.2  0.0  0.1 -0.0  0.0  0.7  0.4  0.1 -0.0 -0.0  0.0  0.0  1.0  0.4  0.4  0.2  0.0 -0.0 -1.1 
-0.0  0.2 -0.0 -0.1 -0.0  0.0  0.4 -1.1 -0.2  0.5  0.0 -0.0  0.0  0.3 -0.8  0.1  0.1 -0.0  0.0  0.6 
 0.0 -1.4  0.0  0.1 -0.0  0.0  0.0 -0.2  1.2  0.4  0.0  0.0  0.0  1.1 -0.1  0.0  0.2  0.0  0.0  0.5 
-0.0  0.6 -0.0 -0.1 -0.0  0.0  0.4  0.1  0.2 -0.2  0.0  0.0 -0.0 -1.2 -0.3 -0.2  0.2  0.0  0.0 -1.3 
-0.0  0.7 -0.0 -0.3 -0.0  0.0  0.2  0.0 -0.1  0.3  0.0 -0.0  0.0  0.5  0.0  0.2 -0.5  0.0 -0.0  0.5 
-0.0  0.9 -0.0 -0.0 -0.0  0.0  0.7 -0.0 -0.1 -0.2  0.0 -0.0  0.0  0.5  0.2 -0.3  0.1  0.0 -0.0 -0.8 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 2.255587
pred: tensor([1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.4 -0.0  0.5 -0.0  0.0 -1.7  0.2 -0.0 -0.7  0.0 -0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.0 -0.0 -0.0 
-0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.2  1.3 -0.4  0.6  0.0 -0.0  0.0 -0.4  0.0 -0.1 -0.3 -0.0  0.0  0.5 
-0.0  0.2  0.0 -0.1 -0.0  0.0  0.4 -0.3 -0.3 -0.0  0.0  0.0 -0.0 -1.1  0.1 -0.1 -0.1 -0.0 -0.0  1.3 
-0.0 -0.2  0.0  0.1 -0.0  0.0  0.7  0.4  0.1 -0.0 -0.0  0.0  0.0  1.0  0.4  0.4  0.2  0.0 -0.0 -1.1 
-0.0  0.2 -0.0 -0.1 -0.0  0.0  0.4 -1.1 -0.2  0.5  0.0 -0.0  0.0  0.3 -0.8  0.1  0.1 -0.0  0.0  0.6 
 0.0 -1.4  0.0  0.1 -0.0  0.0  0.0 -0.2  1.2  0.4  0.0  0.0  0.0  1.1 -0.1  0.0  0.2  0.0  0.0  0.5 
-0.0  0.6 -0.0 -0.1 -0.0  0.0  0.4  0.1  0.2 -0.2  0.0  0.0 -0.0 -1.2 -0.3 -0.2  0.2  0.0  0.0 -1.3 
-0.0  0.7 -0.0 -0.3 -0.0  0.0  0.2  0.0 -0.1  0.3  0.0 -0.0  0.0  0.5  0.0  0.2 -0.5  0.0 -0.0  0.5 
-0.0  0.9 -0.0 -0.0 -0.0  0.0  0.7 -0.0 -0.1 -0.2  0.0 -0.0  0.0  0.5  0.2 -0.3  0.1  0.0 -0.0 -0.8 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 2.9242308
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(62) 
Accuracy (ratio) tensor(0.9688)
Accuracy:
 [[62]]
Accuracy:
 [[1.]]
-0.0  0.4 -0.0  0.5 -0.0  0.0 -1.7  0.2 -0.0 -0.7  0.0 -0.0 -0.0  0.1 -0.1 -0.0 -0.0  0.0 -0.0 -0.0 
-0.0 -1.0 -0.0 -0.2 -0.0  0.0 -0.2  1.3 -0.4  0.6  0.0 -0.0  0.0 -0.4  0.0 -0.1 -0.3 -0.0  0.0  0.5 
-0.0  0.2  0.0 -0.1 -0.0  0.0  0.4 -0.3 -0.3 -0.0  0.0  0.0 -0.0 -1.1  0.1 -0.1 -0.1 -0.0 -0.0  1.3 
-0.0 -0.2  0.0  0.1 -0.0  0.0  0.7  0.4  0.1 -0.0 -0.0  0.0  0.0  1.0  0.4  0.4  0.2  0.0 -0.0 -1.1 
-0.0  0.2 -0.0 -0.1 -0.0  0.0  0.4 -1.1 -0.2  0.5  0.0 -0.0  0.0  0.3 -0.8  0.1  0.1 -0.0  0.0  0.6 
 0.0 -1.4  0.0  0.1 -0.0  0.0  0.0 -0.2  1.2  0.4  0.0  0.0  0.0  1.1 -0.1  0.0  0.2  0.0  0.0  0.5 
-0.0  0.6 -0.0 -0.1 -0.0  0.0  0.4  0.1  0.2 -0.2  0.0  0.0 -0.0 -1.2 -0.3 -0.2  0.2  0.0  0.0 -1.3 
-0.0  0.7 -0.0 -0.3 -0.0  0.0  0.2  0.0 -0.1  0.3  0.0 -0.0  0.0  0.5  0.0  0.2 -0.5  0.0 -0.0  0.5 
-0.0  0.9 -0.0 -0.0 -0.0  0.0  0.7 -0.0 -0.1 -0.2  0.0 -0.0  0.0  0.5  0.2 -0.3  0.1  0.0 -0.0 -0.8 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 2.4345574
results (all): {'reconst_0x0_avg': 0.0625, 'reconst_0x0_all': nan, 'cluster_avg': 0.07190026, 'cluster_all': array([-0.11438243,  0.10709383, -0.04138448, ...,  0.19109997,
        0.28172952,  0.12805593], dtype=float32), 'magnitude_avg': -0.060744730153212724, 'magnitude_all': -0.060744730153212724, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11860940605401993, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.96875, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_cmnist_seed_4', 'model_name': 'VAE_CMNIST', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.0625, 'cluster_avg': 0.07190026, 'magnitude_avg': -0.060744730153212724, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11860940605401993, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.96875}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 0, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_0', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0', 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/train'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment'/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
: 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0',
 'run_id': 'vae_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r2j', 'g7j', 'g3t', 'w8j', 'w7j', 'g3s', 'w4t', 'r8s', 'r8s', 'w6j', 'w9t', 'b5j', 'r4s', 'b5j', 'w6t', 'w9s', 'b3j', 'g7t', 'w2t', 'b2t', 'g2t', 'b1s', 'w5t', 'b3s', 'r8s', 'g3j', 'g9t', 'b9t', 'b7s', 'r8t', 'r6j', 'g1t', 'r3j', 'r4j', 'r1t', 'g9t', 'r8t', 'r4j', 'g7t', 'g5s', 'g4s', 'g2t', 'w7t', 'b5t', 'w8s', 'r4t', 'w5j', 'w4j', 'w1t', 'w5j', 'b1j', 'b3j', 'b1t', 'w3t', 'g3s', 'w9j', 'r4s', 'r1s', 'g5j', 'r9j', 'w9t', 'r5j', 'b2j', 'w2t', 'w1j', 'b1j', 'b3s', 'r5j', 'w5s', 'b1j', 'g9s', 'r4t', 'w7s', 'g4s', 'w9t', 'r6t', 'b4t', 'b9j', 'g1s', 'b3j', 'w1j', 'g9s', 'b5t', 'w1s', 'b4t', 'r8j', 'w7t', 'g4j', 'g7t', 'r5t', 'b2s', 'b3s', 'b1s', 'w4t', 'r6s', 'r9t', 'w1t', 'r1s', 'w7j', 'g2s', 'w2j', 'w3s', 'b6s', 'b8s', 'w3t', 'r5j', 'g1t', 'r3t', 'w7s', 'b8j', 'g2j', 'w1j', 'w9s', 'w6s', 'g3t', 'b9t', 'w6s', 'b7j', 'g3j', 'w2s', 'w1j', 'b9s', 'r7j', 'w6j', 'r2t', 'r1t', 'r2t', 'g7t') 
label[0]: r2j 
label[1]: g7j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.05726913  0.01434292  0.02328787 ...  0.03763659 -0.0422841
  0.06955914]
Average of silhouette coef: 0.0200642
---
  0   1   2   3
0 0.0 1.7 3.2 1.3 
1 1.7 0.0 2.0 2.3 
2 3.2 2.0 0.0 4.1 
3 1.3 2.3 4.1 0.0 
correlation [[ 1.         -0.39840301]
 [-0.39840301  1.        ]]
---
[[], [], [], []] [3 2 2 ... 2 1 3] [[-42.87328   -11.0703   ]
 [ 36.800945  -25.870369 ]
 [ -3.5145867 -21.417025 ]
 ...
 [ 14.305496  -45.075092 ]
 [-30.20195    24.437006 ]
 [ -1.7782053  23.955004 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.0200642, 'cluster_all': array([ 0.05726913,  0.01434292,  0.02328787, ...,  0.03763659,
       -0.0422841 ,  0.06955914], dtype=float32), 'magnitude_avg': -0.3984030060941867, 'magnitude_all': -0.3984030060941867, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_0', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.0200642, 'magnitude_avg': -0.3984030060941867, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('b6t', 'g6j', 'w4t', 'b7s', 'g4s', 'w7j', 'b8j', 'w6t', 'b2j', 'g3s', 'w4j', 'g6t', 'b9j', 'r9j', 'w8j', 'w1t', 'r2j', 'w2s', 'r2s', 'b7j', 'r2s', 'r1j', 'w3j', 'b2j', 'g4j', 'g8j', 'r2s', 'g8j', 'b4t', 'g2t', 'b8s', 'r9t', 'b1s', 'b1t', 'b8t', 'b2s', 'w8t', 'w1t', 'r5t', 'g6j', 'r3t', 'b6s', 'r7s', 'r9s', 'w9t', 'w4t', 'w6j', 'r6j', 'w8s', 'b2j', 'r2t', 'r1s', 'b5s', 'b2t', 'g9j', 'g8j', 'g7s', 'r6t', 'g7s', 'r3t', 'w3t', 'w1j', 'g3t', 'b6s', 'g3t', 'g8s', 'w1s', 'w6j', 'g3j', 'b6t', 'g1j', 'b6j', 'w9s', 'b7j', 'b7j', 'r9t', 'g4j', 'g1s', 'w6j', 'r8s', 'b1j', 'b6j', 'r6s', 'r8j', 'w8t', 'r1s', 'w4j', 'r9t', 'r3j', 'r1t', 'g3s', 'r1j', 'b6j', 'w3t', 'r2s', 'g2t', 'r3s', 'g4j', 'g6s', 'b6j', 'b9j', 'w3j', 'b8j', 'w7j', 'r2s', 'g7t', 'r1j', 'r5j', 'g2t', 'g6j', 'r3t', 'r9t', 'w4t', 'b9t', 'w3s', 'g1j', 'r8s', 'g2j', 'g1t', 'b6t', 'b5t', 'w1j', 'r6j', 'g3s', 'r9s', 'b4j', 'w7s', 'r7t') 
label[0]: b6t 
label[1]: g6j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([8, 3, 5, 7, 4, 9, 9, 1, 1, 7, 3, 5, 2, 3, 4, 6, 4, 3, 2, 8, 1, 1, 6, 5,
        6, 9, 8, 1, 9, 7, 7, 6, 5, 1, 2, 2, 7, 9, 7, 3, 8, 3, 2, 5, 2, 8, 2, 9,
        1, 9, 7, 5, 3, 8, 5, 5, 6, 2, 3, 6, 8, 6, 9, 5, 1, 1, 9, 1, 7, 5, 6, 6,
        2, 1, 9, 4, 1, 7, 7, 5, 3, 9, 4, 1, 7, 1, 4, 1, 5, 4, 1, 3, 2, 5, 4, 6,
        9, 7, 6, 4, 5, 2, 6, 4, 7, 6, 6, 5, 6, 9, 2, 2, 4, 8, 3, 7, 4, 9, 8, 7,
        7, 7, 1, 3, 6, 1, 5, 1])
Accuracy (count): tensor(19) 
Accuracy (ratio) tensor(0.1484)
Accuracy:
 [[19  0  0  0  0  0  0  0  0]
 [13  0  0  0  0  0  0  0  0]
 [12  0  0  0  0  0  0  0  0]
 [12  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]
 [17  0  0  0  0  0  0  0  0]
 [ 9  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [-0.19173515 -0.19139305 -0.13342321 ... -0.1604291  -0.09324714
 -0.13029103]
Average of silhouette coef: -0.09224087
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.4 0.5 0.6 0.8 0.8 1.1 1.2 1.7 
2 0.4 0.0 0.5 0.5 0.6 0.7 0.9 1.0 1.5 
3 0.5 0.5 0.0 0.4 0.5 0.5 0.9 0.9 1.4 
4 0.6 0.5 0.4 0.0 0.6 0.6 0.8 0.8 1.3 
5 0.8 0.6 0.5 0.6 0.0 0.5 0.5 0.6 1.0 
6 0.8 0.7 0.5 0.6 0.5 0.0 0.6 0.7 1.0 
7 1.1 0.9 0.9 0.8 0.5 0.6 0.0 0.2 0.7 
8 1.2 1.0 0.9 0.8 0.6 0.7 0.2 0.0 0.7 
9 1.7 1.5 1.4 1.3 1.0 1.0 0.7 0.7 0.0 
correlation [[1.         0.90271461]
 [0.90271461 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [6 6 4 ... 3 7 6] [[ 19.77022  -39.16905 ]
 [-39.472927  34.462585]
 [ 20.78644    3.470476]
 ...
 [ 35.75279   32.253063]
 [-22.779861 -47.222878]
 [-26.461262 -42.236645]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[45  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [61  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [45  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.1  0.0  0.0 -0.2 -0.0 -0.4 -0.1  0.2 -0.1  0.1 -0.0 -0.1 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0 -0.2 -0.0 -0.2 -0.1 -0.0  0.1 -0.0 -0.0 -0.1 -0.1  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0  0.0 -0.0 -0.1  0.1  0.1 -0.3  0.1 -0.0 -0.0 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
-0.1 -0.1  0.0  0.1 -0.0  0.0  0.2  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0  0.0 
 0.1  0.1  0.0  0.0  0.0  0.2 -0.1  0.1 -0.1  0.0 -0.0  0.0  0.1 -0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.2  0.0  0.2  0.1  0.0 -0.1 -0.0 -0.0  0.3 -0.1 /new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
-0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.1 -0.0  0.0  0.1  0.0  0.5 -0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.3  0.0  0.0  0.0  0.0 -0.0  0.0 
-0.0 -0.0 -0.0  0.1  0.0  0.5  0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.4  0.0 -0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0 -0.0  0.1  0.0  1.1 -0.0 -0.0  0.0 -0.0  0.0  0.3  0.0 -0.3 -0.0 -0.0 -0.0  0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 6 
distance: 0.54321176
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.1  0.0  0.0 -0.2 -0.0 -0.4 -0.1  0.2 -0.1  0.1 -0.0 -0.1 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0 -0.2 -0.0 -0.2 -0.1 -0.0  0.1 -0.0 -0.0 -0.1 -0.1  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0  0.0 -0.0 -0.1  0.1  0.1 -0.3  0.1 -0.0 -0.0 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
-0.1 -0.1  0.0  0.1 -0.0  0.0  0.2  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0  0.0 
 0.1  0.1  0.0  0.0  0.0  0.2 -0.1  0.1 -0.1  0.0 -0.0  0.0  0.1 -0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.2  0.0  0.2  0.1  0.0 -0.1 -0.0 -0.0  0.3 -0.1 -0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.1 -0.0  0.0  0.1  0.0  0.5 -0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.3  0.0  0.0  0.0  0.0 -0.0  0.0 
-0.0 -0.0 -0.0  0.1  0.0  0.5  0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.4  0.0 -0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0 -0.0  0.1  0.0  1.1 -0.0 -0.0  0.0 -0.0  0.0  0.3  0.0 -0.3 -0.0 -0.0 -0.0  0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 5 
distance: 0.40901622
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.1  0.0  0.0 -0.2 -0.0 -0.4 -0.1  0.2 -0.1  0.1 -0.0 -0.1 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0 -0.2 -0.0 -0.2 -0.1 -0.0  0.1 -0.0 -0.0 -0.1 -0.1  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0  0.0 -0.0 -0.1  0.1  0.1 -0.3  0.1 -0.0 -0.0 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
-0.1 -0.1  0.0  0.1 -0.0  0.0  0.2  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0  0.0 
 0.1  0.1  0.0  0.0  0.0  0.2 -0.1  0.1 -0.1  0.0 -0.0  0.0  0.1 -0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.2  0.0  0.2  0.1  0.0 -0.1 -0.0 -0.0  0.3 -0.1 -0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.1 -0.0  0.0  0.1  0.0  0.5 -0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.3  0.0  0.0  0.0  0.0 -0.0  0.0 
-0.0 -0.0 -0.0  0.1  0.0  0.5  0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.4  0.0 -0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0 -0.0  0.1  0.0  1.1 -0.0 -0.0  0.0 -0.0  0.0  0.3  0.0 -0.3 -0.0 -0.0 -0.0  0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.39487466
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.1  0.0  0.0 -0.2 -0.0 -0.4 -0.1  0.2 -0.1  0.1 -0.0 -0.1 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0 -0.2 -0.0 -0.2 -0.1 -0.0  0.1 -0.0 -0.0 -0.1 -0.1  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
 0.1  0.1  0.0  0.0 -0.0 -0.1  0.1  0.1 -0.3  0.1 -0.0 -0.0 -0.2  0.1 -0.0  0.0  0.0 -0.0  0.0  0.0 
-0.1 -0.1  0.0  0.1 -0.0  0.0  0.2  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0  0.0 
 0.1  0.1  0.0  0.0  0.0  0.2 -0.1  0.1 -0.1  0.0 -0.0  0.0  0.1 -0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.2  0.0  0.2  0.1  0.0 -0.1 -0.0 -0.0  0.3 -0.1 -0.1 -0.0  0.0  0.0 -0.0 -0.0  0.0 
 0.1 -0.0  0.0  0.1  0.0  0.5 -0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.3  0.0  0.0  0.0  0.0 -0.0  0.0 
-0.0 -0.0 -0.0  0.1  0.0  0.5  0.1 -0.1 -0.0 -0.1 -0.0  0.1  0.1 -0.4  0.0 -0.0 -0.0  0.0 -0.0  0.0 
 0.0 -0.0 -0.0  0.1  0.0  1.1 -0.0 -0.0  0.0 -0.0  0.0  0.3  0.0 -0.3 -0.0 -0.0 -0.0  0.0 -0.0  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.5034785
results (all): {'reconst_0x0_avg': 0.1484375, 'reconst_0x0_all': nan, 'cluster_avg': -0.09224087, 'cluster_all': array([-0.19173515, -0.19139305, -0.13342321, ..., -0.1604291 ,
       -0.09324714, -0.13029103], dtype=float32), 'magnitude_avg': 0.9027146089534601, 'magnitude_all': 0.9027146089534601, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_0', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1484375, 'cluster_avg': -0.09224087, 'magnitude_avg': 0.9027146089534601, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r7j', 'g2t', 'b2s', 'b8j', 'r8t', 'w4j', 'b1j', 'w9s', 'g8s', 'g2j', 'b1s', 'r4j', 'b9j', 'r6s', 'r3j', 'w3t', 'r4t', 'r1t', 'w3t', 'g3j', 'w5t', 'g7t', 'b5j', 'b8j', 'g2t', 'g2t', 'g7j', 'r3j', 'g9t', 'w8j', 'b2s', 'w6s', 'g6s', 'g8s', 'b8s', 'r8t', 'w3t', 'g6t', 'w8j', 'g9s', 'r3t', 'b7s', 'g1s', 'g8s', 'b1s', 'w7s', 'r3j', 'b3t', 'r2s', 'w7t', 'w3t', 'r2s', 'b2j', 'w5j', 'w8j', 'r5s', 'g2t', 'r1t', 'r2s', 'b8s', 'w8t', 'r7s', 'g9j', 'b5j', 'b7j', 'b4t', 'g2t', 'g7s', 'b5s', 'g4j', 'g8s', 'b8t', 'w4s', 'g8j', 'r7s', 'r1t', 'b6t', 'w5t', 'r2t', 'g3t', 'w5j', 'g3t', 'w8j', 'r3j', 'g3t', 'r6s', 'w1s', 'w2j', 'r4j', 'g1s', 'b6s', 'g5s', 'w2t', 'g9t', 'r1j', 'b8t', 'g7t', 'b6s', 'g9s', 'b9s', 'b8t', 'r4j', 'g4t', 'r7s', 'b3j', 'r3j', 'r6s', 'w5t', 'w2t', 'g2t', 'w8t', 'b6j', 'w3s', 'w9s', 'r2j', 'b4t', 'w3t', 'g4j', 'r8s', 'b8j', 'b7t', 'b5t', 'r5s', 'w9j', 'w1j', 'g4s', 'b5t', 'b9j') 
label[0]: r7j 
label[1]: g2t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.08280855  0.05998278 -0.01671822 ...  0.08364538  0.0922298
  0.01748566]
Average of silhouette coef: 0.080415785
---
  0   1   2
0 0.0 3.0 3.4 
1 3.0 0.0 3.1 
2 3.4 3.1 0.0 
correlation [[1.         0.98625389]
 [0.98625389 1.        ]]
---
[[], [], []] [0 2 1 ... 0 0 1] [[-15.848066    20.167192  ]
 [ 18.006367   -64.453926  ]
 [ -8.523591   -50.323128  ]
 ...
 [-15.054868     8.560955  ]
 [ 47.836483   -24.742788  ]
 [  0.26265097  -4.907805  ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.080415785, 'cluster_all': array([ 0.08280855,  0.05998278, -0.01671822, ...,  0.08364538,
        0.0922298 ,  0.01748566], dtype=float32), 'magnitude_avg': 0.9862538927840526, 'magnitude_all': 0.9862538927840526, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_0', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.080415785, 'magnitude_avg': 0.9862538927840526, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 1, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_1', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1', 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1',
 'run_id': 'vae_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('w4t', 'g8s', 'g2j', 'b2t', 'b4t', 'w2t', 'b8s', 'g1j', 'b3t', 'g5t', 'g2t', 'w8j', 'w2t', 'w3t', 'g7s', 'r5t', 'g8t', 'g5t', 'g3j', 'g8s', 'b8t', 'b2s', 'w3t', 'r2j', 'b3t', 'b5t', 'b8j', 'w5j', 'g6s', 'g1j', 'g8t', 'g5t', 'w5j', 'w6j', 'w2s', 'g3j', 'b9j', 'b9j', 'r7j', 'r5j', 'b9j', 'r8t', 'r8j', 'b4t', 'b1j', 'r6j', 'g1j', 'b7s', 'r9s', 'w6j', 'r6t', 'g4t', 'b6j', 'b6j', 'g4t', 'w6t', 'g6t', 'w5j', 'g6t', 'r2s', 'w8t', 'b1s', 'g6t', 'b2s', 'g6t', 'b8t', 'g2j', 'g9j', 'b1s', 'r5j', 'r1j', 'b6j', 'g8j', 'g6j', 'r6t', 'r6t', 'w8s', 'b4t', 'g3j', 'b9t', 'b9s', 'g3s', 'w7s', 'g4s', 'r8s', 'g2s', 'g8s', 'g9j', 'w6s', 'w9t', 'b7s', 'w2j', 'w2s', 'r2j', 'w4j', 'r2s', 'w1j', 'w8j', 'r5s', 'w7s', 'g8j', 'w8j', 'r1s', 'g1t', 'b4s', 'g7j', 'w1j', 'b7j', 'g1s', 'w3j', 'w6s', 'b1s', 'r6t', 'b6j', 'g3t', 'g6j', 'w5s', 'w6t', 'b3t', 'b3s', 'g8j', 'b7s', 'g9s', 'g4s', 'g1t', 'b7s', 'b3j', 'b6j') 
label[0]: w4t 
label[1]: g8s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.11570732  0.11559043  0.0221058  ...  0.01228391 -0.0043418
 -0.02005471]
Average of silhouette coef: 0.023362186
---
  0   1   2   3
0 0.0 1.2 2.9 1.4 
1 1.2 0.0 1.8 2.2 
2 2.9 1.8 0.0 3.8 
3 1.4 2.2 3.8 0.0 
correlation [[ 1.        -0.2221172]
 [-0.2221172  1.       ]]
---
[[], [], [], []] [0 2 2 ... 1 1 1] [[ -0.3827199   -2.0991342 ]
 [ 56.717903    -0.97687346]
 [ -5.008062   -16.368277  ]
 ...
 [ -0.53909457 -22.795301  ]
 [ 34.24745     11.152098  ]
 [-24.455107    10.389301  ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.023362186, 'cluster_all': array([-0.11570732,  0.11559043,  0.0221058 , ...,  0.01228391,
       -0.0043418 , -0.02005471], dtype=float32), 'magnitude_avg': -0.22211719786656026, 'magnitude_all': -0.22211719786656026, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_1', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.023362186, 'magnitude_avg': -0.22211719786656026, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('w5j', 'b3j', 'r5s', 'g3s', 'b5j', 'b1j', 'g9t', 'r4s', 'g8j', 'g8t', 'g5s', 'w9t', 'b8t', 'w5t', 'g8s', 'r2t', 'g2s', 'b9t', 'b1j', 'b1s', 'r6j', 'b2s', 'g9t', 'b6t', 'r8s', 'w6t', 'w1s', 'b2s', 'r8s', 'w3j', 'g4j', 'w6j', 'b2t', 'g1s', 'b3s', 'b7t', 'w2t', 'g3j', 'g7j', 'r6j', 'g4t', 'g2j', 'r2t', 'b8s', 'b8t', 'b1t', 'r6t', 'r1t', 'w3t', 'b9s', 'g2j', 'g5t', 'g9s', 'b5t', 'w4s', 'g2j', 'r5t', 'r1s', 'w7t', 'r9j', 'r9t', 'r2j', 'g7t', 'b3j', 'g3t', 'r7s', 'r2j', 'b4j', 'g1j', 'r6t', 'b1t', 'b6s', 'b5s', 'r8s', 'r1s', 'r6t', 'g7s', 'w2t', 'r3t', 'g8t', 'w2t', 'b7s', 'w4j', 'g6s', 'w8t', 'g6s', 'b2s', 'w8t', 'w7s', 'b6s', 'r9t', 'w7t', 'w2j', 'b1s', 'b9j', 'b5t', 'w7j', 'g6t', 'r1t', 'w8t', 'g6s', 'r8t', 'b4s', 'b7t', 'b4t', 'w9s', 'b9j', 'b1t', 'w3j', 'r9j', 'g8s', 'r5t', 'g6j', 'r8t', 'b1t', 'b1j', 'r7j', 'r7t', 'g7t', 'w7j', 'r1t', 'b7t', 'g4j', 'g8t', 'r5j', 'b6j', 'r6j', 'w4t') 
label[0]: w5j 
label[1]: b3j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([4, 9, 3, 3, 4, 2, 1, 7, 4, 2, 4, 4, 6, 7, 3, 5, 9, 4, 3, 2, 3, 4, 9, 6,
        4, 2, 3, 5, 5, 3, 5, 4, 1, 5, 9, 5, 7, 6, 5, 6, 7, 2, 3, 1, 1, 3, 9, 7,
        2, 3, 3, 3, 3, 7, 1, 9, 4, 4, 8, 6, 9, 2, 2, 8, 2, 2, 6, 5, 2, 6, 1, 1,
        3, 7, 8, 8, 6, 9, 9, 3, 4, 1, 7, 1, 2, 3, 7, 5, 2, 5, 3, 6, 4, 2, 5, 5,
        8, 1, 4, 1, 2, 1, 1, 1, 7, 7, 6, 9, 1, 7, 6, 2, 3, 5, 8, 5, 7, 2, 1, 1,
        3, 4, 7, 5, 1, 9, 2, 9])
Accuracy (count): tensor(14) 
Accuracy (ratio) tensor(0.1094)
Accuracy:
 [[ 0  0  0  0  0  0 18  0  0]
 [ 0  0  0  0  0  0 18  0  0]
 [ 0  0  0  0  0  0 19  0  0]
 [ 0  0  0  0  0  0 15  0  0]
 [ 0  0  0  0  0  0 15  0  0]
 [ 0  0  0  0  0  0 11  0  0]
 [ 0  0  0  0  0  0 14  0  0]
 [ 0  0  0  0  0  0  6  0  0]
 [ 0  0  0  0  0  0 12  0  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]]
---
Silhouette values: [-0.15436882 -0.13813475 -0.17573144 ...  0.12539184 -0.14477174
 -0.09909644]
Average of silhouette coef: -0.091158144
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.5 0.6 0.6 0.7 1.1 1.3 1.3 1.7 
2 0.5 0.0 0.3 0.5 0.5 0.9 1.1 1.1 1.4 
3 0.6 0.3 0.0 0.5 0.4 0.7 0.9 1.0 1.3 
4 0.6 0.5 0.5 0.0 0.6 0.5 0.8 0.8 1.1 
5 0.7 0.5 0.4 0.6 0.0 0.6 0.8 0.8 1.1 
6 1.1 0.9 0.7 0.5 0.6 0.0 0.5 0.5 0.7 
7 1.3 1.1 0.9 0.8 0.8 0.5 0.0 0.4 0.6 
8 1.3 1.1 1.0 0.8 0.8 0.5 0.4 0.0 0.4 
9 1.7 1.4 1.3 1.1 1.1 0.7 0.6 0.4 0.0 
correlation [[1.         0.95188441]
 [0.95188441 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [5 3 5 ... 9 8 2] [[ -3.3146648  11.486349 ]
 [-19.10305    29.117762 ]
 [ 39.746174   11.461624 ]
 ...
 [ 49.567726  -45.287113 ]
 [-63.53688    10.160052 ]
 [ 20.181429   42.23365  ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(57) 
Accuracy (ratio) tensor(0.1166)
Accuracy:
 [[ 0  0  0  0  0  0 45  0  0]
 [ 0  0  0  0  0  0 52  0  0]
 [ 0  0  0  0  0  0 57  0  0]
 [ 0  0  0  0  0  0 60  0  0]
 [ 0  0  0  0  0  0 61  0  0]
 [ 0  0  0  0  0  0 60  0  0]
 [ 0  0  0  0  0  0 57  0  0]
 [ 0  0  0  0  0  0 52  0  0]
 [ 0  0  0  0  0  0 45  0  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.4  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1  0.0 -0.0 -0.0  0.0  0.2  0.0 -0.0 -0.4 -0.0 -0.1 
-0.0  0.1  0.0 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0 -0.0 -0.1 -0.0  0.1  0.0 -0.0 -0.1 -0.0  0.1 
-0.0  0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.1 
-0.0  0.2  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.1 -0.0 -0.1 -0.1 -0.3  0.0  0.0 -0.1  0.0 -0.2 
-0.0  0.1  0.0  0.0 -0.0  0.0 -0.0 -0.1  0.0  0.2  0.1 -0.0 -0.2  0.2 -0.3  0.0  0.0 -0.1 -0.0  0.1 
-0.0 -0.0  0.0 -0.1 -0.0  0.0 -0.2 -0.1  0.0  0.1 -0.0 -0.0 -0.1  0.1 -0.6  0.0  0.0  0.0 -0.0 -0.2 
-0.0 -0.1  0.0  0.1 -0.0  0.0 -0.0  0.2  0.0 -0.1  0.1 -0.0 -0.2 -0.0 -0.8  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.0  0.0 -0.1  0.0  0.0  0.0  0.2 -0.0 -0.1  0.1 -1.0  0.0  0.0 -0.0 -0.0 -0.1 
-0.0 -0.2 -0.0 -0.0 -0.0  0.0 -0.1 -0.0  0.0  0.1  0.3 -0.0 -0.3 -0.0 -1.3  0.0  0.0  0.0 -0.0 -0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.3801622
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.4  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1  0.0 -0.0 -0.0  0.0  0.2  0.0 -0.0 -0.4 -0.0 -0.1 
-0.0  0.1  0.0 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0 -0.0 -0.1 -0.0  0.1  0.0 -0.0 -0.1 -0.0  0.1 
-0.0  0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.1 
-0.0  0.2  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.1 -0.0 -0.1 -0.1 -0.3  0.0  0.0 -0.1  0.0 -0.2 
-0.0  0.1  0.0  0.0 -0.0  0.0 -0.0 -0.1  0.0  0.2  0.1 -0.0 -0.2  0.2 -0.3  0.0  0.0 -0.1 -0.0  0.1 
-0.0 -0.0  0.0 -0.1 -0.0  0.0 -0.2 -0.1  0.0  0.1 -0.0 -0.0 -0.1  0.1 -0.6  0.0  0.0  0.0 -0.0 -0.2 
-0.0 -0.1  0.0  0.1 -0.0  0.0 -0.0  0.2  0.0 -0.1  0.1 -0.0 -0.2 -0.0 -0.8  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.0  0.0 -0.1  0.0  0.0  0.0  0.2 -0.0 -0.1  0.1 -1.0  0.0  0.0 -0.0 -0.0 -0.1 
-0.0 -0.2 -0.0 -0.0 -0.0  0.0 -0.1 -0.0  0.0  0.1  0.3 -0.0 -0.3 -0.0 -1.3  0.0  0.0  0.0 -0.0 -0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 5 
distance: 0.41539723
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.4  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1  0.0 -0.0 -0.0  0.0  0.2  0.0 -0.0 -0.4 -0.0 -0.1 
-0.0  0.1  0.0 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0 -0.0 -0.1 -0.0  0.1  0.0 -0.0 -0.1 -0.0  0.1 
-0.0  0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.1 
-0.0  0.2  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.1 -0.0 -0.1 -0.1 -0.3  0.0  0.0 -0.1  0.0 -0.2 
-0.0  0.1  0.0  0.0 -0.0  0.0 -0.0 -0.1  0.0  0.2  0.1 -0.0 -0.2  0.2 -0.3  0.0  0.0 -0.1 -0.0  0.1 
-0.0 -0.0  0.0 -0.1 -0.0  0.0 -0.2 -0.1  0.0  0.1 -0.0 -0.0 -0.1  0.1 -0.6  0.0  0.0  0.0 -0.0 -0.2 
-0.0 -0.1  0.0  0.1 -0.0  0.0 -0.0  0.2  0.0 -0.1  0.1 -0.0 -0.2 -0.0 -0.8  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.0  0.0 -0.1  0.0  0.0  0.0  0.2 -0.0 -0.1  0.1 -1.0  0.0  0.0 -0.0 -0.0 -0.1 
-0.0 -0.2 -0.0 -0.0 -0.0  0.0 -0.1 -0.0  0.0  0.1  0.3 -0.0 -0.3 -0.0 -1.3  0.0  0.0  0.0 -0.0 -0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.45001596
pred: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.4  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1  0.0 -0.0 -0.0  0.0  0.2  0.0 -0.0 -0.4 -0.0 -0.1 
-0.0  0.1  0.0 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0 -0.0 -0.1 -0.0  0.1  0.0 -0.0 -0.1 -0.0  0.1 
-0.0  0.0  0.0 -0.1 -0.0  0.0 -0.1  0.0  0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.1 
-0.0  0.2  0.0  0.0 -0.0  0.0  0.1 -0.1  0.0 -0.1 -0.1 -0.0 -0.1 -0.1 -0.3  0.0  0.0 -0.1  0.0 -0.2 
-0.0  0.1  0.0  0.0 -0.0  0.0 -0.0 -0.1  0.0  0.2  0.1 -0.0 -0.2  0.2 -0.3  0.0  0.0 -0.1 -0.0  0.1 
-0.0 -0.0  0.0 -0.1 -0.0  0.0 -0.2 -0.1  0.0  0.1 -0.0 -0.0 -0.1  0.1 -0.6  0.0  0.0  0.0 -0.0 -0.2 
-0.0 -0.1  0.0  0.1 -0.0  0.0 -0.0  0.2  0.0 -0.1  0.1 -0.0 -0.2 -0.0 -0.8  0.0  0.0  0.1 -0.0 -0.1 
-0.0 -0.1 -0.0  0.0 -0.0  0.0 -0.1  0.0  0.0  0.0  0.2 -0.0 -0.1  0.1 -1.0  0.0  0.0 -0.0 -0.0 -0.1 
-0.0 -0.2 -0.0 -0.0 -0.0  0.0 -0.1 -0.0  0.0  0.1  0.3 -0.0 -0.3 -0.0 -1.3  0.0  0.0  0.0 -0.0 -0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.33836284
results (all): {'reconst_0x0_avg': 0.109375, 'reconst_0x0_all': nan, 'cluster_avg': -0.091158144, 'cluster_all': array([-0.15436882, -0.13813475, -0.17573144, ...,  0.12539184,
       -0.14477174, -0.09909644], dtype=float32), 'magnitude_avg': 0.951884412837525, 'magnitude_all': 0.951884412837525, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_1', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.109375, 'cluster_avg': -0.091158144, 'magnitude_avg': 0.951884412837525, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g3s', 'b5j', 'r1t', 'w1j', 'g6t', 'r4j', 'r9j', 'r7s', 'g3t', 'g8s', 'r6s', 'g1j', 'r9t', 'w1t', 'r1j', 'g7s', 'b6j', 'g6s', 'w5j', 'g5t', 'g2t', 'w3j', 'w3t', 'g8s', 'g2t', 'g5j', 'b3t', 'g4j', 'b4s', 'g6j', 'r7t', 'g4j', 'b1t', 'r2j', 'w1t', 'r5s', 'w5t', 'w3t', 'g3t', 'b7s', 'w7s', 'b1s', 'g9t', 'g8j', 'g6s', 'g5s', 'w4s', 'r2j', 'b6t', 'r9j', 'g6j', 'b9s', 'b1j', 'g8t', 'b9t', 'g8t', 'w5t', 'b8s', 'w1s', 'g8t', 'w2s', 'w7t', 'r2s', 'r4t', 'g9t', 'r3s', 'w5j', 'w9j', 'w6s', 'g5s', 'w2j', 'g4j', 'w1j', 'w5j', 'g8s', 'w3t', 'r4s', 'g8j', 'b4s', 'b7t', 'g1j', 'g9t', 'b2j', 'r5j', 'r5s', 'g1j', 'b4t', 'r2s', 'g8t', 'b1t', 'r2j', 'w1t', 'g1j', 'g5t', 'g4t', 'r4j', 'r1j', 'r8s', 'g4s', 'r7j', 'b8t', 'g8t', 'r6j', 'r2s', 'w6s', 'b3t', 'g7s', 'g2j', 'b8s', 'b6t', 'g9t', 'g8s', 'w6t', 'b6t', 'w9t', 'r9t', 'b4s', 'w5s', 'b9s', 'b2j', 'b3j', 'b6j', 'w8t', 'w8t', 'r5j', 'g5j', 'g7s', 'g1t') 
label[0]: g3s 
label[1]: b5j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.04334455  0.14322998  0.00616511 ...  0.10594881 -0.03506067
  0.11848536]
Average of silhouette coef: 0.09488855
---
  0   1   2
0 0.0 2.9 3.4 
1 2.9 0.0 3.0 
2 3.4 3.0 0.0 
correlation [[1.         0.98126928]
 [0.98126928 1.        ]]
---
[[], [], []] [1 0 2 ... 2 1 0] [[  3.7624307  11.903189 ]
 [ 17.538593   36.51473  ]
 [-27.310675    5.393124 ]
 ...
 [ 33.39307   -48.658928 ]
 [-35.310356  -39.222816 ]
 [  7.4846     23.743511 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.09488855, 'cluster_all': array([-0.04334455,  0.14322998,  0.00616511, ...,  0.10594881,
       -0.03506067,  0.11848536], dtype=float32), 'magnitude_avg': 0.981269283262149, 'magnitude_all': 0.981269283262149, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_1', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.09488855, 'magnitude_avg': 0.981269283262149, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 2, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_2', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2', 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2',
 'run_id': 'vae_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r7s', 'r7t', 'g9s', 'w9j', 'g2t', 'g8j', 'w2s', 'b4s', 'g2j', 'g7s', 'g2t', 'w8t', 'r9s', 'b8s', 'r4s', 'r9t', 'g1j', 'g1s', 'b2j', 'r3s', 'b2s', 'g7t', 'w9t', 'b1t', 'b8j', 'w2j', 'w3t', 'g3j', 'b9s', 'r6t', 'b8j', 'b8t', 'b1s', 'w9t', 'w7s', 'b3t', 'b3j', 'w5t', 'r6t', 'r6s', 'r4t', 'b5j', 'b9s', 'g2j', 'w2t', 'b9t', 'g5t', 'g8s', 'w6s', 'w5s', 'g2t', 'g1j', 'b8j', 'g1t', 'g7s', 'b3j', 'r4j', 'g3s', 'w3j', 'b1s', 'b2t', 'g3s', 'b3t', 'g3j', 'r4t', 'g9s', 'g7t', 'g1t', 'r1t', 'b4t', 'g4j', 'g6s', 'b7j', 'w4s', 'w3t', 'b8j', 'w9t', 'r6s', 'b8t', 'g5t', 'r9s', 'b6j', 'g4s', 'w3j', 'w4t', 'g1t', 'g2t', 'b9t', 'g2t', 'g3s', 'w9t', 'r6j', 'r2j', 'b2j', 'w3j', 'w8t', 'w9s', 'g3t', 'b8t', 'r2t', 'r3s', 'b9j', 'w4s', 'b9t', 'r2s', 'g5s', 'w1t', 'w7j', 'g9t', 'g8s', 'w7j', 'w3s', 'b3j', 'w8t', 'w7s', 'r8t', 'b6j', 'r8t', 'r9j', 'g1s', 'g7t', 'g3t', 'w9s', 'g7s', 'r1t', 'b9s', 'b5t', 'g4j') 
label[0]: r7s 
label[1]: r7t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.05207588  0.00628838  0.07458957 ... -0.09705202  0.06724063
 -0.04294316]
Average of silhouette coef: 0.037336
---
  0   1   2   3
0 0.0 1.9 3.4 1.5 
1 1.9 0.0 3.9 2.6 
2 3.4 3.9 0.0 2.0 
3 1.5 2.6 2.0 0.0 
correlation [[ 1.         -0.31696017]
 [-0.31696017  1.        ]]
---
[[], [], [], []] [3 3 2 ... 0 2 0] [[-45.102592  -39.127518 ]
 [-52.75679     3.879346 ]
 [-55.597393  -21.107069 ]
 ...
 [-11.178031   -6.9391375]
 [ 38.026173   57.437454 ]
 [-33.177383  -11.870068 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.037336, 'cluster_all': array([ 0.05207588,  0.00628838,  0.07458957, ..., -0.09705202,
        0.06724063, -0.04294316], dtype=float32), 'magnitude_avg': -0.3169601711878573, 'magnitude_all': -0.3169601711878573, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_2', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.037336, 'magnitude_avg': -0.3169601711878573, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g1t', 'g7t', 'g2j', 'g6t', 'w2t', 'g3s', 'w3s', 'g9s', 'b6j', 'b3s', 'b9s', 'r9j', 'g1s', 'b1t', 'r3t', 'w8j', 'g6t', 'w1s', 'b9t', 'r7t', 'w5j', 'g8j', 'w8j', 'g5t', 'w1t', 'w1s', 'g8s', 'r7s', 'g3j', 'r2s', 'b8t', 'b6j', 'g1s', 'g4j', 'g4t', 'w9t', 'r9j', 'r5t', 'r2t', 'w8t', 'g5j', 'g3s', 'b1s', 'w4j', 'w1s', 'b4j', 'g6t', 'r5t', 'g5j', 'g8j', 'w5t', 'w6j', 'w5t', 'b7s', 'r4j', 'r4t', 'r4t', 'r1s', 'b4j', 'r1j', 'b1t', 'r5j', 'g9t', 'r9j', 'g5t', 'g6j', 'w9t', 'b9j', 'b2j', 'r6j', 'w5j', 'r9j', 'b2s', 'b5j', 'b9j', 'g1s', 'r1s', 'g5s', 'b7t', 'r6t', 'w4s', 'b5j', 'b4t', 'r3t', 'w4s', 'g7j', 'r4t', 'r3t', 'r8s', 'w9t', 'r3j', 'b5s', 'g1s', 'b8t', 'w3s', 'b1j', 'g8t', 'r6s', 'g5t', 'b7s', 'r9t', 'g6s', 'r5j', 'b8t', 'b8j', 'g1t', 'w1s', 'r7s', 'r5j', 'w7t', 'w7s', 'r2s', 'w5j', 'g1s', 'r8s', 'b2t', 'g1s', 'b8j', 'b2j', 'g2s', 'b4j', 'w8t', 'b2j', 'g6j', 'b4s', 'r4j', 'w6s', 'g3t') 
label[0]: g1t 
label[1]: g7t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([3, 4, 5, 6, 5, 7, 1, 9, 1, 9, 7, 8, 2, 6, 6, 8, 4, 7, 1, 5, 4, 5, 5, 7,
        2, 2, 6, 5, 7, 6, 3, 1, 7, 1, 1, 6, 5, 3, 2, 6, 7, 9, 7, 2, 9, 1, 3, 1,
        2, 5, 2, 6, 2, 1, 1, 3, 1, 4, 1, 8, 1, 4, 3, 9, 1, 6, 7, 8, 2, 9, 5, 4,
        7, 6, 9, 7, 5, 8, 5, 2, 9, 8, 9, 9, 6, 9, 1, 5, 9, 3, 8, 4, 3, 8, 7, 9,
        3, 1, 5, 4, 6, 7, 3, 9, 4, 5, 4, 3, 8, 6, 2, 7, 5, 2, 3, 7, 4, 8, 4, 3,
        2, 7, 1, 2, 1, 2, 4, 7])
Accuracy (count): tensor(15) 
Accuracy (ratio) tensor(0.1172)
Accuracy:
 [[ 0  0  0  0 18  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 17  0  0  0  0]
 [ 0  0  0  0 10  0  0  0  0]
 [ 0  0  0  0 14  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
Silhouette values: [ 0.17344664 -0.15990053 -0.12652795 ... -0.12076802 -0.15142877
 -0.14735602]
Average of silhouette coef: -0.100904346
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.4 0.5 0.5 0.7 0.6 0.8 0.9 1.1 
2 0.4 0.0 0.5 0.5 0.6 0.5 0.6 0.7 1.0 
3 0.5 0.5 0.0 0.5 0.5 0.5 0.7 0.6 0.8 
4 0.5 0.5 0.5 0.0 0.4 0.4 0.7 0.6 0.8 
5 0.7 0.6 0.5 0.4 0.0 0.5 0.7 0.5 0.6 
6 0.6 0.5 0.5 0.4 0.5 0.0 0.5 0.5 0.7 
7 0.8 0.6 0.7 0.7 0.7 0.5 0.0 0.5 0.6 
8 0.9 0.7 0.6 0.6 0.5 0.5 0.5 0.0 0.5 
9 1.1 1.0 0.8 0.8 0.6 0.7 0.6 0.5 0.0 
correlation [[1.         0.85713367]
 [0.85713367 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [1 7 2 ... 3 8 4] [[ 38.293945  -51.03391  ]
 [-35.72821   -33.706944 ]
 [ 32.230568   18.308525 ]
 ...
 [ 31.477165    4.9221954]
 [-35.34949   -39.87739  ]
 [ 15.766442  -13.512517 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(61) 
Accuracy (ratio) tensor(0.1247)
Accuracy:
 [[ 0  0  0  0 45  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 61  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 45  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.1 -0.0  0.2 -0.1  0.0 -0.0  0.1  0.1  0.3 -0.0 -0.0  0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0 -0.0 
-0.0  0.0 -0.0  0.2 -0.0  0.1  0.0  0.1  0.0 -0.1 -0.0  0.1 -0.0 -0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.0 
-0.0  0.0 -0.0 -0.1 -0.1  0.1  0.0 -0.1 -0.1  0.0 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0  0.2  0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.2  0.1  0.0  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0 -0.1 -0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.3  0.2 -0.0 -0.1 -0.0 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0 
-0.0  0.1 -0.0 -0.0  0.1  0.1  0.0  0.1 -0.2  0.0 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.0  0.0  0.1  0.0 
-0.0  0.0 -0.0 -0.0 -0.2  0.2  0.0  0.1 -0.4 -0.2  0.0  0.2 -0.0 -0.0 -0.0 -0.0 -0.0 -0.2  0.1  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.3  0.0 -0.1 -0.5  0.0 -0.0  0.2 -0.0 -0.1 -0.0 -0.0 -0.0 -0.0 -0.1  0.0 
 0.0  0.1 -0.0 -0.1  0.1  0.2  0.0  0.0 -0.8  0.1 -0.0  0.1 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.45145527
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
-0.0 -0.1 -0.0  0.2 -0.1  0.0 -0.0  0.1  0.1  0.3 -0.0 -0.0  0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0 -0.0 
-0.0  0.0 -0.0  0.2 -0.0  0.1  0.0  0.1  0.0 -0.1 -0.0  0.1 -0.0 -0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.0 
-0.0  0.0 -0.0 -0.1 -0.1  0.1  0.0 -0.1 -0.1  0.0 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0  0.2  0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.2  0.1  0.0  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0 -0.1 -0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.3  0.2 -0.0 -0.1 -0.0 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0 
-0.0  0.1 -0.0 -0.0  0.1  0.1  0.0  0.1 -0.2  0.0 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.0  0.0  0.1  0.0 
-0.0  0.0 -0.0 -0.0 -0.2  0.2  0.0  0.1 -0.4 -0.2  0.0  0.2 -0.0 -0.0 -0.0 -0.0 -0.0 -0.2  0.1  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.3  0.0 -0.1 -0.5  0.0 -0.0  0.2 -0.0 -0.1 -0.0 -0.0 -0.0 -0.0 -0.1  0.0 
 0.0  0.1 -0.0 -0.1  0.1  0.2  0.0  0.0 -0.8  0.1 -0.0  0.1 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 0.5460201
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.1 -0.0  0.2 -0.1  0.0 -0.0  0.1  0.1  0.3 -0.0 -0.0  0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0 -0.0 
-0.0  0.0 -0.0  0.2 -0.0  0.1  0.0  0.1  0.0 -0.1 -0.0  0.1 -0.0 -0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.0 
-0.0  0.0 -0.0 -0.1 -0.1  0.1  0.0 -0.1 -0.1  0.0 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0  0.2  0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.2  0.1  0.0  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0 -0.1 -0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.3  0.2 -0.0 -0.1 -0.0 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0 
-0.0  0.1 -0.0 -0.0  0.1  0.1  0.0  0.1 -0.2  0.0 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.0  0.0  0.1  0.0 
-0.0  0.0 -0.0 -0.0 -0.2  0.2  0.0  0.1 -0.4 -0.2  0.0  0.2 -0.0 -0.0 -0.0 -0.0 -0.0 -0.2  0.1  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.3  0.0 -0.1 -0.5  0.0 -0.0  0.2 -0.0 -0.1 -0.0 -0.0 -0.0 -0.0 -0.1  0.0 
 0.0  0.1 -0.0 -0.1  0.1  0.2  0.0  0.0 -0.8  0.1 -0.0  0.1 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.43242207
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.1 -0.0  0.2 -0.1  0.0 -0.0  0.1  0.1  0.3 -0.0 -0.0  0.0  0.1  0.0 -0.0 -0.0 -0.1  0.0 -0.0 
-0.0  0.0 -0.0  0.2 -0.0  0.1  0.0  0.1  0.0 -0.1 -0.0  0.1 -0.0 -0.1 -0.0  0.0 -0.0 -0.1 -0.0 -0.0 
-0.0  0.0 -0.0 -0.1 -0.1  0.1  0.0 -0.1 -0.1  0.0 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0  0.2  0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.2  0.1  0.0  0.1 -0.1  0.2 -0.0 -0.1 -0.0  0.1  0.0  0.0 -0.0 -0.1 -0.1  0.0 
-0.0  0.1 -0.0 -0.1  0.0  0.1  0.0  0.0 -0.3  0.2 -0.0 -0.1 -0.0 -0.0  0.0  0.0 -0.0  0.1 -0.2  0.0 
-0.0  0.1 -0.0 -0.0  0.1  0.1  0.0  0.1 -0.2  0.0 -0.0  0.2 -0.0  0.0  0.0  0.0 -0.0  0.0  0.1  0.0 
-0.0  0.0 -0.0 -0.0 -0.2  0.2  0.0  0.1 -0.4 -0.2  0.0  0.2 -0.0 -0.0 -0.0 -0.0 -0.0 -0.2  0.1  0.0 
 0.0 -0.0 -0.0 -0.1  0.0  0.3  0.0 -0.1 -0.5  0.0 -0.0  0.2 -0.0 -0.1 -0.0 -0.0 -0.0 -0.0 -0.1  0.0 
 0.0  0.1 -0.0 -0.1  0.1  0.2  0.0  0.0 -0.8  0.1 -0.0  0.1 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.5254083
results (all): {'reconst_0x0_avg': 0.1171875, 'reconst_0x0_all': nan, 'cluster_avg': -0.100904346, 'cluster_all': array([ 0.17344664, -0.15990053, -0.12652795, ..., -0.12076802,
       -0.15142877, -0.14735602], dtype=float32), 'magnitude_avg': 0.8571336740295524, 'magnitude_all': 0.8571336740295524, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 1.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_2', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1171875, 'cluster_avg': -0.100904346, 'magnitude_avg': 0.8571336740295524, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 1.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('w2s', 'b4t', 'g2s', 'g7j', 'r7t', 'r4t', 'g2t', 'b8t', 'b5s', 'b9t', 'w2t', 'b1j', 'b9t', 'g8j', 'w8s', 'r3t', 'w5t', 'g3t', 'g6t', 'b1t', 'g2t', 'b5s', 'g3j', 'r1t', 'b8s', 'b7j', 'r3t', 'b3t', 'w3t', 'w2j', 'b9s', 'w6t', 'r2s', 'b9s', 'w7j', 'r2s', 'g5t', 'g9j', 'b4j', 'r1j', 'r7t', 'g9t', 'b2s', 'w7s', 'g5s', 'b4t', 'w7s', 'g1t', 'w4s', 'b2j', 'b7s', 'w5j', 'r5s', 'r3t', 'b2s', 'w4t', 'b2t', 'g9j', 'g7t', 'g7t', 'g4t', 'r8j', 'w8j', 'w9t', 'b4s', 'b4t', 'g6j', 'b4j', 'w9t', 'g1j', 'w3t', 'w2j', 'g4t', 'b9s', 'b6s', 'b8j', 'g6s', 'g5s', 'r6j', 'b2j', 'g4t', 'r6s', 'g1j', 'r9j', 'g3t', 'b3t', 'g4s', 'b5j', 'g3s', 'g6s', 'w8t', 'w9s', 'w8j', 'w9t', 'r8j', 'g6j', 'w7s', 'w1t', 'b7t', 'r2s', 'g9t', 'g1t', 'r7j', 'g9j', 'g8j', 'g7j', 'g7t', 'w3j', 'r2s', 'g7j', 'r9j', 'b9s', 'g6s', 'b7j', 'g8j', 'g9s', 'r8t', 'w6j', 'g6j', 'r5t', 'r3s', 'w9j', 'b7t', 'b8s', 'g6t', 'w2j', 'r6t', 'b7j') 
label[0]: w2s 
label[1]: b4t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.0188883   0.02905835 -0.03628417 ...  0.08768402  0.09767398
  0.18347315]
Average of silhouette coef: 0.081451625
---
  0   1   2
0 0.0 2.8 3.5 
1 2.8 0.0 2.7 
2 3.5 2.7 0.0 
correlation [[1.         0.99460729]
 [0.99460729 1.        ]]
---
[[], [], []] [1 2 1 ... 0 0 2] [[ 20.024271    2.0129788]
 [ -1.3336468  25.377005 ]
 [ -2.5776992 -42.325546 ]
 ...
 [ 35.319763   16.51997  ]
 [ 31.452095   27.71732  ]
 [-52.843666  -32.47051  ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.081451625, 'cluster_all': array([ 0.0188883 ,  0.02905835, -0.03628417, ...,  0.08768402,
        0.09767398,  0.18347315], dtype=float32), 'magnitude_avg': 0.9946072948989311, 'magnitude_all': 0.9946072948989311, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_2', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.081451625, 'magnitude_avg': 0.9946072948989311, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 3, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_3', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3', 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3',
 'run_id': 'vae_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('w4j', 'b6j', 'w1j', 'g7t', 'w5t', 'w9t', 'r5j', 'b9t', 'w2j', 'r1t', 'b8j', 'g1t', 'r3s', 'b8j', 'b1j', 'w1t', 'w7j', 'g4j', 'r4j', 'w8j', 'b5s', 'g5s', 'g2j', 'b8t', 'b3t', 'g4t', 'b5j', 'b1s', 'r7s', 'r4j', 'b3j', 'r8j', 'w4j', 'g8j', 'r8j', 'r4t', 'g5j', 'r8s', 'w6t', 'b4t', 'r6s', 'b3j', 'g9s', 'g7j', 'w1t', 'g4t', 'r4t', 'g6t', 'r4s', 'w6s', 'b3j', 'r9j', 'b7j', 'g6j', 'r1t', 'r7t', 'w2s', 'r4s', 'b3j', 'r1t', 'g8j', 'w1t', 'w7j', 'r1j', 'g6s', 'w9j', 'r5s', 'r8j', 'g7j', 'b6s', 'w4j', 'r7t', 'w4j', 'b6t', 'r7s', 'b2j', 'b3t', 'g2t', 'r1j', 'r3j', 'b7s', 'w5t', 'w7t', 'r6t', 'g1s', 'g7t', 'r3s', 'g6s', 'w5j', 'w4j', 'w4j', 'w7t', 'g4s', 'w2t', 'b2t', 'b5t', 'r3s', 'w8t', 'r6s', 'g8s', 'w7j', 'g6j', 'w8s', 'g1j', 'r7s', 'r3t', 'w9s', 'g9j', 'w2t', 'b1s', 'b6j', 'r5t', 'r2t', 'w5t', 'w8t', 'g2s', 'r3t', 'r8j', 'g1j', 'b1j', 'b9s', 'g7s', 'g1t', 'g3j', 'b5j', 'r3t', 'b2s', 'r5s') 
label[0]: w4j 
label[1]: b6j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.07780597  0.14651969 -0.13230254 ... -0.09186359  0.18643218
 -0.10389215]
Average of silhouette coef: 0.03652481
---
  0   1   2   3
0 0.0 1.8 1.5 3.1 
1 1.8 0.0 3.2 4.1 
2 1.5 3.2 0.0 2.2 
3 3.1 4.1 2.2 0.0 
correlation [[1.         0.30421461]
 [0.30421461 1.        ]]
---
[[], [], [], []] [0 1 0 ... 3 1 3] [[ 23.94251   43.6589  ]
 [-27.999527 -10.319466]
 [ 11.514899  39.784046]
 ...
 [ 17.388609   8.034866]
 [-40.17042   -4.582553]
 [ 47.818417  33.99409 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.03652481, 'cluster_all': array([-0.07780597,  0.14651969, -0.13230254, ..., -0.09186359,
        0.18643218, -0.10389215], dtype=float32), 'magnitude_avg': 0.3042146066589007, 'magnitude_all': 0.3042146066589007, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_3', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.03652481, 'magnitude_avg': 0.3042146066589007, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g2j', 'r5j', 'g1s', 'r4s', 'b9j', 'g8t', 'w8j', 'b6j', 'g6j', 'w3s', 'g9j', 'g1j', 'b5t', 'w4t', 'g3j', 'r8j', 'r7j', 'r2s', 'r2t', 'b8s', 'b8j', 'w7t', 'b2j', 'b9s', 'w8s', 'w7s', 'r5t', 'w9s', 'r5s', 'r2s', 'r7t', 'w5s', 'g5s', 'g6t', 'g5j', 'r7j', 'b2t', 'g3j', 'b1t', 'b2t', 'r8s', 'b7j', 'r8s', 'w2s', 'g5j', 'b1j', 'w3j', 'w7j', 'b1j', 'r1t', 'g9t', 'b2s', 'r2s', 'g2t', 'b4t', 'b7t', 'b7s', 'r2s', 'r5t', 'b5t', 'b1s', 'r1j', 'g1t', 'g3j', 'g8j', 'w6s', 'w4t', 'r3s', 'b5t', 'r5t', 'b4t', 'b1j', 'b8j', 'r9s', 'w4j', 'b4s', 'r4j', 'b4j', 'w3t', 'b4s', 'w2s', 'g3s', 'g4t', 'g7t', 'r1j', 'w9j', 'g8t', 'g2t', 'b1t', 'r1t', 'b2t', 'w6t', 'g7t', 'b9t', 'w5s', 'g7s', 'w7s', 'b2j', 'w9s', 'r2s', 'g4s', 'r9s', 'w8s', 'r4t', 'g4j', 'g1j', 'b2j', 'w3t', 'b4t', 'b3t', 'g2s', 'g2s', 'w3j', 'r2t', 'b5s', 'r5t', 'g7j', 'g8s', 'w8t', 'r5j', 'r3j', 'w3j', 'r7t', 'w2j', 'r6j', 'r8t', 'g4s', 'w8s') 
label[0]: g2j 
label[1]: r5j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([3, 4, 1, 1, 6, 1, 4, 6, 8, 6, 4, 6, 8, 1, 8, 3, 1, 6, 2, 2, 7, 1, 1, 2,
        6, 6, 4, 1, 8, 3, 5, 3, 4, 1, 1, 6, 1, 2, 1, 7, 4, 1, 2, 7, 1, 4, 8, 1,
        1, 5, 5, 3, 4, 4, 1, 5, 9, 8, 2, 3, 7, 2, 2, 6, 8, 3, 9, 9, 9, 5, 2, 1,
        9, 8, 7, 7, 4, 9, 9, 2, 1, 9, 3, 8, 6, 5, 3, 3, 7, 2, 1, 4, 6, 2, 9, 3,
        4, 2, 2, 8, 2, 9, 3, 1, 9, 6, 8, 2, 6, 3, 4, 4, 1, 2, 1, 9, 8, 9, 1, 5,
        7, 4, 7, 4, 1, 4, 5, 6])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0  0 25  0  0  0  0  0  0]
 [ 0  0 17  0  0  0  0  0  0]
 [ 0  0 13  0  0  0  0  0  0]
 [ 0  0 17  0  0  0  0  0  0]
 [ 0  0  8  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]
 [ 0  0  8  1  0  0  0  0  0]
 [ 0  0 11  1  0  0  0  0  0]
 [ 0  0 13  0  0  0  0  0  0]]
Accuracy:
 [[0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.889 0.111 0.    0.    0.    0.    0.   ]
 [0.    0.    0.917 0.083 0.    0.    0.    0.    0.   ]
 [0.    0.    1.    0.    0.    0.    0.    0.    0.   ]]
---
Silhouette values: [-0.1135737  -0.17060623  0.12911771 ...  0.18053688 -0.14572117
 -0.21808566]
Average of silhouette coef: -0.10406309
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.4 0.4 0.5 0.5 0.8 0.6 0.7 0.6 
2 0.4 0.0 0.4 0.4 0.6 0.7 0.6 0.7 0.6 
3 0.4 0.4 0.0 0.4 0.4 0.6 0.4 0.5 0.5 
4 0.5 0.4 0.4 0.0 0.5 0.6 0.5 0.4 0.5 
5 0.5 0.6 0.4 0.5 0.0 0.6 0.5 0.5 0.4 
6 0.8 0.7 0.6 0.6 0.6 0.0 0.5 0.5 0.5 
7 0.6 0.6 0.4 0.5 0.5 0.5 0.0 0.4 0.4 
8 0.7 0.7 0.5 0.4 0.5 0.5 0.4 0.0 0.2 
9 0.6 0.6 0.5 0.5 0.4 0.5 0.4 0.2 0.0 
correlation [[1.         0.60527828]
 [0.60527828 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [2 5 1 ... 1 6 5] [[ 34.54487    -6.431053 ]
 [-54.456493  -23.9411   ]
 [ 19.073933   14.570027 ]
 ...
 [ 58.49696    -4.1695876]
 [-34.91162   -18.80317  ]
 [ 40.52097    11.133304 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(57) 
Accuracy (ratio) tensor(0.1166)
Accuracy:
 [[ 0  0 45  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 61  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 45  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0  0.0  0.0  0.0 -0.2 -0.1 -0.0 -0.0 -0.1  0.0 -0.2  0.2  0.1  0.0 -0.0 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.0  0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.3  0.2 -0.0  0.2 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.2 -0.0  0.1 -0.0 -0.0 -0.1  0.0 -0.0  0.1  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1 -0.2 -0.0 -0.0 -0.0  0.0  0.0 -0.0  0.1  0.0 -0.0  0.2  0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0 -0.1  0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.1  0.0 -0.0 -0.1  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.0  0.2 -0.1 -0.0 -0.0  0.3  0.0  0.0 -0.1  0.0 -0.0  0.2  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.1  0.2 -0.0 -0.0  0.1  0.0  0.0  0.1  0.1 -0.0  0.0 -0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1 -0.1  0.0  0.0 -0.0  0.1  0.0  0.1 -0.1  0.2 -0.0  0.1  0.1 -0.4  0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1  0.0  0.1  0.0 -0.0  0.0  0.0  0.1 -0.1  0.3 -0.0  0.1  0.1 -0.3  0.0 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.18535443
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0  0.0  0.0  0.0 -0.2 -0.1 -0.0 -0.0 -0.1  0.0 -0.2  0.2  0.1  0.0 -0.0 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.0  0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.3  0.2 -0.0  0.2 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.2 -0.0  0.1 -0.0 -0.0 -0.1  0.0 -0.0  0.1  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1 -0.2 -0.0 -0.0 -0.0  0.0  0.0 -0.0  0.1  0.0 -0.0  0.2  0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0 -0.1  0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.1  0.0 -0.0 -0.1  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.0  0.2 -0.1 -0.0 -0.0  0.3  0.0  0.0 -0.1  0.0 -0.0  0.2  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.1  0.2 -0.0 -0.0  0.1  0.0  0.0  0.1  0.1 -0.0  0.0 -0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1 -0.1  0.0  0.0 -0.0  0.1  0.0  0.1 -0.1  0.2 -0.0  0.1  0.1 -0.4  0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1  0.0  0.1  0.0 -0.0  0.0  0.0  0.1 -0.1  0.3 -0.0  0.1  0.1 -0.3  0.0 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 9 
distance: 0.40100276
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0  0.0  0.0  0.0 -0.2 -0.1 -0.0 -0.0 -0.1  0.0 -0.2  0.2  0.1  0.0 -0.0 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.0  0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.3  0.2 -0.0  0.2 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.2 -0.0  0.1 -0.0 -0.0 -0.1  0.0 -0.0  0.1  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1 -0.2 -0.0 -0.0 -0.0  0.0  0.0 -0.0  0.1  0.0 -0.0  0.2  0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0 -0.1  0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.1  0.0 -0.0 -0.1  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.0  0.2 -0.1 -0.0 -0.0  0.3  0.0  0.0 -0.1  0.0 -0.0  0.2  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.1  0.2 -0.0 -0.0  0.1  0.0  0.0  0.1  0.1 -0.0  0.0 -0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1 -0.1  0.0  0.0 -0.0  0.1  0.0  0.1 -0.1  0.2 -0.0  0.1  0.1 -0.4  0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1  0.0  0.1  0.0 -0.0  0.0  0.0  0.1 -0.1  0.3 -0.0  0.1  0.1 -0.3  0.0 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.42088592
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0  0.0  0.0  0.0 -0.2 -0.1 -0.0 -0.0 -0.1  0.0 -0.2  0.2  0.1  0.0 -0.0 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.0  0.2 -0.1 -0.0 -0.0 -0.0  0.0  0.0  0.0  0.3  0.2 -0.0  0.2 -0.1  0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.2 -0.0  0.1 -0.0 -0.0 -0.1  0.0 -0.0  0.1  0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1 -0.2 -0.0 -0.0 -0.0  0.0  0.0 -0.0  0.1  0.0 -0.0  0.2  0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0 -0.1  0.0 -0.0 -0.1 -0.0 -0.0 -0.0  0.0  0.1  0.1  0.0 -0.0 -0.1  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.0  0.2 -0.1 -0.0 -0.0  0.3  0.0  0.0 -0.1  0.0 -0.0  0.2  0.1 -0.1 -0.0 -0.0 
 0.0  0.0  0.0  0.1  0.1  0.1  0.2 -0.0 -0.0  0.1  0.0  0.0  0.1  0.1 -0.0/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]
  0.0 -0.0 -0.2 -0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1 -0.1  0.0  0.0 -0.0  0.1  0.0  0.1 -0.1  0.2 -0.0  0.1  0.1 -0.4  0.0 -0.0 
 0.0  0.0  0.0  0.1 -0.1  0.0  0.1  0.0 -0.0  0.0  0.0  0.1 -0.1  0.3 -0.0  0.1  0.1 -0.3  0.0 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.407587
results (all): {'reconst_0x0_avg': 0.1015625, 'reconst_0x0_all': nan, 'cluster_avg': -0.10406309, 'cluster_all': array([-0.1135737 , -0.17060623,  0.12911771, ...,  0.18053688,
       -0.14572117, -0.21808566], dtype=float32), 'magnitude_avg': 0.6052782785121993, 'magnitude_all': 0.6052782785121993, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_3', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1015625, 'cluster_avg': -0.10406309, 'magnitude_avg': 0.6052782785121993, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r6j', 'w3s', 'g6t', 'b1j', 'w8j', 'g7t', 'g2t', 'w2j', 'b7j', 'w2s', 'b1t', 'r1s', 'b5j', 'r1j', 'g7t', 'g9s', 'r4j', 'g1j', 'g1s', 'g8s', 'w1s', 'r2t', 'w9t', 'r8j', 'r8j', 'g5t', 'b3t', 'b2t', 'r9s', 'b9s', 'b9j', 'b7t', 'r5s', 'b2t', 'b6s', 'r8t', 'g8s', 'r3t', 'g1t', 'r4j', 'r6t', 'b5t', 'b1t', 'g9s', 'r5j', 'g4j', 'r4t', 'g5s', 'w3t', 'b1s', 'b2t', 'w4t', 'r4t', 'r6j', 'w7t', 'b8t', 'r3j', 'w9s', 'b3j', 'g8s', 'w7s', 'b3t', 'b1s', 'g2j', 'b3j', 'w5j', 'w3t', 'b2j', 'w1t', 'w4s', 'b7t', 'g7t', 'w7j', 'b1s', 'w4j', 'r8j', 'w8j', 'g9j', 'r2s', 'r4j', 'g9j', 'w8j', 'r9s', 'g7t', 'b3s', 'r5t', 'r3j', 'g1s', 'b1j', 'b9s', 'b7s', 'r5j', 'r5s', 'b7t', 'g6j', 'r1j', 'w7t', 'w3j', 'g9s', 'w5t', 'r9t', 'g4j', 'g2t', 'w7t', 'b7s', 'w5t', 'b5t', 'r3j', 'g7s', 'g6j', 'g5s', 'g5t', 'w4s', 'w9t', 'r5j', 'r2s', 'r2t', 'w7t', 'r6s', 'w6t', 'b1s', 'g8j', 'b1j', 'r4t', 'g3s', 'r2s', 'r5t', 'b4t') 
label[0]: r6j 
label[1]: w3s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.1275677   0.0260567   0.06778551 ...  0.17961353  0.2379659
 -0.00767678]
Average of silhouette coef: 0.07254806
---
  0   1   2
0 0.0 2.8 3.3 
1 2.8 0.0 3.1 
2 3.3 3.1 0.0 
correlation [[1.        0.7364587]
 [0.7364587 1.       ]]
---
[[], [], []] [0 1 2 ... 1 0 1] [[ 43.46036     3.8363388]
 [  4.846006   15.651708 ]
 [-35.581985    0.7614671]
 ...
 [ 12.180873  -46.4866   ]
 [ 55.639336   -7.561498 ]
 [  1.0936981  75.493774 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.07254806, 'cluster_all': array([ 0.1275677 ,  0.0260567 ,  0.06778551, ...,  0.17961353,
        0.2379659 , -0.00767678], dtype=float32), 'magnitude_avg': 0.7364587003490699, 'magnitude_all': 0.7364587003490699, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_3', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.07254806, 'magnitude_avg': 0.7364587003490699, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'VAE_OSCN', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'elbo', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'vae_oscn_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4', 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 vae_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'VAE_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'elbo',
 'output_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4',
 'run_id': 'vae_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7             [-1, 20, 1, 1]          40,980
            Conv2d-8             [-1, 20, 1, 1]          40,980
               Enc-9       [[-1, 20], [-1, 20]]               0
  ConvTranspose2d-10            [-1, 128, 4, 4]          41,088
             ReLU-11            [-1, 128, 4, 4]               0
  ConvTranspose2d-12             [-1, 64, 8, 8]         131,136
             ReLU-13             [-1, 64, 8, 8]               0
  ConvTranspose2d-14           [-1, 32, 16, 16]          32,800
             ReLU-15           [-1, 32, 16, 16]               0
  ConvTranspose2d-16            [-1, 3, 32, 32]           1,539
          Sigmoid-17            [-1, 3, 32, 32]               0
Print of model summary was skipped because can't multiply sequence by non-int of type 'list'
Loading model VAE_OSCN from ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: elbo 
t_objectives: iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('r6s', 'g6j', 'r3s', 'w3s', 'b1t', 'w3j', 'b8j', 'b9s', 'r6t', 'w1s', 'w7t', 'r2t', 'g9t', 'w3s', 'g2s', 'b4s', 'w8s', 'b9s', 'b5j', 'r5s', 'g4t', 'w5s', 'g8s', 'g9j', 'r3j', 'b6t', 'r7j', 'w5j', 'r6j', 'b1t', 'b9j', 'r5t', 'w2s', 'w7s', 'b4j', 'w2j', 'r8t', 'r2t', 'g7j', 'r3s', 'g2t', 'b3s', 'b2j', 'w6t', 'w6j', 'r7s', 'r4t', 'b9s', 'r3s', 'g4t', 'g1j', 'w4t', 'w8s', 'r9t', 'b6s', 'b6j', 'g3j', 'g2t', 'g3s', 'b1t', 'g2j', 'r3s', 'r1j', 'b8t', 'r4t', 'r1j', 'r6s', 'g9t', 'g1t', 'b7j', 'r3j', 'r7j', 'g8j', 'g3s', 'w3s', 'r4j', 'g5t', 'r8s', 'g4s', 'w3t', 'g2j', 'r5t', 'r6s', 'g1s', 'b1s', 'w2s', 'r9s', 'b7s', 'b3t', 'w7j', 'g3s', 'w4t', 'b8s', 'b2s', 'r3t', 'b3s', 'g2j', 'g2j', 'b8t', 'b3j', 'g4j', 'w8j', 'r7s', 'w7s', 'w5j', 'g9j', 'g7t', 'g9t', 'r8t', 'g3t', 'g1j', 'r8s', 'w5j', 'g6j', 'r8s', 'b2j', 'b5j', 'g2s', 'w6t', 'g9t', 'b3j', 'g7j', 'g3s', 'b6j', 'r8s', 'b3j', 'g7s', 'r5s') 
label[0]: r6s 
label[1]: g6j 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.13190761 -0.02263584  0.09329528 ... -0.0267664   0.00060439
 -0.09482969]
Average of silhouette coef: 0.030272793
---
  0   1   2   3
0 0.0 3.0 1.4 1.9 
1 3.0 0.0 1.6 4.1 
2 1.4 1.6 0.0 2.8 
3 1.9 4.1 2.8 0.0 
correlation [[ 1.         -0.15603238]
 [-0.15603238  1.        ]]
---
[[], [], [], []] [3 2 3 ... 2 0 0] [[ 3.5671539e+01  4.6172085e+00]
 [ 3.8666187e+01  6.8784943e+00]
 [ 1.7885374e+01  2.6848352e+01]
 ...
 [-1.0528840e+01  2.9930009e-02]
 [ 2.0915735e+01  6.1075943e+01]
 [-2.1558584e+01  2.8496511e+01]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.030272793, 'cluster_all': array([ 0.13190761, -0.02263584,  0.09329528, ..., -0.0267664 ,
        0.00060439, -0.09482969], dtype=float32), 'magnitude_avg': -0.1560323821926654, 'magnitude_all': -0.1560323821926654, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_4', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.030272793, 'magnitude_avg': -0.1560323821926654, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('w8t', 'r8s', 'r9t', 'r3j', 'w8j', 'b3j', 'g4s', 'w9t', 'r8j', 'w8s', 'b2s', 'r3t', 'g2j', 'w7t', 'g9j', 'r7s', 'w7j', 'b4t', 'w1t', 'b5j', 'r9s', 'g6j', 'r2s', 'r6t', 'b8s', 'w3j', 'w5j', 'b4t', 'b3t', 'w1s', 'r4j', 'b7s', 'w8s', 'w4s', 'b8j', 'b3s', 'b8j', 'b8j', 'b8s', 'b9j', 'r2s', 'b6s', 'w2j', 'g8s', 'w1t', 'r1s', 'w5s', 'g7j', 'w1j', 'r4t', 'g6t', 'b4t', 'b8s', 'w3s', 'b9s', 'b1j', 'g6s', 'b6s', 'b6t', 'w1j', 'g4s', 'b1s', 'g5t', 'g6j', 'r5s', 'r2s', 'r6t', 'w9s', 'g7j', 'r9s', 'g6s', 'w4t', 'w8j', 'g2t', 'r6s', 'g3t', 'g7t', 'g4s', 'b8j', 'b6t', 'r7s', 'b3t', 'g9t', 'r3s', 'g3s', 'g5s', 'r4s', 'g1t', 'r7j', 'r1j', 'r6j', 'g8s', 'w3s', 'w8t', 'b7s', 'r6t', 'g3t', 'b3s', 'r1s', 'b9j', 'b4s', 'w5s', 'b3j', 'w7t', 'b5t', 'b4s', 'w1t', 'r6j', 'b6j', 'g7t', 'g8j', 'g1t', 'w8t', 'w1t', 'r9j', 'w1t', 'g2t', 'w4t', 'r5s', 'w6s', 'w8j', 'g3t', 'w4t', 'w1j', 'b8j', 'b4s', 'w9j', 'w1t') 
label[0]: w8t 
label[1]: r8s 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 7, 7, 2, 2, 2, 1, 4, 2, 9, 5, 1, 8, 4, 9, 6, 5, 9, 9, 2, 4, 8, 7, 9,
        3, 9, 8, 3, 5, 6, 6, 3, 2, 3, 3, 3, 3, 5, 4, 7, 2, 3, 9, 2, 7, 3, 4, 2,
        1, 6, 6, 8, 2, 9, 8, 9, 1, 7, 4, 7, 1, 3, 4, 6, 7, 3, 3, 2, 2, 5, 8, 6,
        2, 2, 2, 4, 6, 9, 3, 4, 3, 6, 6, 2, 9, 8, 9, 4, 2, 8, 8, 2, 1, 9, 8, 1,
        2, 8, 1, 5, 1, 9, 1, 1, 9, 2, 9, 1, 2, 2, 2, 3, 3, 2, 6, 3, 6, 2, 6, 6,
        1, 9, 8, 7, 9, 1, 1, 4])
Accuracy (count): tensor(12) 
Accuracy (ratio) tensor(0.0938)
Accuracy:
 [[ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 25  0]
 [ 0  0  0  0  0  0  0 17  0]
 [ 0  0  0  0  0  0  0 11  0]
 [ 0  0  0  0  0  0  0  6  0]
 [ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0  9  0]
 [ 0  0  0  0  0  0  0 12  0]
 [ 0  0  0  0  0  0  0 18  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
Silhouette values: [-0.12168448 -0.10472091 -0.05171819 ...  0.09773552  0.18174149
 -0.08014438]
Average of silhouette coef: -0.09406488
---
  1   2   3   4   5   6   7   8   9
1 0.0 0.5 0.4 0.5 0.9 1.1 1.0 1.2 1.4 
2 0.5 0.0 0.4 0.2 0.6 0.7 0.8 0.9 1.1 
3 0.4 0.4 0.0 0.3 0.6 0.9 0.7 0.9 1.1 
4 0.5 0.2 0.3 0.0 0.5 0.7 0.7 0.8 1.0 
5 0.9 0.6 0.6 0.5 0.0 0.6 0.5 0.5 0.8 
6 1.1 0.7 0.9 0.7 0.6 0.0 0.6 0.4 0.6 
7 1.0 0.8 0.7 0.7 0.5 0.6 0.0 0.5 0.6 
8 1.2 0.9 0.9 0.8 0.5 0.4 0.5 0.0 0.4 
9 1.4 1.1 1.1 1.0 0.8 0.6 0.6 0.4 0.0 
correlation [[1.         0.89390857]
 [0.89390857 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [8 8 9 ... 1 1 8] [[-61.85499     4.2707195]
 [-24.426653  -33.788353 ]
 [-37.879204  -23.218233 ]
 ...
 [ 14.484549   33.33138  ]
 [ -8.272115   55.92883  ]
 [ 55.28588    -5.0241113]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0  0  0  0  0  0  0 45  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 61  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 45  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1 -0.2 -0.0 -0.0  0.0 -0.3  0.1 -0.0  0.0  0.1  0.3 -0.0  0.0 
 0.0  0.0  0.0  0.1 -0.0  0.1 -0.1 -0.0 -0.1  0.1 -0.0  0.0 -0.2 -0.2 -0.0  0.0 -0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0  0.1 -0.1  0.1 -0.1  0.2 -0.0  0.0 -0.3 -0.0 -0.0  0.2  0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.1 -0.1  0.3 -0.0  0.0 -0.2 -0.1 -0.0  0.0  0.1  0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.2 -0.0  0.5  0.0  0.0 -0.0 -0.1 -0.0  0.1  0.3 -0.1 -0.0 -0.0 
-0.0  0.0  0.0  0.2 -0.0  0.1  0.2 -0.0 -0.0  0.7  0.0  0.0  0.1 -0.2 -0.0 -0.2 -0.0 -0.1 -0.0  0.0 
-0.0  0.0  0.0  0.0 -0.0  0.0  0.2 -0.0  0.0  0.6  0.0  0.0 -0.1  0.2 -0.0  0.1  0.0 -0.3  0.0 -0.0 
-0.0  0.0  0.0  0.0 -0.0 -0.1  0.3 -0.0  0.1  0.8  0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.1  0.0 -0.0 
-0.0  0.0  0.0 -0.0 -0.0  0.2  0.4  0.0  0.1  1.0  0.0 -0.0  0.1 -0.0 -0.0  0.0  0.0 -0.2  0.0 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 3 
distance: 0.38087335
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1 -0.2 -0.0 -0.0  0.0 -0.3  0.1 -0.0  0.0  0.1  0.3 -0.0  0.0 
 0.0  0.0  0.0  0.1 -0.0  0.1 -0.1 -0.0 -0.1  0.1 -0.0  0.0 -0.2 -0.2 -0.0  0.0 -0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0  0.1 -0.1  0.1 -0.1  0.2 -0.0  0.0 -0.3 -0.0 -0.0  0.2  0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.1 -0.1  0.3 -0.0  0.0 -0.2 -0.1 -0.0  0.0  0.1  0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.2 -0.0  0.5  0.0  0.0 -0.0 -0.1 -0.0  0.1  0.3 -0.1 -0.0 -0.0 
-0.0  0.0  0.0  0.2 -0.0  0.1  0.2 -0.0 -0.0  0.7  0.0  0.0  0.1 -0.2 -0.0 -0.2 -0.0 -0.1 -0.0  0.0 
-0.0  0.0  0.0  0.0 -0.0  0.0  0.2 -0.0  0.0  0.6  0.0  0.0 -0.1  0.2 -0.0  0.1  0.0 -0.3  0.0 -0.0 
-0.0  0.0  0.0  0.0 -0.0 -0.1  0.3 -0.0  0.1  0.8  0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.1  0.0 -0.0 
-0.0  0.0  0.0 -0.0 -0.0  0.2  0.4  0.0  0.1  1.0  0.0 -0.0  0.1 -0.0 -0.0  0.0  0.0 -0.2  0.0 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 7 
distance: 0.5034217
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 0.0  0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1 -0.2 -0.0 -0.0  0.0 -0.3  0.1 -0.0  0.0  0.1  0.3 -0.0  0.0 
 0.0  0.0  0.0  0.1 -0.0  0.1 -0.1 -0.0 -0.1  0.1 -0.0  0.0 -0.2 -0.2 -0.0  0.0 -0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0  0.1 -0.1  0.1 -0.1  0.2 -0.0  0.0 -0.3 -0.0 -0.0  0.2  0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.1 -0.1  0.3 -0.0  0.0 -0.2 -0.1 -0.0  0.0  0.1  0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.2 -0.0  0.5  0.0  0.0 -0.0 -0.1 -0.0  0.1  0.3 -0.1 -0.0 -0.0 
-0.0  0.0  0.0  0.2 -0.0  0.1  0.2 -0.0 -0.0  0.7  0.0  0.0  0.1 -0.2 -0.0 -0.2 -0.0 -0.1 -0.0  0.0 
-0.0  0.0  0.0  0.0 -0.0  0.0  0.2 -0.0  0.0  0.6  0.0  0.0 -0.1  0.2 -0.0  0.1  0.0 -0.3  0.0 -0.0 
-0.0  0.0  0.0  0.0 -0.0 -0.1  0.3 -0.0  0.1  0.8  0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.1  0.0 -0.0 
-0.0  0.0  0.0 -0.0 -0.0  0.2  0.4  0.0  0.1  1.0  0.0 -0.0  0.1 -0.0 -0.0  0.0  0.0 -0.2  0.0 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 0.48651817
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.0  0.0 -0.0  0.0 -0.0  0.1 -0.1  0.1 -0.2 -0.0 -0.0  0.0 -0.3  0.1 -0.0  0.0  0.1  0.3 -0.0  0.0 
 0.0  0.0  0.0  0.1 -0.0  0.1 -0.1 -0.0 -0.1  0.1 -0.0  0.0 -0.2 -0.2 -0.0  0.0 -0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0  0.1 -0.1  0.1 -0.1  0.2 -0.0  0.0 -0.3 -0.0 -0.0  0.2  0.0  0.1 -0.0  0.0 
 0.0  0.0  0.0 -0.0 -0.0  0.1 -0.0 -0.1 -0.1  0.3 -0.0  0.0 -0.2 -0.1 -0.0  0.0  0.1  0.0 -0.0  0.0 
 0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.2 -0.0  0.5  0.0  0.0 -0.0 -0.1 -0.0  0.1  0.3 -0.1 -0.0 -0.0 
-0.0  0.0  0.0  0.2 -0.0  0.1  0.2 -0.0 -0.0  0.7  0.0  0.0  0.1 -0.2 -0.0 -0.2 -0.0 -0.1 -0.0  0.0 
-0.0  0.0  0.0  0.0 -0.0  0.0  0.2 -0.0  0.0  0.6  0.0  0.0 -0.1  0.2 -0.0  0.1  0.0 -0.3  0.0 -0.0 
-0.0  0.0  0.0  0.0 -0.0 -0.1  0.3 -0.0  0.1  0.8  0.0  0.0  0.0 -0.1 -0.0 -0.0  0.1 -0.1  0.0 -0.0 
-0.0  0.0  0.0 -0.0 -0.0  0.2  0.4  0.0  0.1  1.0  0.0 -0.0  0.1 -0.0 -0.0  0.0  0.0 -0.2  0.0 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 0.39956665
results (all): {'reconst_0x0_avg': 0.09375, 'reconst_0x0_all': nan, 'cluster_avg': -0.09406488, 'cluster_all': array([-0.12168448, -0.10472091, -0.05171819, ...,  0.09773552,
        0.18174149, -0.08014438], dtype=float32), 'magnitude_avg': 0.893908570380916, 'magnitude_all': 0.893908570380916, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 1.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'vae_oscn_seed_4', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.09375, 'cluster_avg': -0.09406488, 'magnitude_avg': 0.893908570380916, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 1.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'tuple'> 
label: ('g2t', 'r7t', 'g4s', 'w5t', 'r4j', 'w1s', 'w5j', 'r5j', 'g1t', 'w5j', 'w6t', 'w5s', 'r2t', 'r8j', 'b9s', 'w4s', 'b2t', 'g5t', 'w3s', 'b6s', 'r5s', 'r8j', 'b3s', 'g8t', 'g4t', 'w6j', 'g5s', 'r2t', 'r8t', 'w8s', 'w3j', 'b5j', 'w4j', 'g3s', 'g5t', 'w6j', 'r3t', 'b6s', 'b5t', 'r7j', 'b1s', 'w9j', 'g1s', 'b6s', 'g7s', 'w6t', 'w7j', 'b5t', 'g2t', 'w5j', 'g7t', 'w3t', 'w2s', 'r5s', 'b4t', 'r3j', 'r7j', 'w2t', 'b4j', 'w9t', 'b4s', 'g2t', 'b3s', 'r7j', 'r2s', 'b5j', 'w5s', 'b3t', 'w6t', 'r6s', 'r9t', 'b9j', 'b1s', 'w2s', 'r1s', 'g2t', 'r9j', 'r1t', 'b9t', 'w2j', 'g6j', 'b2t', 'w1j', 'g2t', 'b7s', 'b5t', 'w8j', 'b4t', 'b1s', 'r3j', 'g5t', 'g7s', 'g6j', 'g4s', 'r8s', 'w9t', 'w1j', 'w9j', 'g1s', 'w2j', 'g4s', 'r8j', 'w5t', 'w4s', 'b2s', 'r2t', 'w2s', 'b1s', 'r9j', 'r7s', 'g7j', 'b2s', 'w1t', 'g7t', 'w5j', 'b6j', 'g5j', 'b7s', 'g5j', 'w2j', 'g7t', 'g7j', 'g5t', 'g6t', 'r1j', 'r1t', 'r4t', 'b5t') 
label[0]: g2t 
label[1]: r7t 
type(data): <class 'torch.Tensor'> 
data[0]: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') 
data[0].shape: torch.Size([3, 32, 32])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.01479694  0.09323119  0.05060424 ... -0.00629165  0.1336321
  0.22937256]
Average of silhouette coef: 0.08862027
---
  0   1   2
0 0.0 3.2 3.4 
1 3.2 0.0 3.3 
2 3.4 3.3 0.0 
correlation [[1.         0.65158672]
 [0.65158672 1.        ]]
---
[[], [], []] [2 2 1 ... 2 0 2] [[ 28.981277 -31.865719]
 [-10.389349  41.93    ]
 [-13.35277  -37.835373]
 ...
 [  7.143461  18.915274]
 [ 67.335365  -9.971285]
 [  8.130669  60.56615 ]]
saved ./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': 0.08862027, 'cluster_all': array([ 0.01479694,  0.09323119,  0.05060424, ..., -0.00629165,
        0.1336321 ,  0.22937256], dtype=float32), 'magnitude_avg': 0.6515867249895335, 'magnitude_all': 0.6515867249895335, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'vae_oscn_seed_4', 'model_name': 'VAE_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': 0.08862027, 'magnitude_avg': 0.6515867249895335, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 0, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_0', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0', 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_0
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0',
 'run_id': 'mmvae_cmnist_oscn_seed_0',
 'run_type': 'train',
 'seed': 0,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 6, 7, 7, 4, 9, 1, 6, 2, 9, 3, 3, 4, 5, 5, 7, 5, 4, 8, 2, 7, 9, 1, 9,
        5, 1, 8, 5, 4, 4, 8, 3, 8, 3, 6, 3, 2, 7, 6, 7, 3, 7, 4, 2, 3, 7, 8, 2,
        8, 1, 3, 5, 9, 6, 6, 8, 4, 9, 8, 9, 9, 2, 5, 2, 9, 3, 3, 6, 2, 1, 8, 6,
        6, 6, 6, 3, 3, 2, 3, 7, 1, 2, 5, 6, 1, 5, 4, 8, 3, 2, 7, 5, 9, 6, 2, 8,
        1, 2, 5, 9, 9, 8, 6, 2, 6, 9, 5, 4, 2, 7, 9, 7, 7, 4, 4, 6, 2, 9, 3, 3,
        3, 5, 4, 1, 6, 5, 2, 2]), ['w7s', 'b6j', 'r7s', 'r7j', 'b4t', 'b9t', 'r1j', 'g6j', 'r2s', 'w9j', 'w3s', 'g3t', 'r4j', 'g5s', 'w5t', 'r7s', 'g5s', 'b4j', 'g8j', 'b2s', 'b7t', 'g9t', 'b1t', 'r9s', 'g5s', 'r1s', 'g8j', 'r5t', 'w4s', 'r4j', 'g8s', 'w3s', 'g8s', 'b3s', 'w6j', 'w3s', 'w2s', 'g7s', 'r6j', 'g7t', 'r3j', 'w7t', 'r4s', 'b2j', 'b3s', 'b7s', 'b8t', 'r2t', 'r8t', 'g1s', 'r3j', 'g5j', 'w9j', 'w6j', 'w6t', 'g8t', 'b4s', 'r9t', 'r8j', 'r9s', 'b9s', 'g2t', 'w5s', 'g2j', 'b9t', 'g3j', 'r3t', 'b6s', 'b2t', 'g1s', 'g8t', 'g6t', 'w6j', 'r6s', 'g6j', 'r3s', 'b3s', 'r2j', 'g3j', 'w7t', 'b1t', 'b2s', 'b5j', 'w6s', 'r1s', 'w5s', 'r4t', 'g8s', 'g3s', 'r2s', 'w7t', 'b5s', 'b9t', 'b6t', 'b2s', 'b8j', 'w1s', 'w2j', 'w5t', 'w9s', 'w9t', 'b8s', 'r6t', 'r2j', 'g6j', 'w9t', 'r5j', 'w4s', 'r2t', 'g7j', 'w9s', 'b7j', 'b7j', 'b4t', 'g4s', 'w6s', 'w2t', 'b9s', 'g3j', 'r3j', 'r3s', 'w5t', 'b4t', 'g1s', 'r6s', 'r5t', 'w2t', 'r2j']] 
label[0]: tensor([7, 6, 7, 7, 4, 9, 1, 6, 2, 9, 3, 3, 4, 5, 5, 7, 5, 4, 8, 2, 7, 9, 1, 9,
        5, 1, 8, 5, 4, 4, 8, 3, 8, 3, 6, 3, 2, 7, 6, 7, 3, 7, 4, 2, 3, 7, 8, 2,
        8, 1, 3, 5, 9, 6, 6, 8, 4, 9, 8, 9, 9, 2, 5, 2, 9, 3, 3, 6, 2, 1, 8, 6,
        6, 6, 6, 3, 3, 2, 3, 7, 1, 2, 5, 6, 1, 5, 4, 8, 3, 2, 7, 5, 9, 6, 2, 8,
        1, 2, 5, 9, 9, 8, 6, 2, 6, 9, 5, 4, 2, 7, 9, 7, 7, 4, 4, 6, 2, 9, 3, 3,
        3, 5, 4, 1, 6, 5, 2, 2]) 
label[1]: ['w7s', 'b6j', 'r7s', 'r7j', 'b4t', 'b9t', 'r1j', 'g6j', 'r2s', 'w9j', 'w3s', 'g3t', 'r4j', 'g5s', 'w5t', 'r7s', 'g5s', 'b4j', 'g8j', 'b2s', 'b7t', 'g9t', 'b1t', 'r9s', 'g5s', 'r1s', 'g8j', 'r5t', 'w4s', 'r4j', 'g8s', 'w3s', 'g8s', 'b3s', 'w6j', 'w3s', 'w2s', 'g7s', 'r6j', 'g7t', 'r3j', 'w7t', 'r4s', 'b2j', 'b3s', 'b7s', 'b8t', 'r2t', 'r8t', 'g1s', 'r3j', 'g5j', 'w9j', 'w6j', 'w6t', 'g8t', 'b4s', 'r9t', 'r8j', 'r9s', 'b9s', 'g2t', 'w5s', 'g2j', 'b9t', 'g3j', 'r3t', 'b6s', 'b2t', 'g1s', 'g8t', 'g6t', 'w6j', 'r6s', 'g6j', 'r3s', 'b3s', 'r2j', 'g3j', 'w7t', 'b1t', 'b2s', 'b5j', 'w6s', 'r1s', 'w5s', 'r4t', 'g8s', 'g3s', 'r2s', 'w7t', 'b5s', 'b9t', 'b6t', 'b2s', 'b8j', 'w1s', 'w2j', 'w5t', 'w9s', 'w9t', 'b8s', 'r6t', 'r2j', 'g6j', 'w9t', 'r5j', 'w4s', 'r2t', 'g7j', 'w9s', 'b7j', 'b7j', 'b4t', 'g4s', 'w6s', 'w2t', 'b9s', 'g3j', 'r3j', 'r3s', 'w5t', 'b4t', 'g1s', 'r6s', 'r5t', 'w2t', 'r2j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.05952721  0.07963942  0.11339267 ...  0.09947778  0.08082998
  0.05568701]
Average of silhouette coef: 0.05167777
---
  0   1   2   3
0 0.0 2.3 3.0 2.7 
1 2.3 0.0 4.0 3.8 
2 3.0 4.0 0.0 3.8 
3 2.7 3.8 3.8 0.0 
correlation [[ 1.        -0.3012855]
 [-0.3012855  1.       ]]
---
[[], [], [], []] [0 1 3 ... 3 2 3] [[-31.167942  54.778023]
 [ 62.335182 -20.872047]
 [-15.170806  54.902966]
 ...
 [ 25.926544   1.212488]
 [-14.251592  38.55657 ]
 [ 42.9031    -7.031547]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.05167777, 'cluster_all': array([-0.05952721,  0.07963942,  0.11339267, ...,  0.09947778,
        0.08082998,  0.05568701], dtype=float32), 'magnitude_avg': -0.3012855025949572, 'magnitude_all': -0.3012855025949572, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.05167777, 'magnitude_avg': -0.3012855025949572, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 9, 2, 4, 3, 8, 8, 4, 1, 2, 4, 3, 9, 5, 4, 8, 3, 4, 2, 4, 2, 5, 6, 1,
        6, 2, 4, 3, 3, 3, 1, 9, 6, 5, 4, 7, 9, 5, 2, 8, 7, 8, 3, 9, 9, 9, 2, 2,
        6, 2, 9, 1, 3, 4, 1, 2, 2, 5, 8, 6, 4, 8, 4, 1, 6, 1, 2, 9, 2, 4, 2, 4,
        8, 1, 7, 9, 1, 7, 1, 8, 8, 6, 8, 4, 1, 8, 6, 3, 8, 4, 1, 8, 8, 2, 3, 6,
        7, 1, 2, 1, 9, 4, 8, 8, 8, 1, 5, 1, 1, 8, 7, 5, 6, 9, 6, 5, 7, 8, 4, 4,
        8, 1, 2, 1, 2, 1, 1, 3]), ['w2j', 'w9s', 'w2s', 'r4j', 'b3j', 'r8s', 'r8t', 'w4s', 'b1s', 'b2s', 'w4j', 'b3t', 'w9s', 'g5s', 'g4j', 'r8s', 'g3t', 'g4j', 'g2j', 'g4t', 'b2j', 'w5j', 'b6t', 'r1s', 'w6t', 'g2j', 'g4s', 'g3s', 'g3j', 'g3j', 'g1j', 'g9j', 'r6j', 'g5t', 'w4j', 'g7t', 'r9t', 'b5j', 'w2j', 'g8j', 'g7j', 'g8j', 'w3t', 'g9t', 'r9s', 'g9s', 'g2s', 'g2j', 'r6s', 'w2j', 'w9j', 'b1t', 'w3t', 'w4s', 'w1s', 'b2t', 'g2t', 'w5j', 'b8t', 'b6t', 'w4t', 'r8s', 'g4j', 'g1t', 'r6t', 'g1j', 'b2s', 'r9t', 'g2j', 'b4j', 'g2s', 'g4s', 'w8t', 'r1j', 'b7s', 'g9s', 'b1s', 'b7j', 'g1j', 'w8s', 'r8s', 'g6s', 'b8j', 'w4s', 'w1j', 'b8j', 'r6s', 'r3s', 'b8s', 'g4t', 'w1j', 'r8j', 'w8s', 'r2s', 'w3s', 'r6t', 'b7t', 'r1j', 'g2s', 'b1s', 'r9j', 'r4s', 'w8j', 'g8t', 'w8j', 'b1t', 'g5t', 'g1t', 'w1s', 'b8j', 'r7s', 'r5t', 'r6t', 'r9s', 'w6s', 'g5s', 'w7s', 'g8t', 'r4t', 'b4j', 'b8s', 'b1t', 'r2t', 'g1j', 'g2s', 'g1j', 'w1s', 'b3j']] 
label[0]: tensor([2, 9, 2, 4, 3, 8, 8, 4, 1, 2, 4, 3, 9, 5, 4, 8, 3, 4, 2, 4, 2, 5, 6, 1,
        6, 2, 4, 3, 3, 3, 1, 9, 6, 5, 4, 7, 9, 5, 2, 8, 7, 8, 3, 9, 9, 9, 2, 2,
        6, 2, 9, 1, 3, 4, 1, 2, 2, 5, 8, 6, 4, 8, 4, 1, 6, 1, 2, 9, 2, 4, 2, 4,
        8, 1, 7, 9, 1, 7, 1, 8, 8, 6, 8, 4, 1, 8, 6, 3, 8, 4, 1, 8, 8, 2, 3, 6,
        7, 1, 2, 1, 9, 4, 8, 8, 8, 1, 5, 1, 1, 8, 7, 5, 6, 9, 6, 5, 7, 8, 4, 4,
        8, 1, 2, 1, 2, 1, 1, 3]) 
label[1]: ['w2j', 'w9s', 'w2s', 'r4j', 'b3j', 'r8s', 'r8t', 'w4s', 'b1s', 'b2s', 'w4j', 'b3t', 'w9s', 'g5s', 'g4j', 'r8s', 'g3t', 'g4j', 'g2j', 'g4t', 'b2j', 'w5j', 'b6t', 'r1s', 'w6t', 'g2j', 'g4s', 'g3s', 'g3j', 'g3j', 'g1j', 'g9j', 'r6j', 'g5t', 'w4j', 'g7t', 'r9t', 'b5j', 'w2j', 'g8j', 'g7j', 'g8j', 'w3t', 'g9t', 'r9s', 'g9s', 'g2s', 'g2j', 'r6s', 'w2j', 'w9j', 'b1t', 'w3t', 'w4s', 'w1s', 'b2t', 'g2t', 'w5j', 'b8t', 'b6t', 'w4t', 'r8s', 'g4j', 'g1t', 'r6t', 'g1j', 'b2s', 'r9t', 'g2j', 'b4j', 'g2s', 'g4s', 'w8t', 'r1j', 'b7s', 'g9s', 'b1s', 'b7j', 'g1j', 'w8s', 'r8s', 'g6s', 'b8j', 'w4s', 'w1j', 'b8j', 'r6s', 'r3s', 'b8s', 'g4t', 'w1j', 'r8j', 'w8s', 'r2s', 'w3s', 'r6t', 'b7t', 'r1j', 'g2s', 'b1s', 'r9j', 'r4s', 'w8j', 'g8t', 'w8j', 'b1t', 'g5t', 'g1t', 'w1s', 'b8j', 'r7s', 'r5t', 'r6t', 'r9s', 'w6s', 'g5s', 'w7s', 'g8t', 'r4t', 'b4j', 'b8s', 'b1t', 'r2t', 'g1j', 'g2s', 'g1j', 'w1s', 'b3j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([3, 3, 2, 2, 8, 7, 2, 3, 7, 2, 4, 4, 7, 4, 6, 1, 3, 8, 2, 4, 9, 8, 7, 6,
        9, 7, 3, 2, 5, 6, 4, 9, 9, 7, 5, 9, 9, 5, 6, 4, 9, 9, 8, 2, 7, 2, 6, 7,
        4, 8, 8, 5, 9, 7, 1, 1, 2, 4, 7, 5, 7, 6, 9, 3, 3, 9, 1, 3, 1, 6, 1, 4,
        8, 2, 3, 5, 8, 1, 5, 2, 4, 1, 9, 1, 7, 4, 7, 7, 9, 7, 4, 8, 9, 1, 5, 5,
        3, 1, 7, 3, 3, 9, 1, 8, 5, 8, 8, 2, 3, 2, 2, 5, 2, 4, 8, 1, 6, 6, 2, 8,
        3, 3, 8, 9, 4, 3, 3, 5])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[13  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]
 [17  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [12  0  0  0  0  0  0  0  0]
 [ 9  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]
 [15  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([5, 7, 1, 6, 2, 4, 4, 1, 8, 8, 1, 1, 8, 5, 7, 4, 9, 1, 5, 9, 5, 2, 6, 2,
        4, 3, 9, 1, 9, 2, 2, 8, 4, 9, 7, 1, 8, 9, 6, 5, 9, 1, 8, 2, 2, 7, 4, 1,
        9, 3, 1, 6, 3, 6, 4, 9, 1, 6, 6, 5, 3, 3, 8, 2, 3, 4, 5, 4, 1, 5, 4, 6,
        2, 6, 7, 8, 5, 2, 3, 4, 6, 7, 7, 9, 1, 8, 2, 7, 3, 4, 8, 2, 4, 6, 1, 2,
        4, 7, 3, 6, 5, 1, 1, 9, 1, 3, 8, 8, 5, 6, 3, 5, 3, 2, 3, 6, 9, 1, 4, 4,
        4, 9, 8, 5, 4, 9, 1, 7])
Accuracy (count): tensor(19) 
Accuracy (ratio) tensor(0.1484)
Accuracy:
 [[19  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [13  0  0  0  0  0  0  0  0]
 [18  0  0  0  0  0  0  0  0]
 [13  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [10  0  0  0  0  0  0  0  0]
 [13  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [ 0.12426463  0.12644762  0.0211892  ...  0.09732153  0.24025786
 -0.0261967 ]
Average of silhouette coef: 0.10082565
---
  1   2   3   4   5   6   7   8   9
1 0.0 4.2 4.7 6.1 5.0 5.7 5.7 5.2 6.6 
2 4.2 0.0 4.4 4.7 4.9 5.1 5.2 4.0 5.6 
3 4.7 4.4 0.0 5.6 3.1 6.0 5.1 3.8 5.7 
4 6.1 4.7 5.6 0.0 5.0 5.3 4.9 5.2 4.2 
5 5.0 4.9 3.1 5.0 0.0 5.1 5.3 3.7 5.0 
6 5.7 5.1 6.0 5.3 5.1 0.0 7.1 4.7 5.5 
7 5.7 5.2 5.1 4.9 5.3 7.1 0.0 5.2 3.8 
8 5.2 4.0 3.8 5.2 3.7 4.7 5.2 0.0 3.6 
9 6.6 5.6 5.7 4.2 5.0 5.5 3.8 3.6 0.0 
correlation [[1.         0.22992326]
 [0.22992326 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [2 9 2 ... 3 6 7] [[ 31.747833  22.90819 ]
 [ 16.980392 -30.724604]
 [ 14.617913  23.011322]
 ...
 [ -5.86948   44.39597 ]
 [-37.818684 -25.465956]
 [ 12.006411 -20.936646]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[45  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [61  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [45  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.3  0.1  0.2 -0.1 -0.2 -0.0 -0.0 -0.6  3.0  0.6  0.1 -0.1  0.3 -0.2 -0.0  0.7 -0.6 -0.1  0.3 
-0.3 -0.2  0.1 -0.1  0.6 -1.4 -0.9 -0.4 -2.2  0.5  0.3  0.9 -0.6  1.3 -0.3 -0.3  1.0 -1.5  0.7 -1.1 
 0.3 -0.2  0.2  0.1  0.2 -1.2  2.3  1.2 -1.5  0.1  0.2  1.1  0.1  1.1  0.2 -0.3  0.7 -1.8 -0.6  0.2 
 0.1  0.2  0.2  0.1  0.6 -0.1 -0.8 -0.3 -0.3 -1.5 -1.1  0.6 -2.9  0.4 -0.7 -0.2  0.1 -2.3  0.1  0.4 
-0.6 -0.2  0.3  0.1 -0.0 -0.4  2.4  0.1 -0.7 -0.6 -0.1  0.9 -0.2  0.2 -1.8 -0.3  0.9 -1.5 -0.9 -0.1 
-3.7 -0.1  0.1 -0.2  0.5 -0.4 -1.2 -0.1 -0.6 -0.5 -1.0  0.8  0.3  0.3 -0.8 -0.3  0.9 -1.4 -0.7  0.9 
 3.1  0.0  0.3  0.1  0.4 -0.3 -0.5 -0.3 -0.5 -1.5  0.2  0.3 -0.1 -0.0 -0.1  0.0  1.5 -1.2 -0.9  0.3 
-1.2 -0.0  0.1 -0.2  0.0 -0.7  0.8  0.3 -1.4 -1.7  0.8  0.1  0.5  0.4  0.1 -0.2  0.1 -0.5  0.3 -0.4 
 0.3  0.0  0.2  0.0  0.4  0.1 -0.3 -0.2  0.0 -3.4 -0.2 -0.1 -0.2 -0.2 -0.1 -0.1  0.8 -0.5  0.4  0.7 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 3.597743
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.3  0.1  0.2 -0.1 -0.2 -0.0 -0.0 -0.6  3.0  0.6  0.1 -0.1  0.3 -0.2 -0.0  0.7 -0.6 -0.1  0.3 
-0.3 -0.2  0.1 -0.1  0.6 -1.4 -0.9 -0.4 -2.2  0.5  0.3  0.9 -0.6  1.3 -0.3 -0.3  1.0 -1.5  0.7 -1.1 
 0.3 -0.2  0.2  0.1  0.2 -1.2  2.3  1.2 -1.5  0.1  0.2  1.1  0.1  1.1  0.2 -0.3  0.7 -1.8 -0.6  0.2 
 0.1  0.2  0.2  0.1  0.6 -0.1 -0.8 -0.3 -0.3 -1.5 -1.1  0.6 -2.9  0.4 -0.7 -0.2  0.1 -2.3  0.1  0.4 
-0.6 -0.2  0.3  0.1 -0.0 -0.4  2.4  0.1 -0.7 -0.6 -0.1  0.9 -0.2  0.2 -1.8 -0.3  0.9 -1.5 -0.9 -0.1 
-3.7 -0.1  0.1 -0.2  0.5 -0.4 -1.2 -0.1 -0.6 -0.5 -1.0  0.8  0.3  0.3 -0.8 -0.3  0.9 -1.4 -0.7  0.9 
 3.1  0.0  0.3  0.1  0.4 -0.3 -0.5 -0.3 -0.5 -1.5  0.2  0.3 -0.1 -0.0 -0.1  0.0  1.5 -1.2 -0.9  0.3 
-1.2 -0.0  0.1 -0.2  0.0 -0.7  0.8  0.3 -1.4 -1.7  0.8  0.1  0.5  0.4  0.1 -0.2  0.1 -0.5  0.3 -0.4 
 0.3  0.0  0.2  0.0  0.4  0.1 -0.3 -0.2  0.0 -3.4 -0.2 -0.1 -0.2 -0.2 -0.1 -0.1  0.8 -0.5  0.4  0.7 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 5.1784153
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.3  0.1  0.2 -0.1 -0.2 -0.0 -0.0 -0.6  3.0  0.6  0.1 -0.1  0.3 -0.2 -0.0  0.7 -0.6 -0.1  0.3 
-0.3 -0.2  0.1 -0.1  0.6 -1.4 -0.9 -0.4 -2.2  0.5  0.3  0.9 -0.6  1.3 -0.3 -0.3  1.0 -1.5  0.7 -1.1 
 0.3 -0.2  0.2  0.1  0.2 -1.2  2.3  1.2 -1.5  0.1  0.2  1.1  0.1  1.1  0.2 -0.3  0.7 -1.8 -0.6  0.2 
 0.1  0.2  0.2  0.1  0.6 -0.1 -0.8 -0.3 -0.3 -1.5 -1.1  0.6 -2.9  0.4 -0.7 -0.2  0.1 -2.3  0.1  0.4 
-0.6 -0.2  0.3  0.1 -0.0 -0.4  2.4  0.1 -0.7 -0.6 -0.1  0.9 -0.2  0.2 -1.8 -0.3  0.9 -1.5 -0.9 -0.1 
-3.7 -0.1  0.1 -0.2  0.5 -0.4 -1.2 -0.1 -0.6 -0.5 -1.0  0.8  0.3  0.3 -0.8 -0.3  0.9 -1.4 -0.7  0.9 
 3.1  0.0  0.3  0.1  0.4 -0.3 -0.5 -0.3 -0.5 -1.5  0.2  0.3 -0.1 -0.0 -0.1  0.0  1.5 -1.2 -0.9  0.3 
-1.2 -0.0  0.1 -0.2  0.0 -0.7  0.8  0.3 -1.4 -1.7  0.8  0.1  0.5  0.4  0.1 -0.2  0.1 -0.5  0.3 -0.4 
 0.3  0.0  0.2  0.0  0.4  0.1 -0.3 -0.2  0.0 -3.4 -0.2 -0.1 -0.2 -0.2 -0.1 -0.1  0.8 -0.5  0.4  0.7 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 4.1654696
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.3  0.1  0.2 -0.1 -0.2 -0.0 -0.0 -0.6  3.0  0.6  0.1 -0.1  0.3 -0.2 -0.0  0.7 -0.6 -0.1  0.3 
-0.3 -0.2  0.1 -0.1  0.6 -1.4 -0.9 -0.4 -2.2  0.5  0.3  0.9 -0.6  1.3 -0.3 -0.3  1.0 -1.5  0.7 -1.1 
 0.3 -0.2  0.2  0.1  0.2 -1.2  2.3  1.2 -1.5  0.1  0.2  1.1  0.1  1.1  0.2 -0.3  0.7 -1.8 -0.6  0.2 
 0.1  0.2  0.2  0.1  0.6 -0.1 -0.8 -0.3 -0.3 -1.5 -1.1  0.6 -2.9  0.4 -0.7 -0.2  0.1 -2.3  0.1  0.4 
-0.6 -0.2  0.3  0.1 -0.0 -0.4  2.4  0.1 -0.7 -0.6 -0.1  0.9 -0.2  0.2 -1.8 -0.3  0.9 -1.5 -0.9 -0.1 
-3.7 -0.1  0.1 -0.2  0.5 -0.4 -1.2 -0.1 -0.6 -0.5 -1.0  0.8  0.3  0.3 -0.8 -0.3  0.9 -1.4 -0.7  0.9 
 3.1  0.0  0.3  0.1  0.4 -0.3 -0.5 -0.3 -0.5 -1.5  0.2  0.3 -0.1 -0.0 -0.1  0.0  1.5 -1.2 -0.9  0.3 
-1.2 -0.0  0.1 -0.2  0.0 -0.7  0.8  0.3 -1.4 -1.7  0.8  0.1  0.5  0.4  0.1 -0.2  0.1 -0.5  0.3 -0.4 
 0.3  0.0  0.2  0.0  0.4  0.1 -0.3 -0.2  0.0 -3.4 -0.2 -0.1 -0.2 -0.2 -0.1 -0.1  0.8 -0.5  0.4  0.7 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 4.3728833
results (all): {'reconst_0x0_avg': 0.1015625, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.1484375, 'cross_1x0_all': nan, 'cluster_avg': 0.10082565, 'cluster_all': array([ 0.12426463,  0.12644762,  0.0211892 , ...,  0.09732153,
        0.24025786, -0.0261967 ], dtype=float32), 'magnitude_avg': 0.2299232587582342, 'magnitude_all': 0.2299232587582342, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1015625, 'cross_1x0_avg': 0.1484375, 'cluster_avg': 0.10082565, 'magnitude_avg': 0.2299232587582342, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 6, 3, 2, 9, 7, 7, 4, 1, 1, 5, 3, 6, 2, 8, 9, 2, 1, 7, 3, 1, 6, 2, 2,
        9, 5, 5, 3, 3, 7, 3, 8, 3, 5, 9, 6, 7, 9, 9, 5, 3, 7, 1, 4, 7, 2, 2, 5,
        3, 3, 1, 2, 3, 2, 7, 7, 8, 5, 4, 6, 1, 7, 1, 6, 3, 5, 1, 9, 7, 1, 4, 7,
        2, 2, 5, 9, 8, 8, 9, 4, 9, 3, 4, 3, 4, 2, 7, 3, 8, 7, 1, 9, 9, 2, 6, 6,
        3, 9, 3, 7, 1, 7, 2, 5, 1, 4, 6, 6, 2, 3, 6, 8, 6, 2, 1, 5, 4, 6, 7, 1,
        2, 4, 2, 3, 8, 4, 5, 6]), ['w6j', 'b6t', 'r3t', 'w2t', 'g9j', 'g7j', 'g7s', 'b4s', 'w1j', 'b1s', 'b5j', 'g3j', 'g6t', 'b2j', 'g8s', 'w9j', 'g2s', 'w1t', 'w7j', 'w3j', 'b1t', 'g6t', 'b2t', 'g2j', 'w9j', 'w5s', 'b5j', 'w3s', 'b3s', 'g7t', 'b3j', 'b8s', 'r3s', 'b5j', 'g9t', 'g6t', 'w7s', 'g9s', 'g9j', 'w5s', 'b3s', 'b7t', 'w1t', 'g4t', 'r7t', 'g2j', 'g2s', 'r5t', 'w3j', 'b3j', 'b1t', 'b2s', 'w3j', 'g2t', 'r7t', 'r7s', 'b8s', 'w5s', 'w4t', 'b6j', 'b1s', 'w7j', 'w1t', 'r6t', 'g3t', 'r5s', 'w1t', 'g9s', 'r7t', 'w1j', 'w4t', 'g7t', 'b2t', 'r2s', 'g5s', 'r9t', 'r8t', 'w8j', 'w9j', 'w4t', 'g9t', 'g3t', 'b4j', 'g3t', 'b4t', 'g2s', 'r7j', 'b3j', 'r8t', 'g7j', 'b1t', 'g9j', 'w9s', 'r2j', 'r6s', 'g6s', 'r3s', 'w9s', 'b3s', 'b7t', 'b1j', 'g7j', 'r2t', 'r5j', 'b1s', 'b4s', 'g6s', 'b6s', 'r2j', 'r3t', 'b6t', 'r8t', 'w6j', 'r2t', 'w1t', 'g5s', 'r4j', 'w6s', 'g7t', 'w1s', 'r2t', 'g4j', 'r2s', 'b3t', 'g8j', 'b4s', 'g5t', 'w6j']] 
label[0]: tensor([6, 6, 3, 2, 9, 7, 7, 4, 1, 1, 5, 3, 6, 2, 8, 9, 2, 1, 7, 3, 1, 6, 2, 2,
        9, 5, 5, 3, 3, 7, 3, 8, 3, 5, 9, 6, 7, 9, 9, 5, 3, 7, 1, 4, 7, 2, 2, 5,
        3, 3, 1, 2, 3, 2, 7, 7, 8, 5, 4, 6, 1, 7, 1, 6, 3, 5, 1, 9, 7, 1, 4, 7,
        2, 2, 5, 9, 8, 8, 9, 4, 9, 3, 4, 3, 4, 2, 7, 3, 8, 7, 1, 9, 9, 2, 6, 6,
        3, 9, 3, 7, 1, 7, 2, 5, 1, 4, 6, 6, 2, 3, 6, 8, 6, 2, 1, 5, 4, 6, 7, 1,
        2, 4, 2, 3, 8, 4, 5, 6]) 
label[1]: ['w6j', 'b6t', 'r3t', 'w2t', 'g9j', 'g7j', 'g7s', 'b4s', 'w1j', 'b1s', 'b5j', 'g3j', 'g6t', 'b2j', 'g8s', 'w9j', 'g2s', 'w1t', 'w7j', 'w3j', 'b1t', 'g6t', 'b2t', 'g2j', 'w9j', 'w5s', 'b5j', 'w3s', 'b3s', 'g7t', 'b3j', 'b8s', 'r3s', 'b5j', 'g9t', 'g6t', 'w7s', 'g9s', 'g9j', 'w5s', 'b3s', 'b7t', 'w1t', 'g4t', 'r7t', 'g2j', 'g2s', 'r5t', 'w3j', 'b3j', 'b1t', 'b2s', 'w3j', 'g2t', 'r7t', 'r7s', 'b8s', 'w5s', 'w4t', 'b6j', 'b1s', 'w7j', 'w1t', 'r6t', 'g3t', 'r5s', 'w1t', 'g9s', 'r7t', 'w1j', 'w4t', 'g7t', 'b2t', 'r2s', 'g5s', 'r9t', 'r8t', 'w8j', 'w9j', 'w4t', 'g9t', 'g3t', 'b4j', 'g3t', 'b4t', 'g2s', 'r7j', 'b3j', 'r8t', 'g7j', 'b1t', 'g9j', 'w9s', 'r2j', 'r6s', 'g6s', 'r3s', 'w9s', 'b3s', 'b7t', 'b1j', 'g7j', 'r2t', 'r5j', 'b1s', 'b4s', 'g6s', 'b6s', 'r2j', 'r3t', 'b6t', 'r8t', 'w6j', 'r2t', 'w1t', 'g5s', 'r4j', 'w6s', 'g7t', 'w1s', 'r2t', 'g4j', 'r2s', 'b3t', 'g8j', 'b4s', 'g5t', 'w6j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.01615573  0.0042995   0.00151957 ... -0.0128085  -0.00671156
 -0.00480412]
Average of silhouette coef: -0.0043741735
---
  0   1   2
0 0.0 0.4 0.4 
1 0.4 0.0 0.2 
2 0.4 0.2 0.0 
correlation [[1.         0.50687379]
 [0.50687379 1.        ]]
---
[[], [], []] [0 2 2 ... 1 1 0] [[  2.0673509 -52.784092 ]
 [ 31.48      -43.07942  ]
 [ 31.843142   47.370922 ]
 ...
 [-47.335983  -31.112776 ]
 [ -8.123836   22.560297 ]
 [-12.419484   24.389877 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.0043741735, 'cluster_all': array([-0.01615573,  0.0042995 ,  0.00151957, ..., -0.0128085 ,
       -0.00671156, -0.00480412], dtype=float32), 'magnitude_avg': 0.5068737899608866, 'magnitude_all': 0.5068737899608866, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.0043741735, 'magnitude_avg': 0.5068737899608866, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 7, 3, 8, 4, 4, 7, 7, 9, 9, 5, 8, 2, 1, 2, 9, 1, 8, 5, 3, 3, 8, 6, 5,
        9, 2, 8, 9, 7, 8, 6, 6, 4, 2, 7, 5, 6, 2, 1, 4, 6, 9, 6, 7, 7, 3, 4, 4,
        4, 9, 6, 6, 1, 2, 8, 5, 4, 9, 9, 7, 3, 5, 8, 8, 9, 1, 9, 8, 5, 8, 4, 2,
        1, 1, 8, 3, 7, 8, 7, 5, 8, 1, 4, 6, 4, 6, 9, 6, 9, 4, 4, 2, 8, 4, 8, 9,
        2, 7, 2, 8, 7, 5, 3, 3, 5, 9, 6, 1, 7, 1, 5, 9, 9, 1, 3, 4, 4, 6, 4, 5,
        1, 6, 4, 8, 5, 1, 3, 1]), ['w2s', 'g7t', 'w3s', 'w8s', 'g4t', 'r4t', 'g7j', 'r7j', 'g9t', 'g9j', 'w5s', 'w8s', 'w2s', 'b1j', 'g2j', 'g9j', 'w1t', 'g8j', 'b5s', 'r3t', 'w3j', 'w8t', 'r6t', 'w5s', 'r9t', 'w2t', 'b8t', 'b9s', 'b7j', 'g8j', 'g6j', 'r6s', 'w4t', 'b2j', 'w7s', 'r5j', 'g6t', 'b2s', 'w1j', 'r4t', 'b6t', 'b9t', 'g6t', 'w7t', 'b7s', 'b3s', 'g4s', 'b4j', 'g4j', 'w9s', 'w6j', 'b6t', 'r1s', 'b2t', 'w8j', 'b5t', 'g4s', 'b9j', 'w9j', 'w7j', 'w3s', 'b5j', 'w8j', 'g8t', 'w9j', 'g1j', 'b9j', 'w8t', 'g5s', 'b8t', 'b4t', 'g2t', 'g1s', 'g1j', 'b8s', 'b3s', 'g7j', 'w8j', 'r7j', 'w5t', 'r8j', 'b1t', 'r4s', 'r6t', 'r4t', 'g6j', 'g9t', 'w6t', 'g9t', 'g4s', 'w4t', 'g2j', 'w8j', 'b4s', 'r8s', 'r9s', 'b2s', 'b7t', 'r2s', 'b8t', 'w7t', 'g5s', 'g3t', 'r3s', 'w5j', 'b9s', 'w6j', 'b1t', 'g7t', 'w1j', 'w5t', 'b9j', 'w9j', 'g1t', 'w3s', 'g4t', 'g4j', 'w6t', 'r4j', 'r5j', 'g1j', 'b6s', 'g4j', 'r8t', 'b5j', 'g1s', 'b3s', 'r1s']] 
label[0]: tensor([2, 7, 3, 8, 4, 4, 7, 7, 9, 9, 5, 8, 2, 1, 2, 9, 1, 8, 5, 3, 3, 8, 6, 5,
        9, 2, 8, 9, 7, 8, 6, 6, 4, 2, 7, 5, 6, 2, 1, 4, 6, 9, 6, 7, 7, 3, 4, 4,
        4, 9, 6, 6, 1, 2, 8, 5, 4, 9, 9, 7, 3, 5, 8, 8, 9, 1, 9, 8, 5, 8, 4, 2,
        1, 1, 8, 3, 7, 8, 7, 5, 8, 1, 4, 6, 4, 6, 9, 6, 9, 4, 4, 2, 8, 4, 8, 9,
        2, 7, 2, 8, 7, 5, 3, 3, 5, 9, 6, 1, 7, 1, 5, 9, 9, 1, 3, 4, 4, 6, 4, 5,
        1, 6, 4, 8, 5, 1, 3, 1]) 
label[1]: ['w2s', 'g7t', 'w3s', 'w8s', 'g4t', 'r4t', 'g7j', 'r7j', 'g9t', 'g9j', 'w5s', 'w8s', 'w2s', 'b1j', 'g2j', 'g9j', 'w1t', 'g8j', 'b5s', 'r3t', 'w3j', 'w8t', 'r6t', 'w5s', 'r9t', 'w2t', 'b8t', 'b9s', 'b7j', 'g8j', 'g6j', 'r6s', 'w4t', 'b2j', 'w7s', 'r5j', 'g6t', 'b2s', 'w1j', 'r4t', 'b6t', 'b9t', 'g6t', 'w7t', 'b7s', 'b3s', 'g4s', 'b4j', 'g4j', 'w9s', 'w6j', 'b6t', 'r1s', 'b2t', 'w8j', 'b5t', 'g4s', 'b9j', 'w9j', 'w7j', 'w3s', 'b5j', 'w8j', 'g8t', 'w9j', 'g1j', 'b9j', 'w8t', 'g5s', 'b8t', 'b4t', 'g2t', 'g1s', 'g1j', 'b8s', 'b3s', 'g7j', 'w8j', 'r7j', 'w5t', 'r8j', 'b1t', 'r4s', 'r6t', 'r4t', 'g6j', 'g9t', 'w6t', 'g9t', 'g4s', 'w4t', 'g2j', 'w8j', 'b4s', 'r8s', 'r9s', 'b2s', 'b7t', 'r2s', 'b8t', 'w7t', 'g5s', 'g3t', 'r3s', 'w5j', 'b9s', 'w6j', 'b1t', 'g7t', 'w1j', 'w5t', 'b9j', 'w9j', 'g1t', 'w3s', 'g4t', 'g4j', 'w6t', 'r4j', 'r5j', 'g1j', 'b6s', 'g4j', 'r8t', 'b5j', 'g1s', 'b3s', 'r1s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.02859375  0.17167048  0.06097048 ...  0.11507547  0.20029493
 -0.01100881]
Average of silhouette coef: 0.13649961
---
  0   1   2   3
0 0.0 3.1 3.0 3.2 
1 3.1 0.0 4.3 4.4 
2 3.0 4.3 0.0 4.4 
3 3.2 4.4 4.4 0.0 
correlation [[ 1.         -0.40336234]
 [-0.40336234  1.        ]]
---
[[], [], [], []] [0 2 0 ... 3 3 0] [[ 26.913668    1.6607391]
 [ 12.781438  -34.656063 ]
 [  2.772615  -20.28823  ]
 ...
 [ 27.426773   15.454145 ]
 [  3.2587738  46.430264 ]
 [ -4.790084   -7.6773434]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': 0.13649961, 'cluster_all': array([ 0.02859375,  0.17167048,  0.06097048, ...,  0.11507547,
        0.20029493, -0.01100881], dtype=float32), 'magnitude_avg': -0.40336234152712974, 'magnitude_all': -0.40336234152712974, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': 0.13649961, 'magnitude_avg': -0.40336234152712974, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 2, 1, 6, 1, 7, 3, 7, 2, 3, 8, 7, 9, 7, 6, 8, 8, 4, 8, 6, 3, 8, 7, 7,
        7, 3, 5, 7, 5, 3, 3, 8, 2, 7, 9, 9, 2, 9, 2, 6, 3, 4, 2, 6, 7, 7, 1, 2,
        4, 1, 2, 2, 4, 2, 5, 7, 5, 9, 8, 6, 3, 6, 3, 4, 4, 9, 2, 3, 8, 8, 9, 7,
        8, 1, 9, 6, 8, 6, 8, 1, 2, 5, 7, 1, 3, 9, 1, 4, 6, 5, 8, 6, 6, 3, 7, 2,
        6, 6, 1, 4, 9, 8, 1, 4, 6, 2, 6, 6, 5, 6, 9, 3, 8, 2, 6, 2, 7, 9, 4, 7,
        3, 9, 3, 7, 6, 6, 8, 9]), ['b7t', 'g2t', 'r1s', 'g6s', 'r1s', 'b7j', 'b3t', 'r7j', 'r2s', 'b3s', 'g8s', 'r7s', 'g9j', 'r7s', 'r6s', 'g8s', 'r8s', 'w4j', 'b8t', 'b6t', 'g3j', 'g8s', 'w7j', 'r7t', 'r7j', 'w3t', 'b5s', 'g7s', 'b5s', 'b3j', 'w3s', 'b8j', 'r2j', 'b7t', 'r9s', 'w9s', 'b2s', 'b9t', 'b2t', 'w6t', 'r3j', 'r4j', 'g2t', 'g6j', 'b7j', 'r7s', 'b1j', 'r2s', 'b4j', 'r1j', 'w2j', 'g2s', 'b4t', 'w2s', 'g5s', 'w7t', 'r5t', 'b9s', 'w8t', 'r6j', 'w3s', 'g6t', 'g3t', 'b4s', 'w4t', 'g9t', 'r2s', 'g3s', 'g8s', 'g8t', 'w9t', 'r7t', 'g8s', 'r1j', 'w9j', 'b6t', 'b8s', 'r6j', 'b8j', 'b1s', 'g2t', 'g5t', 'w7s', 'b1j', 'b3t', 'w9j', 'b1j', 'g4j', 'r6j', 'r5s', 'g8j', 'r6j', 'b6j', 'r3s', 'g7t', 'w2j', 'w6t', 'w6s', 'g1j', 'g4j', 'w9s', 'b8j', 'w1j', 'w4t', 'w6j', 'w2t', 'b6j', 'g6t', 'w5j', 'w6j', 'b9j', 'w3s', 'w8s', 'w2t', 'g6t', 'g2j', 'g7t', 'b9s', 'r4j', 'r7s', 'b3j', 'r9s', 'b3t', 'b7s', 'w6s', 'g6j', 'g8s', 'b9t']] 
label[0]: tensor([7, 2, 1, 6, 1, 7, 3, 7, 2, 3, 8, 7, 9, 7, 6, 8, 8, 4, 8, 6, 3, 8, 7, 7,
        7, 3, 5, 7, 5, 3, 3, 8, 2, 7, 9, 9, 2, 9, 2, 6, 3, 4, 2, 6, 7, 7, 1, 2,
        4, 1, 2, 2, 4, 2, 5, 7, 5, 9, 8, 6, 3, 6, 3, 4, 4, 9, 2, 3, 8, 8, 9, 7,
        8, 1, 9, 6, 8, 6, 8, 1, 2, 5, 7, 1, 3, 9, 1, 4, 6, 5, 8, 6, 6, 3, 7, 2,
        6, 6, 1, 4, 9, 8, 1, 4, 6, 2, 6, 6, 5, 6, 9, 3, 8, 2, 6, 2, 7, 9, 4, 7,
        3, 9, 3, 7, 6, 6, 8, 9]) 
label[1]: ['b7t', 'g2t', 'r1s', 'g6s', 'r1s', 'b7j', 'b3t', 'r7j', 'r2s', 'b3s', 'g8s', 'r7s', 'g9j', 'r7s', 'r6s', 'g8s', 'r8s', 'w4j', 'b8t', 'b6t', 'g3j', 'g8s', 'w7j', 'r7t', 'r7j', 'w3t', 'b5s', 'g7s', 'b5s', 'b3j', 'w3s', 'b8j', 'r2j', 'b7t', 'r9s', 'w9s', 'b2s', 'b9t', 'b2t', 'w6t', 'r3j', 'r4j', 'g2t', 'g6j', 'b7j', 'r7s', 'b1j', 'r2s', 'b4j', 'r1j', 'w2j', 'g2s', 'b4t', 'w2s', 'g5s', 'w7t', 'r5t', 'b9s', 'w8t', 'r6j', 'w3s', 'g6t', 'g3t', 'b4s', 'w4t', 'g9t', 'r2s', 'g3s', 'g8s', 'g8t', 'w9t', 'r7t', 'g8s', 'r1j', 'w9j', 'b6t', 'b8s', 'r6j', 'b8j', 'b1s', 'g2t', 'g5t', 'w7s', 'b1j', 'b3t', 'w9j', 'b1j', 'g4j', 'r6j', 'r5s', 'g8j', 'r6j', 'b6j', 'r3s', 'g7t', 'w2j', 'w6t', 'w6s', 'g1j', 'g4j', 'w9s', 'b8j', 'w1j', 'w4t', 'w6j', 'w2t', 'b6j', 'g6t', 'w5j', 'w6j', 'b9j', 'w3s', 'w8s', 'w2t', 'g6t', 'g2j', 'g7t', 'b9s', 'r4j', 'r7s', 'b3j', 'r9s', 'b3t', 'b7s', 'w6s', 'g6j', 'g8s', 'b9t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([3, 6, 9, 6, 4, 7, 1, 3, 5, 9, 3, 1, 1, 8, 2, 2, 5, 9, 5, 7, 7, 2, 6, 5,
        1, 6, 1, 1, 9, 7, 9, 6, 4, 6, 2, 6, 2, 6, 1, 7, 8, 4, 4, 1, 2, 1, 4, 6,
        5, 3, 6, 2, 2, 2, 5, 3, 6, 4, 1, 5, 7, 9, 6, 4, 9, 5, 8, 2, 4, 7, 8, 3,
        9, 1, 8, 3, 7, 1, 5, 4, 1, 3, 9, 6, 3, 4, 9, 3, 7, 1, 4, 9, 5, 2, 4, 2,
        4, 9, 1, 9, 1, 2, 2, 5, 9, 4, 1, 9, 4, 1, 6, 5, 2, 2, 8, 3, 2, 2, 4, 2,
        5, 8, 6, 2, 6, 2, 3, 1])
Accuracy (count): tensor(21) 
Accuracy (ratio) tensor(0.1641)
Accuracy:
 [[ 0 19  0  0  0  0  0  0  0]
 [ 0 21  0  0  0  0  0  0  0]
 [ 0 12  0  0  0  0  0  0  0]
 [ 0 16  0  0  0  0  0  0  0]
 [ 0 13  0  0  0  0  0  0  0]
 [ 0 16  0  0  0  0  0  0  0]
 [ 0  9  0  0  0  0  0  0  0]
 [ 0  7  0  0  0  0  0  0  0]
 [ 0 15  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([3, 9, 3, 8, 1, 1, 3, 6, 6, 1, 3, 8, 1, 4, 2, 7, 7, 7, 4, 1, 1, 9, 1, 6,
        9, 7, 4, 9, 1, 4, 8, 9, 4, 8, 3, 4, 8, 7, 7, 5, 5, 5, 9, 2, 9, 4, 5, 6,
        1, 9, 5, 2, 8, 4, 6, 3, 4, 1, 3, 3, 5, 3, 4, 7, 4, 6, 1, 8, 1, 8, 5, 6,
        1, 8, 9, 4, 1, 6, 8, 7, 5, 4, 7, 9, 1, 7, 5, 3, 9, 5, 9, 9, 8, 2, 4, 4,
        5, 2, 6, 1, 7, 7, 8, 4, 5, 3, 8, 5, 1, 4, 2, 4, 3, 5, 5, 2, 5, 2, 1, 8,
        2, 6, 6, 1, 6, 6, 9, 5])
Accuracy (count): tensor(9) 
Accuracy (ratio) tensor(0.0703)
Accuracy:
 [[ 0 19  0  0  0  0  0  0  0]
 [ 0  9  0  0  0  0  0  0  0]
 [ 0 12  0  0  0  0  0  0  0]
 [ 0 18  0  0  0  0  0  0  0]
 [ 0 17  0  0  0  0  0  0  0]
 [ 0 13  0  0  0  0  0  0  0]
 [ 0 12  0  0  0  0  0  0  0]
 [ 0 14  0  0  0  0  0  0  0]
 [ 0 14  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [-0.15028796 -0.08145949  0.15339011 ... -0.13732456 -0.1097163
 -0.11457435]
Average of silhouette coef: -0.03821373
---
  1   2   3   4   5   6   7   8   9
1 0.0 1.3 2.2 2.9 3.5 4.1 4.7 5.3 5.9 
2 1.3 0.0 1.0 1.7 2.3 3.0 3.6 4.2 4.9 
3 2.2 1.0 0.0 0.8 1.5 2.1 2.7 3.4 4.1 
4 2.9 1.7 0.8 0.0 0.9 1.4 2.0 2.7 3.4 
5 3.5 2.3 1.5 0.9 0.0 0.9 1.5 2.1 2.8 
6 4.1 3.0 2.1 1.4 0.9 0.0 0.8 1.4 2.2 
7 4.7 3.6 2.7 2.0 1.5 0.8 0.0 0.8 1.5 
8 5.3 4.2 3.4 2.7 2.1 1.4 0.8 0.0 0.8 
9 5.9 4.9 4.1 3.4 2.8 2.2 1.5 0.8 0.0 
correlation [[1.         0.98407059]
 [0.98407059 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 2 1 ... 2 5 5] [[-55.955265  -13.953009 ]
 [-12.222707   13.436757 ]
 [-15.902246  -32.698032 ]
 ...
 [-23.820055   24.90924  ]
 [ 38.199226    6.6683564]
 [ 40.32737   -23.66358  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0 45  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 61  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 45  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
-0.0 -0.0 -0.1 -0.3  0.1  0.4  0.0 -0.0  0.3  3.1  0.0  0.0  0.0 -0.2  0.0  0.1 -0.1 -0.3  0.1 -0.1 
 0.0  0.3 -0.2 -0.2 -0.1  0.4  0.1 -0.0  0.5  2.1 -0.0 -0.0 -0.0 -0.3  0.0 -0.1 -0.5  0.2  0.0 -0.1 
 0.0  0.1  0.1  0.0 -0.1  0.5  0.1 -0.0  0.5  1.3 -0.1  0.1 -0.1 -0.5  0.1 -0.0 -0.8  0.5 -0.0 -0.0 
-0.0 -0.1 -0.0  0.1 -0.2  0.6  0.0 -0.0  0.7  0.6 -0.1  0.1 -0.1 -0.6  0.1  0.1 -0.8  0.7 -0.0 -0.0 
-0.1  0.3 -0.0 -0.3 -0.3  0.8  0.0  0.0  0.8 -0.0 -0.0  0.0 -0.0 -0.6  0.2 -0.1 -0.9  0.5 -0.1 -0.0 
-0.1  0.2 -0.0 -0.1 -0.4  1.0  0.0  0.0  0.8 -0.7  0.1  0.2 -0.0 -0.7 -0.2  0.1 -0.8  0.5 -0.1 -0.0 
-0.1  0.0  0.0  0.0 -0.2  0.8  0.0  0.0  0.9 -1.3  0.1  0.3  0.0 -0.8  0.1  0.0 -0.7  0.4 -0.1 -0.1 
-0.1 -0.1  0.0  0.1 -0.3  0.8  0.0  0.1  0.7 -2.0  0.2  0.1  0.1 -0.8  0.1 -0.0 -0.7  0.5 -0.0 -0.1 
-0.1 -0.0 -0.1  0.1 -0.3  0.7  0.1  0.1  0.8 -2.7  0.3 -0.1  0.1 -0.8  0.1  0.1 -0.3  0.3 -0.0 -0.2 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 0.8342966
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.1 -0.3  0.1  0.4  0.0 -0.0  0.3  3.1  0.0  0.0  0.0 -0.2  0.0  0.1 -0.1 -0.3  0.1 -0.1 
 0.0  0.3 -0.2 -0.2 -0.1  0.4  0.1 -0.0  0.5  2.1 -0.0 -0.0 -0.0 -0.3  0.0 -0.1 -0.5  0.2  0.0 -0.1 
 0.0  0.1  0.1  0.0 -0.1  0.5  0.1 -0.0  0.5  1.3 -0.1  0.1 -0.1 -0.5  0.1 -0.0 -0.8  0.5 -0.0 -0.0 
-0.0 -0.1 -0.0  0.1 -0.2  0.6  0.0 -0.0  0.7  0.6 -0.1  0.1 -0.1 -0.6  0.1  0.1 -0.8  0.7 -0.0 -0.0 
-0.1  0.3 -0.0 -0.3 -0.3  0.8  0.0  0.0  0.8 -0.0 -0.0  0.0 -0.0 -0.6  0.2 -0.1 -0.9  0.5 -0.1 -0.0 
-0.1  0.2 -0.0 -0.1 -0.4  1.0  0.0  0.0  0.8 -0.7  0.1  0.2 -0.0 -0.7 -0.2  0.1 -0.8  0.5 -0.1 -0.0 
-0.1  0.0  0.0  0.0 -0.2  0.8  0.0  0.0  0.9 -1.3  0.1  0.3  0.0 -0.8  0.1  0.0 -0.7  0.4 -0.1 -0.1 
-0.1 -0.1  0.0  0.1 -0.3  0.8  0.0  0.1  0.7 -2.0  0.2  0.1  0.1 -0.8  0.1 -0.0 -0.7  0.5 -0.0 -0.1 
-0.1 -0.0 -0.1  0.1 -0.3  0.7  0.1  0.1  0.8 -2.7  0.3 -0.1  0.1 -0.8  0.1  0.1 -0.3  0.3 -0.0 -0.2 
Minimum distance of calculation. 
true answer: 5 
indices: 3 
distance: 1.6623615
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.1 -0.3  0.1  0.4  0.0 -0.0  0.3  3.1  0.0  0.0  0.0 -0.2  0.0  0.1 -0.1 -0.3  0.1 -0.1 
 0.0  0.3 -0.2 -0.2 -0.1  0.4  0.1 -0.0  0.5  2.1 -0.0 -0.0 -0.0 -0.3  0.0 -0.1 -0.5  0.2  0.0 -0.1 
 0.0  0.1  0.1  0.0 -0.1  0.5  0.1 -0.0  0.5  1.3 -0.1  0.1 -0.1 -0.5  0.1 -0.0 -0.8  0.5 -0.0 -0.0 
-0.0 -0.1 -0.0  0.1 -0.2  0.6  0.0 -0.0  0.7  0.6 -0.1  0.1 -0.1 -0.6  0.1  0.1 -0.8  0.7 -0.0 -0.0 
-0.1  0.3 -0.0 -0.3 -0.3  0.8  0.0  0.0  0.8 -0.0 -0.0  0.0 -0.0 -0.6  0.2 -0.1 -0.9  0.5 -0.1 -0.0 
-0.1  0.2 -0.0 -0.1 -0.4  1.0  0.0  0.0  0.8 -0.7  0.1  0.2 -0.0 -0.7 -0.2  0.1 -0.8  0.5 -0.1 -0.0 
-0.1  0.0  0.0  0.0 -0.2  0.8  0.0  0.0  0.9 -1.3  0.1  0.3  0.0 -0.8  0.1  0.0 -0.7  0.4 -0.1 -0.1 
-0.1 -0.1  0.0  0.1 -0.3  0.8  0.0  0.1  0.7 -2.0  0.2  0.1  0.1 -0.8  0.1 -0.0 -0.7  0.5 -0.0 -0.1 
-0.1 -0.0 -0.1  0.1 -0.3  0.7  0.1  0.1  0.8 -2.7  0.3 -0.1  0.1 -0.8  0.1  0.1 -0.3  0.3 -0.0 -0.2 
Minimum distance of calculation. 
true answer: 8 
indices: 8 
distance: 0.9081954
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0 -0.0 -0.1 -0.3  0.1  0.4  0.0 -0.0  0.3  3.1  0.0  0.0  0.0 -0.2  0.0  0.1 -0.1 -0.3  0.1 -0.1 
 0.0  0.3 -0.2 -0.2 -0.1  0.4  0.1 -0.0  0.5  2.1 -0.0 -0.0 -0.0 -0.3  0.0 -0.1 -0.5  0.2  0.0 -0.1 
 0.0  0.1  0.1  0.0 -0.1  0.5  0.1 -0.0  0.5  1.3 -0.1  0.1 -0.1 -0.5  0.1 -0.0 -0.8  0.5 -0.0 -0.0 
-0.0 -0.1 -0.0  0.1 -0.2  0.6  0.0 -0.0  0.7  0.6 -0.1  0.1 -0.1 -0.6  0.1  0.1 -0.8  0.7 -0.0 -0.0 
-0.1  0.3 -0.0 -0.3 -0.3  0.8  0.0  0.0  0.8 -0.0 -0.0  0.0 -0.0 -0.6  0.2 -0.1 -0.9  0.5 -0.1 -0.0 
-0.1  0.2 -0.0 -0.1 -0.4  1.0  0.0  0.0  0.8 -0.7  0.1  0.2 -0.0 -0.7 -0.2  0.1 -0.8  0.5 -0.1 -0.0 
-0.1  0.0  0.0  0.0 -0.2  0.8  0.0  0.0  0.9 -1.3  0.1  0.3  0.0 -0.8  0.1  0.0 -0.7  0.4 -0.1 -0.1 
-0.1 -0.1  0.0  0.1 -0.3  0.8  0.0  0.1  0.7 -2.0  0.2  0.1  0.1 -0.8  0.1 -0.0 -0.7  0.5 -0.0 -0.1 
-0.1 -0.0 -0.1  0.1 -0.3  0.7  0.1  0.1  0.8 -2.7  0.3 -0.1  0.1 -0.8  0.1  0.1 -0.3  0.3 -0.0 -0.2 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 0.85998356
results (all): {'reconst_1x1_avg': 0.1640625, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.0703125, 'cross_0x1_all': nan, 'cluster_avg': -0.03821373, 'cluster_all': array([-0.15028796, -0.08145949,  0.15339011, ..., -0.13732456,
       -0.1097163 , -0.11457435], dtype=float32), 'magnitude_avg': 0.984070585937432, 'magnitude_all': 0.984070585937432, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.1640625, 'cross_0x1_avg': 0.0703125, 'cluster_avg': -0.03821373, 'magnitude_avg': 0.984070585937432, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 9, 9, 4, 9, 2, 3, 4, 7, 3, 8, 9, 6, 9, 8, 4, 7, 4, 4, 1, 8, 7, 9, 4,
        7, 6, 1, 1, 7, 5, 3, 9, 4, 2, 9, 3, 2, 2, 8, 1, 7, 7, 9, 5, 2, 4, 6, 5,
        5, 6, 6, 5, 9, 5, 8, 3, 8, 6, 7, 4, 6, 3, 1, 8, 5, 4, 7, 1, 4, 5, 5, 3,
        4, 3, 3, 9, 9, 6, 3, 1, 1, 9, 9, 9, 5, 5, 1, 8, 9, 2, 2, 9, 9, 3, 9, 5,
        3, 4, 6, 7, 8, 6, 6, 1, 3, 4, 2, 7, 4, 1, 2, 7, 7, 8, 6, 9, 8, 3, 8, 6,
        3, 8, 5, 6, 8, 4, 5, 3]), ['w7j', 'r9j', 'g9s', 'g4j', 'g9j', 'g2t', 'w3j', 'w4s', 'r7s', 'g3s', 'w8j', 'g9t', 'r6s', 'b9t', 'g8t', 'w4j', 'g7t', 'g4j', 'g4j', 'g1t', 'b8j', 'b7t', 'r9j', 'w4t', 'g7j', 'g6s', 'w1j', 'w1s', 'g7t', 'g5t', 'w3j', 'b9t', 'w4s', 'w2j', 'g9j', 'b3j', 'r2j', 'g2t', 'b8j', 'g1t', 'g7t', 'r7j', 'g9t', 'b5t', 'r2t', 'g4s', 'w6j', 'g5t', 'r5t', 'g6s', 'w6t', 'g5t', 'w9t', 'r5j', 'g8s', 'r3s', 'g8j', 'g6j', 'r7s', 'r4j', 'g6s', 'w3s', 'b1j', 'b8t', 'b5t', 'w4s', 'r7j', 'g1t', 'g4s', 'r5s', 'g5s', 'g3t', 'w4t', 'g3j', 'b3t', 'b9j', 'r9j', 'g6s', 'b3s', 'g1t', 'r1j', 'w9j', 'w9j', 'b9s', 'g5t', 'w5t', 'b1j', 'w8j', 'r9s', 'b2t', 'g2j', 'r9t', 'r9s', 'w3t', 'g9t', 'b5s', 'g3j', 'w4t', 'r6t', 'w7j', 'r8s', 'b6j', 'g6j', 'r1t', 'b3j', 'g4s', 'g2t', 'r7j', 'r4t', 'b1t', 'g2s', 'b7t', 'g7j', 'g8t', 'g6t', 'r9t', 'w8t', 'g3s', 'r8s', 'b6t', 'r3j', 'b8t', 'r5j', 'r6j', 'r8s', 'w4t', 'w5t', 'r3j']] 
label[0]: tensor([7, 9, 9, 4, 9, 2, 3, 4, 7, 3, 8, 9, 6, 9, 8, 4, 7, 4, 4, 1, 8, 7, 9, 4,
        7, 6, 1, 1, 7, 5, 3, 9, 4, 2, 9, 3, 2, 2, 8, 1, 7, 7, 9, 5, 2, 4, 6, 5,
        5, 6, 6, 5, 9, 5, 8, 3, 8, 6, 7, 4, 6, 3, 1, 8, 5, 4, 7, 1, 4, 5, 5, 3,
        4, 3, 3, 9, 9, 6, 3, 1, 1, 9, 9, 9, 5, 5, 1, 8, 9, 2, 2, 9, 9, 3, 9, 5,
        3, 4, 6, 7, 8, 6, 6, 1, 3, 4, 2, 7, 4, 1, 2, 7, 7, 8, 6, 9, 8, 3, 8, 6,
        3, 8, 5, 6, 8, 4, 5, 3]) 
label[1]: ['w7j', 'r9j', 'g9s', 'g4j', 'g9j', 'g2t', 'w3j', 'w4s', 'r7s', 'g3s', 'w8j', 'g9t', 'r6s', 'b9t', 'g8t', 'w4j', 'g7t', 'g4j', 'g4j', 'g1t', 'b8j', 'b7t', 'r9j', 'w4t', 'g7j', 'g6s', 'w1j', 'w1s', 'g7t', 'g5t', 'w3j', 'b9t', 'w4s', 'w2j', 'g9j', 'b3j', 'r2j', 'g2t', 'b8j', 'g1t', 'g7t', 'r7j', 'g9t', 'b5t', 'r2t', 'g4s', 'w6j', 'g5t', 'r5t', 'g6s', 'w6t', 'g5t', 'w9t', 'r5j', 'g8s', 'r3s', 'g8j', 'g6j', 'r7s', 'r4j', 'g6s', 'w3s', 'b1j', 'b8t', 'b5t', 'w4s', 'r7j', 'g1t', 'g4s', 'r5s', 'g5s', 'g3t', 'w4t', 'g3j', 'b3t', 'b9j', 'r9j', 'g6s', 'b3s', 'g1t', 'r1j', 'w9j', 'w9j', 'b9s', 'g5t', 'w5t', 'b1j', 'w8j', 'r9s', 'b2t', 'g2j', 'r9t', 'r9s', 'w3t', 'g9t', 'b5s', 'g3j', 'w4t', 'r6t', 'w7j', 'r8s', 'b6j', 'g6j', 'r1t', 'b3j', 'g4s', 'g2t', 'r7j', 'r4t', 'b1t', 'g2s', 'b7t', 'g7j', 'g8t', 'g6t', 'r9t', 'w8t', 'g3s', 'r8s', 'b6t', 'r3j', 'b8t', 'r5j', 'r6j', 'r8s', 'w4t', 'w5t', 'r3j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.10459962 0.12828687 0.02668167 ... 0.08223518 0.00199218 0.07409751]
Average of silhouette coef: 0.064037815
---
  0   1   2
0 0.0 2.4 4.2 
1 2.4 0.0 2.4 
2 4.2 2.4 0.0 
correlation [[1.         0.99931996]
 [0.99931996 1.        ]]
---
[[], [], []] [0 0 1 ... 0 2 0] [[ 22.39583    30.340076 ]
 [ 56.745907   20.800848 ]
 [-24.71743    55.363453 ]
 ...
 [ 59.975903    1.2469195]
 [ 21.901327   12.078017 ]
 [-15.267717  -35.002666 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': 0.064037815, 'cluster_all': array([0.10459962, 0.12828687, 0.02668167, ..., 0.08223518, 0.00199218,
       0.07409751], dtype=float32), 'magnitude_avg': 0.9993199637120899, 'magnitude_all': 0.9993199637120899, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_0', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': 0.064037815, 'magnitude_avg': 0.9993199637120899, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 1, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_1', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1', 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_1
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1',
 'run_id': 'mmvae_cmnist_oscn_seed_1',
 'run_type': 'train',
 'seed': 1,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([1, 8, 7, 3, 7, 6, 6, 2, 1, 5, 6, 3, 6, 3, 8, 5, 5, 1, 5, 4, 8, 4, 5, 9,
        3, 6, 5, 7, 7, 2, 6, 2, 3, 9, 2, 9, 7, 5, 7, 4, 7, 9, 2, 3, 4, 9, 3, 4,
        5, 6, 7, 7, 8, 2, 2, 4, 7, 3, 5, 4, 1, 7, 8, 7, 9, 8, 4, 8, 8, 6, 3, 9,
        9, 9, 3, 6, 7, 3, 9, 3, 1, 7, 3, 4, 6, 3, 7, 6, 7, 1, 1, 6, 1, 5, 7, 7,
        7, 3, 6, 7, 1, 2, 9, 6, 9, 3, 5, 2, 2, 5, 8, 2, 1, 2, 9, 9, 3, 7, 2, 2,
        4, 9, 1, 6, 6, 1, 5, 9]), ['g1s', 'b8t', 'b7s', 'r3t', 'r7j', 'g6j', 'g6t', 'r2j', 'r1t', 'w5j', 'w6t', 'r3j', 'b6j', 'b3t', 'r8j', 'g5j', 'g5t', 'b1s', 'g5j', 'g4j', 'r8t', 'g4s', 'w5j', 'w9t', 'r3t', 'w6t', 'r5s', 'b7j', 'w7t', 'g2s', 'g6j', 'b2t', 'b3j', 'r9s', 'g2s', 'b9t', 'w7t', 'b5s', 'g7j', 'g4s', 'b7j', 'b9j', 'r2j', 'r3t', 'b4t', 'b9j', 'r3s', 'r4t', 'g5j', 'w6j', 'w7s', 'w7j', 'b8t', 'w2j', 'w2j', 'b4j', 'b7s', 'w3j', 'b5j', 'b4s', 'r1s', 'w7s', 'r8s', 'w7t', 'b9t', 'r8j', 'g4j', 'g8s', 'g8s', 'g6s', 'g3s', 'w9t', 'g9t', 'g9s', 'g3j', 'r6j', 'w7j', 'g3j', 'r9j', 'g3t', 'b1j', 'w7s', 'r3t', 'w4j', 'w6t', 'b3j', 'w7s', 'r6s', 'b7s', 'b1t', 'b1s', 'b6s', 'r1j', 'g5j', 'g7s', 'g7j', 'r7t', 'w3s', 'r6t', 'b7j', 'w1j', 'b2t', 'r9t', 'b6j', 'b9j', 'b3s', 'w5j', 'r2t', 'w2t', 'r5t', 'w8s', 'g2j', 'g1t', 'r2s', 'r9s', 'w9s', 'b3t', 'b7s', 'b2t', 'r2s', 'w4j', 'w9j', 'b1s', 'w6j', 'w6t', 'b1t', 'w5j', 'g9t']] 
label[0]: tensor([1, 8, 7, 3, 7, 6, 6, 2, 1, 5, 6, 3, 6, 3, 8, 5, 5, 1, 5, 4, 8, 4, 5, 9,
        3, 6, 5, 7, 7, 2, 6, 2, 3, 9, 2, 9, 7, 5, 7, 4, 7, 9, 2, 3, 4, 9, 3, 4,
        5, 6, 7, 7, 8, 2, 2, 4, 7, 3, 5, 4, 1, 7, 8, 7, 9, 8, 4, 8, 8, 6, 3, 9,
        9, 9, 3, 6, 7, 3, 9, 3, 1, 7, 3, 4, 6, 3, 7, 6, 7, 1, 1, 6, 1, 5, 7, 7,
        7, 3, 6, 7, 1, 2, 9, 6, 9, 3, 5, 2, 2, 5, 8, 2, 1, 2, 9, 9, 3, 7, 2, 2,
        4, 9, 1, 6, 6, 1, 5, 9]) 
label[1]: ['g1s', 'b8t', 'b7s', 'r3t', 'r7j', 'g6j', 'g6t', 'r2j', 'r1t', 'w5j', 'w6t', 'r3j', 'b6j', 'b3t', 'r8j', 'g5j', 'g5t', 'b1s', 'g5j', 'g4j', 'r8t', 'g4s', 'w5j', 'w9t', 'r3t', 'w6t', 'r5s', 'b7j', 'w7t', 'g2s', 'g6j', 'b2t', 'b3j', 'r9s', 'g2s', 'b9t', 'w7t', 'b5s', 'g7j', 'g4s', 'b7j', 'b9j', 'r2j', 'r3t', 'b4t', 'b9j', 'r3s', 'r4t', 'g5j', 'w6j', 'w7s', 'w7j', 'b8t', 'w2j', 'w2j', 'b4j', 'b7s', 'w3j', 'b5j', 'b4s', 'r1s', 'w7s', 'r8s', 'w7t', 'b9t', 'r8j', 'g4j', 'g8s', 'g8s', 'g6s', 'g3s', 'w9t', 'g9t', 'g9s', 'g3j', 'r6j', 'w7j', 'g3j', 'r9j', 'g3t', 'b1j', 'w7s', 'r3t', 'w4j', 'w6t', 'b3j', 'w7s', 'r6s', 'b7s', 'b1t', 'b1s', 'b6s', 'r1j', 'g5j', 'g7s', 'g7j', 'r7t', 'w3s', 'r6t', 'b7j', 'w1j', 'b2t', 'r9t', 'b6j', 'b9j', 'b3s', 'w5j', 'r2t', 'w2t', 'r5t', 'w8s', 'g2j', 'g1t', 'r2s', 'r9s', 'w9s', 'b3t', 'b7s', 'b2t', 'r2s', 'w4j', 'w9j', 'b1s', 'w6j', 'w6t', 'b1t', 'w5j', 'g9t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.12267447 0.12592946 0.07379701 ... 0.13615859 0.18239304 0.13740382]
Average of silhouette coef: 0.076019414
---
  0   1   2   3
0 0.0 3.5 2.6 2.5 
1 3.5 0.0 4.2 4.1 
2 2.6 4.2 0.0 4.0 
3 2.5 4.1 4.0 0.0 
correlation [[ 1.         -0.70972499]
 [-0.70972499  1.        ]]
---
[[], [], [], []] [2 1 1 ... 3 1 2] [[-23.203394  -49.4238   ]
 [ 41.28609    25.742643 ]
 [-42.382206   44.51396  ]
 ...
 [  3.7552598 -19.597578 ]
 [-45.744358   -1.7973237]
 [ 32.131264  -25.137487 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.076019414, 'cluster_all': array([0.12267447, 0.12592946, 0.07379701, ..., 0.13615859, 0.18239304,
       0.13740382], dtype=float32), 'magnitude_avg': -0.7097249892302372, 'magnitude_all': -0.7097249892302372, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.076019414, 'magnitude_avg': -0.7097249892302372, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 6, 6, 1, 1, 4, 3, 9, 9, 9, 5, 6, 9, 1, 9, 9, 8, 3, 7, 5, 7, 8, 7, 9,
        9, 1, 1, 9, 3, 4, 8, 3, 6, 9, 9, 7, 8, 8, 2, 9, 9, 1, 3, 1, 3, 7, 5, 9,
        7, 1, 3, 6, 8, 2, 3, 4, 9, 2, 1, 5, 8, 1, 7, 6, 3, 9, 5, 9, 1, 2, 5, 1,
        9, 8, 7, 5, 8, 1, 1, 8, 8, 6, 4, 6, 9, 7, 6, 7, 3, 9, 5, 6, 6, 9, 5, 4,
        8, 2, 6, 5, 5, 9, 8, 4, 6, 9, 5, 2, 4, 7, 4, 2, 4, 2, 8, 8, 7, 3, 9, 5,
        4, 9, 9, 3, 4, 3, 8, 8]), ['g7j', 'g6t', 'w6t', 'w1t', 'g1t', 'g4s', 'g3t', 'g9t', 'g9s', 'w9t', 'b5t', 'b6t', 'g9s', 'r1j', 'r9s', 'b9s', 'b8t', 'g3s', 'r7j', 'b5j', 'r7j', 'w8s', 'w7j', 'w9t', 'g9s', 'b1t', 'r1j', 'r9s', 'b3j', 'g4j', 'w8s', 'g3j', 'g6j', 'b9s', 'b9t', 'b7t', 'w8t', 'w8j', 'r2j', 'g9j', 'r9j', 'r1t', 'b3j', 'w1j', 'b3t', 'r7s', 'r5s', 'b9s', 'w7s', 'b1t', 'w3t', 'b6j', 'b8t', 'g2j', 'b3j', 'b4j', 'w9t', 'g2s', 'w1s', 'g5t', 'g8t', 'g1t', 'w7j', 'r6s', 'r3t', 'r9j', 'r5j', 'g9s', 'r1j', 'b2s', 'w5s', 'w1s', 'b9t', 'g8j', 'w7s', 'w5s', 'g8s', 'g1s', 'w1s', 'g8j', 'w8s', 'g6j', 'r4j', 'w6s', 'w9j', 'b7t', 'r6j', 'b7j', 'b3t', 'r9s', 'g5s', 'w6j', 'g6s', 'b9s', 'g5s', 'w4t', 'b8t', 'w2j', 'w6s', 'r5j', 'g5j', 'r9t', 'r8s', 'g4s', 'w6t', 'r9j', 'r5s', 'b2j', 'b4s', 'r7j', 'w4t', 'w2s', 'w4t', 'w2s', 'g8t', 'g8s', 'g7s', 'b3j', 'b9j', 'w5s', 'w4j', 'g9j', 'r9s', 'r3j', 'r4t', 'w3t', 'b8j', 'r8j']] 
label[0]: tensor([7, 6, 6, 1, 1, 4, 3, 9, 9, 9, 5, 6, 9, 1, 9, 9, 8, 3, 7, 5, 7, 8, 7, 9,
        9, 1, 1, 9, 3, 4, 8, 3, 6, 9, 9, 7, 8, 8, 2, 9, 9, 1, 3, 1, 3, 7, 5, 9,
        7, 1, 3, 6, 8, 2, 3, 4, 9, 2, 1, 5, 8, 1, 7, 6, 3, 9, 5, 9, 1, 2, 5, 1,
        9, 8, 7, 5, 8, 1, 1, 8, 8, 6, 4, 6, 9, 7, 6, 7, 3, 9, 5, 6, 6, 9, 5, 4,
        8, 2, 6, 5, 5, 9, 8, 4, 6, 9, 5, 2, 4, 7, 4, 2, 4, 2, 8, 8, 7, 3, 9, 5,
        4, 9, 9, 3, 4, 3, 8, 8]) 
label[1]: ['g7j', 'g6t', 'w6t', 'w1t', 'g1t', 'g4s', 'g3t', 'g9t', 'g9s', 'w9t', 'b5t', 'b6t', 'g9s', 'r1j', 'r9s', 'b9s', 'b8t', 'g3s', 'r7j', 'b5j', 'r7j', 'w8s', 'w7j', 'w9t', 'g9s', 'b1t', 'r1j', 'r9s', 'b3j', 'g4j', 'w8s', 'g3j', 'g6j', 'b9s', 'b9t', 'b7t', 'w8t', 'w8j', 'r2j', 'g9j', 'r9j', 'r1t', 'b3j', 'w1j', 'b3t', 'r7s', 'r5s', 'b9s', 'w7s', 'b1t', 'w3t', 'b6j', 'b8t', 'g2j', 'b3j', 'b4j', 'w9t', 'g2s', 'w1s', 'g5t', 'g8t', 'g1t', 'w7j', 'r6s', 'r3t', 'r9j', 'r5j', 'g9s', 'r1j', 'b2s', 'w5s', 'w1s', 'b9t', 'g8j', 'w7s', 'w5s', 'g8s', 'g1s', 'w1s', 'g8j', 'w8s', 'g6j', 'r4j', 'w6s', 'w9j', 'b7t', 'r6j', 'b7j', 'b3t', 'r9s', 'g5s', 'w6j', 'g6s', 'b9s', 'g5s', 'w4t', 'b8t', 'w2j', 'w6s', 'r5j', 'g5j', 'r9t', 'r8s', 'g4s', 'w6t', 'r9j', 'r5s', 'b2j', 'b4s', 'r7j', 'w4t', 'w2s', 'w4t', 'w2s', 'g8t', 'g8s', 'g7s', 'b3j', 'b9j', 'w5s', 'w4j', 'g9j', 'r9s', 'r3j', 'r4t', 'w3t', 'b8j', 'r8j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([4, 5, 9, 2, 4, 5, 9, 5, 2, 7, 4, 4, 1, 8, 5, 1, 4, 4, 4, 4, 2, 3, 3, 1,
        7, 2, 8, 1, 8, 4, 7, 9, 9, 4, 4, 5, 4, 4, 3, 1, 9, 4, 4, 5, 1, 3, 9, 8,
        5, 5, 1, 4, 3, 7, 3, 8, 9, 2, 4, 6, 7, 1, 4, 2, 1, 3, 7, 4, 9, 3, 9, 2,
        4, 7, 4, 6, 7, 5, 2, 3, 7, 4, 9, 1, 6, 1, 2, 1, 2, 1, 1, 3, 8, 1, 7, 3,
        3, 1, 3, 5, 1, 4, 5, 9, 6, 2, 1, 3, 5, 6, 4, 1, 8, 6, 8, 7, 4, 8, 6, 3,
        9, 4, 5, 3, 7, 6, 4, 1])
Accuracy (count): tensor(8) 
Accuracy (ratio) tensor(0.0625)
Accuracy:
 [[ 0  0  0  0  0 20  0  0  0]
 [ 0  0  0  0  0 11  0  0  0]
 [ 0  0  0  0  0 16  0  0  0]
 [ 0  0  0  0  0 27  0  0  0]
 [ 0  0  0  0  0 13  0  0  0]
 [ 0  0  0  0  0  8  0  0  0]
 [ 0  0  0  0  0 12  0  0  0]
 [ 0  0  0  0  0  9  0  0  0]
 [ 0  0  0  0  0 12  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([7, 4, 4, 1, 2, 1, 4, 9, 2, 2, 5, 1, 1, 7, 6, 8, 4, 6, 5, 7, 3, 8, 7, 3,
        3, 1, 2, 1, 2, 6, 8, 5, 9, 5, 3, 4, 4, 5, 7, 7, 5, 7, 4, 9, 3, 8, 6, 1,
        5, 1, 5, 9, 8, 7, 5, 8, 2, 9, 7, 2, 5, 7, 2, 5, 1, 1, 2, 4, 8, 2, 6, 1,
        2, 3, 2, 3, 6, 6, 7, 5, 5, 1, 7, 6, 5, 4, 2, 8, 6, 8, 5, 7, 8, 4, 4, 7,
        5, 6, 1, 4, 5, 1, 5, 1, 7, 2, 6, 8, 7, 6, 3, 7, 6, 1, 6, 8, 7, 4, 9, 6,
        1, 8, 1, 3, 6, 8, 7, 1])
Accuracy (count): tensor(16) 
Accuracy (ratio) tensor(0.1250)
Accuracy:
 [[ 0  0  0  0  0 19  0  0  0]
 [ 0  0  0  0  0 14  0  0  0]
 [ 0  0  0  0  0  9  0  0  0]
 [ 0  0  0  0  0 13  0  0  0]
 [ 0  0  0  0  0 18  0  0  0]
 [ 0  0  0  0  0 16  0  0  0]
 [ 0  0  0  0  0 19  0  0  0]
 [ 0  0  0  0  0 14  0  0  0]
 [ 0  0  0  0  0  6  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]]
---
Silhouette values: [0.09706739 0.0894123  0.16631359 ... 0.0778986  0.18875498 0.07385454]
Average of silhouette coef: 0.05318614
---
  1   2   3   4   5   6   7   8   9
1 0.0 3.8 5.4 6.6 5.6 4.9 5.7 5.9 7.0 
2 3.8 0.0 4.8 5.0 4.8 3.6 5.3 3.8 5.7 
3 5.4 4.8 0.0 5.5 2.5 4.5 5.4 3.0 5.1 
4 6.6 5.0 5.5 0.0 4.4 3.5 3.7 3.8 1.6 
5 5.6 4.8 2.5 4.4 0.0 4.2 4.9 3.1 4.3 
6 4.9 3.6 4.5 3.5 4.2 0.0 5.1 3.6 4.0 
7 5.7 5.3 5.4 3.7 4.9 5.1 0.0 4.9 3.2 
8 5.9 3.8 3.0 3.8 3.1 3.6 4.9 0.0 3.9 
9 7.0 5.7 5.1 1.6 4.3 4.0 3.2 3.9 0.0 
correlation [[1.         0.27347091]
 [0.27347091 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 6 6 ... 7 7 3] [[-43.332092   44.957813 ]
 [  6.0014596  18.537378 ]
 [ -7.2535577  11.927683 ]
 ...
 [-12.696175   43.296803 ]
 [-41.690502   46.0994   ]
 [ 19.243753  -40.797665 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(60) 
Accuracy (ratio) tensor(0.1227)
Accuracy:
 [[ 0  0  0  0  0 45  0  0  0]
 [ 0  0  0  0  0 52  0  0  0]
 [ 0  0  0  0  0 57  0  0  0]
 [ 0  0  0  0  0 60  0  0  0]
 [ 0  0  0  0  0 61  0  0  0]
 [ 0  0  0  0  0 60  0  0  0]
 [ 0  0  0  0  0 57  0  0  0]
 [ 0  0  0  0  0 52  0  0  0]
 [ 0  0  0  0  0 45  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 2.6  0.0 -0.2 -0.1  0.3  0.2 -0.1 -0.3  0.4 -0.0  0.0 -0.4 -0.4  0.4 -0.1 -0.2  0.1  0.6  0.1  0.2 
 0.4 -0.1 -0.5  1.1 -1.4 -0.2  0.5 -0.4  0.1 -0.2  0.1 -0.9 -0.3  1.7 -1.7 -0.3  0.3  0.4  0.3  0.1 
-1.3 -0.0 -1.2 -0.1  0.4  0.0 -3.0 -0.3 -0.0 -0.0 -0.0 -1.5  0.0  2.1 -0.4 -0.3 -0.1  0.1  0.5  0.1 
-3.7 -0.2  0.4  0.9 -0.7 -0.1  0.6 -0.3 -0.2  0.1  0.0 -0.7 -0.0 -0.3  0.0 -0.0  0.1  0.1  0.4  0.1 
-1.8 -0.1 -0.0 -0.5 -1.0  0.1 -2.9 -0.2 -0.3 -0.2  0.1 -1.3 -0.0  0.8 -0.2 -0.1 -0.2  0.1  0.1  0.3 
-1.3 -0.1 -0.5  1.2  0.1 -0.0  0.1 -0.3 -0.2 -0.2 -0.1 -2.5 -0.2 -0.3 -1.3 -0.1 -0.0  0.2  0.4  0.1 
-2.4 -0.1 -0.4 -0.8  0.5 -0.0  0.2 -0.3 -0.1  0.2  0.0  1.9 -0.2 -0.0 -0.2 -0.0  0.0 -0.1  0.4  0.1 
-2.4 -0.2 -0.1  0.3 -0.5 -0.1 -0.9 -0.4 -0.2  0.2 -0.0 -1.8 -0.1  2.5 -0.7  0.0 -0.1  0.3  0.2  0.2 
-4.3 -0.2 -0.2  0.2  0.2 -0.0  0.0 -0.2 -0.2  0.1  0.0 -0.4 -0.0 -0.3 -0.0 -0.1  0.0  0.0  0.5  0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 3.9134495
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 2.6  0.0 -0.2 -0.1  0.3  0.2 -0.1 -0.3  0.4 -0.0  0.0 -0.4 -0.4  0.4 -0.1 -0.2  0.1  0.6  0.1  0.2 
 0.4 -0.1 -0.5  1.1 -1.4 -0.2  0.5 -0.4  0.1 -0.2  0.1 -0.9 -0.3  1.7 -1.7 -0.3  0.3  0.4  0.3  0.1 
-1.3 -0.0 -1.2 -0.1  0.4  0.0 -3.0 -0.3 -0.0 -0.0 -0.0 -1.5  0.0  2.1 -0.4 -0.3 -0.1  0.1  0.5  0.1 
-3.7 -0.2  0.4  0.9 -0.7 -0.1  0.6 -0.3 -0.2  0.1  0.0 -0.7 -0.0 -0.3  0.0 -0.0  0.1  0.1  0.4  0.1 
-1.8 -0.1 -0.0 -0.5 -1.0  0.1 -2.9 -0.2 -0.3 -0.2  0.1 -1.3 -0.0  0.8 -0.2 -0.1 -0.2  0.1  0.1  0.3 
-1.3 -0.1 -0.5  1.2  0.1 -0.0  0.1 -0.3 -0.2 -0.2 -0.1 -2.5 -0.2 -0.3 -1.3 -0.1 -0.0  0.2  0.4  0.1 
-2.4 -0.1 -0.4 -0.8  0.5 -0.0  0.2 -0.3 -0.1  0.2  0.0  1.9 -0.2 -0.0 -0.2 -0.0  0.0 -0.1  0.4  0.1 
-2.4 -0.2 -0.1  0.3 -0.5 -0.1 -0.9 -0.4 -0.2  0.2 -0.0 -1.8 -0.1  2.5 -0.7  0.0 -0.1  0.3  0.2  0.2 
-4.3 -0.2 -0.2  0.2  0.2 -0.0  0.0 -0.2 -0.2  0.1  0.0 -0.4 -0.0 -0.3 -0.0 -0.1  0.0  0.0  0.5  0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 3.788988
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 2.6  0.0 -0.2 -0.1  0.3  0.2 -0.1 -0.3  0.4 -0.0  0.0 -0.4 -0.4  0.4 -0.1 -0.2  0.1  0.6  0.1  0.2 
 0.4 -0.1 -0.5  1.1 -1.4 -0.2  0.5 -0.4  0.1 -0.2  0.1 -0.9 -0.3  1.7 -1.7 -0.3  0.3  0.4  0.3  0.1 
-1.3 -0.0 -1.2 -0.1  0.4  0.0 -3.0 -0.3 -0.0 -0.0 -0.0 -1.5  0.0  2.1 -0.4 -0.3 -0.1  0.1  0.5  0.1 
-3.7 -0.2  0.4  0.9 -0.7 -0.1  0.6 -0.3 -0.2  0.1  0.0 -0.7 -0.0 -0.3  0.0 -0.0  0.1  0.1  0.4  0.1 
-1.8 -0.1 -0.0 -0.5 -1.0  0.1 -2.9 -0.2 -0.3 -0.2  0.1 -1.3 -0.0  0.8 -0.2 -0.1 -0.2  0.1  0.1  0.3 
-1.3 -0.1 -0.5  1.2  0.1 -0.0  0.1 -0.3 -0.2 -0.2 -0.1 -2.5 -0.2 -0.3 -1.3 -0.1 -0.0  0.2  0.4  0.1 
-2.4 -0.1 -0.4 -0.8  0.5 -0.0  0.2 -0.3 -0.1  0.2  0.0  1.9 -0.2 -0.0 -0.2 -0.0  0.0 -0.1  0.4  0.1 
-2.4 -0.2 -0.1  0.3 -0.5 -0.1 -0.9 -0.4 -0.2  0.2 -0.0 -1.8 -0.1  2.5 -0.7  0.0 -0.1  0.3  0.2  0.2 
-4.3 -0.2 -0.2  0.2  0.2 -0.0  0.0 -0.2 -0.2  0.1  0.0 -0.4 -0.0 -0.3 -0.0 -0.1  0.0  0.0  0.5  0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 9 
distance: 3.5513332
pred: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 2.6  0.0 -0.2 -0.1  0.3  0.2 -0.1 -0.3  0.4 -0.0  0.0 -0.4 -0.4  0.4 -0.1 -0.2  0.1  0.6  0.1  0.2 
 0.4 -0.1 -0.5  1.1 -1.4 -0.2  0.5 -0.4  0.1 -0.2  0.1 -0.9 -0.3  1.7 -1.7 -0.3  0.3  0.4  0.3  0.1 
-1.3 -0.0 -1.2 -0.1  0.4  0.0 -3.0 -0.3 -0.0 -0.0 -0.0 -1.5  0.0  2.1 -0.4 -0.3 -0.1  0.1  0.5  0.1 
-3.7 -0.2  0.4  0.9 -0.7 -0.1  0.6 -0.3 -0.2  0.1  0.0 -0.7 -0.0 -0.3  0.0 -0.0  0.1  0.1  0.4  0.1 
-1.8 -0.1 -0.0 -0.5 -1.0  0.1 -2.9 -0.2 -0.3 -0.2  0.1 -1.3 -0.0  0.8 -0.2 -0.1 -0.2  0.1  0.1  0.3 
-1.3 -0.1 -0.5  1.2  0.1 -0.0  0.1 -0.3 -0.2 -0.2 -0.1 -2.5 -0.2 -0.3 -1.3 -0.1 -0.0  0.2  0.4  0.1 
-2.4 -0.1 -0.4 -0.8  0.5 -0.0  0.2 -0.3 -0.1  0.2  0.0  1.9 -0.2 -0.0 -0.2 -0.0  0.0 -0.1  0.4  0.1 
-2.4 -0.2 -0.1  0.3 -0.5 -0.1 -0.9 -0.4 -0.2  0.2 -0.0 -1.8 -0.1  2.5 -0.7  0.0 -0.1  0.3  0.2  0.2 
-4.3 -0.2 -0.2  0.2  0.2 -0.0  0.0 -0.2 -0.2  0.1  0.0 -0.4 -0.0 -0.3 -0.0 -0.1  0.0  0.0  0.5  0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 4.765863
results (all): {'reconst_0x0_avg': 0.0625, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.125, 'cross_1x0_all': nan, 'cluster_avg': 0.05318614, 'cluster_all': array([0.09706739, 0.0894123 , 0.16631359, ..., 0.0778986 , 0.18875498,
       0.07385454], dtype=float32), 'magnitude_avg': 0.2734709090352406, 'magnitude_all': 0.2734709090352406, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 1.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.0625, 'cross_1x0_avg': 0.125, 'cluster_avg': 0.05318614, 'magnitude_avg': 0.2734709090352406, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12269938737154007, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 1.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([8, 4, 1, 8, 9, 8, 3, 6, 2, 3, 3, 6, 7, 9, 8, 2, 4, 7, 1, 5, 8, 2, 9, 8,
        1, 1, 5, 8, 6, 5, 3, 1, 3, 3, 2, 1, 3, 1, 5, 2, 7, 5, 7, 4, 4, 4, 7, 3,
        6, 8, 8, 5, 9, 1, 5, 1, 6, 6, 9, 6, 8, 5, 1, 3, 1, 2, 7, 3, 4, 3, 5, 3,
        7, 7, 8, 1, 4, 9, 6, 5, 8, 5, 2, 1, 4, 7, 2, 4, 3, 1, 8, 7, 2, 4, 1, 1,
        1, 3, 8, 3, 3, 7, 6, 1, 4, 6, 1, 3, 1, 4, 4, 4, 6, 3, 1, 4, 3, 6, 5, 4,
        1, 8, 4, 2, 1, 3, 5, 2]), ['r8t', 'w4t', 'b1j', 'r8t', 'g9j', 'g8s', 'w3t', 'g6s', 'g2j', 'b3j', 'w3j', 'g6s', 'b7t', 'g9j', 'b8t', 'w2j', 'w4t', 'b7j', 'g1j', 'w5s', 'g8j', 'w2j', 'w9s', 'r8s', 'g1j', 'w1s', 'b5t', 'b8t', 'g6t', 'r5t', 'w3j', 'w1j', 'g3t', 'r3s', 'g2s', 'r1j', 'g3s', 'w1j', 'r5s', 'r2j', 'g7s', 'b5s', 'r7s', 'b4t', 'b4s', 'w4s', 'r7s', 'r3t', 'g6t', 'r8j', 'r8t', 'g5j', 'b9s', 'b1s', 'w5s', 'b1t', 'r6j', 'r6t', 'b9j', 'r6j', 'r8s', 'b5s', 'w1j', 'b3t', 'w1j', 'g2j', 'g7j', 'r3t', 'b4t', 'b3j', 'g5s', 'w3s', 'r7t', 'b7t', 'g8t', 'r1s', 'w4s', 'w9t', 'r6t', 'g5t', 'g8s', 'w5j', 'r2s', 'g1t', 'b4s', 'g7t', 'g2t', 'g4s', 'g3t', 'g1s', 'w8s', 'g7s', 'r2s', 'b4s', 'r1t', 'g1s', 'b1t', 'r3j', 'w8s', 'g3s', 'b3j', 'b7s', 'b6t', 'g1s', 'g4j', 'g6s', 'g1j', 'b3j', 'w1t', 'w4s', 'w4s', 'g4j', 'g6t', 'r3t', 'r1s', 'r4s', 'b3t', 'b6s', 'b5s', 'r4j', 'w1j', 'g8j', 'g4j', 'r2t', 'w1j', 'r3t', 'b5j', 'g2t']] 
label[0]: tensor([8, 4, 1, 8, 9, 8, 3, 6, 2, 3, 3, 6, 7, 9, 8, 2, 4, 7, 1, 5, 8, 2, 9, 8,
        1, 1, 5, 8, 6, 5, 3, 1, 3, 3, 2, 1, 3, 1, 5, 2, 7, 5, 7, 4, 4, 4, 7, 3,
        6, 8, 8, 5, 9, 1, 5, 1, 6, 6, 9, 6, 8, 5, 1, 3, 1, 2, 7, 3, 4, 3, 5, 3,
        7, 7, 8, 1, 4, 9, 6, 5, 8, 5, 2, 1, 4, 7, 2, 4, 3, 1, 8, 7, 2, 4, 1, 1,
        1, 3, 8, 3, 3, 7, 6, 1, 4, 6, 1, 3, 1, 4, 4, 4, 6, 3, 1, 4, 3, 6, 5, 4,
        1, 8, 4, 2, 1, 3, 5, 2]) 
label[1]: ['r8t', 'w4t', 'b1j', 'r8t', 'g9j', 'g8s', 'w3t', 'g6s', 'g2j', 'b3j', 'w3j', 'g6s', 'b7t', 'g9j', 'b8t', 'w2j', 'w4t', 'b7j', 'g1j', 'w5s', 'g8j', 'w2j', 'w9s', 'r8s', 'g1j', 'w1s', 'b5t', 'b8t', 'g6t', 'r5t', 'w3j', 'w1j', 'g3t', 'r3s', 'g2s', 'r1j', 'g3s', 'w1j', 'r5s', 'r2j', 'g7s', 'b5s', 'r7s', 'b4t', 'b4s', 'w4s', 'r7s', 'r3t', 'g6t', 'r8j', 'r8t', 'g5j', 'b9s', 'b1s', 'w5s', 'b1t', 'r6j', 'r6t', 'b9j', 'r6j', 'r8s', 'b5s', 'w1j', 'b3t', 'w1j', 'g2j', 'g7j', 'r3t', 'b4t', 'b3j', 'g5s', 'w3s', 'r7t', 'b7t', 'g8t', 'r1s', 'w4s', 'w9t', 'r6t', 'g5t', 'g8s', 'w5j', 'r2s', 'g1t', 'b4s', 'g7t', 'g2t', 'g4s', 'g3t', 'g1s', 'w8s', 'g7s', 'r2s', 'b4s', 'r1t', 'g1s', 'b1t', 'r3j', 'w8s', 'g3s', 'b3j', 'b7s', 'b6t', 'g1s', 'g4j', 'g6s', 'g1j', 'b3j', 'w1t', 'w4s', 'w4s', 'g4j', 'g6t', 'r3t', 'r1s', 'r4s', 'b3t', 'b6s', 'b5s', 'r4j', 'w1j', 'g8j', 'g4j', 'r2t', 'w1j', 'r3t', 'b5j', 'g2t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.01060539  0.00800548  0.00356257 ... -0.00190371 -0.01172817
 -0.00914907]
Average of silhouette coef: -0.0045082364
---
  0   1   2
0 0.0 0.2 0.4 
1 0.2 0.0 0.2 
2 0.4 0.2 0.0 
correlation [[1.         0.97686352]
 [0.97686352 1.        ]]
---
[[], [], []] [2 2 0 ... 0 1 0] [[ -9.986809  -26.04253  ]
 [  4.2221437  38.88869  ]
 [-42.523354  -50.48305  ]
 ...
 [-40.61546   -22.675179 ]
 [ -8.160631   45.359386 ]
 [  6.2923594  -3.9835515]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.0045082364, 'cluster_all': array([ 0.01060539,  0.00800548,  0.00356257, ..., -0.00190371,
       -0.01172817, -0.00914907], dtype=float32), 'magnitude_avg': 0.9768635166428102, 'magnitude_all': 0.9768635166428102, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.0045082364, 'magnitude_avg': 0.9768635166428102, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 2, 4, 4, 9, 1, 4, 7, 9, 6, 4, 3, 9, 5, 1, 2, 1, 2, 5, 8, 7, 1, 8, 5,
        8, 9, 1, 2, 4, 2, 3, 9, 8, 2, 1, 1, 4, 1, 2, 3, 2, 1, 7, 3, 3, 3, 4, 9,
        9, 1, 4, 8, 5, 1, 1, 3, 5, 5, 2, 5, 6, 6, 1, 7, 6, 7, 8, 2, 5, 1, 4, 1,
        4, 3, 6, 5, 3, 4, 4, 4, 2, 9, 8, 8, 2, 6, 5, 8, 9, 6, 8, 3, 4, 5, 1, 9,
        8, 1, 6, 3, 9, 5, 4, 2, 7, 7, 1, 4, 1, 4, 4, 2, 8, 2, 2, 6, 5, 9, 1, 8,
        5, 9, 3, 8, 8, 5, 9, 5]), ['w2j', 'r2j', 'w4s', 'g4s', 'b9t', 'r1t', 'g4s', 'b7j', 'w9j', 'g6t', 'r4j', 'w3t', 'w9j', 'g5j', 'r1s', 'r2s', 'w1j', 'r2t', 'r5s', 'g8s', 'r7j', 'b1s', 'r8j', 'b5j', 'w8t', 'w9s', 'r1s', 'w2s', 'w4t', 'g2j', 'b3s', 'w9j', 'g8t', 'r2j', 'b1s', 'g1t', 'b4s', 'r1j', 'b2t', 'g3j', 'b2j', 'r1t', 'b7s', 'g3s', 'b3s', 'g3t', 'r4t', 'g9t', 'g9s', 'w1j', 'b4s', 'w8s', 'r5t', 'b1t', 'g1t', 'g3j', 'b5s', 'r5j', 'g2s', 'b5s', 'r6t', 'g6t', 'r1j', 'r7t', 'g6j', 'b7s', 'g8j', 'r2j', 'r5j', 'b1j', 'r4s', 'w1s', 'b4s', 'g3t', 'w6s', 'w5j', 'g3t', 'w4j', 'g4s', 'b4s', 'g2s', 'w9s', 'r8t', 'r8s', 'b2t', 'b6t', 'g5j', 'b8s', 'r9s', 'r6j', 'g8t', 'r3s', 'r4j', 'w5t', 'r1j', 'g9t', 'w8t', 'g1t', 'b6t', 'w3j', 'w9s', 'w5s', 'g4s', 'r2s', 'w7j', 'b7j', 'g1s', 'b4s', 'r1j', 'b4s', 'b4j', 'b2t', 'b8j', 'b2s', 'w2s', 'g6t', 'w5s', 'b9t', 'g1t', 'b8s', 'b5t', 'b9t', 'g3t', 'g8t', 'r8t', 'b5s', 'r9t', 'r5t']] 
label[0]: tensor([2, 2, 4, 4, 9, 1, 4, 7, 9, 6, 4, 3, 9, 5, 1, 2, 1, 2, 5, 8, 7, 1, 8, 5,
        8, 9, 1, 2, 4, 2, 3, 9, 8, 2, 1, 1, 4, 1, 2, 3, 2, 1, 7, 3, 3, 3, 4, 9,
        9, 1, 4, 8, 5, 1, 1, 3, 5, 5, 2, 5, 6, 6, 1, 7, 6, 7, 8, 2, 5, 1, 4, 1,
        4, 3, 6, 5, 3, 4, 4, 4, 2, 9, 8, 8, 2, 6, 5, 8, 9, 6, 8, 3, 4, 5, 1, 9,
        8, 1, 6, 3, 9, 5, 4, 2, 7, 7, 1, 4, 1, 4, 4, 2, 8, 2, 2, 6, 5, 9, 1, 8,
        5, 9, 3, 8, 8, 5, 9, 5]) 
label[1]: ['w2j', 'r2j', 'w4s', 'g4s', 'b9t', 'r1t', 'g4s', 'b7j', 'w9j', 'g6t', 'r4j', 'w3t', 'w9j', 'g5j', 'r1s', 'r2s', 'w1j', 'r2t', 'r5s', 'g8s', 'r7j', 'b1s', 'r8j', 'b5j', 'w8t', 'w9s', 'r1s', 'w2s', 'w4t', 'g2j', 'b3s', 'w9j', 'g8t', 'r2j', 'b1s', 'g1t', 'b4s', 'r1j', 'b2t', 'g3j', 'b2j', 'r1t', 'b7s', 'g3s', 'b3s', 'g3t', 'r4t', 'g9t', 'g9s', 'w1j', 'b4s', 'w8s', 'r5t', 'b1t', 'g1t', 'g3j', 'b5s', 'r5j', 'g2s', 'b5s', 'r6t', 'g6t', 'r1j', 'r7t', 'g6j', 'b7s', 'g8j', 'r2j', 'r5j', 'b1j', 'r4s', 'w1s', 'b4s', 'g3t', 'w6s', 'w5j', 'g3t', 'w4j', 'g4s', 'b4s', 'g2s', 'w9s', 'r8t', 'r8s', 'b2t', 'b6t', 'g5j', 'b8s', 'r9s', 'r6j', 'g8t', 'r3s', 'r4j', 'w5t', 'r1j', 'g9t', 'w8t', 'g1t', 'b6t', 'w3j', 'w9s', 'w5s', 'g4s', 'r2s', 'w7j', 'b7j', 'g1s', 'b4s', 'r1j', 'b4s', 'b4j', 'b2t', 'b8j', 'b2s', 'w2s', 'g6t', 'w5s', 'b9t', 'g1t', 'b8s', 'b5t', 'b9t', 'g3t', 'g8t', 'r8t', 'b5s', 'r9t', 'r5t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [0.0190122  0.08399423 0.02658464 ... 0.10891575 0.12527359 0.11131395]
Average of silhouette coef: 0.07834926
---
  0   1   2   3
0 0.0 2.6 2.0 2.2 
1 2.6 0.0 3.5 3.8 
2 2.0 3.5 0.0 3.7 
3 2.2 3.8 3.7 0.0 
correlation [[ 1.       -0.513541]
 [-0.513541  1.      ]]
---
[[], [], [], []] [0 3 0 ... 2 2 2] [[ 52.18235    25.502691 ]
 [  9.27259    45.031094 ]
 [ 14.9812765   4.7538447]
 ...
 [ -2.8820317  25.279516 ]
 [-36.495228   -1.8076984]
 [  7.142425   14.222777 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': 0.07834926, 'cluster_all': array([0.0190122 , 0.08399423, 0.02658464, ..., 0.10891575, 0.12527359,
       0.11131395], dtype=float32), 'magnitude_avg': -0.513540997558723, 'magnitude_all': -0.513540997558723, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': 0.07834926, 'magnitude_avg': -0.513540997558723, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 7, 5, 4, 1, 8, 4, 4, 7, 8, 2, 6, 2, 6, 3, 4, 2, 1, 3, 8, 8, 4, 7, 7,
        1, 6, 8, 4, 1, 9, 7, 2, 9, 9, 1, 2, 6, 8, 9, 2, 7, 6, 7, 1, 6, 2, 5, 8,
        4, 9, 4, 2, 6, 8, 7, 5, 9, 5, 2, 1, 8, 8, 8, 5, 4, 9, 6, 8, 5, 1, 3, 4,
        4, 2, 1, 8, 6, 8, 8, 4, 8, 6, 2, 1, 8, 1, 1, 1, 2, 1, 6, 2, 9, 4, 9, 9,
        8, 2, 4, 4, 8, 7, 6, 6, 1, 4, 3, 5, 4, 9, 8, 9, 1, 1, 6, 6, 2, 1, 4, 1,
        7, 9, 3, 5, 1, 4, 1, 3]), ['b2s', 'w7t', 'b5s', 'g4s', 'r1s', 'r8s', 'b4j', 'w4t', 'g7s', 'w8s', 'r2s', 'r6s', 'r2j', 'g6j', 'w3s', 'g4t', 'g2j', 'g1t', 'g3t', 'w8t', 'w8j', 'w4t', 'w7j', 'w7t', 'b1t', 'w6t', 'g8j', 'w4j', 'w1j', 'b9j', 'b7j', 'b2t', 'g9t', 'w9s', 'g1j', 'g2j', 'r6j', 'b8s', 'w9s', 'b2j', 'b7j', 'r6s', 'b7s', 'w1s', 'r6s', 'b2s', 'r5j', 'b8t', 'g4j', 'g9j', 'g4t', 'r2t', 'r6t', 'w8t', 'r7j', 'r5s', 'g9t', 'r5t', 'w2s', 'r1j', 'b8t', 'r8s', 'g8s', 'r5t', 'b4t', 'r9t', 'w6s', 'w8j', 'g5s', 'g1s', 'r3s', 'r4t', 'w4t', 'w2j', 'g1s', 'r8j', 'b6t', 'r8s', 'g8j', 'r4j', 'w8t', 'b6t', 'b2j', 'r1s', 'w8j', 'r1j', 'g1j', 'g1t', 'w2s', 'r1s', 'g6s', 'g2t', 'g9t', 'r4j', 'w9s', 'b9s', 'g8t', 'r2s', 'r4s', 'r4j', 'g8s', 'b7t', 'r6t', 'g6t', 'w1s', 'b4j', 'g3s', 'g5j', 'g4j', 'b9j', 'b8t', 'r9s', 'g1j', 'b1s', 'w6s', 'g6t', 'w2s', 'g1s', 'w4j', 'b1j', 'w7t', 'g9j', 'r3s', 'b5j', 'b1s', 'g4t', 'w1j', 'b3t']] 
label[0]: tensor([2, 7, 5, 4, 1, 8, 4, 4, 7, 8, 2, 6, 2, 6, 3, 4, 2, 1, 3, 8, 8, 4, 7, 7,
        1, 6, 8, 4, 1, 9, 7, 2, 9, 9, 1, 2, 6, 8, 9, 2, 7, 6, 7, 1, 6, 2, 5, 8,
        4, 9, 4, 2, 6, 8, 7, 5, 9, 5, 2, 1, 8, 8, 8, 5, 4, 9, 6, 8, 5, 1, 3, 4,
        4, 2, 1, 8, 6, 8, 8, 4, 8, 6, 2, 1, 8, 1, 1, 1, 2, 1, 6, 2, 9, 4, 9, 9,
        8, 2, 4, 4, 8, 7, 6, 6, 1, 4, 3, 5, 4, 9, 8, 9, 1, 1, 6, 6, 2, 1, 4, 1,
        7, 9, 3, 5, 1, 4, 1, 3]) 
label[1]: ['b2s', 'w7t', 'b5s', 'g4s', 'r1s', 'r8s', 'b4j', 'w4t', 'g7s', 'w8s', 'r2s', 'r6s', 'r2j', 'g6j', 'w3s', 'g4t', 'g2j', 'g1t', 'g3t', 'w8t', 'w8j', 'w4t', 'w7j', 'w7t', 'b1t', 'w6t', 'g8j', 'w4j', 'w1j', 'b9j', 'b7j', 'b2t', 'g9t', 'w9s', 'g1j', 'g2j', 'r6j', 'b8s', 'w9s', 'b2j', 'b7j', 'r6s', 'b7s', 'w1s', 'r6s', 'b2s', 'r5j', 'b8t', 'g4j', 'g9j', 'g4t', 'r2t', 'r6t', 'w8t', 'r7j', 'r5s', 'g9t', 'r5t', 'w2s', 'r1j', 'b8t', 'r8s', 'g8s', 'r5t', 'b4t', 'r9t', 'w6s', 'w8j', 'g5s', 'g1s', 'r3s', 'r4t', 'w4t', 'w2j', 'g1s', 'r8j', 'b6t', 'r8s', 'g8j', 'r4j', 'w8t', 'b6t', 'b2j', 'r1s', 'w8j', 'r1j', 'g1j', 'g1t', 'w2s', 'r1s', 'g6s', 'g2t', 'g9t', 'r4j', 'w9s', 'b9s', 'g8t', 'r2s', 'r4s', 'r4j', 'g8s', 'b7t', 'r6t', 'g6t', 'w1s', 'b4j', 'g3s', 'g5j', 'g4j', 'b9j', 'b8t', 'r9s', 'g1j', 'b1s', 'w6s', 'g6t', 'w2s', 'g1s', 'w4j', 'b1j', 'w7t', 'g9j', 'r3s', 'b5j', 'b1s', 'g4t', 'w1j', 'b3t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([5, 7, 6, 9, 9, 3, 6, 1, 8, 8, 3, 8, 7, 7, 5, 4, 7, 9, 7, 3, 7, 8, 3, 8,
        4, 7, 2, 1, 9, 1, 7, 7, 5, 1, 1, 7, 2, 2, 5, 3, 3, 7, 4, 7, 5, 3, 8, 9,
        1, 4, 6, 1, 9, 8, 9, 1, 4, 4, 1, 1, 6, 5, 4, 2, 5, 3, 2, 7, 2, 7, 8, 1,
        5, 9, 8, 3, 2, 5, 5, 5, 6, 9, 5, 8, 8, 2, 7, 8, 6, 2, 4, 4, 5, 5, 1, 1,
        7, 1, 6, 8, 5, 7, 4, 2, 2, 8, 1, 1, 4, 5, 2, 2, 5, 7, 9, 2, 9, 4, 9, 4,
        1, 2, 1, 9, 4, 7, 8, 2])
Accuracy (count): tensor(17) 
Accuracy (ratio) tensor(0.1328)
Accuracy:
 [[ 0  0  0  0 18  0  0  0  0]
 [ 0  0  0  0 16  0  0  0  0]
 [ 0  0  0  0  9  0  0  0  0]
 [ 0  0  0  0 14  0  0  0  0]
 [ 0  0  0  0 17  0  0  0  0]
 [ 0  0  0  0  7  0  0  0  0]
 [ 0  0  0  0 19  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([4, 1, 3, 8, 7, 1, 2, 1, 1, 4, 5, 5, 1, 2, 5, 4, 3, 5, 1, 6, 5, 5, 6, 9,
        4, 9, 4, 9, 8, 2, 6, 5, 9, 3, 1, 3, 6, 1, 1, 8, 6, 1, 6, 4, 6, 1, 3, 4,
        8, 8, 8, 6, 6, 2, 3, 4, 4, 9, 4, 9, 9, 2, 7, 4, 4, 9, 9, 2, 2, 2, 6, 3,
        1, 5, 6, 4, 5, 1, 6, 8, 2, 2, 3, 2, 9, 5, 3, 3, 4, 1, 8, 4, 4, 6, 3, 2,
        9, 8, 2, 1, 4, 1, 4, 6, 8, 2, 4, 3, 3, 3, 6, 8, 8, 3, 2, 2, 2, 2, 8, 1,
        1, 2, 6, 6, 9, 2, 9, 7])
Accuracy (count): tensor(10) 
Accuracy (ratio) tensor(0.0781)
Accuracy:
 [[ 0  0  0  0 18  0  0  0  0]
 [ 0  0  0  0 20  0  0  0  0]
 [ 0  0  0  0 15  0  0  0  0]
 [ 0  0  0  0 19  0  0  0  0]
 [ 0  0  0  0 10  0  0  0  0]
 [ 0  0  0  0 17  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]
 [ 0  0  0  0 13  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
Silhouette values: [-0.05580832  0.01540545 -0.0482063  ... -0.08796035  0.4227528
  0.00705143]
Average of silhouette coef: 0.037250612
---
  1   2   3   4   5   6   7   8   9
1 0.0 1.2 2.0 2.7 3.3 3.7 4.2 5.4 7.5 
2 1.2 0.0 1.0 1.8 2.4 2.8 3.2 4.6 6.9 
3 2.0 1.0 0.0 0.9 1.6 2.0 2.6 4.3 6.8 
4 2.7 1.8 0.9 0.0 0.8 1.3 2.3 4.3 6.9 
5 3.3 2.4 1.6 0.8 0.0 0.9 2.1 4.3 7.0 
6 3.7 2.8 2.0 1.3 0.9 0.0 1.6 3.9 6.6 
7 4.2 3.2 2.6 2.3 2.1 1.6 0.0 2.3 5.1 
8 5.4 4.6 4.3 4.3 4.3 3.9 2.3 0.0 2.8 
9 7.5 6.9 6.8 6.9 7.0 6.6 5.1 2.8 0.0 
correlation [[1.         0.76744402]
 [0.76744402 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [2 7 5 ... 3 9 6] [[-41.21936      0.09822673]
 [ 19.519382     8.630573  ]
 [-39.481728   -26.478243  ]
 ...
 [-15.620925   -11.027363  ]
 [ 63.604702    27.310432  ]
 [ 24.96741     31.231268  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(61) 
Accuracy (ratio) tensor(0.1247)
Accuracy:
 [[ 0  0  0  0 45  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 61  0  0  0  0]
 [ 0  0  0  0 60  0  0  0  0]
 [ 0  0  0  0 57  0  0  0  0]
 [ 0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0 45  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 2.7 -0.1 -0.0  0.0  0.0 -0.1 -0.0  0.2  0.1 -0.1 -0.0 -0.0  0.0  0.1  0.3  0.0 -0.2  0.1  0.4  0.1 
 1.7  0.1  0.0  0.0 -0.1 -0.1 -0.0  0.1  0.2 -0.3 -0.0  0.1 -0.1  0.1 -0.1  0.2  0.0 -0.1  0.4  0.1 
 0.9  0.0  0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1 -0.0  0.1 -0.1 -0.3 -0.3 -0.1 -0.0  0.1  0.3  0.0 
 0.2  0.2  0.0  0.0 -0.0  0.0 -0.1 -0.1  0.2 -0.1 -0.0 -0.0 -0.2 -0.7 -0.3 -0.1 -0.0  0.1  0.4 -0.1 
-0.3 -0.0 -0.0  0.0  0.0 -0.1 -0.1 -0.2  0.1 -0.0 -0.0 -0.1 -0.1 -1.1 -0.2  0.2  0.1  0.1  0.3 -0.2 
-0.9 -0.0 -0.0 -0.0  0.1  0.0 -0.0  0.2 -0.0  0.0 -0.1 -0.1  0.0 -0.8  0.0 -0.1 -0.1 -0.1  0.5 -0.1 
-1.4 -0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.2  0.5  0.2  0.2  0.0 -0.3  0.6 -0.0 
-2.0  0.0  0.1 -0.1 -0.1 -0.1  0.1  0.1  0.1  0.0  0.0  0.4  0.4  2.7  0.4  0.1 -0.1 -0.4  0.5  0.1 
-2.6  0.1  0.2 -0.2 -0.2 -0.2  0.0  0.2 -0.0  0.1  0.1  0.6  0.4  5.4  0.6  0.0 -0.1 -0.4  0.3 -0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 2.755511
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 2.7 -0.1 -0.0  0.0  0.0 -0.1 -0.0  0.2  0.1 -0.1 -0.0 -0.0  0.0  0.1  0.3  0.0 -0.2  0.1  0.4  0.1 
 1.7  0.1  0.0  0.0 -0.1 -0.1 -0.0  0.1  0.2 -0.3 -0.0  0.1 -0.1  0.1 -0.1  0.2  0.0 -0.1  0.4  0.1 
 0.9  0.0  0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1 -0.0  0.1 -0.1 -0.3 -0.3 -0.1 -0.0  0.1  0.3  0.0 
 0.2  0.2  0.0  0.0 -0.0  0.0 -0.1 -0.1  0.2 -0.1 -0.0 -0.0 -0.2 -0.7 -0.3 -0.1 -0.0  0.1  0.4 -0.1 
-0.3 -0.0 -0.0  0.0  0.0 -0.1 -0.1 -0.2  0.1 -0.0 -0.0 -0.1 -0.1 -1.1 -0.2  0.2  0.1  0.1  0.3 -0.2 
-0.9 -0.0 -0.0 -0.0  0.1  0.0 -0.0  0.2 -0.0  0.0 -0.1 -0.1  0.0 -0.8  0.0 -0.1 -0.1 -0.1  0.5 -0.1 
-1.4 -0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.2  0.5  0.2  0.2  0.0 -0.3  0.6 -0.0 
-2.0  0.0  0.1 -0.1 -0.1 -0.1  0.1  0.1  0.1  0.0  0.0  0.4  0.4  2.7  0.4  0.1 -0.1 -0.4  0.5  0.1 
-2.6  0.1  0.2 -0.2 -0.2 -0.2  0.0  0.2 -0.0  0.1  0.1  0.6  0.4  5.4  0.6  0.0 -0.1 -0.4  0.3 -0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 2.6795065
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 2.7 -0.1 -0.0  0.0  0.0 -0.1 -0.0  0.2  0.1 -0.1 -0.0 -0.0  0.0  0.1  0.3  0.0 -0.2  0.1  0.4  0.1 
 1.7  0.1  0.0  0.0 -0.1 -0.1 -0.0  0.1  0.2 -0.3 -0.0  0.1 -0.1  0.1 -0.1  0.2  0.0 -0.1  0.4  0.1 
 0.9  0.0  0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1 -0.0  0.1 -0.1 -0.3 -0.3 -0.1 -0.0  0.1  0.3  0.0 
 0.2  0.2  0.0  0.0 -0.0  0.0 -0.1 -0.1  0.2 -0.1 -0.0 -0.0 -0.2 -0.7 -0.3 -0.1 -0.0  0.1  0.4 -0.1 
-0.3 -0.0 -0.0  0.0  0.0 -0.1 -0.1 -0.2  0.1 -0.0 -0.0 -0.1 -0.1 -1.1 -0.2  0.2  0.1  0.1  0.3 -0.2 
-0.9 -0.0 -0.0 -0.0  0.1  0.0 -0.0  0.2 -0.0  0.0 -0.1 -0.1  0.0 -0.8  0.0 -0.1 -0.1 -0.1  0.5 -0.1 
-1.4 -0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.2  0.5  0.2  0.2  0.0 -0.3  0.6 -0.0 
-2.0  0.0  0.1 -0.1 -0.1 -0.1  0.1  0.1  0.1  0.0  0.0  0.4  0.4  2.7  0.4  0.1 -0.1 -0.4  0.5  0.1 
-2.6  0.1  0.2 -0.2 -0.2 -0.2  0.0  0.2 -0.0  0.1  0.1  0.6  0.4  5.4  0.6  0.0 -0.1 -0.4  0.3 -0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 1.1758007
pred: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 2.7 -0.1 -0.0  0.0  0.0 -0.1 -0.0  0.2  0.1 -0.1 -0.0 -0.0  0.0  0.1  0.3  0.0 -0.2  0.1  0.4  0.1 
 1.7  0.1  0.0  0.0 -0.1 -0.1 -0.0  0.1  0.2 -0.3 -0.0  0.1 -0.1  0.1 -0.1  0.2  0.0 -0.1  0.4  0.1 
 0.9  0.0  0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1 -0.0  0.1 -0.1 -0.3 -0.3 -0.1 -0.0  0.1  0.3  0.0 
 0.2  0.2  0.0  0.0 -0.0  0.0 -0.1 -0.1  0.2 -0.1 -0.0 -0.0 -0.2 -0.7 -0.3 -0.1 -0.0  0.1  0.4 -0.1 
-0.3 -0.0 -0.0  0.0  0.0 -0.1 -0.1 -0.2  0.1 -0.0 -0.0 -0.1 -0.1 -1.1 -0.2  0.2  0.1  0.1  0.3 -0.2 
-0.9 -0.0 -0.0 -0.0  0.1  0.0 -0.0  0.2 -0.0  0.0 -0.1 -0.1  0.0 -0.8  0.0 -0.1 -0.1 -0.1  0.5 -0.1 
-1.4 -0.1  0.0 -0.1  0.0 -0.1  0.0  0.0 -0.1  0.0 -0.0  0.1  0.2  0.5  0.2  0.2  0.0 -0.3  0.6 -0.0 
-2.0  0.0  0.1 -0.1 -0.1 -0.1  0.1  0.1  0.1  0.0  0.0  0.4  0.4  2.7  0.4  0.1 -0.1 -0.4  0.5  0.1 
-2.6  0.1  0.2 -0.2 -0.2 -0.2  0.0  0.2 -0.0  0.1  0.1  0.6  0.4  5.4  0.6  0.0 -0.1 -0.4  0.3 -0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 1.0100291
results (all): {'reconst_1x1_avg': 0.1328125, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.078125, 'cross_0x1_all': nan, 'cluster_avg': 0.037250612, 'cluster_all': array([-0.05580832,  0.01540545, -0.0482063 , ..., -0.08796035,
        0.4227528 ,  0.00705143], dtype=float32), 'magnitude_avg': 0.7674440157530062, 'magnitude_all': 0.7674440157530062, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 1.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.1328125, 'cross_0x1_avg': 0.078125, 'cluster_avg': 0.037250612, 'magnitude_avg': 0.7674440157530062, 'tsne-2d_avg': nan, 'mathematics_avg': 0.12474437803030014, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 1.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([9, 4, 8, 3, 2, 7, 5, 4, 3, 8, 9, 9, 4, 1, 3, 7, 5, 4, 2, 2, 5, 4, 3, 9,
        3, 3, 9, 4, 7, 6, 4, 7, 2, 6, 2, 2, 1, 5, 5, 4, 6, 6, 7, 6, 5, 9, 1, 9,
        7, 4, 1, 5, 7, 5, 2, 9, 6, 5, 7, 3, 1, 9, 1, 9, 2, 4, 6, 3, 7, 7, 5, 1,
        8, 9, 8, 4, 3, 1, 9, 5, 3, 9, 8, 9, 8, 9, 5, 5, 4, 8, 6, 5, 3, 8, 2, 2,
        3, 2, 7, 9, 4, 3, 9, 7, 5, 1, 1, 4, 3, 1, 6, 8, 2, 3, 9, 8, 7, 4, 2, 8,
        3, 3, 5, 2, 6, 8, 4, 6]), ['w9j', 'g4j', 'r8t', 'b3s', 'w2j', 'b7s', 'g5s', 'r4s', 'g3j', 'r8t', 'b9t', 'b9j', 'b4t', 'g1j', 'g3s', 'g7s', 'w5j', 'w4s', 'g2j', 'w2j', 'w5j', 'r4j', 'r3t', 'b9s', 'b3s', 'b3j', 'b9t', 'b4j', 'r7t', 'b6s', 'g4j', 'b7j', 'b2s', 'g6t', 'b2t', 'g2t', 'b1j', 'r5t', 'b5s', 'w4s', 'g6t', 'r6j', 'g7j', 'w6j', 'w5t', 'w9j', 'g1j', 'r9j', 'w7s', 'g4s', 'g1t', 'w5t', 'w7s', 'r5s', 'r2s', 'b9s', 'w6j', 'g5s', 'g7t', 'g3j', 'b1t', 'r9j', 'r1s', 'w9j', 'r2j', 'w4t', 'b6j', 'g3s', 'g7j', 'g7s', 'w5t', 'r1s', 'g8j', 'b9t', 'w8s', 'w4t', 'w3t', 'b1j', 'b9s', 'b5t', 'w3t', 'r9s', 'g8s', 'w9s', 'w8j', 'r9s', 'w5t', 'b5s', 'b4j', 'r8s', 'w6t', 'g5j', 'r3t', 'b8j', 'w2t', 'g2t', 'w3j', 'g2j', 'w7t', 'b9s', 'w4s', 'g3t', 'g9s', 'w7s', 'b5t', 'g1j', 'b1t', 'r4j', 'w3s', 'b1t', 'w6s', 'b8s', 'g2s', 'r3j', 'g9s', 'r8t', 'w7j', 'r4j', 'r2j', 'g8s', 'r3t', 'g3t', 'r5t', 'r2j', 'b6t', 'w8s', 'g4j', 'g6t']] 
label[0]: tensor([9, 4, 8, 3, 2, 7, 5, 4, 3, 8, 9, 9, 4, 1, 3, 7, 5, 4, 2, 2, 5, 4, 3, 9,
        3, 3, 9, 4, 7, 6, 4, 7, 2, 6, 2, 2, 1, 5, 5, 4, 6, 6, 7, 6, 5, 9, 1, 9,
        7, 4, 1, 5, 7, 5, 2, 9, 6, 5, 7, 3, 1, 9, 1, 9, 2, 4, 6, 3, 7, 7, 5, 1,
        8, 9, 8, 4, 3, 1, 9, 5, 3, 9, 8, 9, 8, 9, 5, 5, 4, 8, 6, 5, 3, 8, 2, 2,
        3, 2, 7, 9, 4, 3, 9, 7, 5, 1, 1, 4, 3, 1, 6, 8, 2, 3, 9, 8, 7, 4, 2, 8,
        3, 3, 5, 2, 6, 8, 4, 6]) 
label[1]: ['w9j', 'g4j', 'r8t', 'b3s', 'w2j', 'b7s', 'g5s', 'r4s', 'g3j', 'r8t', 'b9t', 'b9j', 'b4t', 'g1j', 'g3s', 'g7s', 'w5j', 'w4s', 'g2j', 'w2j', 'w5j', 'r4j', 'r3t', 'b9s', 'b3s', 'b3j', 'b9t', 'b4j', 'r7t', 'b6s', 'g4j', 'b7j', 'b2s', 'g6t', 'b2t', 'g2t', 'b1j', 'r5t', 'b5s', 'w4s', 'g6t', 'r6j', 'g7j', 'w6j', 'w5t', 'w9j', 'g1j', 'r9j', 'w7s', 'g4s', 'g1t', 'w5t', 'w7s', 'r5s', 'r2s', 'b9s', 'w6j', 'g5s', 'g7t', 'g3j', 'b1t', 'r9j', 'r1s', 'w9j', 'r2j', 'w4t', 'b6j', 'g3s', 'g7j', 'g7s', 'w5t', 'r1s', 'g8j', 'b9t', 'w8s', 'w4t', 'w3t', 'b1j', 'b9s', 'b5t', 'w3t', 'r9s', 'g8s', 'w9s', 'w8j', 'r9s', 'w5t', 'b5s', 'b4j', 'r8s', 'w6t', 'g5j', 'r3t', 'b8j', 'w2t', 'g2t', 'w3j', 'g2j', 'w7t', 'b9s', 'w4s', 'g3t', 'g9s', 'w7s', 'b5t', 'g1j', 'b1t', 'r4j', 'w3s', 'b1t', 'w6s', 'b8s', 'g2s', 'r3j', 'g9s', 'r8t', 'w7j', 'r4j', 'r2j', 'g8s', 'r3t', 'g3t', 'r5t', 'r2j', 'b6t', 'w8s', 'g4j', 'g6t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.0217189  0.01888128 0.01978491 ... 0.02695193 0.04306464 0.01557987]
Average of silhouette coef: 0.030720878
---
  0   1   2
0 0.0 1.6 1.7 
1 1.6 0.0 3.3 
2 1.7 3.3 0.0 
correlation [[ 1.         -0.46534892]
 [-0.46534892  1.        ]]
---
[[], [], []] [0 0 2 ... 1 2 0] [[ -2.0399575 -86.58111  ]
 [  6.3438993 -25.780693 ]
 [-56.166225  -25.156155 ]
 ...
 [-18.955286   42.359642 ]
 [-28.623661    8.619934 ]
 [ 52.168884  -16.495213 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': 0.030720878, 'cluster_all': array([0.0217189 , 0.01888128, 0.01978491, ..., 0.02695193, 0.04306464,
       0.01557987], dtype=float32), 'magnitude_avg': -0.4653489175133122, 'magnitude_all': -0.4653489175133122, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_1', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': 0.030720878, 'magnitude_avg': -0.4653489175133122, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 2, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_2', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2', 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_2
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2',
 'run_id': 'mmvae_cmnist_oscn_seed_2',
 'run_type': 'train',
 'seed': 2,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 1, 6, 5, 5, 6, 4, 4, 9, 3, 2, 2, 3, 4, 8, 8, 7, 6, 8, 9, 7, 8, 6, 3,
        5, 9, 4, 6, 5, 1, 1, 5, 9, 1, 8, 8, 6, 7, 5, 9, 1, 3, 4, 8, 1, 6, 5, 5,
        5, 8, 1, 6, 2, 7, 2, 2, 4, 5, 8, 3, 6, 9, 1, 1, 8, 6, 7, 4, 8, 1, 3, 5,
        7, 3, 9, 6, 2, 4, 1, 4, 4, 5, 1, 6, 5, 8, 8, 6, 8, 2, 3, 2, 9, 9, 2, 7,
        3, 2, 9, 8, 5, 2, 5, 7, 3, 9, 8, 7, 8, 4, 4, 7, 3, 8, 7, 1, 5, 8, 4, 7,
        2, 3, 8, 3, 8, 3, 2, 7]), ['r6t', 'g1s', 'g6t', 'r5s', 'b5j', 'r6s', 'g4s', 'w4j', 'b9s', 'g3t', 'w2t', 'r2t', 'g3j', 'r4t', 'b8j', 'g8t', 'b7j', 'w6t', 'b8j', 'g9t', 'r7j', 'w8s', 'g6j', 'b3t', 'w5t', 'g9j', 'r4j', 'w6s', 'r5j', 'g1j', 'g1j', 'b5s', 'g9s', 'g1j', 'w8t', 'g8t', 'w6s', 'b7t', 'w5j', 'b9j', 'r1j', 'w3s', 'g4j', 'b8s', 'r1j', 'w6t', 'b5j', 'g5j', 'r5j', 'r8t', 'w1s', 'w6j', 'r2j', 'w7s', 'g2t', 'b2j', 'r4j', 'r5s', 'b8j', 'r3t', 'r6t', 'b9s', 'r1j', 'w1t', 'r8j', 'r6j', 'w7s', 'g4j', 'b8t', 'r1j', 'r3s', 'g5j', 'r7s', 'w3j', 'b9s', 'b6j', 'g2j', 'r4s', 'g1j', 'b4j', 'b4j', 'b5j', 'r1s', 'w6t', 'g5t', 'w8j', 'b8t', 'b6s', 'w8j', 'b2s', 'w3j', 'r2j', 'b9t', 'g9j', 'b2s', 'g7s', 'r3j', 'b2t', 'r9t', 'r8s', 'w5s', 'r2j', 'g5j', 'r7t', 'g3j', 'g9t', 'w8t', 'w7t', 'g8j', 'r4s', 'r4j', 'r7j', 'b3s', 'b8s', 'w7s', 'r1s', 'w5j', 'g8t', 'b4s', 'g7s', 'b2j', 'g3t', 'r8t', 'g3t', 'g8j', 'w3s', 'r2s', 'g7s']] 
label[0]: tensor([6, 1, 6, 5, 5, 6, 4, 4, 9, 3, 2, 2, 3, 4, 8, 8, 7, 6, 8, 9, 7, 8, 6, 3,
        5, 9, 4, 6, 5, 1, 1, 5, 9, 1, 8, 8, 6, 7, 5, 9, 1, 3, 4, 8, 1, 6, 5, 5,
        5, 8, 1, 6, 2, 7, 2, 2, 4, 5, 8, 3, 6, 9, 1, 1, 8, 6, 7, 4, 8, 1, 3, 5,
        7, 3, 9, 6, 2, 4, 1, 4, 4, 5, 1, 6, 5, 8, 8, 6, 8, 2, 3, 2, 9, 9, 2, 7,
        3, 2, 9, 8, 5, 2, 5, 7, 3, 9, 8, 7, 8, 4, 4, 7, 3, 8, 7, 1, 5, 8, 4, 7,
        2, 3, 8, 3, 8, 3, 2, 7]) 
label[1]: ['r6t', 'g1s', 'g6t', 'r5s', 'b5j', 'r6s', 'g4s', 'w4j', 'b9s', 'g3t', 'w2t', 'r2t', 'g3j', 'r4t', 'b8j', 'g8t', 'b7j', 'w6t', 'b8j', 'g9t', 'r7j', 'w8s', 'g6j', 'b3t', 'w5t', 'g9j', 'r4j', 'w6s', 'r5j', 'g1j', 'g1j', 'b5s', 'g9s', 'g1j', 'w8t', 'g8t', 'w6s', 'b7t', 'w5j', 'b9j', 'r1j', 'w3s', 'g4j', 'b8s', 'r1j', 'w6t', 'b5j', 'g5j', 'r5j', 'r8t', 'w1s', 'w6j', 'r2j', 'w7s', 'g2t', 'b2j', 'r4j', 'r5s', 'b8j', 'r3t', 'r6t', 'b9s', 'r1j', 'w1t', 'r8j', 'r6j', 'w7s', 'g4j', 'b8t', 'r1j', 'r3s', 'g5j', 'r7s', 'w3j', 'b9s', 'b6j', 'g2j', 'r4s', 'g1j', 'b4j', 'b4j', 'b5j', 'r1s', 'w6t', 'g5t', 'w8j', 'b8t', 'b6s', 'w8j', 'b2s', 'w3j', 'r2j', 'b9t', 'g9j', 'b2s', 'g7s', 'r3j', 'b2t', 'r9t', 'r8s', 'w5s', 'r2j', 'g5j', 'r7t', 'g3j', 'g9t', 'w8t', 'w7t', 'g8j', 'r4s', 'r4j', 'r7j', 'b3s', 'b8s', 'w7s', 'r1s', 'w5j', 'g8t', 'b4s', 'g7s', 'b2j', 'g3t', 'r8t', 'g3t', 'g8j', 'w3s', 'r2s', 'g7s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.070717    0.09650099  0.10883086 ... -0.06953656 -0.07462063
  0.09060827]
Average of silhouette coef: 0.06539648
---
  0   1   2   3
0 0.0 2.3 2.8 2.4 
1 2.3 0.0 4.3 3.8 
2 2.8 4.3 0.0 4.2 
3 2.4 3.8 4.2 0.0 
correlation [[ 1.         -0.51872179]
 [-0.51872179  1.        ]]
---
[[], [], [], []] [3 2 2 ... 0 0 3] [[  4.6049156 -28.463865 ]
 [ 22.60119   -79.82301  ]
 [ 32.825157  -21.904541 ]
 ...
 [-70.00587     4.9352074]
 [-31.418056  -40.278057 ]
 [  2.2323122 -30.003555 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.06539648, 'cluster_all': array([ 0.070717  ,  0.09650099,  0.10883086, ..., -0.06953656,
       -0.07462063,  0.09060827], dtype=float32), 'magnitude_avg': -0.5187217919814369, 'magnitude_all': -0.5187217919814369, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.06539648, 'magnitude_avg': -0.5187217919814369, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 4, 7, 5, 3, 3, 4, 3, 1, 5, 2, 5, 7, 1, 2, 6, 7, 4, 4, 4, 7, 1, 8, 7,
        6, 2, 4, 2, 2, 7, 3, 6, 6, 2, 5, 3, 5, 8, 2, 2, 1, 3, 5, 4, 7, 4, 3, 5,
        4, 4, 3, 3, 6, 4, 5, 5, 1, 6, 1, 2, 1, 5, 2, 2, 5, 3, 8, 5, 2, 9, 8, 3,
        1, 8, 8, 5, 5, 7, 7, 7, 6, 6, 9, 1, 4, 6, 1, 7, 7, 1, 4, 2, 9, 3, 6, 8,
        2, 1, 2, 9, 9, 2, 3, 8, 9, 2, 8, 1, 1, 6, 9, 4, 3, 8, 2, 9, 9, 4, 2, 5,
        2, 1, 9, 4, 1, 7, 9, 4]), ['r7j', 'g4s', 'w7j', 'b5j', 'w3j', 'b3j', 'w4j', 'b3t', 'g1t', 'r5t', 'w2t', 'w5s', 'w7j', 'r1t', 'g2j', 'g6s', 'b7s', 'g4t', 'w4s', 'r4t', 'g7s', 'g1s', 'w8t', 'b7t', 'b6s', 'g2j', 'g4j', 'w2s', 'g2s', 'g7j', 'w3t', 'w6t', 'w6s', 'b2s', 'w5j', 'g3s', 'w5t', 'b8j', 'b2t', 'r2t', 'r1t', 'w3j', 'r5j', 'b4j', 'g7t', 'g4t', 'w3t', 'b5j', 'r4j', 'w4t', 'g3j', 'r3s', 'g6j', 'r4t', 'w5t', 'b5j', 'b1s', 'b6s', 'w1s', 'b2t', 'w1s', 'r5s', 'w2j', 'r2s', 'b5s', 'r3s', 'r8t', 'g5s', 'g2j', 'w9s', 'w8j', 'w3j', 'w1t', 'g8t', 'g8j', 'g5t', 'r5t', 'b7t', 'b7t', 'b7s', 'r6s', 'r6s', 'w9s', 'r1s', 'r4s', 'r6j', 'g1t', 'b7t', 'r7t', 'b1j', 'b4s', 'r2j', 'r9j', 'b3s', 'b6t', 'b8t', 'g2s', 'w1s', 'b2s', 'r9t', 'g9j', 'g2t', 'w3j', 'b8s', 'g9s', 'w2j', 'g8j', 'g1t', 'g1s', 'g6s', 'g9j', 'b4s', 'g3s', 'r8j', 'r2j', 'w9s', 'b9j', 'g4s', 'g2j', 'g5j', 'w2s', 'b1j', 'r9t', 'w4j', 'r1s', 'b7s', 'w9j', 'w4s']] 
label[0]: tensor([7, 4, 7, 5, 3, 3, 4, 3, 1, 5, 2, 5, 7, 1, 2, 6, 7, 4, 4, 4, 7, 1, 8, 7,
        6, 2, 4, 2, 2, 7, 3, 6, 6, 2, 5, 3, 5, 8, 2, 2, 1, 3, 5, 4, 7, 4, 3, 5,
        4, 4, 3, 3, 6, 4, 5, 5, 1, 6, 1, 2, 1, 5, 2, 2, 5, 3, 8, 5, 2, 9, 8, 3,
        1, 8, 8, 5, 5, 7, 7, 7, 6, 6, 9, 1, 4, 6, 1, 7, 7, 1, 4, 2, 9, 3, 6, 8,
        2, 1, 2, 9, 9, 2, 3, 8, 9, 2, 8, 1, 1, 6, 9, 4, 3, 8, 2, 9, 9, 4, 2, 5,
        2, 1, 9, 4, 1, 7, 9, 4]) 
label[1]: ['r7j', 'g4s', 'w7j', 'b5j', 'w3j', 'b3j', 'w4j', 'b3t', 'g1t', 'r5t', 'w2t', 'w5s', 'w7j', 'r1t', 'g2j', 'g6s', 'b7s', 'g4t', 'w4s', 'r4t', 'g7s', 'g1s', 'w8t', 'b7t', 'b6s', 'g2j', 'g4j', 'w2s', 'g2s', 'g7j', 'w3t', 'w6t', 'w6s', 'b2s', 'w5j', 'g3s', 'w5t', 'b8j', 'b2t', 'r2t', 'r1t', 'w3j', 'r5j', 'b4j', 'g7t', 'g4t', 'w3t', 'b5j', 'r4j', 'w4t', 'g3j', 'r3s', 'g6j', 'r4t', 'w5t', 'b5j', 'b1s', 'b6s', 'w1s', 'b2t', 'w1s', 'r5s', 'w2j', 'r2s', 'b5s', 'r3s', 'r8t', 'g5s', 'g2j', 'w9s', 'w8j', 'w3j', 'w1t', 'g8t', 'g8j', 'g5t', 'r5t', 'b7t', 'b7t', 'b7s', 'r6s', 'r6s', 'w9s', 'r1s', 'r4s', 'r6j', 'g1t', 'b7t', 'r7t', 'b1j', 'b4s', 'r2j', 'r9j', 'b3s', 'b6t', 'b8t', 'g2s', 'w1s', 'b2s', 'r9t', 'g9j', 'g2t', 'w3j', 'b8s', 'g9s', 'w2j', 'g8j', 'g1t', 'g1s', 'g6s', 'g9j', 'b4s', 'g3s', 'r8j', 'r2j', 'w9s', 'b9j', 'g4s', 'g2j', 'g5j', 'w2s', 'b1j', 'r9t', 'w4j', 'r1s', 'b7s', 'w9j', 'w4s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([4, 5, 3, 1, 9, 9, 7, 4, 8, 9, 7, 3, 6, 7, 8, 1, 3, 8, 1, 5, 6, 8, 2, 5,
        2, 6, 6, 3, 7, 9, 1, 4, 3, 2, 4, 1, 2, 2, 8, 8, 4, 3, 3, 6, 6, 6, 7, 4,
        7, 2, 5, 1, 6, 5, 9, 9, 1, 5, 6, 1, 7, 5, 2, 7, 1, 3, 2, 3, 1, 9, 5, 2,
        7, 1, 5, 2, 4, 1, 1, 5, 8, 8, 6, 3, 5, 5, 4, 6, 3, 1, 7, 4, 9, 3, 7, 5,
        6, 5, 2, 4, 5, 3, 6, 4, 8, 2, 3, 9, 7, 9, 6, 6, 9, 6, 8, 5, 5, 1, 5, 1,
        7, 4, 8, 7, 4, 5, 5, 4])
Accuracy (count): tensor(16) 
Accuracy (ratio) tensor(0.1250)
Accuracy:
 [[16  0  0  0  0  0  0  0  0]
 [12  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [20  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]
 [11  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([8, 8, 7, 7, 9, 3, 6, 6, 9, 4, 4, 5, 7, 6, 1, 7, 8, 6, 5, 2, 9, 1, 3, 7,
        5, 1, 6, 5, 6, 4, 6, 3, 7, 6, 4, 6, 1, 6, 4, 5, 1, 1, 8, 6, 3, 4, 2, 6,
        9, 8, 6, 1, 9, 3, 8, 2, 9, 9, 5, 8, 6, 3, 1, 3, 3, 2, 4, 9, 3, 3, 7, 9,
        5, 4, 1, 2, 4, 2, 1, 1, 9, 7, 1, 6, 7, 8, 2, 7, 2, 9, 3, 9, 1, 9, 5, 8,
        4, 8, 7, 3, 1, 1, 7, 6, 6, 8, 4, 3, 2, 7, 5, 3, 7, 6, 6, 3, 4, 3, 7, 4,
        4, 8, 2, 1, 9, 4, 4, 1])
Accuracy (count): tensor(17) 
Accuracy (ratio) tensor(0.1328)
Accuracy:
 [[17  0  0  0  0  0  0  0  0]
 [10  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]
 [16  0  0  0  0  0  0  0  0]
 [ 9  0  0  0  0  0  0  0  0]
 [19  0  0  0  0  0  0  0  0]
 [15  0  0  0  0  0  0  0  0]
 [12  0  0  0  0  0  0  0  0]
 [14  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [ 0.15830833 -0.10833273  0.11611718 ...  0.10341384 -0.06964149
  0.0187919 ]
Average of silhouette coef: 0.06641148
---
  1   2   3   4   5   6   7   8   9
1 0.0 4.1 5.5 6.4 5.4 5.8 6.5 6.1 7.6 
2 4.1 0.0 4.4 4.3 4.5 3.4 5.3 3.7 5.6 
3 5.5 4.4 0.0 5.7 2.5 4.4 5.3 4.0 5.5 
4 6.4 4.3 5.7 0.0 4.6 4.0 3.7 4.2 2.4 
5 5.4 4.5 2.5 4.6 0.0 4.1 4.5 4.1 4.6 
6 5.8 3.4 4.4 4.0 4.1 0.0 6.0 4.5 4.6 
7 6.5 5.3 5.3 3.7 4.5 6.0 0.0 4.5 4.0 
8 6.1 3.7 4.0 4.2 4.1 4.5 4.5 0.0 4.0 
9 7.6 5.6 5.5 2.4 4.6 4.6 4.0 4.0 0.0 
correlation [[1.         0.37544129]
 [0.37544129 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 4 7 ... 6 6 2] [[ 68.51867   -18.902674 ]
 [ -9.879434  -32.412    ]
 [ 56.10656   -21.687153 ]
 ...
 [  1.8342695  25.121178 ]
 [ 26.183174    2.508348 ]
 [ 11.941861   48.50598  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(45) 
Accuracy (ratio) tensor(0.0920)
Accuracy:
 [[45  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [59  0  0  0  0  1  0  0  0]
 [61  0  0  0  0  0  0  0  0]
 [60  0  0  0  0  0  0  0  0]
 [57  0  0  0  0  0  0  0  0]
 [52  0  0  0  0  0  0  0  0]
 [45  0  0  0  0  0  0  0  0]]
Accuracy:
 [[1.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [1.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [1.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.983 0.    0.    0.    0.    0.017 0.    0.    0.   ]
 [1.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [1.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [1.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [1.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [1.    0.    0.    0.    0.    0.    0.    0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  0.2 -0.1 -0.0  0.3 -0.1  0.2  0.2  0.2 -0.0  0.3  0.0  0.1  0.1 -0.1 -0.1  0.1 -0.0  3.7 -0.1 
 0.8  0.0  0.4  1.0 -0.2 -1.4 -0.1  1.3 -0.6 -0.6  1.0  0.3  0.0 -1.4 -0.1 -0.3  0.3 -0.1  0.9  0.3 
 1.2  0.2  0.2 -0.3 -0.4 -1.2 -0.0  1.5 -0.0  3.1  0.8  0.4  0.1 -0.1  0.1  0.5  0.2 -0.1 -0.2  0.0 
-0.0 -0.2 -0.0  0.2  0.3 -0.5 -0.0 -0.5 -0.3 -1.5  1.0 -0.1  0.2 -0.4  0.3  0.4  0.2  0.0 -2.3  0.1 
-0.1  0.3  0.2  0.8  0.2 -1.1 -0.2  0.0 -0.4  2.6  0.4  0.2  0.3  0.4  0.1  0.4  0.2 -0.0 -0.8  0.1 
 0.4  0.0  0.1 -0.1 -0.1 -1.6 -0.3 -0.2 -0.5  0.6  0.4  0.6  0.1 -2.9  0.1 -0.1 -0.0 -0.1 -0.8  0.1 
 0.9 -0.0  0.2  0.0  0.2 -1.0 -0.4  0.6 -0.1 -0.8 -0.0  0.4  0.5  2.6  0.1  0.4  0.2 -0.1 -2.0  0.4 
-0.1 -0.0 -0.0  0.8 -0.1 -0.5 -0.4  3.0  0.2  0.4  0.7  0.2  0.2 -0.5  0.2  0.0  0.1  0.0 -1.5 -0.1 
-0.1 -0.2 -0.2 -0.2  0.2  0.0 -0.1  0.1  0.1 -0.1  0.6 -0.1 -0.0 -0.3  0.3 -0.1  0.0 -0.0 -3.8 -0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 3.988088
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  0.2 -0.1 -0.0  0.3 -0.1  0.2  0.2  0.2 -0.0  0.3  0.0  0.1  0.1 -0.1 -0.1  0.1 -0.0  3.7 -0.1 
 0.8  0.0  0.4  1.0 -0.2 -1.4 -0.1  1.3 -0.6 -0.6  1.0  0.3  0.0 -1.4 -0.1 -0.3  0.3 -0.1  0.9  0.3 
 1.2  0.2  0.2 -0.3 -0.4 -1.2 -0.0  1.5 -0.0  3.1  0.8  0.4  0.1 -0.1  0.1  0.5  0.2 -0.1 -0.2  0.0 
-0.0 -0.2 -0.0  0.2  0.3 -0.5 -0.0 -0.5 -0.3 -1.5  1.0 -0.1  0.2 -0.4  0.3  0.4  0.2  0.0 -2.3  0.1 
-0.1  0.3  0.2  0.8  0.2 -1.1 -0.2  0.0 -0.4  2.6  0.4  0.2  0.3  0.4  0.1  0.4  0.2 -0.0 -0.8  0.1 
 0.4  0.0  0.1 -0.1 -0.1 -1.6 -0.3 -0.2 -0.5  0.6  0.4  0.6  0.1 -2.9  0.1 -0.1 -0.0 -0.1 -0.8  0.1 
 0.9 -0.0  0.2  0.0  0.2 -1.0 -0.4  0.6 -0.1 -0.8 -0.0  0.4  0.5  2.6  0.1  0.4  0.2 -0.1 -2.0  0.4 
-0.1 -0.0 -0.0  0.8 -0.1 -0.5 -0.4  3.0  0.2  0.4  0.7  0.2  0.2 -0.5  0.2  0.0  0.1  0.0 -1.5 -0.1 
-0.1 -0.2 -0.2 -0.2  0.2  0.0 -0.1  0.1  0.1 -0.1  0.6 -0.1 -0.0 -0.3  0.3 -0.1  0.0 -0.0 -3.8 -0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 4.2067766
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  0.2 -0.1 -0.0  0.3 -0.1  0.2  0.2  0.2 -0.0  0.3  0.0  0.1  0.1 -0.1 -0.1  0.1 -0.0  3.7 -0.1 
 0.8  0.0  0.4  1.0 -0.2 -1.4 -0.1  1.3 -0.6 -0.6  1.0  0.3  0.0 -1.4 -0.1 -0.3  0.3 -0.1  0.9  0.3 
 1.2  0.2  0.2 -0.3 -0.4 -1.2 -0.0  1.5 -0.0  3.1  0.8  0.4  0.1 -0.1  0.1  0.5  0.2 -0.1 -0.2  0.0 
-0.0 -0.2 -0.0  0.2  0.3 -0.5 -0.0 -0.5 -0.3 -1.5  1.0 -0.1  0.2 -0.4  0.3  0.4  0.2  0.0 -2.3  0.1 
-0.1  0.3  0.2  0.8  0.2 -1.1 -0.2  0.0 -0.4  2.6  0.4  0.2  0.3  0.4  0.1  0.4  0.2 -0.0 -0.8  0.1 
 0.4  0.0  0.1 -0.1 -0.1 -1.6 -0.3 -0.2 -0.5  0.6  0.4  0.6  0.1 -2.9  0.1 -0.1 -0.0 -0.1 -0.8  0.1 
 0.9 -0.0  0.2  0.0  0.2 -1.0 -0.4  0.6 -0.1 -0.8 -0.0  0.4  0.5  2.6  0.1  0.4  0.2 -0.1 -2.0  0.4 
-0.1 -0.0 -0.0  0.8 -0.1 -0.5 -0.4  3.0  0.2  0.4  0.7  0.2  0.2 -0.5  0.2  0.0  0.1  0.0 -1.5 -0.1 
-0.1 -0.2 -0.2 -0.2  0.2  0.0 -0.1  0.1  0.1 -0.1  0.6 -0.1 -0.0 -0.3  0.3 -0.1  0.0 -0.0 -3.8 -0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 4.135059
pred: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  0.2 -0.1 -0.0  0.3 -0.1  0.2  0.2  0.2 -0.0  0.3  0.0  0.1  0.1 -0.1 -0.1  0.1 -0.0  3.7 -0.1 
 0.8  0.0  0.4  1.0 -0.2 -1.4 -0.1  1.3 -0.6 -0.6  1.0  0.3  0.0 -1.4 -0.1 -0.3  0.3 -0.1  0.9  0.3 
 1.2  0.2  0.2 -0.3 -0.4 -1.2 -0.0  1.5 -0.0  3.1  0.8  0.4  0.1 -0.1  0.1  0.5  0.2 -0.1 -0.2  0.0 
-0.0 -0.2 -0.0  0.2  0.3 -0.5 -0.0 -0.5 -0.3 -1.5  1.0 -0.1  0.2 -0.4  0.3  0.4  0.2  0.0 -2.3  0.1 
-0.1  0.3  0.2  0.8  0.2 -1.1 -0.2  0.0 -0.4  2.6  0.4  0.2  0.3  0.4  0.1  0.4  0.2 -0.0 -0.8  0.1 
 0.4  0.0  0.1 -0.1 -0.1 -1.6 -0.3 -0.2 -0.5  0.6  0.4  0.6  0.1 -2.9  0.1 -0.1 -0.0 -0.1 -0.8  0.1 
 0.9 -0.0  0.2  0.0  0.2 -1.0 -0.4  0.6 -0.1 -0.8 -0.0  0.4  0.5  2.6  0.1  0.4  0.2 -0.1 -2.0  0.4 
-0.1 -0.0 -0.0  0.8 -0.1 -0.5 -0.4  3.0  0.2  0.4  0.7  0.2  0.2 -0.5  0.2  0.0  0.1  0.0 -1.5 -0.1 
-0.1 -0.2 -0.2 -0.2  0.2  0.0 -0.1  0.1  0.1 -0.1  0.6 -0.1 -0.0 -0.3  0.3 -0.1  0.0 -0.0 -3.8 -0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 4.4447384
results (all): {'reconst_0x0_avg': 0.125, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.1328125, 'cross_1x0_all': nan, 'cluster_avg': 0.06641148, 'cluster_all': array([ 0.15830833, -0.10833273,  0.11611718, ...,  0.10341384,
       -0.06964149,  0.0187919 ], dtype=float32), 'magnitude_avg': 0.37544128649118635, 'magnitude_all': 0.37544128649118635, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.125, 'cross_1x0_avg': 0.1328125, 'cluster_avg': 0.06641148, 'magnitude_avg': 0.37544128649118635, 'tsne-2d_avg': nan, 'mathematics_avg': 0.0920245423913002, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([5, 7, 8, 8, 4, 5, 5, 5, 4, 7, 1, 4, 3, 7, 2, 3, 8, 8, 8, 2, 9, 3, 2, 4,
        6, 6, 2, 8, 7, 4, 8, 1, 9, 9, 3, 7, 1, 1, 9, 1, 6, 5, 2, 1, 1, 1, 2, 8,
        1, 4, 1, 1, 9, 8, 2, 3, 4, 7, 7, 3, 4, 2, 4, 3, 1, 6, 2, 3, 1, 6, 1, 9,
        5, 2, 4, 1, 7, 8, 2, 6, 4, 8, 7, 8, 1, 2, 2, 2, 1, 7, 5, 2, 6, 9, 7, 8,
        1, 8, 6, 9, 1, 9, 1, 4, 1, 1, 6, 4, 5, 4, 3, 9, 9, 1, 7, 8, 9, 7, 6, 2,
        4, 8, 6, 3, 8, 1, 7, 4]), ['b5s', 'r7j', 'w8s', 'r8j', 'r4t', 'g5s', 'b5j', 'b5j', 'r4t', 'g7j', 'r1s', 'g4t', 'b3j', 'r7t', 'r2j', 'w3j', 'w8t', 'r8j', 'b8s', 'w2j', 'b9j', 'g3s', 'g2s', 'g4j', 'w6s', 'w6j', 'w2s', 'g8t', 'g7t', 'g4s', 'g8s', 'b1t', 'w9t', 'b9t', 'g3j', 'b7t', 'g1j', 'w1j', 'r9j', 'w1s', 'b6j', 'w5s', 'g2j', 'w1t', 'b1t', 'g1j', 'w2t', 'r8t', 'b1s', 'r4t', 'g1t', 'b1s', 'b9t', 'r8j', 'g2s', 'r3t', 'r4s', 'g7t', 'w7t', 'b3t', 'w4j', 'g2j', 'g4j', 'w3j', 'r1s', 'w6s', 'w2j', 'g3t', 'g1s', 'r6t', 'w1s', 'g9t', 'b5s', 'r2t', 'r4t', 'w1s', 'w7j', 'r8t', 'g2j', 'b6t', 'w4j', 'b8j', 'b7s', 'w8j', 'b1s', 'r2t', 'b2s', 'r2s', 'r1j', 'r7j', 'w5t', 'r2s', 'b6j', 'w9j', 'w7s', 'r8t', 'r1t', 'w8t', 'r6t', 'r9s', 'g1t', 'b9t', 'r1s', 'b4s', 'b1t', 'b1j', 'b6j', 'r4s', 'g5s', 'r4s', 'w3s', 'g9t', 'w9j', 'w1j', 'b7t', 'r8j', 'g9t', 'b7j', 'w6s', 'g2t', 'w4t', 'g8j', 'w6s', 'b3j', 'b8j', 'w1t', 'g7j', 'b4s']] 
label[0]: tensor([5, 7, 8, 8, 4, 5, 5, 5, 4, 7, 1, 4, 3, 7, 2, 3, 8, 8, 8, 2, 9, 3, 2, 4,
        6, 6, 2, 8, 7, 4, 8, 1, 9, 9, 3, 7, 1, 1, 9, 1, 6, 5, 2, 1, 1, 1, 2, 8,
        1, 4, 1, 1, 9, 8, 2, 3, 4, 7, 7, 3, 4, 2, 4, 3, 1, 6, 2, 3, 1, 6, 1, 9,
        5, 2, 4, 1, 7, 8, 2, 6, 4, 8, 7, 8, 1, 2, 2, 2, 1, 7, 5, 2, 6, 9, 7, 8,
        1, 8, 6, 9, 1, 9, 1, 4, 1, 1, 6, 4, 5, 4, 3, 9, 9, 1, 7, 8, 9, 7, 6, 2,
        4, 8, 6, 3, 8, 1, 7, 4]) 
label[1]: ['b5s', 'r7j', 'w8s', 'r8j', 'r4t', 'g5s', 'b5j', 'b5j', 'r4t', 'g7j', 'r1s', 'g4t', 'b3j', 'r7t', 'r2j', 'w3j', 'w8t', 'r8j', 'b8s', 'w2j', 'b9j', 'g3s', 'g2s', 'g4j', 'w6s', 'w6j', 'w2s', 'g8t', 'g7t', 'g4s', 'g8s', 'b1t', 'w9t', 'b9t', 'g3j', 'b7t', 'g1j', 'w1j', 'r9j', 'w1s', 'b6j', 'w5s', 'g2j', 'w1t', 'b1t', 'g1j', 'w2t', 'r8t', 'b1s', 'r4t', 'g1t', 'b1s', 'b9t', 'r8j', 'g2s', 'r3t', 'r4s', 'g7t', 'w7t', 'b3t', 'w4j', 'g2j', 'g4j', 'w3j', 'r1s', 'w6s', 'w2j', 'g3t', 'g1s', 'r6t', 'w1s', 'g9t', 'b5s', 'r2t', 'r4t', 'w1s', 'w7j', 'r8t', 'g2j', 'b6t', 'w4j', 'b8j', 'b7s', 'w8j', 'b1s', 'r2t', 'b2s', 'r2s', 'r1j', 'r7j', 'w5t', 'r2s', 'b6j', 'w9j', 'w7s', 'r8t', 'r1t', 'w8t', 'r6t', 'r9s', 'g1t', 'b9t', 'r1s', 'b4s', 'b1t', 'b1j', 'b6j', 'r4s', 'g5s', 'r4s', 'w3s', 'g9t', 'w9j', 'w1j', 'b7t', 'r8j', 'g9t', 'b7j', 'w6s', 'g2t', 'w4t', 'g8j', 'w6s', 'b3j', 'b8j', 'w1t', 'g7j', 'b4s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.00369618 -0.00656427 -0.01666615 ... -0.00452814 -0.01122177
  0.01090287]
Average of silhouette coef: -0.00403499
---
  0   1   2
0 0.0 0.3 0.4 
1 0.3 0.0 0.2 
2 0.4 0.2 0.0 
correlation [[1.         0.78291573]
 [0.78291573 1.        ]]
---
[[], [], []] [1 0 1 ... 0 1 0] [[-34.068436  14.407317]
 [-28.505527 -39.45128 ]
 [ -2.967288  -9.686525]
 ...
 [ 11.098387 -32.560364]
 [ 12.031442  63.201405]
 [ 29.926916  68.7108  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.00403499, 'cluster_all': array([-0.00369618, -0.00656427, -0.01666615, ..., -0.00452814,
       -0.01122177,  0.01090287], dtype=float32), 'magnitude_avg': 0.7829157348097271, 'magnitude_all': 0.7829157348097271, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.00403499, 'magnitude_avg': 0.7829157348097271, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([4, 6, 6, 6, 6, 7, 2, 6, 2, 4, 4, 4, 7, 9, 7, 9, 5, 4, 3, 8, 7, 1, 8, 7,
        2, 3, 5, 3, 2, 9, 4, 7, 6, 7, 3, 1, 1, 6, 7, 8, 9, 3, 9, 4, 1, 9, 8, 8,
        3, 8, 8, 1, 2, 8, 1, 7, 8, 2, 8, 2, 3, 9, 8, 7, 8, 1, 5, 2, 9, 6, 1, 8,
        2, 3, 8, 3, 5, 8, 1, 9, 3, 6, 2, 6, 4, 9, 4, 1, 4, 8, 9, 9, 5, 7, 7, 9,
        4, 9, 9, 9, 2, 2, 7, 5, 1, 2, 9, 4, 4, 2, 9, 2, 4, 3, 7, 3, 6, 6, 3, 7,
        6, 5, 6, 2, 5, 8, 8, 9]), ['g4j', 'b6t', 'g6t', 'r6s', 'g6s', 'w7j', 'w2s', 'r6t', 'g2t', 'r4t', 'r4s', 'r4j', 'w7j', 'r9j', 'w7s', 'g9j', 'r5j', 'g4j', 'r3j', 'b8t', 'g7j', 'g1t', 'b8s', 'r7s', 'r2s', 'b3s', 'b5s', 'w3j', 'g2t', 'r9j', 'g4s', 'g7t', 'b6s', 'r7s', 'w3t', 'b1j', 'g1s', 'g6j', 'w7s', 'w8j', 'g9t', 'r3j', 'r9s', 'g4t', 'b1s', 'b9t', 'w8t', 'w8s', 'w3s', 'g8s', 'r8t', 'r1s', 'b2t', 'b8t', 'r1t', 'b7j', 'b8s', 'g2j', 'w8t', 'g2t', 'r3j', 'w9s', 'w8s', 'w7s', 'g8j', 'r1j', 'w5j', 'r2j', 'r9s', 'b6j', 'w1t', 'w8j', 'r2t', 'w3j', 'g8t', 'g3j', 'w5s', 'b8j', 'w1t', 'g9t', 'g3j', 'b6j', 'r2s', 'b6s', 'g4s', 'r9t', 'b4j', 'b1t', 'r4s', 'w8s', 'g9t', 'b9s', 'b5s', 'b7s', 'g7j', 'w9j', 'b4s', 'g9t', 'r9s', 'g9t', 'w2j', 'g2s', 'g7s', 'r5s', 'r1j', 'w2t', 'b9t', 'g4j', 'w4j', 'r2t', 'b9j', 'b2s', 'g4j', 'b3j', 'w7j', 'g3s', 'r6s', 'g6s', 'w3s', 'r7t', 'g6t', 'b5s', 'g6s', 'b2j', 'b5j', 'r8t', 'b8t', 'w9t']] 
label[0]: tensor([4, 6, 6, 6, 6, 7, 2, 6, 2, 4, 4, 4, 7, 9, 7, 9, 5, 4, 3, 8, 7, 1, 8, 7,
        2, 3, 5, 3, 2, 9, 4, 7, 6, 7, 3, 1, 1, 6, 7, 8, 9, 3, 9, 4, 1, 9, 8, 8,
        3, 8, 8, 1, 2, 8, 1, 7, 8, 2, 8, 2, 3, 9, 8, 7, 8, 1, 5, 2, 9, 6, 1, 8,
        2, 3, 8, 3, 5, 8, 1, 9, 3, 6, 2, 6, 4, 9, 4, 1, 4, 8, 9, 9, 5, 7, 7, 9,
        4, 9, 9, 9, 2, 2, 7, 5, 1, 2, 9, 4, 4, 2, 9, 2, 4, 3, 7, 3, 6, 6, 3, 7,
        6, 5, 6, 2, 5, 8, 8, 9]) 
label[1]: ['g4j', 'b6t', 'g6t', 'r6s', 'g6s', 'w7j', 'w2s', 'r6t', 'g2t', 'r4t', 'r4s', 'r4j', 'w7j', 'r9j', 'w7s', 'g9j', 'r5j', 'g4j', 'r3j', 'b8t', 'g7j', 'g1t', 'b8s', 'r7s', 'r2s', 'b3s', 'b5s', 'w3j', 'g2t', 'r9j', 'g4s', 'g7t', 'b6s', 'r7s', 'w3t', 'b1j', 'g1s', 'g6j', 'w7s', 'w8j', 'g9t', 'r3j', 'r9s', 'g4t', 'b1s', 'b9t', 'w8t', 'w8s', 'w3s', 'g8s', 'r8t', 'r1s', 'b2t', 'b8t', 'r1t', 'b7j', 'b8s', 'g2j', 'w8t', 'g2t', 'r3j', 'w9s', 'w8s', 'w7s', 'g8j', 'r1j', 'w5j', 'r2j', 'r9s', 'b6j', 'w1t', 'w8j', 'r2t', 'w3j', 'g8t', 'g3j', 'w5s', 'b8j', 'w1t', 'g9t', 'g3j', 'b6j', 'r2s', 'b6s', 'g4s', 'r9t', 'b4j', 'b1t', 'r4s', 'w8s', 'g9t', 'b9s', 'b5s', 'b7s', 'g7j', 'w9j', 'b4s', 'g9t', 'r9s', 'g9t', 'w2j', 'g2s', 'g7s', 'r5s', 'r1j', 'w2t', 'b9t', 'g4j', 'w4j', 'r2t', 'b9j', 'b2s', 'g4j', 'b3j', 'w7j', 'g3s', 'r6s', 'g6s', 'w3s', 'r7t', 'g6t', 'b5s', 'g6s', 'b2j', 'b5j', 'r8t', 'b8t', 'w9t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.10345904  0.09098119  0.10937089 ... -0.01408677 -0.00153131
  0.08158278]
Average of silhouette coef: 0.06941342
---
  0   1   2   3
0 0.0 2.0 2.6 2.0 
1 2.0 0.0 3.7 3.8 
2 2.6 3.7 0.0 3.7 
3 2.0 3.8 3.7 0.0 
correlation [[ 1.         -0.42352473]
 [-0.42352473  1.        ]]
---
[[], [], [], []] [2 1 2 ... 0 0 1] [[  1.9966981 -44.54878  ]
 [ 17.678219   -9.441642 ]
 [ 23.213448  -36.40982  ]
 ...
 [-15.459781  -15.410154 ]
 [-34.78748    10.956372 ]
 [-13.195291   -5.324459 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': 0.06941342, 'cluster_all': array([ 0.10345904,  0.09098119,  0.10937089, ..., -0.01408677,
       -0.00153131,  0.08158278], dtype=float32), 'magnitude_avg': -0.42352473296905657, 'magnitude_all': -0.42352473296905657, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': 0.06941342, 'magnitude_avg': -0.42352473296905657, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([4, 6, 2, 2, 3, 7, 9, 4, 7, 3, 8, 1, 3, 9, 1, 7, 3, 7, 9, 3, 5, 3, 9, 4,
        6, 5, 2, 8, 4, 6, 5, 6, 4, 6, 3, 4, 1, 3, 3, 3, 9, 3, 3, 7, 5, 7, 9, 9,
        8, 5, 5, 6, 7, 6, 3, 4, 8, 4, 2, 2, 5, 8, 3, 5, 9, 2, 4, 4, 6, 4, 4, 1,
        2, 8, 3, 7, 2, 2, 8, 1, 7, 5, 6, 7, 7, 1, 2, 2, 1, 8, 5, 9, 1, 1, 4, 7,
        1, 9, 1, 5, 4, 4, 2, 9, 8, 8, 4, 6, 9, 9, 6, 7, 5, 2, 9, 9, 4, 9, 9, 7,
        3, 5, 8, 1, 6, 7, 3, 7]), ['w4s', 'w6t', 'w2j', 'g2j', 'b3t', 'r7t', 'g9s', 'w4j', 'g7t', 'r3t', 'g8j', 'b1s', 'r3s', 'b9t', 'r1j', 'b7t', 'w3j', 'g7t', 'g9t', 'b3t', 'r5j', 'w3t', 'b9s', 'r4t', 'g6s', 'b5s', 'r2s', 'r8j', 'b4t', 'r6s', 'r5j', 'r6j', 'r4t', 'g6s', 'r3j', 'b4s', 'w1t', 'b3s', 'r3t', 'b3s', 'r9s', 'b3s', 'b3t', 'b7s', 'b5t', 'r7j', 'w9s', 'g9s', 'r8j', 'w5j', 'b5t', 'b6s', 'w7s', 'b6t', 'w3j', 'w4j', 'b8j', 'r4j', 'w2s', 'w2t', 'b5t', 'r8j', 'b3j', 'w5s', 'b9t', 'r2t', 'b4t', 'b4t', 'b6t', 'w4s', 'w4j', 'g1t', 'r2t', 'b8j', 'b3t', 'g7t', 'g2s', 'b2s', 'b8s', 'b1t', 'b7t', 'w5t', 'r6s', 'b7j', 'r7j', 'g1s', 'b2t', 'g2s', 'r1s', 'r8s', 'w5s', 'w9s', 'w1j', 'r1s', 'b4j', 'b7s', 'r1s', 'r9s', 'w1t', 'b5t', 'w4j', 'b4s', 'b2j', 'r9s', 'b8j', 'b8s', 'g4t', 'w6t', 'w9s', 'r9t', 'r6j', 'g7s', 'w5s', 'b2t', 'g9s', 'g9s', 'w4t', 'w9j', 'r9t', 'g7s', 'b3s', 'b5t', 'b8j', 'w1t', 'g6j', 'w7s', 'w3j', 'b7t']] 
label[0]: tensor([4, 6, 2, 2, 3, 7, 9, 4, 7, 3, 8, 1, 3, 9, 1, 7, 3, 7, 9, 3, 5, 3, 9, 4,
        6, 5, 2, 8, 4, 6, 5, 6, 4, 6, 3, 4, 1, 3, 3, 3, 9, 3, 3, 7, 5, 7, 9, 9,
        8, 5, 5, 6, 7, 6, 3, 4, 8, 4, 2, 2, 5, 8, 3, 5, 9, 2, 4, 4, 6, 4, 4, 1,
        2, 8, 3, 7, 2, 2, 8, 1, 7, 5, 6, 7, 7, 1, 2, 2, 1, 8, 5, 9, 1, 1, 4, 7,
        1, 9, 1, 5, 4, 4, 2, 9, 8, 8, 4, 6, 9, 9, 6, 7, 5, 2, 9, 9, 4, 9, 9, 7,
        3, 5, 8, 1, 6, 7, 3, 7]) 
label[1]: ['w4s', 'w6t', 'w2j', 'g2j', 'b3t', 'r7t', 'g9s', 'w4j', 'g7t', 'r3t', 'g8j', 'b1s', 'r3s', 'b9t', 'r1j', 'b7t', 'w3j', 'g7t', 'g9t', 'b3t', 'r5j', 'w3t', 'b9s', 'r4t', 'g6s', 'b5s', 'r2s', 'r8j', 'b4t', 'r6s', 'r5j', 'r6j', 'r4t', 'g6s', 'r3j', 'b4s', 'w1t', 'b3s', 'r3t', 'b3s', 'r9s', 'b3s', 'b3t', 'b7s', 'b5t', 'r7j', 'w9s', 'g9s', 'r8j', 'w5j', 'b5t', 'b6s', 'w7s', 'b6t', 'w3j', 'w4j', 'b8j', 'r4j', 'w2s', 'w2t', 'b5t', 'r8j', 'b3j', 'w5s', 'b9t', 'r2t', 'b4t', 'b4t', 'b6t', 'w4s', 'w4j', 'g1t', 'r2t', 'b8j', 'b3t', 'g7t', 'g2s', 'b2s', 'b8s', 'b1t', 'b7t', 'w5t', 'r6s', 'b7j', 'r7j', 'g1s', 'b2t', 'g2s', 'r1s', 'r8s', 'w5s', 'w9s', 'w1j', 'r1s', 'b4j', 'b7s', 'r1s', 'r9s', 'w1t', 'b5t', 'w4j', 'b4s', 'b2j', 'r9s', 'b8j', 'b8s', 'g4t', 'w6t', 'w9s', 'r9t', 'r6j', 'g7s', 'w5s', 'b2t', 'g9s', 'g9s', 'w4t', 'w9j', 'r9t', 'g7s', 'b3s', 'b5t', 'b8j', 'w1t', 'g6j', 'w7s', 'w3j', 'b7t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 8, 5, 8, 8, 8, 8, 8, 8, 5,
        8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 5, 5, 8, 5, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8,
        8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 5, 8, 5, 8, 8, 8, 8,
        8, 8, 8, 5, 8, 8, 8, 8]) 
tar: tensor([7, 2, 8, 1, 2, 3, 4, 6, 7, 4, 6, 1, 1, 7, 6, 8, 9, 5, 5, 5, 1, 6, 1, 7,
        5, 2, 9, 6, 4, 4, 5, 5, 6, 3, 3, 9, 3, 2, 9, 6, 5, 1, 1, 7, 8, 3, 2, 6,
        1, 8, 2, 9, 2, 6, 5, 7, 1, 9, 9, 1, 7, 4, 4, 2, 3, 3, 2, 3, 8, 4, 7, 4,
        3, 4, 5, 2, 2, 7, 8, 9, 6, 9, 2, 5, 7, 6, 6, 3, 7, 3, 6, 3, 1, 2, 9, 7,
        1, 8, 7, 7, 4, 7, 9, 7, 5, 1, 2, 3, 1, 9, 9, 2, 3, 8, 7, 3, 2, 7, 5, 7,
        2, 3, 3, 7, 7, 6, 5, 1])
Accuracy (count): tensor(6) 
Accuracy (ratio) tensor(0.0469)
Accuracy:
 [[ 0  0  0  0  1  0  0 14  0]
 [ 0  0  0  0  5  0  0 12  0]
 [ 0  0  0  0  2  0  0 15  0]
 [ 0  0  0  0  0  0  0 10  0]
 [ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  2  0  0 12  0]
 [ 0  0  0  0  4  0  0 17  0]
 [ 0  0  0  0  2  0  0  6  0]
 [ 0  0  0  0  2  0  0 11  0]]
Accuracy:
 [[0.    0.    0.    0.    0.067 0.    0.    0.933 0.   ]
 [0.    0.    0.    0.    0.294 0.    0.    0.706 0.   ]
 [0.    0.    0.    0.    0.118 0.    0.    0.882 0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    1.    0.   ]
 [0.    0.    0.    0.    0.143 0.    0.    0.857 0.   ]
 [0.    0.    0.    0.    0.19  0.    0.    0.81  0.   ]
 [0.    0.    0.    0.    0.25  0.    0.    0.75  0.   ]
 [0.    0.    0.    0.    0.154 0.    0.    0.846 0.   ]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 5, 8, 5, 8, 8, 5, 5, 5, 8, 5, 8, 8,
        8, 8, 8, 8, 8, 5, 5, 8, 5, 8, 5, 8, 8, 8, 8, 5, 8, 8, 5, 8, 8, 5, 8, 5,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 5, 5, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 5, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 5, 5, 8, 8, 5, 5, 8, 5,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 6, 4, 1, 5, 6, 1, 1, 8, 2, 6, 1, 8, 1, 9, 3, 3, 3, 9, 8, 7, 2, 8, 2,
        7, 1, 5, 2, 6, 2, 3, 1, 9, 6, 9, 6, 1, 5, 1, 6, 1, 2, 7, 5, 7, 9, 1, 8,
        9, 9, 1, 1, 1, 2, 1, 2, 9, 6, 3, 1, 1, 9, 7, 7, 2, 2, 9, 2, 5, 3, 5, 6,
        9, 2, 2, 6, 5, 9, 1, 6, 2, 2, 3, 5, 1, 8, 6, 7, 3, 5, 3, 5, 1, 7, 5, 5,
        5, 1, 4, 3, 2, 1, 6, 1, 5, 2, 6, 5, 3, 8, 5, 7, 8, 5, 1, 3, 4, 4, 9, 2,
        9, 8, 4, 4, 1, 5, 2, 3])
Accuracy (count): tensor(7) 
Accuracy (ratio) tensor(0.0547)
Accuracy:
 [[ 0  0  0  0  1  0  0 25  0]
 [ 0  0  0  0  5  0  0 14  0]
 [ 0  0  0  0  3  0  0 10  0]
 [ 0  0  0  0  2  0  0  4  0]
 [ 0  0  0  0  3  0  0 15  0]
 [ 0  0  0  0  1  0  0 13  0]
 [ 0  0  0  0  2  0  0  7  0]
 [ 0  0  0  0  5  0  0  4  0]
 [ 0  0  0  0  6  0  0  8  0]]
Accuracy:
 [[0.    0.    0.    0.    0.038 0.    0.    0.962 0.   ]
 [0.    0.    0.    0.    0.263 0.    0.    0.737 0.   ]
 [0.    0.    0.    0.    0.231 0.    0.    0.769 0.   ]
 [0.    0.    0.    0.    0.333 0.    0.    0.667 0.   ]
 [0.    0.    0.    0.    0.167 0.    0.    0.833 0.   ]
 [0.    0.    0.    0.    0.071 0.    0.    0.929 0.   ]
 [0.    0.    0.    0.    0.222 0.    0.    0.778 0.   ]
 [0.    0.    0.    0.    0.556 0.    0.    0.444 0.   ]
 [0.    0.    0.    0.    0.429 0.    0.    0.571 0.   ]]
---
Silhouette values: [ 0.00462898 -0.00664624 -0.0384308  ...  0.0017086  -0.06054844
  0.32504714]
Average of silhouette coef: 0.04614501
---
  1   2   3   4   5   6   7   8   9
1 0.0 1.5 2.7 3.5 4.0 4.2 4.4 5.4 7.1 
2 1.5 0.0 1.2 2.1 2.6 2.9 3.6 5.0 7.2 
3 2.7 1.2 0.0 1.0 1.6 2.1 3.1 5.0 7.4 
4 3.5 2.1 1.0 0.0 0.8 1.6 3.0 5.0 7.6 
5 4.0 2.6 1.6 0.8 0.0 1.0 2.5 4.6 7.2 
6 4.2 2.9 2.1 1.6 1.0 0.0 1.6 3.8 6.4 
7 4.4 3.6 3.1 3.0 2.5 1.6 0.0 2.2 4.8 
8 5.4 5.0 5.0 5.0 4.6 3.8 2.2 0.0 2.7 
9 7.1 7.2 7.4 7.6 7.2 6.4 4.8 2.7 0.0 
correlation [[1.         0.78940672]
 [0.78940672 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [4 6 2 ... 5 2 9] [[ 10.746897  -21.41601  ]
 [ 10.83733   -27.860788 ]
 [ 40.39671    53.282333 ]
 ...
 [ -1.1689113  20.636938 ]
 [  5.963259    6.9486423]
 [-19.563793  -57.763737 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([8, 8, 5, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8,
        8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 5, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 5,
        8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 5, 8, 5, 8, 5, 5, 8, 5, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 5, 5, 8, 8, 5, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8,
        8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8,
        5, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 5, 5, 8, 5, 5, 8, 8, 8, 8, 8, 5,
        8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 5, 8, 8, 8,
        5, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8,
        8, 5, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5,
        8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8,
        8, 8, 8, 8, 5, 8, 5, 8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8, 5, 5, 8, 8, 8,
        8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 5, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(58) 
Accuracy (ratio) tensor(0.1186)
Accuracy:
 [[ 0  0  0  0  6  0  0 39  0]
 [ 0  0  0  0 10  0  0 42  0]
 [ 0  0  0  0  9  0  0 48  0]
 [ 0  0  0  0  5  0  0 55  0]
 [ 0  0  0  0  9  0  0 52  0]
 [ 0  0  0  0 11  0  0 49  0]
 [ 0  0  0  0  7  0  0 50  0]
 [ 0  0  0  0  3  0  0 49  0]
 [ 0  0  0  0  8  0  0 37  0]]
Accuracy:
 [[0.    0.    0.    0.    0.133 0.    0.    0.867 0.   ]
 [0.    0.    0.    0.    0.192 0.    0.    0.808 0.   ]
 [0.    0.    0.    0.    0.158 0.    0.    0.842 0.   ]
 [0.    0.    0.    0.    0.083 0.    0.    0.917 0.   ]
 [0.    0.    0.    0.    0.148 0.    0.    0.852 0.   ]
 [0.    0.    0.    0.    0.183 0.    0.    0.817 0.   ]
 [0.    0.    0.    0.    0.123 0.    0.    0.877 0.   ]
 [0.    0.    0.    0.    0.058 0.    0.    0.942 0.   ]
 [0.    0.    0.    0.    0.178 0.    0.    0.822 0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.2 -0.3  0.0  0.2 -0.2 -0.1  0.3 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0  0.1  0.0  3.4  0.2 
-0.0  0.2 -0.1  0.1  0.4 -0.2  0.0 -0.4 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0  0.1 -0.0  2.2  0.2 
-0.0  0.2 -0.0  0.2  0.3 -0.1  0.0 -1.1 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0 -0.0  0.0  1.2  0.1 
-0.0  0.2 -0.1  0.1  0.4 -0.1  0.1 -1.5 -0.6  0.0 -0.0 -0.0 -0.1  0.0 -0.1 -0.0  0.0 -0.1  0.4 -0.1 
-0.0 -0.1 -0.2 -0.0  0.5 -0.0  0.3 -1.3 -0.4  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.1 -0.0 -0.2 -0.2 
-0.0  0.1 -0.0 -0.1  0.6 -0.1  0.1 -0.5  0.0 -0.0 -0.1 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.1 -0.6 -0.2 
-0.1 -0.2 -0.0 -0.0  0.4  0.0 -0.1  0.9  0.6 -0.1 -0.2 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.2 -0.9  0.0 
-0.1 -0.2  0.0  0.1  0.6  0.1  0.1  2.9  1.3 -0.1 -0.2 -0.1  0.2  0.2 -0.1 -0.0 -0.1 -0.2 -1.0 -0.2 
-0.1 -0.4 -0.2  0.3  0.5  0.0  0.0  5.5  2.0 -0.1 -0.2 -0.1  0.2  0.2 -0.0  0.0  0.0 -0.4 -1.0  0.0 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 2.669581
pred: tensor([8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 8, 8, 8, 8, 8, 8]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(4) 
Accuracy (ratio) tensor(0.0625)
Accuracy:
 [[4]]
Accuracy:
 [[1.]]
-0.0  0.2 -0.3  0.0  0.2 -0.2 -0.1  0.3 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0  0.1  0.0  3.4  0.2 
-0.0  0.2 -0.1  0.1  0.4 -0.2  0.0 -0.4 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0  0.1 -0.0  2.2  0.2 
-0.0  0.2 -0.0  0.2  0.3 -0.1  0.0 -1.1 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0 -0.0  0.0  1.2  0.1 
-0.0  0.2 -0.1  0.1  0.4 -0.1  0.1 -1.5 -0.6  0.0 -0.0 -0.0 -0.1  0.0 -0.1 -0.0  0.0 -0.1  0.4 -0.1 
-0.0 -0.1 -0.2 -0.0  0.5 -0.0  0.3 -1.3 -0.4  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.1 -0.0 -0.2 -0.2 
-0.0  0.1 -0.0 -0.1  0.6 -0.1  0.1 -0.5  0.0 -0.0 -0.1 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.1 -0.6 -0.2 
-0.1 -0.2 -0.0 -0.0  0.4  0.0 -0.1  0.9  0.6 -0.1 -0.2 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.2 -0.9  0.0 
-0.1 -0.2  0.0  0.1  0.6  0.1  0.1  2.9  1.3 -0.1 -0.2 -0.1  0.2  0.2 -0.1 -0.0 -0.1 -0.2 -1.0 -0.2 
-0.1 -0.4 -0.2  0.3  0.5  0.0  0.0  5.5  2.0 -0.1 -0.2 -0.1  0.2  0.2 -0.0  0.0  0.0 -0.4 -1.0  0.0 
Minimum distance of calculation. 
true answer: 5 
indices: 9 
distance: 3.106322
pred: tensor([8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 5, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 5, 8, 8, 8, 8, 5, 8, 8,
        8, 8, 8, 8, 8, 5, 8, 8, 5, 8, 8, 8, 8, 8, 5, 8]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.8125)
Accuracy:
 [[52]]
Accuracy:
 [[1.]]
-0.0  0.2 -0.3  0.0  0.2 -0.2 -0.1  0.3 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0  0.1  0.0  3.4  0.2 
-0.0  0.2 -0.1  0.1  0.4 -0.2  0.0 -0.4 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0  0.1 -0.0  2.2  0.2 
-0.0  0.2 -0.0  0.2  0.3 -0.1  0.0 -1.1 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0 -0.0  0.0  1.2  0.1 
-0.0  0.2 -0.1  0.1  0.4 -0.1  0.1 -1.5 -0.6  0.0 -0.0 -0.0 -0.1  0.0 -0.1 -0.0  0.0 -0.1  0.4 -0.1 
-0.0 -0.1 -0.2 -0.0  0.5 -0.0  0.3 -1.3 -0.4  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.1 -0.0 -0.2 -0.2 
-0.0  0.1 -0.0 -0.1  0.6 -0.1  0.1 -0.5  0.0 -0.0 -0.1 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.1 -0.6 -0.2 
-0.1 -0.2 -0.0 -0.0  0.4  0.0 -0.1  0.9  0.6 -0.1 -0.2 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.2 -0.9  0.0 
-0.1 -0.2  0.0  0.1  0.6  0.1  0.1  2.9  1.3 -0.1 -0.2 -0.1  0.2  0.2 -0.1 -0.0 -0.1 -0.2 -1.0 -0.2 
-0.1 -0.4 -0.2  0.3  0.5  0.0  0.0  5.5  2.0 -0.1 -0.2 -0.1  0.2  0.2 -0.0  0.0  0.0 -0.4 -1.0  0.0 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 1.5127083
pred: tensor([5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 5, 8, 8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        5, 8, 8, 8, 8, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.0  0.2 -0.3  0.0  0.2 -0.2 -0.1  0.3 -0.1 -0.0  0.0  0.0 -0.1  0.0 -0.1 -0.0  0.1  0.0  3.4  0.2 
-0.0  0.2 -0.1  0.1  0.4 -0.2  0.0 -0.4 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0  0.1 -0.0  2.2  0.2 
-0.0  0.2 -0.0  0.2  0.3 -0.1  0.0 -1.1 -0.5  0.0  0.0 -0.0 -0.0  0.0 -0.2 -0.0 -0.0  0.0  1.2  0.1 
-0.0  0.2 -0.1  0.1  0.4 -0.1  0.1 -1.5 -0.6  0.0 -0.0 -0.0 -0.1  0.0 -0.1 -0.0  0.0 -0.1  0.4 -0.1 
-0.0 -0.1 -0.2 -0.0  0.5 -0.0  0.3 -1.3 -0.4  0.0 -0.1 -0.0 -0.0  0.0 -0.1 -0.0 -0.1 -0.0 -0.2 -0.2 
-0.0  0.1 -0.0 -0.1  0.6 -0.1  0.1 -0.5  0.0 -0.0 -0.1 -0.0  0.0  0.0  0.0 -0.0 -0.1 -0.1 -0.6 -0.2 
-0.1 -0.2 -0.0 -0.0  0.4  0.0 -0.1  0.9  0.6 -0.1 -0.2 -0.0  0.2  0.1 -0.0 -0.0 -0.1 -0.2 -0.9  0.0 
-0.1 -0.2  0.0  0.1  0.6  0.1  0.1  2.9  1.3 -0.1 -0.2 -0.1  0.2  0.2 -0.1 -0.0 -0.1 -0.2 -1.0 -0.2 
-0.1 -0.4 -0.2  0.3  0.5  0.0  0.0  5.5  2.0 -0.1 -0.2 -0.1  0.2  0.2 -0.0  0.0  0.0 -0.4 -1.0  0.0 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 1.2125477
results (all): {'reconst_1x1_avg': 0.046875, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.0546875, 'cross_0x1_all': nan, 'cluster_avg': 0.04614501, 'cluster_all': array([ 0.00462898, -0.00664624, -0.0384308 , ...,  0.0017086 ,
       -0.06054844,  0.32504714], dtype=float32), 'magnitude_avg': 0.7894067240816596, 'magnitude_all': 0.7894067240816596, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11860940605401993, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0625, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.8125, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.046875, 'cross_0x1_avg': 0.0546875, 'cluster_avg': 0.04614501, 'magnitude_avg': 0.7894067240816596, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11860940605401993, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0625, 'mathematics_2+7-1_avg': 0.8125, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 3, 2, 2, 3, 2, 2, 9, 2, 9, 7, 3, 9, 3, 5, 7, 2, 2, 3, 4, 1, 8, 1, 2,
        8, 6, 3, 6, 8, 6, 9, 3, 4, 5, 8, 9, 6, 4, 3, 7, 2, 7, 3, 6, 2, 2, 1, 9,
        9, 7, 7, 7, 2, 1, 9, 1, 6, 5, 1, 8, 5, 9, 3, 6, 1, 7, 3, 3, 2, 4, 8, 2,
        3, 5, 3, 9, 7, 9, 7, 3, 2, 3, 7, 7, 3, 4, 3, 7, 7, 9, 8, 2, 8, 9, 8, 9,
        7, 5, 7, 9, 2, 1, 4, 5, 2, 6, 3, 5, 1, 7, 4, 6, 8, 7, 5, 9, 7, 3, 4, 6,
        4, 3, 1, 5, 7, 3, 6, 4]), ['g6j', 'b3s', 'b2s', 'w2t', 'w3s', 'g2s', 'w2t', 'w9j', 'w2j', 'w9s', 'b7s', 'r3t', 'g9j', 'b3t', 'b5j', 'g7j', 'b2t', 'w2s', 'g3j', 'r4s', 'w1j', 'w8t', 'w1s', 'g2j', 'r8t', 'r6t', 'r3t', 'b6s', 'r8s', 'r6j', 'r9t', 'r3s', 'r4j', 'g5t', 'w8t', 'g9t', 'r6j', 'b4j', 'b3s', 'b7s', 'r2j', 'g7s', 'r3t', 'b6s', 'w2s', 'w2s', 'b1j', 'b9t', 'r9t', 'r7j', 'r7t', 'w7j', 'r2s', 'r1j', 'g9t', 'w1j', 'r6t', 'b5j', 'r1j', 'r8s', 'w5t', 'g9t', 'b3s', 'g6t', 'b1j', 'w7t', 'g3s', 'r3s', 'g2j', 'b4t', 'b8s', 'b2t', 'g3s', 'r5j', 'g3t', 'b9t', 'b7t', 'w9j', 'g7s', 'g3j', 'g2t', 'r3j', 'w7s', 'g7s', 'b3t', 'r4s', 'g3j', 'b7j', 'r7j', 'r9j', 'b8s', 'b2t', 'g8s', 'w9j', 'w8s', 'r9j', 'w7t', 'g5j', 'r7s', 'b9s', 'w2t', 'w1s', 'r4s', 'r5t', 'g2j', 'g6s', 'b3j', 'b5t', 'g1j', 'g7t', 'b4t', 'r6j', 'b8j', 'b7t', 'r5t', 'g9t', 'w7s', 'w3s', 'b4j', 'b6j', 'g4t', 'g3s', 'w1t', 'b5j', 'w7j', 'w3j', 'b6s', 'r4j']] 
label[0]: tensor([6, 3, 2, 2, 3, 2, 2, 9, 2, 9, 7, 3, 9, 3, 5, 7, 2, 2, 3, 4, 1, 8, 1, 2,
        8, 6, 3, 6, 8, 6, 9, 3, 4, 5, 8, 9, 6, 4, 3, 7, 2, 7, 3, 6, 2, 2, 1, 9,
        9, 7, 7, 7, 2, 1, 9, 1, 6, 5, 1, 8, 5, 9, 3, 6, 1, 7, 3, 3, 2, 4, 8, 2,
        3, 5, 3, 9, 7, 9, 7, 3, 2, 3, 7, 7, 3, 4, 3, 7, 7, 9, 8, 2, 8, 9, 8, 9,
        7, 5, 7, 9, 2, 1, 4, 5, 2, 6, 3, 5, 1, 7, 4, 6, 8, 7, 5, 9, 7, 3, 4, 6,
        4, 3, 1, 5, 7, 3, 6, 4]) 
label[1]: ['g6j', 'b3s', 'b2s', 'w2t', 'w3s', 'g2s', 'w2t', 'w9j', 'w2j', 'w9s', 'b7s', 'r3t', 'g9j', 'b3t', 'b5j', 'g7j', 'b2t', 'w2s', 'g3j', 'r4s', 'w1j', 'w8t', 'w1s', 'g2j', 'r8t', 'r6t', 'r3t', 'b6s', 'r8s', 'r6j', 'r9t', 'r3s', 'r4j', 'g5t', 'w8t', 'g9t', 'r6j', 'b4j', 'b3s', 'b7s', 'r2j', 'g7s', 'r3t', 'b6s', 'w2s', 'w2s', 'b1j', 'b9t', 'r9t', 'r7j', 'r7t', 'w7j', 'r2s', 'r1j', 'g9t', 'w1j', 'r6t', 'b5j', 'r1j', 'r8s', 'w5t', 'g9t', 'b3s', 'g6t', 'b1j', 'w7t', 'g3s', 'r3s', 'g2j', 'b4t', 'b8s', 'b2t', 'g3s', 'r5j', 'g3t', 'b9t', 'b7t', 'w9j', 'g7s', 'g3j', 'g2t', 'r3j', 'w7s', 'g7s', 'b3t', 'r4s', 'g3j', 'b7j', 'r7j', 'r9j', 'b8s', 'b2t', 'g8s', 'w9j', 'w8s', 'r9j', 'w7t', 'g5j', 'r7s', 'b9s', 'w2t', 'w1s', 'r4s', 'r5t', 'g2j', 'g6s', 'b3j', 'b5t', 'g1j', 'g7t', 'b4t', 'r6j', 'b8j', 'b7t', 'r5t', 'g9t', 'w7s', 'w3s', 'b4j', 'b6j', 'g4t', 'g3s', 'w1t', 'b5j', 'w7j', 'w3j', 'b6s', 'r4j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [0.09623324 0.01237874 0.02463771 ... 0.14184801 0.06248568 0.03917602]
Average of silhouette coef: 0.059034377
---
  0   1   2
0 0.0 2.8 2.9 
1 2.8 0.0 2.6 
2 2.9 2.6 0.0 
correlation [[1.         0.87138497]
 [0.87138497 1.        ]]
---
[[], [], []] [0 1 1 ... 0 2 1] [[-32.66014    -8.8754835]
 [ 32.953316  -12.83623  ]
 [ 28.67819    -9.736006 ]
 ...
 [ 31.622635  -48.82268  ]
 [  0.3177614  46.265125 ]
 [ 13.144453   64.04917  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': 0.059034377, 'cluster_all': array([0.09623324, 0.01237874, 0.02463771, ..., 0.14184801, 0.06248568,
       0.03917602], dtype=float32), 'magnitude_avg': 0.8713849718198066, 'magnitude_all': 0.8713849718198066, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_2', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': 0.059034377, 'magnitude_avg': 0.8713849718198066, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 3, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_3', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3', 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_3
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3',
 'run_id': 'mmvae_cmnist_oscn_seed_3',
 'run_type': 'train',
 'seed': 3,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 7, 8, 9, 5, 8, 7, 6, 2, 7, 8, 2, 9, 5, 9, 8, 4, 8, 1, 1, 4, 8, 1, 6,
        8, 9, 7, 8, 2, 4, 9, 7, 6, 3, 6, 3, 7, 5, 7, 8, 2, 2, 9, 5, 5, 5, 5, 2,
        5, 9, 3, 6, 2, 1, 6, 4, 3, 6, 3, 8, 9, 1, 3, 9, 7, 5, 3, 4, 9, 4, 1, 2,
        8, 6, 9, 7, 1, 7, 9, 2, 4, 2, 2, 9, 8, 9, 8, 2, 7, 9, 3, 5, 4, 8, 6, 7,
        8, 3, 5, 3, 9, 1, 2, 4, 2, 3, 7, 8, 4, 7, 9, 1, 9, 6, 7, 1, 8, 8, 1, 8,
        7, 3, 6, 9, 3, 1, 5, 9]), ['b2j', 'g7j', 'g8j', 'w9t', 'w5t', 'r8s', 'g7j', 'g6s', 'w2s', 'b7t', 'w8s', 'w2t', 'w9j', 'r5t', 'w9t', 'w8t', 'w4t', 'r8t', 'r1j', 'b1t', 'w4j', 'w8t', 'g1j', 'b6t', 'w8t', 'b9s', 'w7t', 'r8j', 'g2j', 'b4j', 'r9s', 'w7t', 'g6s', 'g3t', 'r6t', 'b3j', 'w7t', 'g5t', 'w7j', 'w8j', 'b2j', 'w2j', 'g9t', 'g5t', 'g5j', 'w5s', 'w5s', 'w2s', 'b5s', 'r9s', 'w3j', 'g6s', 'w2j', 'r1j', 'r6j', 'r4s', 'g3t', 'b6j', 'b3t', 'r8s', 'b9s', 'r1s', 'g3s', 'b9j', 'r7t', 'g5j', 'g3j', 'b4t', 'b9s', 'w4s', 'w1s', 'r2t', 'r8j', 'b6j', 'r9j', 'b7s', 'w1s', 'w7j', 'g9s', 'g2t', 'g4t', 'r2t', 'b2j', 'b9t', 'w8t', 'g9s', 'g8s', 'g2t', 'r7t', 'w9s', 'r3t', 'b5s', 'r4j', 'w8j', 'r6j', 'b7j', 'r8j', 'b3s', 'g5t', 'w3j', 'g9t', 'w1t', 'b2t', 'w4j', 'w2t', 'r3s', 'r7s', 'g8t', 'b4t', 'r7j', 'g9j', 'r1t', 'g9t', 'w6t', 'w7t', 'g1s', 'g8t', 'w8t', 'g1s', 'g8j', 'r7s', 'w3j', 'g6s', 'b9s', 'g3s', 'b1t', 'w5t', 'b9j']] 
label[0]: tensor([2, 7, 8, 9, 5, 8, 7, 6, 2, 7, 8, 2, 9, 5, 9, 8, 4, 8, 1, 1, 4, 8, 1, 6,
        8, 9, 7, 8, 2, 4, 9, 7, 6, 3, 6, 3, 7, 5, 7, 8, 2, 2, 9, 5, 5, 5, 5, 2,
        5, 9, 3, 6, 2, 1, 6, 4, 3, 6, 3, 8, 9, 1, 3, 9, 7, 5, 3, 4, 9, 4, 1, 2,
        8, 6, 9, 7, 1, 7, 9, 2, 4, 2, 2, 9, 8, 9, 8, 2, 7, 9, 3, 5, 4, 8, 6, 7,
        8, 3, 5, 3, 9, 1, 2, 4, 2, 3, 7, 8, 4, 7, 9, 1, 9, 6, 7, 1, 8, 8, 1, 8,
        7, 3, 6, 9, 3, 1, 5, 9]) 
label[1]: ['b2j', 'g7j', 'g8j', 'w9t', 'w5t', 'r8s', 'g7j', 'g6s', 'w2s', 'b7t', 'w8s', 'w2t', 'w9j', 'r5t', 'w9t', 'w8t', 'w4t', 'r8t', 'r1j', 'b1t', 'w4j', 'w8t', 'g1j', 'b6t', 'w8t', 'b9s', 'w7t', 'r8j', 'g2j', 'b4j', 'r9s', 'w7t', 'g6s', 'g3t', 'r6t', 'b3j', 'w7t', 'g5t', 'w7j', 'w8j', 'b2j', 'w2j', 'g9t', 'g5t', 'g5j', 'w5s', 'w5s', 'w2s', 'b5s', 'r9s', 'w3j', 'g6s', 'w2j', 'r1j', 'r6j', 'r4s', 'g3t', 'b6j', 'b3t', 'r8s', 'b9s', 'r1s', 'g3s', 'b9j', 'r7t', 'g5j', 'g3j', 'b4t', 'b9s', 'w4s', 'w1s', 'r2t', 'r8j', 'b6j', 'r9j', 'b7s', 'w1s', 'w7j', 'g9s', 'g2t', 'g4t', 'r2t', 'b2j', 'b9t', 'w8t', 'g9s', 'g8s', 'g2t', 'r7t', 'w9s', 'r3t', 'b5s', 'r4j', 'w8j', 'r6j', 'b7j', 'r8j', 'b3s', 'g5t', 'w3j', 'g9t', 'w1t', 'b2t', 'w4j', 'w2t', 'r3s', 'r7s', 'g8t', 'b4t', 'r7j', 'g9j', 'r1t', 'g9t', 'w6t', 'w7t', 'g1s', 'g8t', 'w8t', 'g1s', 'g8j', 'r7s', 'w3j', 'g6s', 'b9s', 'g3s', 'b1t', 'w5t', 'b9j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.12898096  0.10084663  0.1351279  ... -0.02495784  0.09772598
  0.13576771]
Average of silhouette coef: 0.069059
---
  0   1   2   3
0 0.0 2.7 2.9 2.8 
1 2.7 0.0 3.8 3.6 
2 2.9 3.8 0.0 3.7 
3 2.8 3.6 3.7 0.0 
correlation [[ 1.         -0.43527721]
 [-0.43527721  1.        ]]
---
[[], [], [], []] [1 2 2 ... 0 3 1] [[-30.986307    4.0023417]
 [ 14.209233   20.202688 ]
 [  2.7815504   4.9934406]
 ...
 [ -1.2157501  -1.7804708]
 [-24.605171  -20.744778 ]
 [  4.3551726 -14.91946  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.069059, 'cluster_all': array([ 0.12898096,  0.10084663,  0.1351279 , ..., -0.02495784,
        0.09772598,  0.13576771], dtype=float32), 'magnitude_avg': -0.43527721321775764, 'magnitude_all': -0.43527721321775764, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.069059, 'magnitude_avg': -0.43527721321775764, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([4, 3, 2, 8, 1, 1, 2, 1, 7, 6, 2, 5, 8, 9, 5, 5, 3, 4, 1, 5, 8, 4, 7, 9,
        9, 6, 2, 1, 9, 7, 8, 6, 6, 7, 7, 2, 7, 5, 1, 8, 5, 1, 9, 6, 5, 2, 1, 3,
        1, 6, 6, 8, 1, 1, 3, 9, 2, 6, 8, 3, 3, 8, 3, 3, 3, 1, 8, 5, 8, 9, 2, 1,
        1, 5, 8, 7, 6, 6, 7, 1, 4, 7, 9, 4, 5, 5, 4, 5, 9, 6, 7, 1, 7, 4, 1, 5,
        7, 3, 4, 5, 7, 6, 5, 8, 7, 7, 1, 9, 8, 3, 8, 7, 9, 6, 7, 9, 4, 9, 9, 6,
        2, 7, 4, 1, 3, 2, 8, 6]), ['w4t', 'r3j', 'b2t', 'g8s', 'g1j', 'b1t', 'g2j', 'g1t', 'r7s', 'b6t', 'g2s', 'b5s', 'g8s', 'w9s', 'w5j', 'w5j', 'g3j', 'b4t', 'w1t', 'g5j', 'w8j', 'r4s', 'b7t', 'b9s', 'b9t', 'r6s', 'g2j', 'w1s', 'w9j', 'g7t', 'r8s', 'w6t', 'g6j', 'g7s', 'g7s', 'g2j', 'r7j', 'w5j', 'g1t', 'b8t', 'r5j', 'w1s', 'g9s', 'w6s', 'w5j', 'b2s', 'w1s', 'r3t', 'b1t', 'b6s', 'w6j', 'w8j', 'g1j', 'w1s', 'w3t', 'b9j', 'r2s', 'b6t', 'g8j', 'g3j', 'g3s', 'w8j', 'w3j', 'b3s', 'b3j', 'b1j', 'r8j', 'b5t', 'w8j', 'g9t', 'g2s', 'b1t', 'w1s', 'g5j', 'w8j', 'w7j', 'w6j', 'b6j', 'w7s', 'g1j', 'r4t', 'w7s', 'r9s', 'r4t', 'b5t', 'g5t', 'w4t', 'b5t', 'w9t', 'r6j', 'w7j', 'r1j', 'w7s', 'g4t', 'g1s', 'b5t', 'r7t', 'g3j', 'b4s', 'g5s', 'r7s', 'w6j', 'r5s', 'g8s', 'r7s', 'b7j', 'r1j', 'g9j', 'g8j', 'w3j', 'g8j', 'w7t', 'g9s', 'b6t', 'g7j', 'g9t', 'r4j', 'w9s', 'w9t', 'g6t', 'g2t', 'g7j', 'w4t', 'r1j', 'w3j', 'w2j', 'r8j', 'r6t']] 
label[0]: tensor([4, 3, 2, 8, 1, 1, 2, 1, 7, 6, 2, 5, 8, 9, 5, 5, 3, 4, 1, 5, 8, 4, 7, 9,
        9, 6, 2, 1, 9, 7, 8, 6, 6, 7, 7, 2, 7, 5, 1, 8, 5, 1, 9, 6, 5, 2, 1, 3,
        1, 6, 6, 8, 1, 1, 3, 9, 2, 6, 8, 3, 3, 8, 3, 3, 3, 1, 8, 5, 8, 9, 2, 1,
        1, 5, 8, 7, 6, 6, 7, 1, 4, 7, 9, 4, 5, 5, 4, 5, 9, 6, 7, 1, 7, 4, 1, 5,
        7, 3, 4, 5, 7, 6, 5, 8, 7, 7, 1, 9, 8, 3, 8, 7, 9, 6, 7, 9, 4, 9, 9, 6,
        2, 7, 4, 1, 3, 2, 8, 6]) 
label[1]: ['w4t', 'r3j', 'b2t', 'g8s', 'g1j', 'b1t', 'g2j', 'g1t', 'r7s', 'b6t', 'g2s', 'b5s', 'g8s', 'w9s', 'w5j', 'w5j', 'g3j', 'b4t', 'w1t', 'g5j', 'w8j', 'r4s', 'b7t', 'b9s', 'b9t', 'r6s', 'g2j', 'w1s', 'w9j', 'g7t', 'r8s', 'w6t', 'g6j', 'g7s', 'g7s', 'g2j', 'r7j', 'w5j', 'g1t', 'b8t', 'r5j', 'w1s', 'g9s', 'w6s', 'w5j', 'b2s', 'w1s', 'r3t', 'b1t', 'b6s', 'w6j', 'w8j', 'g1j', 'w1s', 'w3t', 'b9j', 'r2s', 'b6t', 'g8j', 'g3j', 'g3s', 'w8j', 'w3j', 'b3s', 'b3j', 'b1j', 'r8j', 'b5t', 'w8j', 'g9t', 'g2s', 'b1t', 'w1s', 'g5j', 'w8j', 'w7j', 'w6j', 'b6j', 'w7s', 'g1j', 'r4t', 'w7s', 'r9s', 'r4t', 'b5t', 'g5t', 'w4t', 'b5t', 'w9t', 'r6j', 'w7j', 'r1j', 'w7s', 'g4t', 'g1s', 'b5t', 'r7t', 'g3j', 'b4s', 'g5s', 'r7s', 'w6j', 'r5s', 'g8s', 'r7s', 'b7j', 'r1j', 'g9j', 'g8j', 'w3j', 'g8j', 'w7t', 'g9s', 'b6t', 'g7j', 'g9t', 'r4j', 'w9s', 'w9t', 'g6t', 'g2t', 'g7j', 'w4t', 'r1j', 'w3j', 'w2j', 'r8j', 'r6t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([9, 1, 4, 1, 8, 1, 2, 2, 2, 2, 3, 1, 1, 5, 3, 6, 4, 5, 6, 5, 7, 5, 8, 3,
        5, 6, 9, 9, 7, 7, 4, 8, 1, 8, 7, 9, 5, 9, 1, 5, 2, 9, 6, 6, 7, 1, 1, 2,
        3, 7, 3, 8, 3, 9, 6, 2, 3, 6, 1, 8, 1, 7, 9, 1, 8, 8, 7, 5, 3, 1, 8, 5,
        2, 2, 2, 9, 5, 4, 3, 2, 1, 1, 3, 9, 4, 4, 9, 7, 5, 2, 5, 6, 5, 8, 5, 5,
        5, 2, 3, 8, 8, 2, 1, 2, 4, 4, 7, 3, 9, 1, 4, 5, 3, 5, 1, 6, 5, 4, 1, 7,
        5, 1, 8, 1, 3, 4, 9, 9])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0  0  0  0  0  0  0 21  0]
 [ 0  0  0  0  0  0  0 15  0]
 [ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0 11  0]
 [ 0  0  0  0  0  0  0 20  0]
 [ 0  0  0  0  0  0  0  9  0]
 [ 0  0  0  0  0  0  0 11  0]
 [ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  0  0  0 14  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([3, 6, 1, 5, 7, 2, 4, 3, 2, 8, 5, 5, 1, 2, 3, 4, 6, 4, 9, 2, 1, 7, 1, 9,
        8, 7, 9, 7, 9, 1, 1, 7, 7, 6, 1, 4, 9, 4, 4, 1, 2, 9, 6, 2, 5, 2, 3, 7,
        9, 2, 2, 4, 8, 4, 1, 5, 3, 3, 4, 3, 5, 6, 3, 9, 3, 6, 2, 8, 4, 3, 4, 5,
        5, 4, 6, 4, 9, 5, 2, 6, 2, 9, 7, 4, 9, 4, 1, 5, 7, 1, 4, 1, 6, 4, 7, 8,
        6, 2, 5, 8, 2, 6, 6, 4, 3, 6, 5, 9, 1, 7, 7, 1, 1, 7, 9, 1, 6, 5, 4, 8,
        4, 3, 3, 9, 2, 2, 4, 8])
Accuracy (count): tensor(8) 
Accuracy (ratio) tensor(0.0625)
Accuracy:
 [[ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 16  0]
 [ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  0  0  0 21  0]
 [ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  0  0  0 14  0]
 [ 0  0  0  0  0  0  0 13  0]
 [ 0  0  0  0  0  0  0  8  0]
 [ 0  0  0  0  0  0  0 14  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
Silhouette values: [ 0.17628455  0.10066412 -0.02081167 ...  0.07143622 -0.00937794
 -0.05385195]
Average of silhouette coef: 0.10337191
---
  1   2   3   4   5   6   7   8   9
1 0.0 4.0 4.4 5.1 4.8 4.9 4.8 4.4 5.0 
2 4.0 0.0 3.3 4.6 4.8 4.4 4.6 4.5 5.1 
3 4.4 3.3 0.0 4.9 3.3 4.9 4.3 3.8 4.9 
4 5.1 4.6 4.9 0.0 4.6 4.7 4.7 5.3 4.3 
5 4.8 4.8 3.3 4.6 0.0 4.2 4.2 3.6 4.4 
6 4.9 4.4 4.9 4.7 4.2 0.0 5.1 4.3 4.4 
7 4.8 4.6 4.3 4.7 4.2 5.1 0.0 3.9 3.1 
8 4.4 4.5 3.8 5.3 3.6 4.3 3.9 0.0 3.1 
9 5.0 5.1 4.9 4.3 4.4 4.4 3.1 3.1 0.0 
correlation [[1.         0.40754352]
 [0.40754352 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [4 3 2 ... 2 6 4] [[  2.362442  -51.25632  ]
 [ 30.538021   29.647142 ]
 [-21.464746   39.02369  ]
 ...
 [-22.58562    21.066513 ]
 [ 49.644226  -23.057352 ]
 [ -4.6114078 -57.559998 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]

Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0  0  0  0  0  0  0 45  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 61  0]
 [ 0  0  0  0  0  0  0 60  0]
 [ 0  0  0  0  0  0  0 57  0]
 [ 0  0  0  0  0  0  0 52  0]
 [ 0  0  0  0  0  0  0 45  0]]
Accuracy:
 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.3 -0.1 -2.5  0.1  0.5  0.2  0.2  0.1 -0.2 -1.6  0.2 -0.2  0.2 -0.3  0.0  0.0  0.1  0.2  0.1 -0.0 
 0.1 -0.8  0.6  0.5 -0.3  0.2  0.1  0.4  0.5 -1.0 -0.1 -0.7  0.2  0.1 -0.5  0.2 -0.1  2.0  0.1 -0.1 
 0.7 -0.0 -0.0 -0.4 -1.7  0.3  0.3 -0.1  0.2  0.5 -0.2 -0.5  0.4  0.6  0.9 -0.0 -0.1  0.9  0.2 -0.1 
-1.4  0.1  0.7 -0.5  0.7  0.3  0.3 -0.2 -0.1 -0.1 -0.1  0.4  0.0  0.1 -0.2 -3.0 -0.1  0.2 -0.0 -0.1 
 0.3  0.8  0.5 -0.2 -0.9  0.3  0.5 -0.6 -2.1  0.7 -0.0  0.9  0.2  0.7  0.4  0.1 -0.1 -0.2  0.0 -0.1 
-0.2  0.3 -0.0  0.1 -0.5  0.3  0.4 -0.3 -0.1 -0.1 -0.0  1.8  0.1  0.5 -2.9  0.1 -0.0  0.1  0.1 -0.1 
-0.5  0.1  0.3  0.5  1.2  0.2  0.2 -0.3 -0.1  1.8 -0.1 -0.7  1.1 -0.8  0.2  0.4 -0.0 -0.6  0.1 -0.1 
 0.1  0.4  0.4  0.7 -1.0  0.3  0.4  0.0 -0.1 -0.7 -0.0 -0.2  0.7  0.1 -0.1  0.8 -0.1 -2.0 -0.1 -0.0 
-2.0 -0.1  0.5 -0.0  0.3  0.1  0.4  0.2  0.3  0.6 -0.0  0.2  0.4 -0.1 -0.3  0.3 -0.1 -2.1  0.1 -0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 3.0986192
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.3 -0.1 -2.5  0.1  0.5  0.2  0.2  0.1 -0.2 -1.6  0.2 -0.2  0.2 -0.3  0.0  0.0  0.1  0.2  0.1 -0.0 
 0.1 -0.8  0.6  0.5 -0.3  0.2  0.1  0.4  0.5 -1.0 -0.1 -0.7  0.2  0.1 -0.5  0.2 -0.1  2.0  0.1 -0.1 
 0.7 -0.0 -0.0 -0.4 -1.7  0.3  0.3 -0.1  0.2  0.5 -0.2 -0.5  0.4  0.6  0.9 -0.0 -0.1  0.9  0.2 -0.1 
-1.4  0.1  0.7 -0.5  0.7  0.3  0.3 -0.2 -0.1 -0.1 -0.1  0.4  0.0  0.1 -0.2 -3.0 -0.1  0.2 -0.0 -0.1 
 0.3  0.8  0.5 -0.2 -0.9  0.3  0.5 -0.6 -2.1  0.7 -0.0  0.9  0.2  0.7  0.4  0.1 -0.1 -0.2  0.0 -0.1 
-0.2  0.3 -0.0  0.1 -0.5  0.3  0.4 -0.3 -0.1 -0.1 -0.0  1.8  0.1  0.5 -2.9  0.1 -0.0  0.1  0.1 -0.1 
-0.5  0.1  0.3  0.5  1.2  0.2  0.2 -0.3 -0.1  1.8 -0.1 -0.7  1.1 -0.8  0.2  0.4 -0.0 -0.6  0.1 -0.1 
 0.1  0.4  0.4  0.7 -1.0  0.3  0.4  0.0 -0.1 -0.7 -0.0 -0.2  0.7  0.1 -0.1  0.8 -0.1 -2.0 -0.1 -0.0 
-2.0 -0.1  0.5 -0.0  0.3  0.1  0.4  0.2  0.3  0.6 -0.0  0.2  0.4 -0.1 -0.3  0.3 -0.1 -2.1  0.1 -0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 8 
distance: 5.085869
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
 0.3 -0.1 -2.5  0.1  0.5  0.2  0.2  0.1 -0.2 -1.6  0.2 -0.2  0.2 -0.3  0.0  0.0  0.1  0.2  0.1 -0.0 
 0.1 -0.8  0.6  0.5 -0.3  0.2  0.1  0.4  0.5 -1.0 -0.1 -0.7  0.2  0.1 -0.5  0.2 -0.1  2.0  0.1 -0.1 
 0.7 -0.0 -0.0 -0.4 -1.7  0.3  0.3 -0.1  0.2  0.5 -0.2 -0.5  0.4  0.6  0.9 -0.0 -0.1  0.9  0.2 -0.1 
-1.4  0.1  0.7 -0.5  0.7  0.3  0.3 -0.2 -0.1 -0.1 -0.1  0.4  0.0  0.1 -0.2 -3.0 -0.1  0.2 -0.0 -0.1 
 0.3  0.8  0.5 -0.2 -0.9  0.3  0.5 -0.6 -2.1  0.7 -0.0  0.9  0.2  0.7  0.4  0.1 -0.1 -0.2  0.0 -0.1 
-0.2  0.3 -0.0  0.1 -0.5  0.3  0.4 -0.3 -0.1 -0.1 -0.0  1.8  0.1  0.5 -2.9  0.1 -0.0  0.1  0.1 -0.1 
-0.5  0.1  0.3  0.5  1.2  0.2  0.2 -0.3 -0.1  1.8 -0.1 -0.7  1.1 -0.8  0.2  0.4 -0.0 -0.6  0.1 -0.1 
 0.1  0.4  0.4  0.7 -1.0  0.3  0.4  0.0 -0.1 -0.7 -0.0 -0.2  0.7  0.1 -0.1  0.8 -0.1 -2.0 -0.1 -0.0 
-2.0 -0.1  0.5 -0.0  0.3  0.1  0.4  0.2  0.3  0.6 -0.0  0.2  0.4 -0.1 -0.3  0.3 -0.1 -2.1  0.1 -0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 4.0142913
pred: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 0.3 -0.1 -2.5  0.1  0.5  0.2  0.2  0.1 -0.2 -1.6  0.2 -0.2  0.2 -0.3  0.0  0.0  0.1  0.2  0.1 -0.0 
 0.1 -0.8  0.6  0.5 -0.3  0.2  0.1  0.4  0.5 -1.0 -0.1 -0.7  0.2  0.1 -0.5  0.2 -0.1  2.0  0.1 -0.1 
 0.7 -0.0 -0.0 -0.4 -1.7  0.3  0.3 -0.1  0.2  0.5 -0.2 -0.5  0.4  0.6  0.9 -0.0 -0.1  0.9  0.2 -0.1 
-1.4  0.1  0.7 -0.5  0.7  0.3  0.3 -0.2 -0.1 -0.1 -0.1  0.4  0.0  0.1 -0.2 -3.0 -0.1  0.2 -0.0 -0.1 
 0.3  0.8  0.5 -0.2 -0.9  0.3  0.5 -0.6 -2.1  0.7 -0.0  0.9  0.2  0.7  0.4  0.1 -0.1 -0.2  0.0 -0.1 
-0.2  0.3 -0.0  0.1 -0.5  0.3  0.4 -0.3 -0.1 -0.1 -0.0  1.8  0.1  0.5 -2.9  0.1 -0.0  0.1  0.1 -0.1 
-0.5  0.1  0.3  0.5  1.2  0.2  0.2 -0.3 -0.1  1.8 -0.1 -0.7  1.1 -0.8  0.2  0.4 -0.0 -0.6  0.1 -0.1 
 0.1  0.4  0.4  0.7 -1.0  0.3  0.4  0.0 -0.1 -0.7 -0.0 -0.2  0.7  0.1 -0.1  0.8 -0.1 -2.0 -0.1 -0.0 
-2.0 -0.1  0.5 -0.0  0.3  0.1  0.4  0.2  0.3  0.6 -0.0  0.2  0.4 -0.1 -0.3  0.3 -0.1 -2.1  0.1 -0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 3.310185
results (all): {'reconst_0x0_avg': 0.1015625, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.0625, 'cross_1x0_all': nan, 'cluster_avg': 0.10337191, 'cluster_all': array([ 0.17628455,  0.10066412, -0.02081167, ...,  0.07143622,
       -0.00937794, -0.05385195], dtype=float32), 'magnitude_avg': 0.40754351669101296, 'magnitude_all': 0.40754351669101296, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 1.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1015625, 'cross_1x0_avg': 0.0625, 'cluster_avg': 0.10337191, 'magnitude_avg': 0.40754351669101296, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 1.0, 'mathematics_3+5-2_avg': 0.0}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([1, 9, 7, 2, 1, 9, 4, 6, 9, 8, 5, 3, 5, 3, 7, 2, 3, 1, 6, 9, 4, 7, 5, 1,
        2, 3, 4, 6, 9, 3, 9, 1, 4, 8, 4, 9, 9, 3, 8, 3, 5, 2, 1, 8, 8, 5, 4, 9,
        1, 9, 9, 3, 1, 4, 5, 5, 2, 4, 5, 4, 1, 9, 3, 5, 8, 1, 9, 5, 5, 8, 1, 5,
        9, 7, 8, 9, 8, 6, 2, 7, 3, 1, 6, 4, 7, 8, 8, 8, 5, 5, 4, 4, 1, 4, 9, 5,
        8, 6, 1, 1, 2, 9, 7, 7, 6, 2, 9, 9, 1, 5, 1, 9, 8, 1, 7, 4, 2, 4, 3, 8,
        1, 4, 8, 2, 4, 6, 3, 6]), ['g1s', 'w9s', 'b7s', 'w2t', 'w1j', 'w9s', 'w4t', 'g6j', 'g9t', 'g8j', 'g5j', 'w3s', 'r5s', 'w3s', 'r7j', 'r2t', 'g3t', 'w1t', 'g6s', 'b9t', 'r4j', 'w7j', 'b5s', 'g1j', 'b2j', 'b3t', 'w4j', 'w6j', 'g9s', 'b3j', 'r9s', 'r1j', 'r4t', 'w8j', 'r4s', 'g9t', 'w9t', 'b3t', 'r8j', 'g3j', 'r5j', 'b2t', 'w1t', 'r8s', 'b8j', 'b5t', 'b4t', 'r9t', 'w1t', 'b9s', 'r9j', 'w3s', 'g1j', 'w4j', 'b5j', 'b5j', 'b2s', 'g4j', 'b5s', 'g4s', 'r1s', 'b9s', 'r3t', 'g5t', 'g8s', 'r1s', 'r9j', 'b5s', 'w5s', 'r8s', 'g1t', 'g5j', 'b9t', 'w7s', 'b8t', 'w9t', 'b8j', 'w6j', 'g2t', 'r7j', 'g3t', 'w1s', 'b6t', 'w4j', 'g7j', 'w8s', 'g8t', 'r8t', 'w5j', 'w5j', 'w4j', 'r4j', 'g1t', 'g4j', 'b9t', 'w5t', 'r8s', 'b6s', 'r1t', 'r1t', 'w2t', 'w9s', 'g7j', 'r7s', 'g6j', 'w2t', 'w9t', 'w9t', 'w1s', 'w5t', 'w1j', 'r9j', 'b8s', 'w1t', 'b7t', 'r4s', 'w2t', 'r4t', 'w3j', 'r8t', 'b1j', 'w4j', 'b8j', 'r2t', 'w4s', 'g6s', 'r3t', 'w6s']] 
label[0]: tensor([1, 9, 7, 2, 1, 9, 4, 6, 9, 8, 5, 3, 5, 3, 7, 2, 3, 1, 6, 9, 4, 7, 5, 1,
        2, 3, 4, 6, 9, 3, 9, 1, 4, 8, 4, 9, 9, 3, 8, 3, 5, 2, 1, 8, 8, 5, 4, 9,
        1, 9, 9, 3, 1, 4, 5, 5, 2, 4, 5, 4, 1, 9, 3, 5, 8, 1, 9, 5, 5, 8, 1, 5,
        9, 7, 8, 9, 8, 6, 2, 7, 3, 1, 6, 4, 7, 8, 8, 8, 5, 5, 4, 4, 1, 4, 9, 5,
        8, 6, 1, 1, 2, 9, 7, 7, 6, 2, 9, 9, 1, 5, 1, 9, 8, 1, 7, 4, 2, 4, 3, 8,
        1, 4, 8, 2, 4, 6, 3, 6]) 
label[1]: ['g1s', 'w9s', 'b7s', 'w2t', 'w1j', 'w9s', 'w4t', 'g6j', 'g9t', 'g8j', 'g5j', 'w3s', 'r5s', 'w3s', 'r7j', 'r2t', 'g3t', 'w1t', 'g6s', 'b9t', 'r4j', 'w7j', 'b5s', 'g1j', 'b2j', 'b3t', 'w4j', 'w6j', 'g9s', 'b3j', 'r9s', 'r1j', 'r4t', 'w8j', 'r4s', 'g9t', 'w9t', 'b3t', 'r8j', 'g3j', 'r5j', 'b2t', 'w1t', 'r8s', 'b8j', 'b5t', 'b4t', 'r9t', 'w1t', 'b9s', 'r9j', 'w3s', 'g1j', 'w4j', 'b5j', 'b5j', 'b2s', 'g4j', 'b5s', 'g4s', 'r1s', 'b9s', 'r3t', 'g5t', 'g8s', 'r1s', 'r9j', 'b5s', 'w5s', 'r8s', 'g1t', 'g5j', 'b9t', 'w7s', 'b8t', 'w9t', 'b8j', 'w6j', 'g2t', 'r7j', 'g3t', 'w1s', 'b6t', 'w4j', 'g7j', 'w8s', 'g8t', 'r8t', 'w5j', 'w5j', 'w4j', 'r4j', 'g1t', 'g4j', 'b9t', 'w5t', 'r8s', 'b6s', 'r1t', 'r1t', 'w2t', 'w9s', 'g7j', 'r7s', 'g6j', 'w2t', 'w9t', 'w9t', 'w1s', 'w5t', 'w1j', 'r9j', 'b8s', 'w1t', 'b7t', 'r4s', 'w2t', 'r4t', 'w3j', 'r8t', 'b1j', 'w4j', 'b8j', 'r2t', 'w4s', 'g6s', 'r3t', 'w6s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.00392173  0.00396229 -0.00010957 ...  0.00616492 -0.00953774
 -0.00012778]
Average of silhouette coef: -0.003212501
---
  0   1   2
0 0.0 0.3 0.3 
1 0.3 0.0 0.3 
2 0.3 0.3 0.0 
correlation [[ 1.         -0.07484391]
 [-0.07484391  1.        ]]
---
[[], [], []] [1 1 1 ... 2 1 1] [[-64.81102   -40.1461   ]
 [ 35.19737   -31.740679 ]
 [ -2.3885872 -37.237698 ]
 ...
 [-49.714405  -35.95614  ]
 [-41.259666   16.494007 ]
 [ 40.304054   51.792038 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.003212501, 'cluster_all': array([-0.00392173,  0.00396229, -0.00010957, ...,  0.00616492,
       -0.00953774, -0.00012778], dtype=float32), 'magnitude_avg': -0.07484390535742651, 'magnitude_all': -0.07484390535742651, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.003212501, 'magnitude_avg': -0.07484390535742651, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([2, 7, 9, 4, 9, 2, 9, 4, 1, 9, 6, 7, 6, 3, 2, 9, 3, 6, 6, 7, 9, 1, 6, 2,
        7, 3, 1, 7, 2, 2, 9, 6, 5, 2, 9, 2, 5, 7, 3, 7, 9, 2, 1, 6, 1, 2, 6, 2,
        6, 9, 3, 6, 7, 2, 6, 5, 5, 6, 9, 4, 7, 5, 7, 6, 9, 1, 9, 4, 9, 6, 8, 1,
        9, 8, 6, 9, 8, 6, 8, 4, 4, 8, 8, 3, 9, 4, 3, 1, 1, 1, 7, 8, 9, 3, 6, 4,
        6, 9, 3, 1, 4, 4, 7, 9, 5, 7, 9, 5, 1, 9, 1, 5, 4, 1, 8, 7, 3, 4, 5, 1,
        1, 3, 5, 5, 3, 2, 7, 9]), ['b2s', 'r7j', 'r9j', 'w4j', 'g9j', 'g2j', 'g9t', 'w4j', 'w1j', 'r9s', 'r6s', 'b7s', 'r6j', 'b3j', 'b2t', 'g9j', 'g3s', 'g6s', 'b6s', 'r7s', 'r9t', 'r1t', 'g6t', 'r2t', 'b7j', 'g3j', 'w1j', 'b7t', 'g2t', 'b2s', 'w9j', 'b6j', 'w5s', 'g2s', 'r9j', 'w2t', 'b5s', 'g7j', 'w3s', 'r7t', 'g9t', 'g2s', 'g1j', 'b6j', 'b1j', 'r2s', 'b6s', 'g2s', 'b6t', 'g9s', 'g3t', 'g6t', 'g7j', 'r2j', 'g6t', 'w5s', 'b5t', 'r6j', 'w9t', 'w4j', 'r7j', 'g5s', 'r7s', 'w6j', 'r9s', 'r1s', 'b9j', 'b4t', 'g9t', 'w6j', 'b8t', 'r1j', 'g9t', 'w8s', 'g6t', 'r9s', 'w8j', 'w6j', 'w8t', 'b4j', 'b4t', 'w8t', 'g8t', 'b3t', 'w9j', 'w4j', 'b3j', 'b1j', 'r1t', 'b1j', 'g7j', 'r8s', 'b9j', 'g3j', 'b6j', 'b4j', 'w6j', 'b9j', 'b3s', 'r1t', 'r4j', 'w4s', 'b7s', 'r9t', 'r5j', 'b7j', 'r9s', 'w5j', 'b1t', 'b9t', 'b1t', 'b5t', 'g4t', 'g1s', 'w8s', 'w7t', 'g3j', 'g4s', 'r5j', 'b1j', 'w1t', 'g3s', 'r5j', 'w5t', 'w3j', 'r2s', 'r7s', 'r9t']] 
label[0]: tensor([2, 7, 9, 4, 9, 2, 9, 4, 1, 9, 6, 7, 6, 3, 2, 9, 3, 6, 6, 7, 9, 1, 6, 2,
        7, 3, 1, 7, 2, 2, 9, 6, 5, 2, 9, 2, 5, 7, 3, 7, 9, 2, 1, 6, 1, 2, 6, 2,
        6, 9, 3, 6, 7, 2, 6, 5, 5, 6, 9, 4, 7, 5, 7, 6, 9, 1, 9, 4, 9, 6, 8, 1,
        9, 8, 6, 9, 8, 6, 8, 4, 4, 8, 8, 3, 9, 4, 3, 1, 1, 1, 7, 8, 9, 3, 6, 4,
        6, 9, 3, 1, 4, 4, 7, 9, 5, 7, 9, 5, 1, 9, 1, 5, 4, 1, 8, 7, 3, 4, 5, 1,
        1, 3, 5, 5, 3, 2, 7, 9]) 
label[1]: ['b2s', 'r7j', 'r9j', 'w4j', 'g9j', 'g2j', 'g9t', 'w4j', 'w1j', 'r9s', 'r6s', 'b7s', 'r6j', 'b3j', 'b2t', 'g9j', 'g3s', 'g6s', 'b6s', 'r7s', 'r9t', 'r1t', 'g6t', 'r2t', 'b7j', 'g3j', 'w1j', 'b7t', 'g2t', 'b2s', 'w9j', 'b6j', 'w5s', 'g2s', 'r9j', 'w2t', 'b5s', 'g7j', 'w3s', 'r7t', 'g9t', 'g2s', 'g1j', 'b6j', 'b1j', 'r2s', 'b6s', 'g2s', 'b6t', 'g9s', 'g3t', 'g6t', 'g7j', 'r2j', 'g6t', 'w5s', 'b5t', 'r6j', 'w9t', 'w4j', 'r7j', 'g5s', 'r7s', 'w6j', 'r9s', 'r1s', 'b9j', 'b4t', 'g9t', 'w6j', 'b8t', 'r1j', 'g9t', 'w8s', 'g6t', 'r9s', 'w8j', 'w6j', 'w8t', 'b4j', 'b4t', 'w8t', 'g8t', 'b3t', 'w9j', 'w4j', 'b3j', 'b1j', 'r1t', 'b1j', 'g7j', 'r8s', 'b9j', 'g3j', 'b6j', 'b4j', 'w6j', 'b9j', 'b3s', 'r1t', 'r4j', 'w4s', 'b7s', 'r9t', 'r5j', 'b7j', 'r9s', 'w5j', 'b1t', 'b9t', 'b1t', 'b5t', 'g4t', 'g1s', 'w8s', 'w7t', 'g3j', 'g4s', 'r5j', 'b1j', 'w1t', 'g3s', 'r5j', 'w5t', 'w3j', 'r2s', 'r7s', 'r9t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [-0.21491979  0.75976205  0.6530062  ... -0.11071383 -0.8153594
  0.02150752]
Average of silhouette coef: -0.041237775
---
  0   1   2   3
0 0.0 29.2 38.3 68.1 
1 29.2 0.0 9.1 38.9 
2 38.3 9.1 0.0 29.8 
3 68.1 38.9 29.8 0.0 
correlation [[1.         0.90495375]
 [0.90495375 1.        ]]
---
[[], [], [], []] [1 3 3 ... 1 0 0] [[  7.2419024 -67.34157  ]
 [-31.221912   21.70072  ]
 [  1.9095173  41.91344  ]
 ...
 [  1.120965  -26.874138 ]
 [  7.97825    30.911135 ]
 [ 65.957695   -4.6009684]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': -0.041237775, 'cluster_all': array([-0.21491979,  0.75976205,  0.6530062 , ..., -0.11071383,
       -0.8153594 ,  0.02150752], dtype=float32), 'magnitude_avg': 0.904953746768579, 'magnitude_all': 0.904953746768579, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': -0.041237775, 'magnitude_avg': 0.904953746768579, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([6, 7, 3, 3, 3, 2, 5, 4, 4, 1, 5, 2, 7, 4, 2, 1, 8, 4, 2, 4, 6, 9, 6, 5,
        7, 3, 5, 6, 6, 7, 3, 4, 9, 3, 5, 3, 8, 8, 7, 6, 2, 7, 1, 7, 5, 6, 3, 8,
        7, 7, 5, 2, 4, 2, 2, 6, 9, 8, 7, 1, 5, 1, 8, 2, 9, 1, 7, 9, 6, 9, 1, 7,
        7, 2, 5, 3, 8, 9, 9, 9, 1, 1, 9, 1, 8, 7, 3, 4, 3, 5, 1, 4, 3, 4, 7, 2,
        3, 4, 8, 6, 4, 2, 2, 1, 9, 7, 4, 4, 1, 9, 9, 2, 9, 1, 6, 9, 2, 9, 1, 5,
        4, 8, 4, 3, 1, 7, 9, 6]), ['r6j', 'r7j', 'w3s', 'g3j', 'w3j', 'g2t', 'w5t', 'g4j', 'b4s', 'b1s', 'g5j', 'b2s', 'b7j', 'b4t', 'w2s', 'r1t', 'r8j', 'r4j', 'b2t', 'b4s', 'r6s', 'b9j', 'g6j', 'r5t', 'g7t', 'b3j', 'b5t', 'w6s', 'b6t', 'b7j', 'g3t', 'r4j', 'r9t', 'b3j', 'g5j', 'w3t', 'r8j', 'r8s', 'g7t', 'w6t', 'w2j', 'g7j', 'r1j', 'b7t', 'g5s', 'w6t', 'b3t', 'w8s', 'r7t', 'g7s', 'w5t', 'r2s', 'r4t', 'w2j', 'b2s', 'r6s', 'r9j', 'r8t', 'g7j', 'g1j', 'b5s', 'g1s', 'b8t', 'r2t', 'r9t', 'r1j', 'b7j', 'b9s', 'b6j', 'g9t', 'b1s', 'w7j', 'r7t', 'b2s', 'r5t', 'w3j', 'w8j', 'b9j', 'r9t', 'b9s', 'r1t', 'b1s', 'r9s', 'w1s', 'r8s', 'w7j', 'g3t', 'w4j', 'w3t', 'r5s', 'g1s', 'w4s', 'g3j', 'b4j', 'b7t', 'g2s', 'r3s', 'g4s', 'r8j', 'b6s', 'w4j', 'b2t', 'w2s', 'b1t', 'b9j', 'w7t', 'r4s', 'g4s', 'b1j', 'r9t', 'w9j', 'g2j', 'b9t', 'w1t', 'w6s', 'g9j', 'b2j', 'r9j', 'w1j', 'r5t', 'w4s', 'g8j', 'w4j', 'r3t', 'r1t', 'b7j', 'b9s', 'w6s']] 
label[0]: tensor([6, 7, 3, 3, 3, 2, 5, 4, 4, 1, 5, 2, 7, 4, 2, 1, 8, 4, 2, 4, 6, 9, 6, 5,
        7, 3, 5, 6, 6, 7, 3, 4, 9, 3, 5, 3, 8, 8, 7, 6, 2, 7, 1, 7, 5, 6, 3, 8,
        7, 7, 5, 2, 4, 2, 2, 6, 9, 8, 7, 1, 5, 1, 8, 2, 9, 1, 7, 9, 6, 9, 1, 7,
        7, 2, 5, 3, 8, 9, 9, 9, 1, 1, 9, 1, 8, 7, 3, 4, 3, 5, 1, 4, 3, 4, 7, 2,
        3, 4, 8, 6, 4, 2, 2, 1, 9, 7, 4, 4, 1, 9, 9, 2, 9, 1, 6, 9, 2, 9, 1, 5,
        4, 8, 4, 3, 1, 7, 9, 6]) 
label[1]: ['r6j', 'r7j', 'w3s', 'g3j', 'w3j', 'g2t', 'w5t', 'g4j', 'b4s', 'b1s', 'g5j', 'b2s', 'b7j', 'b4t', 'w2s', 'r1t', 'r8j', 'r4j', 'b2t', 'b4s', 'r6s', 'b9j', 'g6j', 'r5t', 'g7t', 'b3j', 'b5t', 'w6s', 'b6t', 'b7j', 'g3t', 'r4j', 'r9t', 'b3j', 'g5j', 'w3t', 'r8j', 'r8s', 'g7t', 'w6t', 'w2j', 'g7j', 'r1j', 'b7t', 'g5s', 'w6t', 'b3t', 'w8s', 'r7t', 'g7s', 'w5t', 'r2s', 'r4t', 'w2j', 'b2s', 'r6s', 'r9j', 'r8t', 'g7j', 'g1j', 'b5s', 'g1s', 'b8t', 'r2t', 'r9t', 'r1j', 'b7j', 'b9s', 'b6j', 'g9t', 'b1s', 'w7j', 'r7t', 'b2s', 'r5t', 'w3j', 'w8j', 'b9j', 'r9t', 'b9s', 'r1t', 'b1s', 'r9s', 'w1s', 'r8s', 'w7j', 'g3t', 'w4j', 'w3t', 'r5s', 'g1s', 'w4s', 'g3j', 'b4j', 'b7t', 'g2s', 'r3s', 'g4s', 'r8j', 'b6s', 'w4j', 'b2t', 'w2s', 'b1t', 'b9j', 'w7t', 'r4s', 'g4s', 'b1j', 'r9t', 'w9j', 'g2j', 'b9t', 'w1t', 'w6s', 'g9j', 'b2j', 'r9j', 'w1j', 'r5t', 'w4s', 'g8j', 'w4j', 'r3t', 'r1t', 'b7j', 'b9s', 'w6s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 9,
        2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 9, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 9, 2,
        2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2,
        2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([2, 3, 7, 5, 5, 8, 5, 4, 9, 4, 4, 2, 2, 8, 3, 8, 1, 5, 5, 6, 3, 9, 7, 9,
        5, 5, 3, 7, 5, 7, 6, 8, 3, 8, 3, 1, 9, 8, 2, 6, 1, 3, 8, 7, 1, 1, 1, 1,
        9, 4, 2, 4, 4, 3, 2, 4, 3, 2, 1, 1, 9, 1, 8, 7, 3, 7, 3, 9, 4, 4, 7, 7,
        4, 7, 1, 7, 4, 7, 4, 4, 6, 1, 5, 2, 4, 8, 2, 6, 5, 1, 1, 2, 9, 4, 2, 2,
        5, 3, 2, 3, 2, 1, 1, 1, 3, 9, 7, 9, 1, 1, 4, 9, 2, 2, 3, 1, 6, 3, 4, 9,
        3, 2, 7, 4, 6, 6, 1, 5])
Accuracy (count): tensor(15) 
Accuracy (ratio) tensor(0.1172)
Accuracy:
 [[ 0 19  0  0  0  0  0  0  2]
 [ 0 14  0  0  0  0  0  0  3]
 [ 0 13  0  0  0  0  0  0  4]
 [ 0 14  0  0  0  0  0  0  4]
 [ 0 12  0  0  0  0  0  0  0]
 [ 0  8  0  0  0  0  0  0  0]
 [ 0 11  0  0  0  0  0  0  3]
 [ 0  8  0  0  0  0  0  0  1]
 [ 0 11  0  0  0  0  0  0  1]]
Accuracy:
 [[0.    0.905 0.    0.    0.    0.    0.    0.    0.095]
 [0.    0.824 0.    0.    0.    0.    0.    0.    0.176]
 [0.    0.765 0.    0.    0.    0.    0.    0.    0.235]
 [0.    0.778 0.    0.    0.    0.    0.    0.    0.222]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    1.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.786 0.    0.    0.    0.    0.    0.    0.214]
 [0.    0.889 0.    0.    0.    0.    0.    0.    0.111]
 [0.    0.917 0.    0.    0.    0.    0.    0.    0.083]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 2, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 9,
        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2,
        9, 9, 9, 9, 9, 9, 9, 9]) 
tar: tensor([8, 2, 9, 4, 1, 7, 1, 4, 6, 4, 7, 5, 2, 4, 8, 8, 5, 6, 9, 5, 8, 2, 4, 5,
        1, 4, 5, 4, 3, 9, 4, 7, 7, 8, 1, 4, 3, 6, 8, 3, 5, 1, 4, 7, 8, 8, 1, 7,
        8, 4, 3, 8, 4, 2, 8, 7, 1, 1, 8, 4, 6, 1, 6, 4, 6, 7, 6, 8, 1, 8, 5, 6,
        4, 1, 4, 4, 9, 6, 4, 4, 1, 3, 5, 8, 9, 3, 6, 2, 2, 2, 7, 8, 1, 5, 3, 7,
        4, 5, 9, 6, 1, 4, 2, 4, 6, 3, 3, 1, 4, 8, 5, 3, 3, 3, 9, 4, 1, 1, 2, 9,
        5, 5, 1, 2, 9, 6, 5, 7])
Accuracy (count): tensor(7) 
Accuracy (ratio) tensor(0.0547)
Accuracy:
 [[ 0  0  0  0  0  0  0  0 18]
 [ 0  0  0  0  0  0  0  0 10]
 [ 0  1  0  0  0  0  0  0 11]
 [ 0  0  0  0  0  0  0  0 24]
 [ 0  0  0  0  0  0  0  0 14]
 [ 0  0  0  0  0  0  0  0 13]
 [ 0  3  0  0  0  0  0  0  8]
 [ 0  2  0  0  0  0  0  0 15]
 [ 0  2  0  0  0  0  0  0  7]]
Accuracy:
 [[0.    0.    0.    0.    0.    0.    0.    0.    1.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.   ]
 [0.    0.083 0.    0.    0.    0.    0.    0.    0.917]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    1.   ]
 [0.    0.273 0.    0.    0.    0.    0.    0.    0.727]
 [0.    0.118 0.    0.    0.    0.    0.    0.    0.882]
 [0.    0.222 0.    0.    0.    0.    0.    0.    0.778]]
---
Silhouette values: [-0.8521952  -0.8375673  -0.19906315 ... -0.32579434 -0.32579434
 -0.14503919]
Average of silhouette coef: -0.2245089
---
  1   2   3   4   5   6   7   8   9
1 0.0 10.2 16.8 28.7 38.3 51.4 58.3 72.1 70.0 
2 10.2 0.0 6.7 18.5 28.2 41.2 48.1 62.0 59.9 
3 16.8 6.7 0.0 11.8 21.5 34.5 41.5 55.3 53.2 
4 28.7 18.5 11.8 0.0 9.7 22.7 29.6 43.5 41.4 
5 38.3 28.2 21.5 9.7 0.0 13.0 19.9 33.8 31.7 
6 51.4 41.2 34.5 22.7 13.0 0.0 6.9 20.8 18.7 
7 58.3 48.1 41.5 29.6 19.9 6.9 0.0 13.9 11.8 
8 72.1 62.0 55.3 43.5 33.8 20.8 13.9 0.0 2.1 
9 70.0 59.9 53.2 41.4 31.7 18.7 11.8 2.1 0.0 
correlation [[1.        0.9729306]
 [0.9729306 1.       ]]
---
[[], [], [], [], [], [], [], [], [], []] [6 7 3 ... 2 2 5] [[ -1.8778838   4.2916856]
 [-20.55684    -5.112266 ]
 [  1.0770923  57.229675 ]
 ...
 [-10.524955    3.7218816]
 [-10.524955    3.7218816]
 [-34.014965   40.196823 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(52) 
Accuracy (ratio) tensor(0.1063)
Accuracy:
 [[ 0 45  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 61  0  0  0  0  0  0  0]
 [ 0 60  0  0  0  0  0  0  0]
 [ 0 57  0  0  0  0  0  0  0]
 [ 0 52  0  0  0  0  0  0  0]
 [ 0 45  0  0  0  0  0  0  0]]
Accuracy:
 [[0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(64) 
Accuracy (ratio) tensor(1.)
Accuracy:
 [[64]]
Accuracy:
 [[1.]]
-3.5 -23.5  1.7 -0.4 -10.3  13.3  23.7  9.8  14.6  3.9 -16.2  0.6  3.0 -1.7  16.8  10.7 -18.0 -23.3  15.4 -10.1 
-4.1 -27.4  2.0 -0.5 -12.0  15.4  27.7  11.5  17.1  4.6 -19.0  0.7  3.5 -2.0  19.7  12.5 -21.0 -27.2  18.1 -11.8 
-4.5 -30.0  2.3 -0.6 -13.0  16.9  30.3  12.6  18.8  5.1 -20.8  0.8  3.9 -2.2  21.5  13.7 -23.0 -29.7  19.8 -12.9 
-5.2 -34.6  2.7 -0.7 -15.1  19.6  35.0  14.6  21.6  5.9 -23.9  0.9  4.5 -2.5  24.8  15.7 -26.7 -34.3  22.8 -14.9 
-5.7 -38.4  2.9 -0.7 -16.7  21.6  38.8  16.1  24.0  6.5 -26.6  1.0  4.9 -2.8  27.5  17.5 -29.6 -38.1  25.3 -16.5 
-6.5 -43.4  3.4 -0.9 -18.9  24.5  43.9  18.2  27.2  7.4 -30.1  1.1  5.6 -3.2  31.1  19.8 -33.6 -43.1  28.7 -18.7 
-7.0 -46.1  3.6 -0.9 -20.1  26.0  46.7  19.3  28.9  7.9 -32.0  1.2  6.0 -3.4  33.0  21.0 -35.7 -45.7  30.5 -19.9 
-7.7 -51.6  4.0 -1.0 -22.4  29.0  52.1  21.7  32.3  8.8 -35.7  1.3  6.6 -3.8  36.9  23.4 -39.9 -51.1  34.0 -22.2 
-7.6 -50.7  3.9 -1.1 -22.1  28.6  51.3  21.3  31.8  8.7 -35.1  1.3  6.5 -3.7  36.3  23.1 -39.3 -50.3  33.5 -21.9 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 2.101226
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-3.5 -23.5  1.7 -0.4 -10.3  13.3  23.7  9.8  14.6  3.9 -16.2  0.6  3.0 -1.7  16.8  10.7 -18.0 -23.3  15.4 -10.1 
-4.1 -27.4  2.0 -0.5 -12.0  15.4  27.7  11.5  17.1  4.6 -19.0  0.7  3.5 -2.0  19.7  12.5 -21.0 -27.2  18.1 -11.8 
-4.5 -30.0  2.3 -0.6 -13.0  16.9  30.3  12.6  18.8  5.1 -20.8  0.8  3.9 -2.2  21.5  13.7 -23.0 -29.7  19.8 -12.9 
-5.2 -34.6  2.7 -0.7 -15.1  19.6  35.0  14.6  21.6  5.9 -23.9  0.9  4.5 -2.5  24.8  15.7 -26.7 -34.3  22.8 -14.9 
-5.7 -38.4  2.9 -0.7 -16.7  21.6  38.8  16.1  24.0  6.5 -26.6  1.0  4.9 -2.8  27.5  17.5 -29.6 -38.1  25.3 -16.5 
-6.5 -43.4  3.4 -0.9 -18.9  24.5  43.9  18.2  27.2  7.4 -30.1  1.1  5.6 -3.2  31.1  19.8 -33.6 -43.1  28.7 -18.7 
-7.0 -46.1  3.6 -0.9 -20.1  26.0  46.7  19.3  28.9  7.9 -32.0  1.2  6.0 -3.4  33.0  21.0 -35.7 -45.7  30.5 -19.9 
-7.7 -51.6  4.0 -1.0 -22.4  29.0  52.1  21.7  32.3  8.8 -35.7  1.3  6.6 -3.8  36.9  23.4 -39.9 -51.1  34.0 -22.2 
-7.6 -50.7  3.9 -1.1 -22.1  28.6  51.3  21.3  31.8  8.7 -35.1  1.3  6.5 -3.7  36.3  23.1 -39.3 -50.3  33.5 -21.9 
Minimum distance of calculation. 
true answer: 5 
indices: 5 
distance: 5.1489873
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-3.5 -23.5  1.7 -0.4 -10.3  13.3  23.7  9.8  14.6  3.9 -16.2  0.6  3.0 -1.7  16.8  10.7 -18.0 -23.3  15.4 -10.1 
-4.1 -27.4  2.0 -0.5 -12.0  15.4  27.7  11.5  17.1  4.6 -19.0  0.7  3.5 -2.0  19.7  12.5 -21.0 -27.2  18.1 -11.8 
-4.5 -30.0  2.3 -0.6 -13.0  16.9  30.3  12.6  18.8  5.1 -20.8  0.8  3.9 -2.2  21.5  13.7 -23.0 -29.7  19.8 -12.9 
-5.2 -34.6  2.7 -0.7 -15.1  19.6  35.0  14.6  21.6  5.9 -23.9  0.9  4.5 -2.5  24.8  15.7 -26.7 -34.3  22.8 -14.9 
-5.7 -38.4  2.9 -0.7 -16.7  21.6  38.8  16.1  24.0  6.5 -26.6  1.0  4.9 -2.8  27.5  17.5 -29.6 -38.1  25.3 -16.5 
-6.5 -43.4  3.4 -0.9 -18.9  24.5  43.9  18.2  27.2  7.4 -30.1  1.1  5.6 -3.2  31.1  19.8 -33.6 -43.1  28.7 -18.7 
-7.0 -46.1  3.6 -0.9 -20.1  26.0  46.7  19.3  28.9  7.9 -32.0  1.2  6.0 -3.4  33.0  21.0 -35.7 -45.7  30.5 -19.9 
-7.7 -51.6  4.0 -1.0 -22.4  29.0  52.1  21.7  32.3  8.8 -35.7  1.3  6.6 -3.8  36.9  23.4 -39.9 -51.1  34.0 -22.2 
-7.6 -50.7  3.9 -1.1 -22.1  28.6  51.3  21.3  31.8  8.7 -35.1  1.3  6.5 -3.7  36.3  23.1 -39.3 -50.3  33.5 -21.9 
Minimum distance of calculation. 
true answer: 8 
indices: 9 
distance: 1.5986917
pred: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-3.5 -23.5  1.7 -0.4 -10.3  13.3  23.7  9.8  14.6  3.9 -16.2  0.6  3.0 -1.7  16.8  10.7 -18.0 -23.3  15.4 -10.1 
-4.1 -27.4  2.0 -0.5 -12.0  15.4  27.7  11.5  17.1  4.6 -19.0  0.7  3.5 -2.0  19.7  12.5 -21.0 -27.2  18.1 -11.8 
-4.5 -30.0  2.3 -0.6 -13.0  16.9  30.3  12.6  18.8  5.1 -20.8  0.8  3.9 -2.2  21.5  13.7 -23.0 -29.7  19.8 -12.9 
-5.2 -34.6  2.7 -0.7 -15.1  19.6  35.0  14.6  21.6  5.9 -23.9  0.9  4.5 -2.5  24.8  15.7 -26.7 -34.3  22.8 -14.9 
-5.7 -38.4  2.9 -0.7 -16.7  21.6  38.8  16.1  24.0  6.5 -26.6  1.0  4.9 -2.8  27.5  17.5 -29.6 -38.1  25.3 -16.5 
-6.5 -43.4  3.4 -0.9 -18.9  24.5  43.9  18.2  27.2  7.4 -30.1  1.1  5.6 -3.2  31.1  19.8 -33.6 -43.1  28.7 -18.7 
-7.0 -46.1  3.6 -0.9 -20.1  26.0  46.7  19.3  28.9  7.9 -32.0  1.2  6.0 -3.4  33.0  21.0 -35.7 -45.7  30.5 -19.9 
-7.7 -51.6  4.0 -1.0 -22.4  29.0  52.1  21.7  32.3  8.8 -35.7  1.3  6.6 -3.8  36.9  23.4 -39.9 -51.1  34.0 -22.2 
-7.6 -50.7  3.9 -1.1 -22.1  28.6  51.3  21.3  31.8  8.7 -35.1  1.3  6.5 -3.7  36.3  23.1 -39.3 -50.3  33.5 -21.9 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 6.3666267
results (all): {'reconst_1x1_avg': 0.1171875, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.0546875, 'cross_0x1_all': nan, 'cluster_avg': -0.2245089, 'cluster_all': array([-0.8521952 , -0.8375673 , -0.19906315, ..., -0.32579434,
       -0.32579434, -0.14503919], dtype=float32), 'magnitude_avg': 0.972930600437943, 'magnitude_all': 0.972930600437943, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.1171875, 'cross_0x1_avg': 0.0546875, 'cluster_avg': -0.2245089, 'magnitude_avg': 0.972930600437943, 'tsne-2d_avg': nan, 'mathematics_avg': 0.1063394695520401, 'mathematics_1+9-8_avg': 1.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([1, 9, 6, 5, 4, 3, 2, 1, 2, 9, 4, 2, 9, 3, 9, 2, 6, 1, 7, 2, 8, 1, 2, 3,
        6, 4, 8, 5, 6, 9, 2, 6, 7, 1, 4, 1, 4, 7, 7, 7, 5, 5, 4, 1, 9, 6, 5, 7,
        8, 9, 2, 5, 8, 1, 1, 7, 7, 1, 7, 8, 1, 9, 5, 3, 7, 3, 4, 5, 7, 8, 9, 2,
        1, 4, 6, 1, 7, 1, 4, 2, 6, 4, 3, 4, 8, 8, 6, 6, 8, 3, 5, 5, 2, 5, 8, 8,
        2, 7, 1, 3, 3, 1, 5, 5, 3, 7, 1, 7, 5, 2, 3, 3, 7, 7, 5, 4, 9, 9, 2, 7,
        5, 7, 3, 1, 8, 2, 1, 4]), ['b1s', 'g9s', 'g6j', 'r5t', 'r4s', 'g3t', 'g2t', 'g1t', 'b2s', 'w9s', 'w4s', 'w2t', 'b9t', 'r3s', 'w9t', 'g2t', 'w6t', 'r1j', 'g7j', 'g2t', 'b8j', 'b1t', 'r2s', 'b3s', 'r6t', 'b4s', 'r8s', 'w5t', 'r6s', 'r9j', 'b2t', 'b6j', 'w7t', 'w1j', 'b4s', 'g1t', 'b4t', 'g7s', 'r7s', 'r7j', 'b5s', 'b5t', 'w4t', 'w1j', 'b9s', 'b6s', 'g5j', 'r7t', 'r8s', 'w9s', 'g2j', 'r5s', 'g8t', 'w1j', 'g1s', 'w7j', 'w7t', 'w1s', 'r7j', 'w8j', 'r1t', 'g9s', 'g5t', 'b3j', 'r7s', 'r3t', 'g4s', 'b5j', 'g7t', 'g8t', 'w9t', 'w2s', 'b1t', 'w4j', 'g6t', 'b1t', 'b7s', 'b1s', 'g4j', 'w2s', 'b6j', 'g4t', 'b3j', 'r4s', 'r8j', 'b8s', 'r6s', 'r6j', 'w8t', 'w3t', 'b5s', 'b5j', 'r2t', 'g5t', 'w8t', 'b8j', 'w2j', 'g7t', 'g1s', 'g3t', 'g3t', 'w1s', 'g5s', 'r5j', 'w3s', 'r7j', 'w1t', 'r7j', 'g5j', 'g2s', 'r3t', 'b3t', 'b7j', 'b7j', 'r5j', 'r4j', 'g9j', 'r9s', 'b2t', 'r7j', 'r5s', 'g7t', 'r3j', 'w1t', 'g8j', 'g2t', 'b1s', 'w4s']] 
label[0]: tensor([1, 9, 6, 5, 4, 3, 2, 1, 2, 9, 4, 2, 9, 3, 9, 2, 6, 1, 7, 2, 8, 1, 2, 3,
        6, 4, 8, 5, 6, 9, 2, 6, 7, 1, 4, 1, 4, 7, 7, 7, 5, 5, 4, 1, 9, 6, 5, 7,
        8, 9, 2, 5, 8, 1, 1, 7, 7, 1, 7, 8, 1, 9, 5, 3, 7, 3, 4, 5, 7, 8, 9, 2,
        1, 4, 6, 1, 7, 1, 4, 2, 6, 4, 3, 4, 8, 8, 6, 6, 8, 3, 5, 5, 2, 5, 8, 8,
        2, 7, 1, 3, 3, 1, 5, 5, 3, 7, 1, 7, 5, 2, 3, 3, 7, 7, 5, 4, 9, 9, 2, 7,
        5, 7, 3, 1, 8, 2, 1, 4]) 
label[1]: ['b1s', 'g9s', 'g6j', 'r5t', 'r4s', 'g3t', 'g2t', 'g1t', 'b2s', 'w9s', 'w4s', 'w2t', 'b9t', 'r3s', 'w9t', 'g2t', 'w6t', 'r1j', 'g7j', 'g2t', 'b8j', 'b1t', 'r2s', 'b3s', 'r6t', 'b4s', 'r8s', 'w5t', 'r6s', 'r9j', 'b2t', 'b6j', 'w7t', 'w1j', 'b4s', 'g1t', 'b4t', 'g7s', 'r7s', 'r7j', 'b5s', 'b5t', 'w4t', 'w1j', 'b9s', 'b6s', 'g5j', 'r7t', 'r8s', 'w9s', 'g2j', 'r5s', 'g8t', 'w1j', 'g1s', 'w7j', 'w7t', 'w1s', 'r7j', 'w8j', 'r1t', 'g9s', 'g5t', 'b3j', 'r7s', 'r3t', 'g4s', 'b5j', 'g7t', 'g8t', 'w9t', 'w2s', 'b1t', 'w4j', 'g6t', 'b1t', 'b7s', 'b1s', 'g4j', 'w2s', 'b6j', 'g4t', 'b3j', 'r4s', 'r8j', 'b8s', 'r6s', 'r6j', 'w8t', 'w3t', 'b5s', 'b5j', 'r2t', 'g5t', 'w8t', 'b8j', 'w2j', 'g7t', 'g1s', 'g3t', 'g3t', 'w1s', 'g5s', 'r5j', 'w3s', 'r7j', 'w1t', 'r7j', 'g5j', 'g2s', 'r3t', 'b3t', 'b7j', 'b7j', 'r5j', 'r4j', 'g9j', 'r9s', 'b2t', 'r7j', 'r5s', 'g7t', 'r3j', 'w1t', 'g8j', 'g2t', 'b1s', 'w4s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.66726804  0.02442396  0.55204886 ...  0.59769946 -0.34554404
 -0.25857508]
Average of silhouette coef: -0.04075492
---
  0   1   2
0 0.0 36.0 25.1 
1 36.0 0.0 10.8 
2 25.1 10.8 0.0 
correlation [[1.       0.079811]
 [0.079811 1.      ]]
---
[[], [], []] [1 1 0 ... 0 1 1] [[ 21.219831   16.482597 ]
 [ 14.115352   68.30125  ]
 [ -4.495251   32.04494  ]
 ...
 [-38.012817   17.250004 ]
 [  1.8468242 -23.301292 ]
 [-13.544083  -30.275541 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': -0.04075492, 'cluster_all': array([-0.66726804,  0.02442396,  0.55204886, ...,  0.59769946,
       -0.34554404, -0.25857508], dtype=float32), 'magnitude_avg': 0.0798109997067676, 'magnitude_all': 0.0798109997067676, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_3', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': -0.04075492, 'magnitude_avg': 0.0798109997067676, 'tsne-2d_avg': nan}
Arguments (initial):
{'pretrained_path': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'analyse'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4', 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 mmvae_cmnist_oscn_seed_4
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}


Model runner was initialized.
Print of model summary was skipped because 'MMVAE_CMNIST_OSCN' object has no attribute 'data_size'
Loading model MMVAE_CMNIST_OSCN from ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4

length of cmnist and oscn dataset (train): 240000 
length of cmnist and oscn dataset (test): 8000

length of dataset (train): 240000 
length of dataset (test): 8000
objectives: m_dreg 
t_objectives: m_iwae


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 400]         941,200
              ReLU-2                  [-1, 400]               0
            Linear-3                    [-1, 8]           3,208
            Linear-4                    [-1, 9]              81
================================================================
Total params: 944,489
Trainable params: 944,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.01
Params size (MB): 3.60
Estimated Total Size (MB): 3.62
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([5, 9, 7, 7, 9, 8, 6, 4, 2, 8, 7, 1, 8, 4, 1, 6, 7, 6, 4, 8, 6, 8, 1, 5,
        2, 2, 1, 9, 8, 2, 5, 8, 8, 3, 2, 1, 2, 7, 3, 1, 7, 1, 3, 5, 2, 6, 5, 4,
        7, 1, 5, 8, 6, 4, 7, 6, 1, 2, 8, 2, 7, 6, 3, 2, 5, 9, 8, 4, 2, 7, 4, 2,
        7, 4, 1, 2, 2, 9, 9, 8, 5, 3, 9, 2, 8, 7, 2, 8, 7, 2, 8, 5, 4, 4, 2, 4,
        9, 2, 4, 5, 2, 3, 2, 1, 2, 4, 3, 8, 7, 7, 7, 3, 6, 9, 7, 1, 3, 7, 7, 4,
        5, 9, 8, 7, 4, 7, 8, 3]), ['r5s', 'g9t', 'b7s', 'b7j', 'b9j', 'r8t', 'w6t', 'g4j', 'w2j', 'w8s', 'r7s', 'r1t', 'b8s', 'b4t', 'r1s', 'g6j', 'b7t', 'b6j', 'g4s', 'w8j', 'r6s', 'w8s', 'w1s', 'b5t', 'g2s', 'b2s', 'b1s', 'w9s', 'g8j', 'g2j', 'w5s', 'g8s', 'r8t', 'r3s', 'b2t', 'g1s', 'r2s', 'r7t', 'b3j', 'w1j', 'r7j', 'g1s', 'r3j', 'w5t', 'b2s', 'r6s', 'w5t', 'w4j', 'g7j', 'g1j', 'w5j', 'g8j', 'r6t', 'b4s', 'w7t', 'r6s', 'g1j', 'w2j', 'w8s', 'w2t', 'b7s', 'g6j', 'b3t', 'b2j', 'b5j', 'w9s', 'w8j', 'g4j', 'g2s', 'g7j', 'g4s', 'w2s', 'w7s', 'g4t', 'g1s', 'g2t', 'g2j', 'w9j', 'b9s', 'b8j', 'b5s', 'b3t', 'w9t', 'w2s', 'b8t', 'b7j', 'r2s', 'b8j', 'r7j', 'r2j', 'g8t', 'r5j', 'g4s', 'w4j', 'g2t', 'w4t', 'b9j', 'w2t', 'g4j', 'w5t', 'b2t', 'r3s', 'w2j', 'w1s', 'b2s', 'r4t', 'g3t', 'w8t', 'r7t', 'g7t', 'g7s', 'w3s', 'b6j', 'g9t', 'r7j', 'r1j', 'b3j', 'b7t', 'b7j', 'r4s', 'w5j', 'g9s', 'w8t', 'g7j', 'g4s', 'w7t', 'g8t', 'g3j']] 
label[0]: tensor([5, 9, 7, 7, 9, 8, 6, 4, 2, 8, 7, 1, 8, 4, 1, 6, 7, 6, 4, 8, 6, 8, 1, 5,
        2, 2, 1, 9, 8, 2, 5, 8, 8, 3, 2, 1, 2, 7, 3, 1, 7, 1, 3, 5, 2, 6, 5, 4,
        7, 1, 5, 8, 6, 4, 7, 6, 1, 2, 8, 2, 7, 6, 3, 2, 5, 9, 8, 4, 2, 7, 4, 2,
        7, 4, 1, 2, 2, 9, 9, 8, 5, 3, 9, 2, 8, 7, 2, 8, 7, 2, 8, 5, 4, 4, 2, 4,
        9, 2, 4, 5, 2, 3, 2, 1, 2, 4, 3, 8, 7, 7, 7, 3, 6, 9, 7, 1, 3, 7, 7, 4,
        5, 9, 8, 7, 4, 7, 8, 3]) 
label[1]: ['r5s', 'g9t', 'b7s', 'b7j', 'b9j', 'r8t', 'w6t', 'g4j', 'w2j', 'w8s', 'r7s', 'r1t', 'b8s', 'b4t', 'r1s', 'g6j', 'b7t', 'b6j', 'g4s', 'w8j', 'r6s', 'w8s', 'w1s', 'b5t', 'g2s', 'b2s', 'b1s', 'w9s', 'g8j', 'g2j', 'w5s', 'g8s', 'r8t', 'r3s', 'b2t', 'g1s', 'r2s', 'r7t', 'b3j', 'w1j', 'r7j', 'g1s', 'r3j', 'w5t', 'b2s', 'r6s', 'w5t', 'w4j', 'g7j', 'g1j', 'w5j', 'g8j', 'r6t', 'b4s', 'w7t', 'r6s', 'g1j', 'w2j', 'w8s', 'w2t', 'b7s', 'g6j', 'b3t', 'b2j', 'b5j', 'w9s', 'w8j', 'g4j', 'g2s', 'g7j', 'g4s', 'w2s', 'w7s', 'g4t', 'g1s', 'g2t', 'g2j', 'w9j', 'b9s', 'b8j', 'b5s', 'b3t', 'w9t', 'w2s', 'b8t', 'b7j', 'r2s', 'b8j', 'r7j', 'r2j', 'g8t', 'r5j', 'g4s', 'w4j', 'g2t', 'w4t', 'b9j', 'w2t', 'g4j', 'w5t', 'b2t', 'r3s', 'w2j', 'w1s', 'b2s', 'r4t', 'g3t', 'w8t', 'r7t', 'g7t', 'g7s', 'w3s', 'b6j', 'g9t', 'r7j', 'r1j', 'b3j', 'b7t', 'b7j', 'r4s', 'w5j', 'g9s', 'w8t', 'g7j', 'g4s', 'w7t', 'g8t', 'g3j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.09257383  0.14004022  0.11382432 ...  0.12861522 -0.07525567
  0.1738561 ]
Average of silhouette coef: 0.08442715
---
  0   1   2   3
0 0.0 2.4 2.8 2.4 
1 2.4 0.0 3.7 3.9 
2 2.8 3.7 0.0 3.9 
3 2.4 3.9 3.9 0.0 
correlation [[ 1.         -0.38080333]
 [-0.38080333  1.        ]]
---
[[], [], [], []] [3 2 1 ... 1 0 2] [[ 2.2715639e+01  6.4915098e-02]
 [ 3.2388386e+01 -4.4860836e+01]
 [-3.6391911e+01 -5.5798580e+01]
 ...
 [ 4.8662090e+01  3.5306084e+01]
 [-7.0199608e+01  1.1345524e+01]
 [-4.2101238e+01 -1.6428366e+01]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_0/latent__0_0.svg
results (all): {'cluster_avg': 0.08442715, 'cluster_all': array([ 0.09257383,  0.14004022,  0.11382432, ...,  0.12861522,
       -0.07525567,  0.1738561 ], dtype=float32), 'magnitude_avg': -0.3808033264255323, 'magnitude_all': -0.3808033264255323, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 0, 'cluster_avg': 0.08442715, 'magnitude_avg': -0.3808033264255323, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([7, 3, 7, 5, 8, 7, 5, 9, 5, 9, 1, 1, 7, 8, 4, 9, 6, 4, 8, 1, 4, 6, 3, 4,
        7, 1, 3, 6, 9, 8, 7, 4, 7, 7, 1, 6, 7, 1, 2, 3, 4, 3, 8, 1, 4, 9, 3, 9,
        4, 2, 4, 3, 6, 2, 9, 1, 2, 4, 9, 3, 4, 1, 2, 3, 2, 8, 9, 1, 2, 5, 1, 2,
        7, 9, 9, 5, 3, 4, 3, 4, 3, 8, 4, 3, 7, 3, 8, 3, 3, 7, 3, 5, 7, 9, 4, 6,
        2, 4, 3, 1, 2, 3, 9, 8, 1, 7, 4, 7, 5, 9, 9, 9, 1, 3, 6, 9, 9, 6, 9, 4,
        3, 5, 3, 1, 1, 3, 2, 9]), ['b7t', 'b3t', 'w7t', 'g5s', 'w8j', 'b7j', 'r5j', 'r9j', 'r5t', 'g9t', 'r1s', 'w1s', 'r7s', 'g8s', 'r4j', 'g9s', 'r6t', 'b4s', 'b8j', 'b1s', 'b4s', 'b6t', 'g3s', 'b4t', 'r7t', 'r1j', 'b3j', 'w6t', 'g9s', 'w8j', 'b7t', 'r4t', 'g7j', 'g7s', 'g1s', 'w6j', 'w7t', 'b1s', 'w2t', 'b3j', 'b4s', 'w3s', 'b8s', 'b1s', 'g4s', 'r9t', 'b3j', 'g9s', 'w4j', 'b2t', 'r4j', 'w3s', 'w6t', 'w2t', 'r9s', 'g1t', 'w2j', 'b4j', 'g9j', 'b3s', 'w4j', 'r1s', 'b2t', 'g3j', 'b2j', 'g8j', 'w9j', 'b1j', 'r2j', 'r5j', 'g1s', 'w2j', 'g7t', 'w9s', 'g9s', 'r5j', 'b3s', 'g4t', 'b3s', 'g4t', 'r3j', 'b8t', 'b4j', 'w3t', 'g7j', 'w3s', 'r8j', 'g3j', 'r3t', 'b7t', 'w3t', 'w5t', 'r7s', 'r9j', 'g4t', 'b6j', 'g2j', 'g4t', 'r3t', 'w1s', 'w2j', 'g3s', 'r9s', 'w8s', 'r1j', 'r7j', 'w4j', 'r7t', 'g5j', 'g9s', 'w9j', 'g9t', 'w1s', 'w3s', 'r6s', 'b9j', 'w9s', 'g6s', 'w9s', 'b4s', 'w3s', 'b5s', 'r3s', 'g1s', 'w1j', 'g3j', 'g2t', 'r9s']] 
label[0]: tensor([7, 3, 7, 5, 8, 7, 5, 9, 5, 9, 1, 1, 7, 8, 4, 9, 6, 4, 8, 1, 4, 6, 3, 4,
        7, 1, 3, 6, 9, 8, 7, 4, 7, 7, 1, 6, 7, 1, 2, 3, 4, 3, 8, 1, 4, 9, 3, 9,
        4, 2, 4, 3, 6, 2, 9, 1, 2, 4, 9, 3, 4, 1, 2, 3, 2, 8, 9, 1, 2, 5, 1, 2,
        7, 9, 9, 5, 3, 4, 3, 4, 3, 8, 4, 3, 7, 3, 8, 3, 3, 7, 3, 5, 7, 9, 4, 6,
        2, 4, 3, 1, 2, 3, 9, 8, 1, 7, 4, 7, 5, 9, 9, 9, 1, 3, 6, 9, 9, 6, 9, 4,
        3, 5, 3, 1, 1, 3, 2, 9]) 
label[1]: ['b7t', 'b3t', 'w7t', 'g5s', 'w8j', 'b7j', 'r5j', 'r9j', 'r5t', 'g9t', 'r1s', 'w1s', 'r7s', 'g8s', 'r4j', 'g9s', 'r6t', 'b4s', 'b8j', 'b1s', 'b4s', 'b6t', 'g3s', 'b4t', 'r7t', 'r1j', 'b3j', 'w6t', 'g9s', 'w8j', 'b7t', 'r4t', 'g7j', 'g7s', 'g1s', 'w6j', 'w7t', 'b1s', 'w2t', 'b3j', 'b4s', 'w3s', 'b8s', 'b1s', 'g4s', 'r9t', 'b3j', 'g9s', 'w4j', 'b2t', 'r4j', 'w3s', 'w6t', 'w2t', 'r9s', 'g1t', 'w2j', 'b4j', 'g9j', 'b3s', 'w4j', 'r1s', 'b2t', 'g3j', 'b2j', 'g8j', 'w9j', 'b1j', 'r2j', 'r5j', 'g1s', 'w2j', 'g7t', 'w9s', 'g9s', 'r5j', 'b3s', 'g4t', 'b3s', 'g4t', 'r3j', 'b8t', 'b4j', 'w3t', 'g7j', 'w3s', 'r8j', 'g3j', 'r3t', 'b7t', 'w3t', 'w5t', 'r7s', 'r9j', 'g4t', 'b6j', 'g2j', 'g4t', 'r3t', 'w1s', 'w2j', 'g3s', 'r9s', 'w8s', 'r1j', 'r7j', 'w4j', 'r7t', 'g5j', 'g9s', 'w9j', 'g9t', 'w1s', 'w3s', 'r6s', 'b9j', 'w9s', 'g6s', 'w9s', 'b4s', 'w3s', 'b5s', 'r3s', 'g1s', 'w1j', 'g3j', 'g2t', 'r9s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 5, 4, 4, 4, 4]) 
tar: tensor([4, 6, 9, 2, 4, 4, 9, 7, 6, 3, 2, 1, 1, 4, 9, 9, 1, 2, 5, 1, 9, 9, 3, 5,
        6, 9, 3, 9, 7, 4, 9, 7, 2, 7, 5, 5, 8, 9, 3, 1, 8, 3, 1, 5, 3, 2, 1, 7,
        5, 2, 5, 2, 9, 1, 6, 9, 7, 1, 8, 4, 5, 3, 9, 7, 1, 4, 5, 1, 1, 4, 4, 2,
        9, 2, 9, 2, 7, 8, 2, 2, 4, 7, 9, 4, 1, 2, 7, 3, 6, 2, 4, 9, 7, 5, 3, 6,
        4, 7, 3, 6, 8, 3, 9, 7, 1, 8, 2, 3, 4, 7, 2, 9, 2, 7, 4, 3, 5, 8, 1, 6,
        2, 2, 9, 8, 9, 1, 8, 1])
Accuracy (count): tensor(15) 
Accuracy (ratio) tensor(0.1172)
Accuracy:
 [[ 0  0  0 17  0  0  0  0  0]
 [ 0  0  0 18  1  0  0  0  0]
 [ 0  0  0 13  0  0  0  0  0]
 [ 0  0  0 15  0  0  0  0  0]
 [ 0  0  0 11  0  0  0  0  0]
 [ 0  0  0  7  1  0  0  0  0]
 [ 0  0  0 15  0  0  0  0  0]
 [ 0  0  0  8  1  0  0  0  0]
 [ 0  0  0 21  0  0  0  0  0]]
Accuracy:
 [[0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.947 0.053 0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.875 0.125 0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.889 0.111 0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 28, 28])
size of extracted recon in MMVAE: torch.Size([128, 3, 28, 28])
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([9, 1, 1, 7, 7, 5, 6, 9, 7, 2, 5, 1, 5, 1, 9, 6, 4, 9, 2, 2, 9, 1, 3, 6,
        4, 7, 6, 4, 5, 7, 1, 2, 9, 1, 4, 2, 7, 9, 3, 4, 5, 3, 5, 4, 6, 1, 1, 5,
        2, 4, 6, 6, 1, 3, 5, 6, 4, 7, 2, 3, 7, 5, 8, 6, 9, 4, 5, 8, 3, 1, 7, 1,
        6, 1, 1, 5, 2, 3, 9, 1, 3, 6, 5, 8, 9, 6, 7, 7, 8, 7, 2, 6, 1, 4, 6, 2,
        8, 1, 4, 1, 7, 7, 2, 2, 6, 1, 8, 1, 4, 2, 2, 7, 4, 1, 2, 1, 9, 2, 9, 9,
        2, 5, 6, 2, 1, 1, 1, 3])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0  0  0 25  0  0  0  0  0]
 [ 0  0  0 18  0  0  0  0  0]
 [ 0  0  0  9  0  0  0  0  0]
 [ 0  0  0 13  0  0  0  0  0]
 [ 0  0  0 13  0  0  0  0  0]
 [ 0  0  0 16  0  0  0  0  0]
 [ 0  0  0 15  0  0  0  0  0]
 [ 0  0  0  6  0  0  0  0  0]
 [ 0  0  0 13  0  0  0  0  0]]
Accuracy:
 [[0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]]
---
Silhouette values: [0.18807274 0.17150578 0.23126033 ... 0.12268393 0.14845519 0.40373024]
Average of silhouette coef: 0.10403704
---
  1   2   3   4   5   6   7   8   9
1 0.0 4.1 4.1 4.7 4.6 4.5 4.5 4.3 4.6 
2 4.1 0.0 3.6 4.1 4.7 4.5 4.5 4.7 5.0 
3 4.1 3.6 0.0 4.1 3.0 4.7 4.0 3.8 4.3 
4 4.7 4.1 4.1 0.0 3.9 4.1 4.2 4.7 3.8 
5 4.6 4.7 3.0 3.9 0.0 4.0 4.1 3.5 4.2 
6 4.5 4.5 4.7 4.1 4.0 0.0 5.4 4.3 4.3 
7 4.5 4.5 4.0 4.2 4.1 5.4 0.0 4.1 3.2 
8 4.3 4.7 3.8 4.7 3.5 4.3 4.1 0.0 2.7 
9 4.6 5.0 4.3 3.8 4.2 4.3 3.2 2.7 0.0 
correlation [[1.         0.39875531]
 [0.39875531 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [7 3 7 ... 7 7 1] [[ 47.26373    -8.275155 ]
 [-28.6745      9.248033 ]
 [ 54.620663    2.2424333]
 ...
 [ 63.43365   -14.902831 ]
 [ 61.172768   -6.830445 ]
 [  3.8008416  69.22374  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_1/latent__0_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 5, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 4, 4, 5, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(65) 
Accuracy (ratio) tensor(0.1329)
Accuracy:
 [[ 0  0  0 43  2  0  0  0  0]
 [ 0  0  0 49  3  0  0  0  0]
 [ 0  0  0 55  2  0  0  0  0]
 [ 0  0  0 60  0  0  0  0  0]
 [ 0  0  0 56  5  0  0  0  0]
 [ 0  0  0 56  4  0  0  0  0]
 [ 0  0  0 53  4  0  0  0  0]
 [ 0  0  0 45  7  0  0  0  0]
 [ 0  0  0 41  4  0  0  0  0]]
Accuracy:
 [[0.    0.    0.    0.956 0.044 0.    0.    0.    0.   ]
 [0.    0.    0.    0.942 0.058 0.    0.    0.    0.   ]
 [0.    0.    0.    0.965 0.035 0.    0.    0.    0.   ]
 [0.    0.    0.    1.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.918 0.082 0.    0.    0.    0.   ]
 [0.    0.    0.    0.933 0.067 0.    0.    0.    0.   ]
 [0.    0.    0.    0.93  0.07  0.    0.    0.    0.   ]
 [0.    0.    0.    0.865 0.135 0.    0.    0.    0.   ]
 [0.    0.    0.    0.911 0.089 0.    0.    0.    0.   ]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  2.0 -0.1  2.2 -0.0 -0.2 -0.1 -0.1 -0.0  0.2  0.0 -0.1 -0.1  0.1 -0.2  0.0 -0.5 -0.0 -0.1  0.1 
 0.2  1.0  0.4 -0.5 -0.9  0.3 -0.1 -2.6  0.3  0.5 -0.4 -0.1 -0.0  0.2  0.1  0.4 -0.9 -0.2 -0.3  0.1 
 0.1  0.2  0.1 -0.9  1.4  0.3 -0.3 -0.2  0.3  0.4 -0.1  0.2 -0.0  0.2 -0.2  0.8 -1.4 -0.1 -0.1  0.1 
 0.1 -1.1 -0.1  0.3 -0.5  2.2 -0.5 -0.8 -0.3  0.4 -0.2 -0.2 -0.0  0.1 -1.4  0.3  0.5  0.1 -0.1  0.2 
 0.1 -0.5 -0.1 -1.1  0.9 -0.3 -0.4  0.6 -0.5  0.4 -0.3 -0.8 -0.1  0.2 -1.2 -0.7 -0.0 -0.9 -0.0  0.2 
 0.1 -0.0 -0.1 -0.3 -1.6 -0.2 -0.2  0.2 -2.2  0.2 -0.1  1.3 -0.0  0.1 -0.5 -0.2  0.2 -0.3  0.0  0.1 
 0.1 -0.9  0.0  0.4 -0.1 -0.3 -0.2 -0.1  2.6  0.5 -0.0  0.1  0.1  0.2  0.1 -0.2  0.6 -0.6 -0.0  0.2 
 0.1  0.2 -0.0 -0.6  0.1 -0.6 -0.6  0.5 -0.3  0.2  1.7 -0.9 -0.0  0.1  0.7 -0.2  0.5  0.7 -0.0  0.1 
 0.1 -1.4  0.0  0.5 -0.1  0.2 -0.4  0.3  0.2  0.1  1.1 -0.0  0.0  0.0  1.0  0.4  1.4  0.2  0.2  0.1 
Minimum distance of calculation. 
true answer: 2 
indices: 1 
distance: 2.7169726
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  2.0 -0.1  2.2 -0.0 -0.2 -0.1 -0.1 -0.0  0.2  0.0 -0.1 -0.1  0.1 -0.2  0.0 -0.5 -0.0 -0.1  0.1 
 0.2  1.0  0.4 -0.5 -0.9  0.3 -0.1 -2.6  0.3  0.5 -0.4 -0.1 -0.0  0.2  0.1  0.4 -0.9 -0.2 -0.3  0.1 
 0.1  0.2  0.1 -0.9  1.4  0.3 -0.3 -0.2  0.3  0.4 -0.1  0.2 -0.0  0.2 -0.2  0.8 -1.4 -0.1 -0.1  0.1 
 0.1 -1.1 -0.1  0.3 -0.5  2.2 -0.5 -0.8 -0.3  0.4 -0.2 -0.2 -0.0  0.1 -1.4  0.3  0.5  0.1 -0.1  0.2 
 0.1 -0.5 -0.1 -1.1  0.9 -0.3 -0.4  0.6 -0.5  0.4 -0.3 -0.8 -0.1  0.2 -1.2 -0.7 -0.0 -0.9 -0.0  0.2 
 0.1 -0.0 -0.1 -0.3 -1.6 -0.2 -0.2  0.2 -2.2  0.2 -0.1  1.3 -0.0  0.1 -0.5 -0.2  0.2 -0.3  0.0  0.1 
 0.1 -0.9  0.0  0.4 -0.1 -0.3 -0.2 -0.1  2.6  0.5 -0.0  0.1  0.1  0.2  0.1 -0.2  0.6 -0.6 -0.0  0.2 
 0.1  0.2 -0.0 -0.6  0.1 -0.6 -0.6  0.5 -0.3  0.2  1.7 -0.9 -0.0  0.1  0.7 -0.2  0.5  0.7 -0.0  0.1 
 0.1 -1.4  0.0  0.5 -0.1  0.2 -0.4  0.3  0.2  0.1  1.1 -0.0  0.0  0.0  1.0  0.4  1.4  0.2  0.2  0.1 
Minimum distance of calculation. 
true answer: 5 
indices: 1 
distance: 4.664784
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  2.0 -0.1  2.2 -0.0 -0.2 -0.1 -0.1 -0.0  0.2  0.0 -0.1 -0.1  0.1 -0.2  0.0 -0.5 -0.0 -0.1  0.1 
 0.2  1.0  0.4 -0.5 -0.9  0.3 -0.1 -2.6  0.3  0.5 -0.4 -0.1 -0.0  0.2  0.1  0.4 -0.9 -0.2 -0.3  0.1 
 0.1  0.2  0.1 -0.9  1.4  0.3 -0.3 -0.2  0.3  0.4 -0.1  0.2 -0.0  0.2 -0.2  0.8 -1.4 -0.1 -0.1  0.1 
 0.1 -1.1 -0.1  0.3 -0.5  2.2 -0.5 -0.8 -0.3  0.4 -0.2 -0.2 -0.0  0.1 -1.4  0.3  0.5  0.1 -0.1  0.2 
 0.1 -0.5 -0.1 -1.1  0.9 -0.3 -0.4  0.6 -0.5  0.4 -0.3 -0.8 -0.1  0.2 -1.2 -0.7 -0.0 -0.9 -0.0  0.2 
 0.1 -0.0 -0.1 -0.3 -1.6 -0.2 -0.2  0.2 -2.2  0.2 -0.1  1.3 -0.0  0.1 -0.5 -0.2  0.2 -0.3  0.0  0.1 
 0.1 -0.9  0.0  0.4 -0.1 -0.3 -0.2 -0.1  2.6  0.5 -0.0  0.1  0.1  0.2  0.1 -0.2  0.6 -0.6 -0.0  0.2 
 0.1  0.2 -0.0 -0.6  0.1 -0.6 -0.6  0.5 -0.3  0.2  1.7 -0.9 -0.0  0.1  0.7 -0.2  0.5  0.7 -0.0  0.1 
 0.1 -1.4  0.0  0.5 -0.1  0.2 -0.4  0.3  0.2  0.1  1.1 -0.0  0.0  0.0  1.0  0.4  1.4  0.2  0.2  0.1 
Minimum distance of calculation. 
true answer: 8 
indices: 7 
distance: 4.1014886
pred: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
-0.1  2.0 -0.1  2.2 -0.0 -0.2 -0.1 -0.1 -0.0  0.2  0.0 -0.1 -0.1  0.1 -0.2  0.0 -0.5 -0.0 -0.1  0.1 
 0.2  1.0  0.4 -0.5 -0.9  0.3 -0.1 -2.6  0.3  0.5 -0.4 -0.1 -0.0  0.2  0.1  0.4 -0.9 -0.2 -0.3  0.1 
 0.1  0.2  0.1 -0.9  1.4  0.3 -0.3 -0.2  0.3  0.4 -0.1  0.2 -0.0  0.2 -0.2  0.8 -1.4 -0.1 -0.1  0.1 
 0.1 -1.1 -0.1  0.3 -0.5  2.2 -0.5 -0.8 -0.3  0.4 -0.2 -0.2 -0.0  0.1 -1.4  0.3  0.5  0.1 -0.1  0.2 
 0.1 -0.5 -0.1 -1.1  0.9 -0.3 -0.4  0.6 -0.5  0.4 -0.3 -0.8 -0.1  0.2 -1.2 -0.7 -0.0 -0.9 -0.0  0.2 
 0.1 -0.0 -0.1 -0.3 -1.6 -0.2 -0.2  0.2 -2.2  0.2 -0.1  1.3 -0.0  0.1 -0.5 -0.2  0.2 -0.3  0.0  0.1 
 0.1 -0.9  0.0  0.4 -0.1 -0.3 -0.2 -0.1  2.6  0.5 -0.0  0.1  0.1  0.2  0.1 -0.2  0.6 -0.6 -0.0  0.2 
 0.1  0.2 -0.0 -0.6  0.1 -0.6 -0.6  0.5 -0.3  0.2  1.7 -0.9 -0.0  0.1  0.7 -0.2  0.5  0.7 -0.0  0.1 
 0.1 -1.4  0.0  0.5 -0.1  0.2 -0.4  0.3  0.2  0.1  1.1 -0.0  0.0  0.0  1.0  0.4  1.4  0.2  0.2  0.1 
Minimum distance of calculation. 
true answer: 6 
indices: 5 
distance: 3.5858274
results (all): {'reconst_0x0_avg': 0.1171875, 'reconst_0x0_all': nan, 'cross_1x0_avg': 0.1015625, 'cross_1x0_all': nan, 'cluster_avg': 0.10403704, 'cluster_all': array([0.18807274, 0.17150578, 0.23126033, ..., 0.12268393, 0.14845519,
       0.40373024], dtype=float32), 'magnitude_avg': 0.3987553106688324, 'magnitude_all': 0.3987553106688324, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.13292433321475983, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 1, 'reconst_0x0_avg': 0.1171875, 'cross_1x0_avg': 0.1015625, 'cluster_avg': 0.10403704, 'magnitude_avg': 0.3987553106688324, 'tsne-2d_avg': nan, 'mathematics_avg': 0.13292433321475983, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([4, 3, 5, 1, 2, 8, 8, 5, 1, 7, 1, 7, 8, 5, 3, 7, 4, 1, 1, 6, 2, 8, 7, 8,
        2, 4, 8, 1, 2, 9, 9, 4, 7, 7, 4, 5, 6, 1, 1, 6, 6, 3, 6, 5, 9, 6, 5, 1,
        3, 6, 8, 1, 4, 2, 1, 4, 7, 5, 8, 4, 5, 1, 2, 9, 9, 5, 7, 8, 1, 3, 6, 2,
        2, 7, 4, 3, 9, 9, 1, 6, 5, 9, 2, 2, 5, 6, 1, 2, 1, 8, 1, 7, 9, 7, 9, 3,
        3, 4, 4, 5, 2, 8, 3, 7, 4, 1, 4, 8, 8, 1, 1, 1, 4, 3, 7, 3, 2, 3, 8, 3,
        7, 8, 4, 9, 5, 9, 3, 5]), ['w4s', 'b3j', 'g5j', 'g1s', 'b2t', 'r8s', 'b8s', 'b5t', 'g1t', 'w7j', 'b1s', 'b7t', 'w8t', 'r5t', 'g3j', 'w7j', 'w4s', 'r1j', 'w1t', 'b6s', 'b2t', 'b8j', 'r7s', 'b8s', 'g2t', 'b4s', 'w8j', 'r1t', 'w2j', 'b9s', 'g9t', 'w4s', 'r7t', 'g7j', 'r4j', 'w5j', 'r6t', 'w1s', 'g1t', 'w6s', 'r6s', 'g3j', 'b6t', 'g5t', 'w9t', 'r6s', 'w5j', 'w1s', 'b3s', 'b6s', 'g8j', 'g1t', 'w4s', 'r2j', 'r1t', 'g4s', 'b7s', 'r5j', 'w8s', 'r4j', 'w5s', 'g1t', 'r2s', 'g9t', 'g9s', 'g5s', 'g7s', 'b8s', 'w1s', 'w3j', 'b6t', 'g2s', 'g2t', 'g7s', 'g4s', 'w3s', 'w9j', 'b9t', 'g1t', 'w6j', 'w5s', 'r9s', 'w2j', 'r2j', 'b5s', 'b6j', 'r1t', 'w2s', 'w1s', 'r8t', 'g1t', 'g7s', 'w9j', 'w7j', 'r9j', 'b3t', 'g3t', 'b4t', 'w4t', 'w5j', 'w2t', 'w8j', 'b3s', 'g7s', 'b4j', 'g1j', 'g4t', 'w8s', 'g8t', 'b1t', 'b1j', 'g1t', 'g4s', 'b3s', 'g7j', 'b3t', 'b2t', 'r3s', 'g8s', 'g3j', 'w7t', 'r8s', 'g4j', 'b9s', 'g5j', 'g9j', 'r3j', 'g5t']] 
label[0]: tensor([4, 3, 5, 1, 2, 8, 8, 5, 1, 7, 1, 7, 8, 5, 3, 7, 4, 1, 1, 6, 2, 8, 7, 8,
        2, 4, 8, 1, 2, 9, 9, 4, 7, 7, 4, 5, 6, 1, 1, 6, 6, 3, 6, 5, 9, 6, 5, 1,
        3, 6, 8, 1, 4, 2, 1, 4, 7, 5, 8, 4, 5, 1, 2, 9, 9, 5, 7, 8, 1, 3, 6, 2,
        2, 7, 4, 3, 9, 9, 1, 6, 5, 9, 2, 2, 5, 6, 1, 2, 1, 8, 1, 7, 9, 7, 9, 3,
        3, 4, 4, 5, 2, 8, 3, 7, 4, 1, 4, 8, 8, 1, 1, 1, 4, 3, 7, 3, 2, 3, 8, 3,
        7, 8, 4, 9, 5, 9, 3, 5]) 
label[1]: ['w4s', 'b3j', 'g5j', 'g1s', 'b2t', 'r8s', 'b8s', 'b5t', 'g1t', 'w7j', 'b1s', 'b7t', 'w8t', 'r5t', 'g3j', 'w7j', 'w4s', 'r1j', 'w1t', 'b6s', 'b2t', 'b8j', 'r7s', 'b8s', 'g2t', 'b4s', 'w8j', 'r1t', 'w2j', 'b9s', 'g9t', 'w4s', 'r7t', 'g7j', 'r4j', 'w5j', 'r6t', 'w1s', 'g1t', 'w6s', 'r6s', 'g3j', 'b6t', 'g5t', 'w9t', 'r6s', 'w5j', 'w1s', 'b3s', 'b6s', 'g8j', 'g1t', 'w4s', 'r2j', 'r1t', 'g4s', 'b7s', 'r5j', 'w8s', 'r4j', 'w5s', 'g1t', 'r2s', 'g9t', 'g9s', 'g5s', 'g7s', 'b8s', 'w1s', 'w3j', 'b6t', 'g2s', 'g2t', 'g7s', 'g4s', 'w3s', 'w9j', 'b9t', 'g1t', 'w6j', 'w5s', 'r9s', 'w2j', 'r2j', 'b5s', 'b6j', 'r1t', 'w2s', 'w1s', 'r8t', 'g1t', 'g7s', 'w9j', 'w7j', 'r9j', 'b3t', 'g3t', 'b4t', 'w4t', 'w5j', 'w2t', 'w8j', 'b3s', 'g7s', 'b4j', 'g1j', 'g4t', 'w8s', 'g8t', 'b1t', 'b1j', 'g1t', 'g4s', 'b3s', 'g7j', 'b3t', 'b2t', 'r3s', 'g8s', 'g3j', 'w7t', 'r8s', 'g4j', 'b9s', 'g5j', 'g9j', 'r3j', 'g5t'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 0 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [ 0.01117098 -0.01697578 -0.01369563 ...  0.00306697 -0.02703471
  0.0085866 ]
Average of silhouette coef: -0.0040110415
---
  0   1   2
0 0.0 0.3 0.4 
1 0.3 0.0 0.4 
2 0.4 0.4 0.0 
correlation [[ 1.         -0.11425016]
 [-0.11425016  1.        ]]
---
[[], [], []] [1 0 0 ... 2 2 0] [[ 15.061471   53.860653 ]
 [-33.99576    -4.4954076]
 [-16.461256   26.826006 ]
 ...
 [-36.575546  -39.418392 ]
 [ 22.487974   28.502243 ]
 [-40.98314    55.49905  ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_2/latent__0_2.svg
results (all): {'cluster_avg': -0.0040110415, 'cluster_all': array([ 0.01117098, -0.01697578, -0.01369563, ...,  0.00306697,
       -0.02703471,  0.0085866 ], dtype=float32), 'magnitude_avg': -0.11425016318236918, 'magnitude_all': -0.11425016318236918, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 0, 'target_property': 2, 'cluster_avg': -0.0040110415, 'magnitude_avg': -0.11425016318236918, 'tsne-2d_avg': nan}


Model runner was initialized.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 16, 16]           1,568
              ReLU-2           [-1, 32, 16, 16]               0
            Conv2d-3             [-1, 64, 8, 8]          32,832
              ReLU-4             [-1, 64, 8, 8]               0
            Conv2d-5            [-1, 128, 4, 4]         131,200
              ReLU-6            [-1, 128, 4, 4]               0
            Conv2d-7              [-1, 8, 1, 1]          16,392
            Linear-8                    [-1, 9]              81
================================================================
Total params: 182,073
Trainable params: 182,073
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.22
Params size (MB): 0.69
Estimated Total Size (MB): 0.93
----------------------------------------------------------------

length of dataset (train): 120000 
length of dataset (test): 4000
objectives: cross 
t_objectives: cross


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([5, 6, 5, 5, 5, 4, 3, 1, 6, 9, 6, 1, 9, 7, 8, 4, 6, 7, 4, 8, 1, 4, 3, 4,
        7, 6, 8, 7, 1, 7, 2, 7, 8, 3, 3, 7, 6, 5, 4, 6, 1, 9, 6, 4, 8, 8, 9, 3,
        7, 2, 4, 1, 6, 9, 6, 6, 9, 7, 3, 2, 4, 2, 3, 8, 5, 6, 7, 3, 8, 4, 5, 3,
        4, 3, 9, 2, 2, 9, 8, 3, 6, 5, 6, 4, 1, 4, 1, 7, 1, 8, 9, 1, 1, 4, 3, 7,
        3, 1, 8, 6, 3, 7, 9, 7, 8, 6, 4, 3, 2, 9, 2, 3, 8, 2, 6, 3, 8, 1, 6, 3,
        9, 8, 7, 3, 4, 1, 7, 1]), ['b5s', 'w6s', 'w5t', 'r5s', 'r5j', 'w4t', 'r3j', 'g1t', 'r6j', 'r9j', 'b6s', 'w1j', 'g9t', 'w7s', 'g8j', 'w4j', 'w6s', 'w7j', 'g4s', 'r8t', 'r1t', 'b4t', 'b3t', 'b4j', 'g7s', 'w6t', 'w8s', 'r7j', 'r1j', 'g7s', 'w2t', 'g7t', 'w8t', 'b3t', 'w3s', 'g7s', 'w6t', 'g5s', 'r4s', 'w6j', 'r1s', 'r9t', 'r6j', 'r4s', 'b8t', 'w8t', 'b9j', 'r3s', 'w7s', 'w2j', 'r4j', 'g1s', 'w6t', 'g9t', 'w6t', 'r6t', 'g9s', 'g7j', 'r3j', 'w2s', 'b4j', 'w2t', 'w3t', 'b8j', 'g5s', 'b6s', 'r7j', 'g3s', 'r8t', 'g4t', 'b5j', 'g3s', 'w4s', 'b3j', 'r9t', 'b2j', 'r2j', 'r9t', 'w8t', 'w3t', 'r6s', 'w5s', 'g6j', 'g4j', 'g1j', 'w4s', 'b1t', 'b7s', 'r1j', 'w8t', 'w9t', 'g1j', 'w1t', 'r4s', 'b3s', 'r7s', 'g3t', 'g1s', 'r8s', 'r6j', 'w3s', 'r7t', 'r9j', 'g7j', 'b8s', 'b6t', 'g4t', 'r3s', 'r2t', 'w9s', 'r2j', 'r3s', 'g8t', 'w2j', 'g6j', 'w3s', 'b8s', 'r1j', 'b6s', 'g3j', 'w9t', 'g8t', 'w7s', 'w3j', 'g4t', 'g1t', 'g7t', 'g1s']] 
label[0]: tensor([5, 6, 5, 5, 5, 4, 3, 1, 6, 9, 6, 1, 9, 7, 8, 4, 6, 7, 4, 8, 1, 4, 3, 4,
        7, 6, 8, 7, 1, 7, 2, 7, 8, 3, 3, 7, 6, 5, 4, 6, 1, 9, 6, 4, 8, 8, 9, 3,
        7, 2, 4, 1, 6, 9, 6, 6, 9, 7, 3, 2, 4, 2, 3, 8, 5, 6, 7, 3, 8, 4, 5, 3,
        4, 3, 9, 2, 2, 9, 8, 3, 6, 5, 6, 4, 1, 4, 1, 7, 1, 8, 9, 1, 1, 4, 3, 7,
        3, 1, 8, 6, 3, 7, 9, 7, 8, 6, 4, 3, 2, 9, 2, 3, 8, 2, 6, 3, 8, 1, 6, 3,
        9, 8, 7, 3, 4, 1, 7, 1]) 
label[1]: ['b5s', 'w6s', 'w5t', 'r5s', 'r5j', 'w4t', 'r3j', 'g1t', 'r6j', 'r9j', 'b6s', 'w1j', 'g9t', 'w7s', 'g8j', 'w4j', 'w6s', 'w7j', 'g4s', 'r8t', 'r1t', 'b4t', 'b3t', 'b4j', 'g7s', 'w6t', 'w8s', 'r7j', 'r1j', 'g7s', 'w2t', 'g7t', 'w8t', 'b3t', 'w3s', 'g7s', 'w6t', 'g5s', 'r4s', 'w6j', 'r1s', 'r9t', 'r6j', 'r4s', 'b8t', 'w8t', 'b9j', 'r3s', 'w7s', 'w2j', 'r4j', 'g1s', 'w6t', 'g9t', 'w6t', 'r6t', 'g9s', 'g7j', 'r3j', 'w2s', 'b4j', 'w2t', 'w3t', 'b8j', 'g5s', 'b6s', 'r7j', 'g3s', 'r8t', 'g4t', 'b5j', 'g3s', 'w4s', 'b3j', 'r9t', 'b2j', 'r2j', 'r9t', 'w8t', 'w3t', 'r6s', 'w5s', 'g6j', 'g4j', 'g1j', 'w4s', 'b1t', 'b7s', 'r1j', 'w8t', 'w9t', 'g1j', 'w1t', 'r4s', 'b3s', 'r7s', 'g3t', 'g1s', 'r8s', 'r6j', 'w3s', 'r7t', 'r9j', 'g7j', 'b8s', 'b6t', 'g4t', 'r3s', 'r2t', 'w9s', 'r2j', 'r3s', 'g8t', 'w2j', 'g6j', 'w3s', 'b8s', 'r1j', 'b6s', 'g3j', 'w9t', 'g8t', 'w7s', 'w3j', 'g4t', 'g1t', 'g7t', 'g1s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 0 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_0 
Number of category 4 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2, 3} 
Start index: 0 
End index: 4 
withzero: False
---
Silhouette values: [ 0.06674705  0.24666059  0.04093057 ... -0.8197959  -0.41349497
  0.42395192]
Average of silhouette coef: -0.07941234
---
  0   1   2   3
0 0.0 32.9 58.1 35.1 
1 32.9 0.0 25.3 2.2 
2 58.1 25.3 0.0 23.1 
3 35.1 2.2 23.1 0.0 
correlation [[1.         0.17138888]
 [0.17138888 1.        ]]
---
[[], [], [], []] [1 0 0 ... 0 3 2] [[-11.159281  -37.700512 ]
 [-10.307216   72.0878   ]
 [ 63.952087   21.496405 ]
 ...
 [ 29.102512   24.758902 ]
 [  8.487162    0.1083674]
 [  5.616735   -4.1200433]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_0/latent__1_0.svg
results (all): {'cluster_avg': -0.07941234, 'cluster_all': array([ 0.06674705,  0.24666059,  0.04093057, ..., -0.8197959 ,
       -0.41349497,  0.42395192], dtype=float32), 'magnitude_avg': 0.17138888162988544, 'magnitude_all': 0.17138888162988544, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 0, 'cluster_avg': -0.07941234, 'magnitude_avg': 0.17138888162988544, 'tsne-2d_avg': nan}


 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([3, 9, 7, 8, 3, 9, 9, 3, 3, 4, 1, 8, 2, 1, 9, 1, 8, 3, 6, 6, 3, 8, 1, 3,
        4, 3, 5, 1, 9, 4, 9, 2, 1, 3, 3, 5, 3, 1, 6, 4, 2, 8, 7, 5, 8, 5, 8, 8,
        7, 4, 3, 3, 3, 6, 6, 3, 2, 4, 3, 1, 4, 4, 8, 7, 2, 8, 3, 6, 4, 9, 6, 6,
        2, 8, 9, 4, 9, 7, 5, 5, 9, 4, 2, 8, 1, 7, 9, 7, 7, 6, 3, 1, 3, 9, 5, 7,
        7, 9, 9, 5, 7, 3, 9, 5, 5, 1, 9, 4, 6, 9, 6, 6, 6, 7, 7, 3, 6, 3, 6, 7,
        4, 2, 3, 3, 7, 4, 4, 7]), ['g3j', 'g9j', 'b7t', 'g8s', 'g3s', 'w9t', 'g9t', 'b3t', 'b3j', 'b4j', 'b1s', 'b8j', 'g2t', 'w1j', 'w9s', 'g1s', 'r8s', 'w3s', 'b6s', 'g6j', 'g3s', 'b8t', 'b1j', 'w3j', 'r4s', 'w3s', 'g5j', 'w1t', 'r9s', 'b4s', 'b9t', 'r2j', 'g1j', 'w3j', 'w3t', 'b5s', 'w3t', 'g1s', 'b6s', 'b4t', 'g2s', 'w8j', 'b7s', 'r5s', 'g8s', 'g5j', 'b8t', 'w8s', 'g7s', 'g4j', 'b3t', 'w3j', 'r3t', 'r6j', 'r6j', 'g3t', 'w2s', 'g4t', 'r3t', 'r1t', 'g4s', 'g4s', 'b8j', 'b7j', 'r2t', 'b8s', 'w3t', 'w6t', 'g4j', 'b9t', 'b6j', 'b6s', 'r2s', 'r8s', 'w9j', 'r4s', 'r9s', 'r7s', 'b5t', 'w5j', 'r9t', 'g4s', 'b2s', 'r8s', 'g1j', 'b7t', 'w9j', 'b7s', 'w7j', 'b6s', 'r3s', 'r1t', 'w3j', 'g9j', 'b5j', 'w7t', 'r7j', 'w9j', 'r9s', 'b5s', 'g7t', 'g3s', 'g9s', 'w5s', 'b5j', 'g1s', 'b9s', 'r4j', 'g6j', 'b9t', 'w6j', 'g6j', 'w6j', 'w7t', 'g7s', 'w3j', 'b6s', 'w3j', 'r6s', 'r7t', 'r4j', 'w2j', 'r3t', 'g3j', 'r7s', 'r4j', 'w4j', 'r7j']] 
label[0]: tensor([3, 9, 7, 8, 3, 9, 9, 3, 3, 4, 1, 8, 2, 1, 9, 1, 8, 3, 6, 6, 3, 8, 1, 3,
        4, 3, 5, 1, 9, 4, 9, 2, 1, 3, 3, 5, 3, 1, 6, 4, 2, 8, 7, 5, 8, 5, 8, 8,
        7, 4, 3, 3, 3, 6, 6, 3, 2, 4, 3, 1, 4, 4, 8, 7, 2, 8, 3, 6, 4, 9, 6, 6,
        2, 8, 9, 4, 9, 7, 5, 5, 9, 4, 2, 8, 1, 7, 9, 7, 7, 6, 3, 1, 3, 9, 5, 7,
        7, 9, 9, 5, 7, 3, 9, 5, 5, 1, 9, 4, 6, 9, 6, 6, 6, 7, 7, 3, 6, 3, 6, 7,
        4, 2, 3, 3, 7, 4, 4, 7]) 
label[1]: ['g3j', 'g9j', 'b7t', 'g8s', 'g3s', 'w9t', 'g9t', 'b3t', 'b3j', 'b4j', 'b1s', 'b8j', 'g2t', 'w1j', 'w9s', 'g1s', 'r8s', 'w3s', 'b6s', 'g6j', 'g3s', 'b8t', 'b1j', 'w3j', 'r4s', 'w3s', 'g5j', 'w1t', 'r9s', 'b4s', 'b9t', 'r2j', 'g1j', 'w3j', 'w3t', 'b5s', 'w3t', 'g1s', 'b6s', 'b4t', 'g2s', 'w8j', 'b7s', 'r5s', 'g8s', 'g5j', 'b8t', 'w8s', 'g7s', 'g4j', 'b3t', 'w3j', 'r3t', 'r6j', 'r6j', 'g3t', 'w2s', 'g4t', 'r3t', 'r1t', 'g4s', 'g4s', 'b8j', 'b7j', 'r2t', 'b8s', 'w3t', 'w6t', 'g4j', 'b9t', 'b6j', 'b6s', 'r2s', 'r8s', 'w9j', 'r4s', 'r9s', 'r7s', 'b5t', 'w5j', 'r9t', 'g4s', 'b2s', 'r8s', 'g1j', 'b7t', 'w9j', 'b7s', 'w7j', 'b6s', 'r3s', 'r1t', 'w3j', 'g9j', 'b5j', 'w7t', 'r7j', 'w9j', 'r9s', 'b5s', 'g7t', 'g3s', 'g9s', 'w5s', 'b5j', 'g1s', 'b9s', 'r4j', 'g6j', 'b9t', 'w6j', 'g6j', 'w6j', 'w7t', 'g7s', 'w3j', 'b6s', 'w3j', 'r6s', 'r7t', 'r4j', 'w2j', 'r3t', 'g3j', 'r7s', 'r4j', 'w4j', 'r7j'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 1 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_1 
Number of category 10 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {1, 2, 3, 4, 5, 6, 7, 8, 9} 
Start index: 1 
End index: 10 
withzero: False
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([1, 5, 7, 8, 2, 9, 2, 4, 6, 1, 9, 2, 1, 4, 8, 1, 4, 8, 9, 2, 2, 7, 3, 7,
        4, 3, 3, 4, 4, 6, 4, 9, 7, 8, 7, 2, 3, 2, 4, 4, 5, 6, 4, 3, 6, 1, 9, 8,
        3, 5, 4, 9, 7, 4, 4, 5, 7, 8, 2, 4, 8, 9, 7, 3, 1, 3, 5, 7, 5, 7, 4, 2,
        8, 3, 7, 5, 5, 1, 2, 1, 4, 5, 3, 7, 5, 7, 4, 8, 7, 8, 4, 7, 3, 4, 7, 1,
        1, 2, 1, 1, 2, 1, 5, 6, 9, 9, 4, 7, 6, 8, 3, 8, 8, 6, 1, 4, 2, 4, 7, 1,
        2, 6, 1, 3, 5, 2, 2, 4])
Accuracy (count): tensor(13) 
Accuracy (ratio) tensor(0.1016)
Accuracy:
 [[ 0  0 16  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0 13  0  0  0  0  0  0]
 [ 0  0 23  0  0  0  0  0  0]
 [ 0  0 12  0  0  0  0  0  0]
 [ 0  0  8  0  0  0  0  0  0]
 [ 0  0 18  0  0  0  0  0  0]
 [ 0  0 13  0  0  0  0  0  0]
 [ 0  0  9  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
size of extracted recon in MMVAE: <class 'list'>
size of extracted recon in MMVAE: torch.Size([1, 128, 3, 32, 32])
size of extracted recon in MMVAE: torch.Size([128, 3, 32, 32])
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([7, 2, 5, 8, 5, 5, 6, 2, 4, 1, 5, 5, 1, 6, 1, 5, 7, 1, 1, 9, 2, 5, 3, 9,
        9, 3, 3, 3, 5, 8, 9, 3, 4, 1, 2, 7, 1, 6, 9, 2, 7, 9, 8, 6, 4, 1, 2, 3,
        8, 9, 5, 3, 8, 4, 1, 6, 3, 7, 5, 8, 6, 4, 4, 5, 1, 6, 7, 3, 4, 2, 4, 3,
        4, 2, 1, 9, 1, 1, 4, 5, 1, 7, 1, 3, 3, 3, 4, 2, 9, 3, 2, 2, 9, 3, 3, 4,
        3, 7, 4, 5, 4, 2, 8, 5, 1, 2, 3, 8, 5, 8, 7, 2, 5, 9, 9, 2, 1, 7, 3, 8,
        3, 3, 9, 6, 9, 2, 1, 7])
Accuracy (count): tensor(21) 
Accuracy (ratio) tensor(0.1641)
Accuracy:
 [[ 0  0 18  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0 21  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]
 [ 0  0 16  0  0  0  0  0  0]
 [ 0  0  8  0  0  0  0  0  0]
 [ 0  0 11  0  0  0  0  0  0]
 [ 0  0 10  0  0  0  0  0  0]
 [ 0  0 14  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
Silhouette values: [-0.76288617 -0.8596272  -0.11763114 ...  0.02038364 -0.8596272
  0.04256164]
Average of silhouette coef: -0.21226405
---
  1   2   3   4   5   6   7   8   9
1 0.0 8.9 17.0 27.9 36.4 42.2 49.6 58.0 71.0 
2 8.9 0.0 8.1 19.0 27.5 33.3 40.7 49.1 62.1 
3 17.0 8.1 0.0 10.9 19.4 25.2 32.6 41.0 54.0 
4 27.9 19.0 10.9 0.0 8.5 14.3 21.7 30.1 43.1 
5 36.4 27.5 19.4 8.5 0.0 5.8 13.2 21.6 34.6 
6 42.2 33.3 25.2 14.3 5.8 0.0 7.4 15.8 28.8 
7 49.6 40.7 32.6 21.7 13.2 7.4 0.0 8.4 21.4 
8 58.0 49.1 41.0 30.1 21.6 15.8 8.4 0.0 12.9 
9 71.0 62.1 54.0 43.1 34.6 28.8 21.4 12.9 0.0 
correlation [[1.         0.99043851]
 [0.99043851 1.        ]]
---
[[], [], [], [], [], [], [], [], [], []] [3 9 7 ... 6 9 6] [[-25.621983  55.569645]
 [ 26.065733  39.918995]
 [ 13.289209 -24.963081]
 ...
 [ 26.322077 -14.066981]
 [ 26.292185  39.821384]
 [ 32.77733   -5.112108]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_1/latent__1_1.svg
---
length of elements: 489 489
length of 1: 45
length of 2: 52
length of 3: 57
length of 4: 60
length of 5: 61
length of 6: 60
length of 7: 57
length of 8: 52
length of 9: 45
all elements:

[(1, (1, 1, 1)),
 (2, (1, 2, 1)),
 (1, (1, 2, 2)),
 (3, (1, 3, 1)),
 (2, (1, 3, 2)),
 (1, (1, 3, 3)),
 (4, (1, 4, 1)),
 (3, (1, 4, 2)),
 (2, (1, 4, 3)),
 (1, (1, 4, 4)),
 (5, (1, 5, 1)),
 (4, (1, 5, 2)),
 (3, (1, 5, 3)),
 (2, (1, 5, 4)),
 (1, (1, 5, 5)),
 (6, (1, 6, 1)),
 (5, (1, 6, 2)),
 (4, (1, 6, 3)),
 (3, (1, 6, 4)),
 (2, (1, 6, 5)),
 (1, (1, 6, 6)),
 (7, (1, 7, 1)),
 (6, (1, 7, 2)),
 (5, (1, 7, 3)),
 (4, (1, 7, 4)),
 (3, (1, 7, 5)),
 (2, (1, 7, 6)),
 (1, (1, 7, 7)),
 (8, (1, 8, 1)),
 (7, (1, 8, 2)),
 (6, (1, 8, 3)),
 (5, (1, 8, 4)),
 (4, (1, 8, 5)),
 (3, (1, 8, 6)),
 (2, (1, 8, 7)),
 (1, (1, 8, 8)),
 (9, (1, 9, 1)),
 (8, (1, 9, 2)),
 (7, (1, 9, 3)),
 (6, (1, 9, 4)),
 (5, (1, 9, 5)),
 (4, (1, 9, 6)),
 (3, (1, 9, 7)),
 (2, (1, 9, 8)),
 (1, (1, 9, 9)),
 (2, (2, 1, 1)),
 (1, (2, 1, 2)),
 (3, (2, 2, 1)),
 (2, (2, 2, 2)),
 (1, (2, 2, 3)),
 (4, (2, 3, 1)),
 (3, (2, 3, 2)),
 (2, (2, 3, 3)),
 (1, (2, 3, 4)),
 (5, (2, 4, 1)),
 (4, (2, 4, 2)),
 (3, (2, 4, 3)),
 (2, (2, 4, 4)),
 (1, (2, 4, 5)),
 (6, (2, 5, 1)),
 (5, (2, 5, 2)),
 (4, (2, 5, 3)),
 (3, (2, 5, 4)),
 (2, (2, 5, 5)),
 (1, (2, 5, 6)),
 (7, (2, 6, 1)),
 (6, (2, 6, 2)),
 (5, (2, 6, 3)),
 (4, (2, 6, 4)),
 (3, (2, 6, 5)),
 (2, (2, 6, 6)),
 (1, (2, 6, 7)),
 (8, (2, 7, 1)),
 (7, (2, 7, 2)),
 (6, (2, 7, 3)),
 (5, (2, 7, 4)),
 (4, (2, 7, 5)),
 (3, (2, 7, 6)),
 (2, (2, 7, 7)),
 (1, (2, 7, 8)),
 (9, (2, 8, 1)),
 (8, (2, 8, 2)),
 (7, (2, 8, 3)),
 (6, (2, 8, 4)),
 (5, (2, 8, 5)),
 (4, (2, 8, 6)),
 (3, (2, 8, 7)),
 (2, (2, 8, 8)),
 (1, (2, 8, 9)),
 (9, (2, 9, 2)),
 (8, (2, 9, 3)),
 (7, (2, 9, 4)),
 (6, (2, 9, 5)),
 (5, (2, 9, 6)),
 (4, (2, 9, 7)),
 (3, (2, 9, 8)),
 (2, (2, 9, 9)),
 (3, (3, 1, 1)),
 (2, (3, 1, 2)),
 (1, (3, 1, 3)),
 (4, (3, 2, 1)),
 (3, (3, 2, 2)),
 (2, (3, 2, 3)),
 (1, (3, 2, 4)),
 (5, (3, 3, 1)),
 (4, (3, 3, 2)),
 (3, (3, 3, 3)),
 (2, (3, 3, 4)),
 (1, (3, 3, 5)),
 (6, (3, 4, 1)),
 (5, (3, 4, 2)),
 (4, (3, 4, 3)),
 (3, (3, 4, 4)),
 (2, (3, 4, 5)),
 (1, (3, 4, 6)),
 (7, (3, 5, 1)),
 (6, (3, 5, 2)),
 (5, (3, 5, 3)),
 (4, (3, 5, 4)),
 (3, (3, 5, 5)),
 (2, (3, 5, 6)),
 (1, (3, 5, 7)),
 (8, (3, 6, 1)),
 (7, (3, 6, 2)),
 (6, (3, 6, 3)),
 (5, (3, 6, 4)),
 (4, (3, 6, 5)),
 (3, (3, 6, 6)),
 (2, (3, 6, 7)),
 (1, (3, 6, 8)),
 (9, (3, 7, 1)),
 (8, (3, 7, 2)),
 (7, (3, 7, 3)),
 (6, (3, 7, 4)),
 (5, (3, 7, 5)),
 (4, (3, 7, 6)),
 (3, (3, 7, 7)),
 (2, (3, 7, 8)),
 (1, (3, 7, 9)),
 (9, (3, 8, 2)),
 (8, (3, 8, 3)),
 (7, (3, 8, 4)),
 (6, (3, 8, 5)),
 (5, (3, 8, 6)),
 (4, (3, 8, 7)),
 (3, (3, 8, 8)),
 (2, (3, 8, 9)),
 (9, (3, 9, 3)),
 (8, (3, 9, 4)),
 (7, (3, 9, 5)),
 (6, (3, 9, 6)),
 (5, (3, 9, 7)),
 (4, (3, 9, 8)),
 (3, (3, 9, 9)),
 (4, (4, 1, 1)),
 (3, (4, 1, 2)),
 (2, (4, 1, 3)),
 (1, (4, 1, 4)),
 (5, (4, 2, 1)),
 (4, (4, 2, 2)),
 (3, (4, 2, 3)),
 (2, (4, 2, 4)),
 (1, (4, 2, 5)),
 (6, (4, 3, 1)),
 (5, (4, 3, 2)),
 (4, (4, 3, 3)),
 (3, (4, 3, 4)),
 (2, (4, 3, 5)),
 (1, (4, 3, 6)),
 (7, (4, 4, 1)),
 (6, (4, 4, 2)),
 (5, (4, 4, 3)),
 (4, (4, 4, 4)),
 (3, (4, 4, 5)),
 (2, (4, 4, 6)),
 (1, (4, 4, 7)),
 (8, (4, 5, 1)),
 (7, (4, 5, 2)),
 (6, (4, 5, 3)),
 (5, (4, 5, 4)),
 (4, (4, 5, 5)),
 (3, (4, 5, 6)),
 (2, (4, 5, 7)),
 (1, (4, 5, 8)),
 (9, (4, 6, 1)),
 (8, (4, 6, 2)),
 (7, (4, 6, 3)),
 (6, (4, 6, 4)),
 (5, (4, 6, 5)),
 (4, (4, 6, 6)),
 (3, (4, 6, 7)),
 (2, (4, 6, 8)),
 (1, (4, 6, 9)),
 (9, (4, 7, 2)),
 (8, (4, 7, 3)),
 (7, (4, 7, 4)),
 (6, (4, 7, 5)),
 (5, (4, 7, 6)),
 (4, (4, 7, 7)),
 (3, (4, 7, 8)),
 (2, (4, 7, 9)),
 (9, (4, 8, 3)),
 (8, (4, 8, 4)),
 (7, (4, 8, 5)),
 (6, (4, 8, 6)),
 (5, (4, 8, 7)),
 (4, (4, 8, 8)),
 (3, (4, 8, 9)),
 (9, (4, 9, 4)),
 (8, (4, 9, 5)),
 (7, (4, 9, 6)),
 (6, (4, 9, 7)),
 (5, (4, 9, 8)),
 (4, (4, 9, 9)),
 (5, (5, 1, 1)),
 (4, (5, 1, 2)),
 (3, (5, 1, 3)),
 (2, (5, 1, 4)),
 (1, (5, 1, 5)),
 (6, (5, 2, 1)),
 (5, (5, 2, 2)),
 (4, (5, 2, 3)),
 (3, (5, 2, 4)),
 (2, (5, 2, 5)),
 (1, (5, 2, 6)),
 (7, (5, 3, 1)),
 (6, (5, 3, 2)),
 (5, (5, 3, 3)),
 (4, (5, 3, 4)),
 (3, (5, 3, 5)),
 (2, (5, 3, 6)),
 (1, (5, 3, 7)),
 (8, (5, 4, 1)),
 (7, (5, 4, 2)),
 (6, (5, 4, 3)),
 (5, (5, 4, 4)),
 (4, (5, 4, 5)),
 (3, (5, 4, 6)),
 (2, (5, 4, 7)),
 (1, (5, 4, 8)),
 (9, (5, 5, 1)),
 (8, (5, 5, 2)),
 (7, (5, 5, 3)),
 (6, (5, 5, 4)),
 (5, (5, 5, 5)),
 (4, (5, 5, 6)),
 (3, (5, 5, 7)),
 (2, (5, 5, 8)),
 (1, (5, 5, 9)),
 (9, (5, 6, 2)),
 (8, (5, 6, 3)),
 (7, (5, 6, 4)),
 (6, (5, 6, 5)),
 (5, (5, 6, 6)),
 (4, (5, 6, 7)),
 (3, (5, 6, 8)),
 (2, (5, 6, 9)),
 (9, (5, 7, 3)),
 (8, (5, 7, 4)),
 (7, (5, 7, 5)),
 (6, (5, 7, 6)),
 (5, (5, 7, 7)),
 (4, (5, 7, 8)),
 (3, (5, 7, 9)),
 (9, (5, 8, 4)),
 (8, (5, 8, 5)),
 (7, (5, 8, 6)),
 (6, (5, 8, 7)),
 (5, (5, 8, 8)),
 (4, (5, 8, 9)),
 (9, (5, 9, 5)),
 (8, (5, 9, 6)),
 (7, (5, 9, 7)),
 (6, (5, 9, 8)),
 (5, (5, 9, 9)),
 (6, (6, 1, 1)),
 (5, (6, 1, 2)),
 (4, (6, 1, 3)),
 (3, (6, 1, 4)),
 (2, (6, 1, 5)),
 (1, (6, 1, 6)),
 (7, (6, 2, 1)),
 (6, (6, 2, 2)),
 (5, (6, 2, 3)),
 (4, (6, 2, 4)),
 (3, (6, 2, 5)),
 (2, (6, 2, 6)),
 (1, (6, 2, 7)),
 (8, (6, 3, 1)),
 (7, (6, 3, 2)),
 (6, (6, 3, 3)),
 (5, (6, 3, 4)),
 (4, (6, 3, 5)),
 (3, (6, 3, 6)),
 (2, (6, 3, 7)),
 (1, (6, 3, 8)),
 (9, (6, 4, 1)),
 (8, (6, 4, 2)),
 (7, (6, 4, 3)),
 (6, (6, 4, 4)),
 (5, (6, 4, 5)),
 (4, (6, 4, 6)),
 (3, (6, 4, 7)),
 (2, (6, 4, 8)),
 (1, (6, 4, 9)),
 (9, (6, 5, 2)),
 (8, (6, 5, 3)),
 (7, (6, 5, 4)),
 (6, (6, 5, 5)),
 (5, (6, 5, 6)),
 (4, (6, 5, 7)),
 (3, (6, 5, 8)),
 (2, (6, 5, 9)),
 (9, (6, 6, 3)),
 (8, (6, 6, 4)),
 (7, (6, 6, 5)),
 (6, (6, 6, 6)),
 (5, (6, 6, 7)),
 (4, (6, 6, 8)),
 (3, (6, 6, 9)),
 (9, (6, 7, 4)),
 (8, (6, 7, 5)),
 (7, (6, 7, 6)),
 (6, (6, 7, 7)),
 (5, (6, 7, 8)),
 (4, (6, 7, 9)),
 (9, (6, 8, 5)),
 (8, (6, 8, 6)),
 (7, (6, 8, 7)),
 (6, (6, 8, 8)),
 (5, (6, 8, 9)),
 (9, (6, 9, 6)),
 (8, (6, 9, 7)),
 (7, (6, 9, 8)),
 (6, (6, 9, 9)),
 (7, (7, 1, 1)),
 (6, (7, 1, 2)),
 (5, (7, 1, 3)),
 (4, (7, 1, 4)),
 (3, (7, 1, 5)),
 (2, (7, 1, 6)),
 (1, (7, 1, 7)),
 (8, (7, 2, 1)),
 (7, (7, 2, 2)),
 (6, (7, 2, 3)),
 (5, (7, 2, 4)),
 (4, (7, 2, 5)),
 (3, (7, 2, 6)),
 (2, (7, 2, 7)),
 (1, (7, 2, 8)),
 (9, (7, 3, 1)),
 (8, (7, 3, 2)),
 (7, (7, 3, 3)),
 (6, (7, 3, 4)),
 (5, (7, 3, 5)),
 (4, (7, 3, 6)),
 (3, (7, 3, 7)),
 (2, (7, 3, 8)),
 (1, (7, 3, 9)),
 (9, (7, 4, 2)),
 (8, (7, 4, 3)),
 (7, (7, 4, 4)),
 (6, (7, 4, 5)),
 (5, (7, 4, 6)),
 (4, (7, 4, 7)),
 (3, (7, 4, 8)),
 (2, (7, 4, 9)),
 (9, (7, 5, 3)),
 (8, (7, 5, 4)),
 (7, (7, 5, 5)),
 (6, (7, 5, 6)),
 (5, (7, 5, 7)),
 (4, (7, 5, 8)),
 (3, (7, 5, 9)),
 (9, (7, 6, 4)),
 (8, (7, 6, 5)),
 (7, (7, 6, 6)),
 (6, (7, 6, 7)),
 (5, (7, 6, 8)),
 (4, (7, 6, 9)),
 (9, (7, 7, 5)),
 (8, (7, 7, 6)),
 (7, (7, 7, 7)),
 (6, (7, 7, 8)),
 (5, (7, 7, 9)),
 (9, (7, 8, 6)),
 (8, (7, 8, 7)),
 (7, (7, 8, 8)),
 (6, (7, 8, 9)),
 (9, (7, 9, 7)),
 (8, (7, 9, 8)),
 (7, (7, 9, 9)),
 (8, (8, 1, 1)),
 (7, (8, 1, 2)),
 (6, (8, 1, 3)),
 (5, (8, 1, 4)),
 (4, (8, 1, 5)),
 (3, (8, 1, 6)),
 (2, (8, 1, 7)),
 (1, (8, 1, 8)),
 (9, (8, 2, 1)),
 (8, (8, 2, 2)),
 (7, (8, 2, 3)),
 (6, (8, 2, 4)),
 (5, (8, 2, 5)),
 (4, (8, 2, 6)),
 (3, (8, 2, 7)),
 (2, (8, 2, 8)),
 (1, (8, 2, 9)),
 (9, (8, 3, 2)),
 (8, (8, 3, 3)),
 (7, (8, 3, 4)),
 (6, (8, 3, 5)),
 (5, (8, 3, 6)),
 (4, (8, 3, 7)),
 (3, (8, 3, 8)),
 (2, (8, 3, 9)),
 (9, (8, 4, 3)),
 (8, (8, 4, 4)),
 (7, (8, 4, 5)),
 (6, (8, 4, 6)),
 (5, (8, 4, 7)),
 (4, (8, 4, 8)),
 (3, (8, 4, 9)),
 (9, (8, 5, 4)),
 (8, (8, 5, 5)),
 (7, (8, 5, 6)),
 (6, (8, 5, 7)),
 (5, (8, 5, 8)),
 (4, (8, 5, 9)),
 (9, (8, 6, 5)),
 (8, (8, 6, 6)),
 (7, (8, 6, 7)),
 (6, (8, 6, 8)),
 (5, (8, 6, 9)),
 (9, (8, 7, 6)),
 (8, (8, 7, 7)),
 (7, (8, 7, 8)),
 (6, (8, 7, 9)),
 (9, (8, 8, 7)),
 (8, (8, 8, 8)),
 (7, (8, 8, 9)),
 (9, (8, 9, 8)),
 (8, (8, 9, 9)),
 (9, (9, 1, 1)),
 (8, (9, 1, 2)),
 (7, (9, 1, 3)),
 (6, (9, 1, 4)),
 (5, (9, 1, 5)),
 (4, (9, 1, 6)),
 (3, (9, 1, 7)),
 (2, (9, 1, 8)),
 (1, (9, 1, 9)),
 (9, (9, 2, 2)),
 (8, (9, 2, 3)),
 (7, (9, 2, 4)),
 (6, (9, 2, 5)),
 (5, (9, 2, 6)),
 (4, (9, 2, 7)),
 (3, (9, 2, 8)),
 (2, (9, 2, 9)),
 (9, (9, 3, 3)),
 (8, (9, 3, 4)),
 (7, (9, 3, 5)),
 (6, (9, 3, 6)),
 (5, (9, 3, 7)),
 (4, (9, 3, 8)),
 (3, (9, 3, 9)),
 (9, (9, 4, 4)),
 (8, (9, 4, 5)),
 (7, (9, 4, 6)),
 (6, (9, 4, 7)),
 (5, (9, 4, 8)),
 (4, (9, 4, 9)),
 (9, (9, 5, 5)),
 (8, (9, 5, 6)),
 (7, (9, 5, 7)),
 (6, (9, 5, 8)),
 (5, (9, 5, 9)),
 (9, (9, 6, 6)),
 (8, (9, 6, 7)),
 (7, (9, 6, 8)),
 (6, (9, 6, 9)),
 (9, (9, 7, 7)),
 (8, (9, 7, 8)),
 (7, (9, 7, 9)),
 (9, (9, 8, 8)),
 (8, (9, 8, 9)),
 (9, (9, 9, 9))]
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([1, 2, 1, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5,
        4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 1, 3,
        2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 3, 2, 1, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3,
        2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5,
        4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 4, 3, 2, 1, 5, 4, 3, 2, 1, 6, 5, 4, 3, 2,
        1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 5, 4,
        3, 2, 1, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1,
        9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3,
        9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1,
        8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3,
        2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9, 8, 7, 6, 7,
        6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9,
        8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6,
        5, 9, 8, 7, 6, 9, 8, 7, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7, 6, 5, 4, 3, 2,
        1, 9, 8, 7, 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8,
        7, 6, 5, 9, 8, 7, 6, 9, 8, 7, 9, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 9, 8, 7,
        6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 9, 8, 7, 6, 5, 4, 9, 8, 7, 6, 5, 9,
        8, 7, 6, 9, 8, 7, 9, 8, 9])
Accuracy (count): tensor(57) 
Accuracy (ratio) tensor(0.1166)
Accuracy:
 [[ 0  0 45  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 61  0  0  0  0  0  0]
 [ 0  0 60  0  0  0  0  0  0]
 [ 0  0 57  0  0  0  0  0  0]
 [ 0  0 52  0  0  0  0  0  0]
 [ 0  0 45  0  0  0  0  0  0]]
Accuracy:
 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]]
---
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
shape (20,)
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 12.4  14.0  1.2  8.9  3.4 -0.0 -3.1 -3.5  14.8 -0.6  16.6  15.9  11.7 -1.7  15.0 -22.4  6.2  7.9  14.4 -3.7 
 14.7  16.5  1.4  10.5  4.0 -0.1 -3.8 -4.1  17.4 -0.8  19.7  18.8  13.8 -2.0  17.7 -26.4  7.2  9.3  17.1 -4.4 
 16.7  18.8  1.6  11.9  4.6 -0.1 -4.3 -4.7  19.9 -0.9  22.4  21.5  15.7 -2.3  20.2 -30.1  8.2  10.6  19.5 -5.0 
 19.5  21.9  1.8  13.9  5.4 -0.0 -5.0 -5.5  23.2 -1.1  26.1  25.0  18.4 -2.7  23.6 -35.1  9.7  12.3  22.7 -5.9 
 21.6  24.3  2.1  15.4  6.0 -0.0 -5.6 -6.1  25.7 -1.2  29.0  27.8  20.4 -3.0  26.2 -39.0  10.7  13.7  25.2 -6.5 
 23.1  25.9  2.2  16.5  6.4 -0.1 -6.0 -6.5  27.5 -1.3  31.0  29.7  21.8 -3.1  28.0 -41.7  11.4  14.6  26.9 -6.9 
 25.0  28.1  2.3  17.9  6.9 -0.1 -6.4 -7.0  29.7 -1.3  33.5  32.1  23.6 -3.4  30.3 -45.1  12.4  15.8  29.0 -7.4 
 27.1  30.4  2.6  19.4  7.5  0.0 -7.0 -7.6  32.2 -1.5  36.4  34.8  25.6 -3.7  32.8 -48.9  13.4  17.2  31.6 -8.2 
 30.3  34.1  2.8  21.8  8.4 -0.0 -7.8 -8.5  36.1 -1.7  40.8  39.0  28.7 -4.1  36.8 -54.8  15.1  19.2  35.4 -9.1 
Minimum distance of calculation. 
true answer: 2 
indices: 2 
distance: 4.063436
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 12.4  14.0  1.2  8.9  3.4 -0.0 -3.1 -3.5  14.8 -0.6  16.6  15.9  11.7 -1.7  15.0 -22.4  6.2  7.9  14.4 -3.7 
 14.7  16.5  1.4  10.5  4.0 -0.1 -3.8 -4.1  17.4 -0.8  19.7  18.8  13.8 -2.0  17.7 -26.4  7.2  9.3  17.1 -4.4 
 16.7  18.8  1.6  11.9  4.6 -0.1 -4.3 -4.7  19.9 -0.9  22.4  21.5  15.7 -2.3  20.2 -30.1  8.2  10.6  19.5 -5.0 
 19.5  21.9  1.8  13.9  5.4 -0.0 -5.0 -5.5  23.2 -1.1  26.1  25.0  18.4 -2.7  23.6 -35.1  9.7  12.3  22.7 -5.9 
 21.6  24.3  2.1  15.4  6.0 -0.0 -5.6 -6.1  25.7 -1.2  29.0  27.8  20.4 -3.0  26.2 -39.0  10.7  13.7  25.2 -6.5 
 23.1  25.9  2.2  16.5  6.4 -0.1 -6.0 -6.5  27.5 -1.3  31.0  29.7  21.8 -3.1  28.0 -41.7  11.4  14.6  26.9 -6.9 
 25.0  28.1  2.3  17.9  6.9 -0.1 -6.4 -7.0  29.7 -1.3  33.5  32.1  23.6 -3.4  30.3 -45.1  12.4  15.8  29.0 -7.4 
 27.1  30.4  2.6  19.4  7.5  0.0 -7.0 -7.6  32.2 -1.5  36.4  34.8  25.6 -3.7  32.8 -48.9  13.4  17.2  31.6 -8.2 
 30.3  34.1  2.8  21.8  8.4 -0.0 -7.8 -8.5  36.1 -1.7  40.8  39.0  28.7 -4.1  36.8 -54.8  15.1  19.2  35.4 -9.1 
Minimum distance of calculation. 
true answer: 5 
indices: 4 
distance: 2.2057703
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 12.4  14.0  1.2  8.9  3.4 -0.0 -3.1 -3.5  14.8 -0.6  16.6  15.9  11.7 -1.7  15.0 -22.4  6.2  7.9  14.4 -3.7 
 14.7  16.5  1.4  10.5  4.0 -0.1 -3.8 -4.1  17.4 -0.8  19.7  18.8  13.8 -2.0  17.7 -26.4  7.2  9.3  17.1 -4.4 
 16.7  18.8  1.6  11.9  4.6 -0.1 -4.3 -4.7  19.9 -0.9  22.4  21.5  15.7 -2.3  20.2 -30.1  8.2  10.6  19.5 -5.0 
 19.5  21.9  1.8  13.9  5.4 -0.0 -5.0 -5.5  23.2 -1.1  26.1  25.0  18.4 -2.7  23.6 -35.1  9.7  12.3  22.7 -5.9 
 21.6  24.3  2.1  15.4  6.0 -0.0 -5.6 -6.1  25.7 -1.2  29.0  27.8  20.4 -3.0  26.2 -39.0  10.7  13.7  25.2 -6.5 
 23.1  25.9  2.2  16.5  6.4 -0.1 -6.0 -6.5  27.5 -1.3  31.0  29.7  21.8 -3.1  28.0 -41.7  11.4  14.6  26.9 -6.9 
 25.0  28.1  2.3  17.9  6.9 -0.1 -6.4 -7.0  29.7 -1.3  33.5  32.1  23.6 -3.4  30.3 -45.1  12.4  15.8  29.0 -7.4 
 27.1  30.4  2.6  19.4  7.5  0.0 -7.0 -7.6  32.2 -1.5  36.4  34.8  25.6 -3.7  32.8 -48.9  13.4  17.2  31.6 -8.2 
 30.3  34.1  2.8  21.8  8.4 -0.0 -7.8 -8.5  36.1 -1.7  40.8  39.0  28.7 -4.1  36.8 -54.8  15.1  19.2  35.4 -9.1 
Minimum distance of calculation. 
true answer: 8 
indices: 8 
distance: 0.51798654
pred: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) 
tar: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])
Accuracy (count): tensor(0) 
Accuracy (ratio) tensor(0.)
Accuracy:
 [[0]]
Accuracy:
 [[nan]]
 12.4  14.0  1.2  8.9  3.4 -0.0 -3.1 -3.5  14.8 -0.6  16.6  15.9  11.7 -1.7  15.0 -22.4  6.2  7.9  14.4 -3.7 
 14.7  16.5  1.4  10.5  4.0 -0.1 -3.8 -4.1  17.4 -0.8  19.7  18.8  13.8 -2.0  17.7 -26.4  7.2  9.3  17.1 -4.4 
 16.7  18.8  1.6  11.9  4.6 -0.1 -4.3 -4.7  19.9 -0.9  22.4  21.5  15.7 -2.3  20.2 -30.1  8.2  10.6  19.5 -5.0 
 19.5  21.9  1.8  13.9  5.4 -0.0 -5.0 -5.5  23.2 -1.1  26.1  25.0  18.4 -2.7  23.6 -35.1  9.7  12.3  22.7 -5.9 
 21.6  24.3  2.1  15.4  6.0 -0.0 -5.6 -6.1  25.7 -1.2  29.0  27.8  20.4 -3.0  26.2 -39.0  10.7  13.7  25.2 -6.5 
 23.1  25.9  2.2  16.5  6.4 -0.1 -6.0 -6.5  27.5 -1.3  31.0  29.7  21.8 -3.1  28.0 -41.7  11.4  14.6  26.9 -6.9 
 25.0  28.1  2.3  17.9  6.9 -0.1 -6.4 -7.0  29.7 -1.3  33.5  32.1  23.6 -3.4  30.3 -45.1  12.4  15.8  29.0 -7.4 
 27.1  30.4  2.6  19.4  7.5  0.0 -7.0 -7.6  32.2 -1.5  36.4  34.8  25.6 -3.7  32.8 -48.9  13.4  17.2  31.6 -8.2 
 30.3  34.1  2.8  21.8  8.4 -0.0 -7.8 -8.5  36.1 -1.7  40.8  39.0  28.7 -4.1  36.8 -54.8  15.1  19.2  35.4 -9.1 
Minimum distance of calculation. 
true answer: 6 
indices: 6 
distance: 2.336007
results (all): {'reconst_1x1_avg': 0.1015625, 'reconst_1x1_all': nan, 'cross_0x1_avg': 0.1640625, 'cross_0x1_all': nan, 'cluster_avg': -0.21226405, 'cluster_all': array([-0.76288617, -0.8596272 , -0.11763114, ...,  0.02038364,
       -0.8596272 ,  0.04256164], dtype=float32), 'magnitude_avg': 0.9904385139326073, 'magnitude_all': 0.9904385139326073, 'tsne-2d_avg': nan, 'tsne-2d_all': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_all': nan, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+9-8_all': nan, 'mathematics_1+8-4_avg': 0.0, 'mathematics_1+8-4_all': nan, 'mathematics_2+7-1_avg': 0.0, 'mathematics_2+7-1_all': nan, 'mathematics_3+5-2_avg': 0.0, 'mathematics_3+5-2_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 1, 'reconst_1x1_avg': 0.1015625, 'cross_0x1_avg': 0.1640625, 'cluster_avg': -0.21226405, 'magnitude_avg': 0.9904385139326073, 'tsne-2d_avg': nan, 'mathematics_avg': 0.11656441539525986, 'mathematics_1+9-8_avg': 0.0, 'mathematics_1+8-4_avg': 0.0, 'mathematics_2+7-1_avg': 0.0, 'mathematics_3+5-2_avg': 0.0}/new_nas/taka/MMVAE/NumberSenseAndMultimodalLearning/src/analyse.py:86: RuntimeWarning: invalid value encountered in true_divide
  conf_mat_nrm = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]



 ==============
 START ANALYSIS
 ==============


===
Data and label info... 
type(label): <class 'list'> 
label: [tensor([8, 8, 7, 5, 4, 7, 5, 1, 9, 2, 1, 3, 5, 5, 7, 2, 4, 2, 1, 8, 7, 1, 6, 4,
        9, 7, 1, 8, 7, 3, 3, 6, 8, 3, 6, 5, 2, 7, 8, 2, 4, 7, 8, 8, 8, 9, 6, 2,
        3, 4, 6, 9, 8, 2, 8, 3, 8, 7, 2, 4, 5, 4, 1, 1, 6, 9, 8, 6, 9, 4, 6, 4,
        1, 6, 9, 7, 4, 2, 7, 5, 4, 5, 9, 1, 6, 5, 2, 4, 2, 6, 3, 8, 1, 8, 3, 3,
        3, 8, 2, 8, 4, 1, 5, 9, 9, 8, 6, 3, 7, 2, 5, 9, 2, 1, 4, 3, 3, 7, 4, 1,
        2, 5, 1, 9, 7, 9, 2, 8]), ['w8t', 'g8j', 'w7j', 'r5t', 'b4j', 'g7j', 'w5j', 'w1s', 'g9j', 'w2t', 'g1t', 'g3s', 'w5t', 'g5t', 'w7s', 'w2t', 'g4j', 'r2s', 'r1j', 'b8s', 'w7s', 'g1t', 'b6j', 'r4s', 'g9j', 'r7t', 'b1s', 'w8s', 'b7s', 'g3t', 'r3t', 'r6t', 'g8j', 'w3j', 'b6s', 'g5s', 'g2t', 'w7t', 'b8s', 'g2j', 'g4s', 'g7s', 'r8t', 'w8j', 'b8j', 'b9t', 'b6s', 'w2j', 'w3t', 'b4s', 'g6s', 'b9t', 'r8j', 'b2s', 'r8t', 'g3t', 'g8j', 'b7t', 'w2s', 'r4s', 'w5t', 'r4s', 'w1j', 'r1j', 'g6s', 'r9t', 'w8t', 'r6j', 'r9t', 'w4t', 'g6t', 'b4j', 'r1t', 'b6j', 'w9s', 'r7s', 'b4t', 'w2j', 'g7s', 'b5s', 'b4j', 'b5j', 'g9j', 'w1j', 'r6j', 'b5j', 'b2j', 'g4s', 'b2t', 'g6j', 'b3j', 'w8j', 'w1j', 'g8t', 'b3s', 'w3t', 'w3s', 'w8t', 'b2j', 'r8t', 'g4s', 'g1s', 'w5t', 'r9s', 'b9j', 'w8j', 'r6j', 'g3s', 'g7s', 'b2t', 'w5j', 'r9j', 'b2t', 'g1j', 'g4t', 'g3s', 'g3s', 'b7t', 'w4j', 'b1j', 'b2j', 'w5j', 'g1s', 'w9s', 'b7s', 'g9t', 'w2t', 'w8s']] 
label[0]: tensor([8, 8, 7, 5, 4, 7, 5, 1, 9, 2, 1, 3, 5, 5, 7, 2, 4, 2, 1, 8, 7, 1, 6, 4,
        9, 7, 1, 8, 7, 3, 3, 6, 8, 3, 6, 5, 2, 7, 8, 2, 4, 7, 8, 8, 8, 9, 6, 2,
        3, 4, 6, 9, 8, 2, 8, 3, 8, 7, 2, 4, 5, 4, 1, 1, 6, 9, 8, 6, 9, 4, 6, 4,
        1, 6, 9, 7, 4, 2, 7, 5, 4, 5, 9, 1, 6, 5, 2, 4, 2, 6, 3, 8, 1, 8, 3, 3,
        3, 8, 2, 8, 4, 1, 5, 9, 9, 8, 6, 3, 7, 2, 5, 9, 2, 1, 4, 3, 3, 7, 4, 1,
        2, 5, 1, 9, 7, 9, 2, 8]) 
label[1]: ['w8t', 'g8j', 'w7j', 'r5t', 'b4j', 'g7j', 'w5j', 'w1s', 'g9j', 'w2t', 'g1t', 'g3s', 'w5t', 'g5t', 'w7s', 'w2t', 'g4j', 'r2s', 'r1j', 'b8s', 'w7s', 'g1t', 'b6j', 'r4s', 'g9j', 'r7t', 'b1s', 'w8s', 'b7s', 'g3t', 'r3t', 'r6t', 'g8j', 'w3j', 'b6s', 'g5s', 'g2t', 'w7t', 'b8s', 'g2j', 'g4s', 'g7s', 'r8t', 'w8j', 'b8j', 'b9t', 'b6s', 'w2j', 'w3t', 'b4s', 'g6s', 'b9t', 'r8j', 'b2s', 'r8t', 'g3t', 'g8j', 'b7t', 'w2s', 'r4s', 'w5t', 'r4s', 'w1j', 'r1j', 'g6s', 'r9t', 'w8t', 'r6j', 'r9t', 'w4t', 'g6t', 'b4j', 'r1t', 'b6j', 'w9s', 'r7s', 'b4t', 'w2j', 'g7s', 'b5s', 'b4j', 'b5j', 'g9j', 'w1j', 'r6j', 'b5j', 'b2j', 'g4s', 'b2t', 'g6j', 'b3j', 'w8j', 'w1j', 'g8t', 'b3s', 'w3t', 'w3s', 'w8t', 'b2j', 'r8t', 'g4s', 'g1s', 'w5t', 'r9s', 'b9j', 'w8j', 'r6j', 'g3s', 'g7s', 'b2t', 'w5j', 'r9j', 'b2t', 'g1j', 'g4t', 'g3s', 'g3s', 'b7t', 'w4j', 'b1j', 'b2j', 'w5j', 'g1s', 'w9s', 'b7s', 'g9t', 'w2t', 'w8s'] 
type(data): <class 'list'> 
data[0]: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0') 
data[0].shape: torch.Size([128, 3, 28, 28])

===
Setting info... 
Target modality: 1 
Target property: 2 
Output directory: ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_2 
Number of category 3 
Shape of latent_all: (2048, 20) 
Shape of label_all:  (2048,) 
Set of label_all:  {0, 1, 2} 
Start index: 0 
End index: 3 
withzero: False
---
Silhouette values: [-0.1353142   0.50626385 -0.13581802 ...  0.10758652  0.13228555
 -0.0467803 ]
Average of silhouette coef: -0.057688355
---
  0   1   2
0 0.0 35.7 18.2 
1 35.7 0.0 17.5 
2 18.2 17.5 0.0 
correlation [[ 1.         -0.47160811]
 [-0.47160811  1.        ]]
---
[[], [], []] [2 0 0 ... 1 1 1] [[ 22.748592 -50.470104]
 [-28.847343  10.572842]
 [-26.770884  36.127346]
 ...
 [-21.493185 -66.34493 ]
 [ 15.547026 -50.913742]
 [-13.729963 -24.98632 ]]
saved ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_2/latent__1_2.svg
results (all): {'cluster_avg': -0.057688355, 'cluster_all': array([-0.1353142 ,  0.50626385, -0.13581802, ...,  0.10758652,
        0.13228555, -0.0467803 ], dtype=float32), 'magnitude_avg': -0.4716081112278706, 'magnitude_all': -0.4716081112278706, 'tsne-2d_avg': nan, 'tsne-2d_all': nan}
results (only averages): {'id': 'mmvae_cmnist_oscn_seed_4', 'model_name': 'MMVAE_CMNIST_OSCN', 'target_modality': 1, 'target_property': 2, 'cluster_avg': -0.057688355, 'magnitude_avg': -0.4716081112278706, 'tsne-2d_avg': nan}
Arguments (initial):
{'device': 'cuda',
 'experiment': 'test-pbs-1',
 'no_cuda': False,
 'pretrained_path': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': '2023-12-04T12:37:09.612504',
 'run_type': 'synthesize'}
args are updated using pretrained_path:
 {'experiment': 'test-pbs-1', 'model': 'MMVAE_CMNIST_OSCN', 'run_type': 'train', 'seed': 4, 'num_hidden_layers': 1, 'use_conditional': False, 'latent_dim': 20, 'obj': 'dreg', 'batch_size': 128, 'epochs': 30, 'K': 20, 'learn_prior': False, 'llik_scaling': 0.0, 'logp': False, 'looser': False, 'print_freq': 100, 'no_analytics': False, 'no_cuda': False, 'run_id': 'mmvae_cmnist_oscn_seed_4', 'pretrained_path': '', 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4', 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train', 'device': 'cuda'}
Parameters were loaded for classifier
Run ID:
 2023-12-04T12:37:09.612504
Run Directory:
 ./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/2023-12-04T12:37:09.612504
Arguments (after settings):
{'K': 20,
 'batch_size': 128,
 'device': 'cuda',
 'epochs': 30,
 'experiment': 'test-pbs-1',
 'latent_dim': 20,
 'learn_prior': False,
 'llik_scaling': 0.0,
 'logp': False,
 'looser': False,
 'model': 'MMVAE_CMNIST_OSCN',
 'no_analytics': False,
 'no_cuda': False,
 'num_hidden_layers': 1,
 'obj': 'dreg',
 'output_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/train',
 'pretrained_path': '',
 'print_freq': 100,
 'run_dir': './rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4',
 'run_id': 'mmvae_cmnist_oscn_seed_4',
 'run_type': 'train',
 'seed': 4,
 'use_conditional': False}
run_ids_dict was automatically constructed.
run_ids_dict for synthesized: {'VAE_CMNIST': ['vae_cmnist_seed_0', 'vae_cmnist_seed_1', 'vae_cmnist_seed_2', 'vae_cmnist_seed_3', 'vae_cmnist_seed_4'], 'VAE_OSCN': ['vae_oscn_seed_4', 'vae_oscn_seed_3', 'vae_oscn_seed_0', 'vae_oscn_seed_2', 'vae_oscn_seed_1'], 'MMVAE_CMNIST_OSCN': ['mmvae_cmnist_oscn_seed_0', 'mmvae_cmnist_oscn_seed_3', 'mmvae_cmnist_oscn_seed_2', 'mmvae_cmnist_oscn_seed_1', 'mmvae_cmnist_oscn_seed_4']}
{'VAE_CMNIST': ['vae_cmnist_seed_0', 'vae_cmnist_seed_1', 'vae_cmnist_seed_2', 'vae_cmnist_seed_3', 'vae_cmnist_seed_4'], 'VAE_OSCN': ['vae_oscn_seed_4', 'vae_oscn_seed_3', 'vae_oscn_seed_0', 'vae_oscn_seed_2', 'vae_oscn_seed_1'], 'MMVAE_CMNIST_OSCN': ['mmvae_cmnist_oscn_seed_0', 'mmvae_cmnist_oscn_seed_3', 'mmvae_cmnist_oscn_seed_2', 'mmvae_cmnist_oscn_seed_1', 'mmvae_cmnist_oscn_seed_4']}
./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_0/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_1/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_2/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_3/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_CMNIST/vae_cmnist_seed_4/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_4/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_3/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_0/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_2/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/VAE_OSCN/vae_oscn_seed_1/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_0/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_3/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_2/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_1/analyse/1_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/0_1/analyse_result.csv was loaded.
./rslt/test-pbs-1/MMVAE_CMNIST_OSCN/mmvae_cmnist_oscn_seed_4/analyse/1_1/analyse_result.csv was loaded.
---
analyzed data (all):
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
10  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...              NaN            NaN
11  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...         0.164062       0.070312
12  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...              NaN            NaN
13  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...         0.117188       0.054688
14  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...              NaN            NaN
15  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...         0.046875       0.054688
16  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...              NaN            NaN
17  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...         0.132812       0.078125
18  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...              NaN            NaN
19  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...         0.101562       0.164062

[20 rows x 16 columns]
analyzed data (selected):
(target_modality == @target_modality and model_name in ["MMVAE_CMNIST_OSCN"]) or (not model_name in ["MMVAE_CMNIST_OSCN"])
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
10  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...              NaN            NaN
12  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...              NaN            NaN
14  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...              NaN            NaN
16  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...              NaN            NaN
18  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...              NaN            NaN

[15 rows x 16 columns]
---
pairplot
---
analyzed data (all):
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
10  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...              NaN            NaN
11  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...         0.164062       0.070312
12  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...              NaN            NaN
13  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...         0.117188       0.054688
14  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...              NaN            NaN
15  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...         0.046875       0.054688
16  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...              NaN            NaN
17  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...         0.132812       0.078125
18  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...              NaN            NaN
19  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...         0.101562       0.164062

[20 rows x 16 columns]
analyzed data (selected):
(target_modality == @target_modality and model_name in ["MMVAE_CMNIST_OSCN"]) or (not model_name in ["MMVAE_CMNIST_OSCN"])
                          id         model_name  ...  reconst_1x1_avg  cross_0x1_avg
0          vae_cmnist_seed_0         VAE_CMNIST  ...              NaN            NaN
1          vae_cmnist_seed_1         VAE_CMNIST  ...              NaN            NaN
2          vae_cmnist_seed_2         VAE_CMNIST  ...              NaN            NaN
3          vae_cmnist_seed_3         VAE_CMNIST  ...              NaN            NaN
4          vae_cmnist_seed_4         VAE_CMNIST  ...              NaN            NaN
5            vae_oscn_seed_4           VAE_OSCN  ...              NaN            NaN
6            vae_oscn_seed_3           VAE_OSCN  ...              NaN            NaN
7            vae_oscn_seed_0           VAE_OSCN  ...              NaN            NaN
8            vae_oscn_seed_2           VAE_OSCN  ...              NaN            NaN
9            vae_oscn_seed_1           VAE_OSCN  ...              NaN            NaN
11  mmvae_cmnist_oscn_seed_0  MMVAE_CMNIST_OSCN  ...         0.164062       0.070312
13  mmvae_cmnist_oscn_seed_3  MMVAE_CMNIST_OSCN  ...         0.117188       0.054688
15  mmvae_cmnist_oscn_seed_2  MMVAE_CMNIST_OSCN  ...         0.046875       0.054688
17  mmvae_cmnist_oscn_seed_1  MMVAE_CMNIST_OSCN  ...         0.132812       0.078125
19  mmvae_cmnist_oscn_seed_4  MMVAE_CMNIST_OSCN  ...         0.101562       0.164062

[15 rows x 16 columns]
---
pairplot
2023/12/06 AM 02:03:05
start time:  1701769573
end time:  1701795785
run time:  26212
