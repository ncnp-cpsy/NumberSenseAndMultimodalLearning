Arguments (initial):
 Namespace(K=20, batch_size=256, epochs=10, experiment='test/Classifier_OSCN', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Expt: None
RunID: 2023-12-01T22:50:19.471765
Arguments (after settings):
 Namespace(K=20, batch_size=256, cuda=True, device='cuda', epochs=10, experiment='test/Classifier_OSCN', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Parameters were loaded for classifier
Model runner was initialized.
Loading model Classifier_OSCN from ./rslt/test/Classifier_OSCN/2023-12-01T15:42:21.987845yxi7qjfw
objectives:  cross 
t_objectives:  cross
====> [MM-VAE] Time:   4.194s or 00:00:04
Traceback (most recent call last):
  File "main.py", line 157, in <module>
    main(run_type=args.run_type, args=args)
  File "main.py", line 141, in main
    run_train(args=args_classifier, run_path=run_path)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 187, in run_train
    _ = runner.train(epoch, agg)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 94, in train
    device=self.args.device)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/objectives.py", line 320, in cross
    loss = F.nll_loss(F.log_softmax(pred, dim=1), labels)
  File "/home/taka/.pyenv/versions/3.6.9/lib/python3.6/site-packages/torch/nn/functional.py", line 2532, in nll_loss
    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: "nll_loss_forward_reduce_cuda_kernel_2d_index" not implemented for 'Float'
Arguments (initial):
 Namespace(K=20, batch_size=256, epochs=10, experiment='test/Classifier_CMNIST', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Expt: None
RunID: 2023-12-01T22:50:26.491154
Arguments (after settings):
 Namespace(K=20, batch_size=256, cuda=True, device='cuda', epochs=10, experiment='test/Classifier_CMNIST', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Parameters were loaded for classifier
Model runner was initialized.
Loading model Classifier_CMNIST from ./rslt/test/Classifier_CMNIST/2023-12-01T15:43:23.118510rruaoa16/
objectives:  cross 
t_objectives:  cross
====> Epoch: 001 Train loss: 0.0001  took : 2.007380723953247
====> [MM-VAE] Time:   4.862s or 00:00:04
Traceback (most recent call last):
  File "main.py", line 157, in <module>
    main(run_type=args.run_type, args=args)
  File "main.py", line 141, in main
    run_train(args=args_classifier, run_path=run_path)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 188, in run_train
    save_model(runner.model, run_path + '/model.rar')
TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'
Arguments (initial):
 Namespace(K=20, batch_size=256, epochs=10, experiment='test/Classifier_OSCN', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Expt: None
RunID: 2023-12-01T22:53:38.639691
Arguments (after settings):
 Namespace(K=20, batch_size=256, cuda=True, device='cuda', epochs=10, experiment='test/Classifier_OSCN', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Parameters were loaded for classifier
Model runner was initialized.
objectives:  cross 
t_objectives:  cross
====> [MM-VAE] Time:   2.555s or 00:00:02
Traceback (most recent call last):
  File "main.py", line 157, in <module>
    main(run_type=args.run_type, args=args)
  File "main.py", line 141, in main
    run_train(args=args_classifier, run_path=run_path)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 187, in run_train
    _ = runner.train(epoch, agg)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 94, in train
    device=self.args.device)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/objectives.py", line 320, in cross
    loss = F.nll_loss(F.log_softmax(pred, dim=1), labels)
  File "/home/taka/.pyenv/versions/3.6.9/lib/python3.6/site-packages/torch/nn/functional.py", line 2532, in nll_loss
    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: "nll_loss_forward_reduce_cuda_kernel_2d_index" not implemented for 'Float'
Arguments (initial):
 Namespace(K=20, batch_size=256, epochs=10, experiment='test/Classifier_CMNIST', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Expt: None
RunID: 2023-12-01T22:53:43.859142
Arguments (after settings):
 Namespace(K=20, batch_size=256, cuda=True, device='cuda', epochs=10, experiment='test/Classifier_CMNIST', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Parameters were loaded for classifier
Model runner was initialized.
objectives:  cross 
t_objectives:  cross
====> Epoch: 001 Train loss: 0.0027  took : 2.022840976715088
====> [MM-VAE] Time:   4.295s or 00:00:04
Traceback (most recent call last):
  File "main.py", line 157, in <module>
    main(run_type=args.run_type, args=args)
  File "main.py", line 141, in main
    run_train(args=args_classifier, run_path=run_path)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 188, in run_train
    save_model(runner.model, run_path + '/model.rar')
TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'
Traceback (most recent call last):
  File "main.py", line 157, in <module>
    main(run_type=args.run_type, args=args)
  File "main.py", line 141, in main
    run_train(args=args_classifier, run_path=run_path)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 187, in run_train
    _ = runner.train(epoch, agg)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/runner.py", line 94, in train
    device=self.args.device)
  File "/new_nas/taka/NumberSenseAndMultimodalLearning/src/objectives.py", line 320, in cross
    loss = F.nll_loss(F.log_softmax(pred, dim=1), labels)
  File "/home/taka/.pyenv/versions/3.6.9/lib/python3.6/site-packages/torch/nn/functional.py", line 2532, in nll_loss
    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: "nll_loss_forward_reduce_cuda_kernel_2d_index" not implemented for 'Float'
Arguments (initial):
 Namespace(K=20, batch_size=256, epochs=10, experiment='test/Classifier_OSCN', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Expt: rslt/test/Classifier_OSCN/2023-12-01T22:55:39.295820eiy4avtk
RunID: 2023-12-01T22:55:39.295820
Arguments (after settings):
 Namespace(K=20, batch_size=256, cuda=True, device='cuda', epochs=10, experiment='test/Classifier_OSCN', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Parameters were loaded for classifier
Model runner was initialized.
objectives:  cross 
t_objectives:  cross
====> [MM-VAE] Time:   2.544s or 00:00:02
Arguments (initial):
 Namespace(K=20, batch_size=256, epochs=10, experiment='test/Classifier_CMNIST', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Expt: rslt/test/Classifier_CMNIST/2023-12-01T22:55:44.547313ygo0w_cg
RunID: 2023-12-01T22:55:44.547313
Arguments (after settings):
 Namespace(K=20, batch_size=256, cuda=True, device='cuda', epochs=10, experiment='test/Classifier_CMNIST', latent_dim=20, learn_prior=False, llik_scaling=0.0, logp=False, looser=False, model='Classifier_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='classify', seed=1, use_conditional=False)
Parameters were loaded for classifier
Model runner was initialized.
objectives:  cross 
t_objectives:  cross
====> Epoch: 001 Train loss: 0.0027  took : 1.9522933959960938
====> Test loss: -0.0012, Test accuracy: 0.9530
====> Epoch: 002 Train loss: 0.0008  took : 2.0208826065063477
====> Test loss: -0.0008, Test accuracy: 0.9728
====> Epoch: 003 Train loss: 0.0004  took : 2.0092833042144775
====> Test loss: -0.0007, Test accuracy: 0.9730
====> Epoch: 004 Train loss: 0.0002  took : 2.0100531578063965
====> Test loss: -0.0008, Test accuracy: 0.9740
====> Epoch: 005 Train loss: 0.0001  took : 2.0294179916381836
====> Test loss: -0.0008, Test accuracy: 0.9740
====> Epoch: 006 Train loss: 0.0001  took : 1.9848532676696777
====> Test loss: -0.0009, Test accuracy: 0.9732
====> Epoch: 007 Train loss: 0.0000  took : 2.0075626373291016
====> Test loss: -0.0009, Test accuracy: 0.9760
====> Epoch: 008 Train loss: 0.0000  took : 2.0170962810516357
====> Test loss: -0.0009, Test accuracy: 0.9758
====> Epoch: 009 Train loss: 0.0000  took : 1.999009370803833
====> Test loss: -0.0010, Test accuracy: 0.9748
====> Epoch: 010 Train loss: 0.0000  took : 1.9842863082885742
====> Test loss: -0.0010, Test accuracy: 0.9758
====> Epoch: 011 Train loss: 0.0000  took : 2.0143771171569824
====> Test loss: -0.0010, Test accuracy: 0.9750
====> Epoch: 012 Train loss: 0.0000  took : 1.9800217151641846
====> Test loss: -0.0010, Test accuracy: 0.9748
====> Epoch: 013 Train loss: 0.0000  took : 1.9904720783233643
====> Test loss: -0.0010, Test accuracy: 0.9748
====> Epoch: 014 Train loss: 0.0000  took : 2.0204904079437256
====> Test loss: -0.0010, Test accuracy: 0.9742
====> Epoch: 015 Train loss: 0.0000  took : 1.9965150356292725
====> Test loss: -0.0011, Test accuracy: 0.9752
====> Epoch: 016 Train loss: 0.0000  took : 2.0011990070343018
====> Test loss: -0.0011, Test accuracy: 0.9750
====> Epoch: 017 Train loss: 0.0000  took : 1.9587550163269043
====> Test loss: -0.0011, Test accuracy: 0.9748
====> Epoch: 018 Train loss: 0.0000  took : 1.9985127449035645
====> Test loss: -0.0011, Test accuracy: 0.9755
====> Epoch: 019 Train loss: 0.0000  took : 1.9937610626220703
====> Test loss: -0.0011, Test accuracy: 0.9742
====> Epoch: 020 Train loss: 0.0000  took : 2.0224032402038574
====> Test loss: -0.0011, Test accuracy: 0.9745
====> Epoch: 021 Train loss: 0.0000  took : 1.9683494567871094
====> Test loss: -0.0012, Test accuracy: 0.9750
====> Epoch: 022 Train loss: 0.0000  took : 2.0444552898406982
====> Test loss: -0.0011, Test accuracy: 0.9758
====> Epoch: 023 Train loss: 0.0000  took : 2.0189049243927
====> Test loss: -0.0012, Test accuracy: 0.9748
====> Epoch: 024 Train loss: 0.0000  took : 2.0067710876464844
====> Test loss: -0.0012, Test accuracy: 0.9740
====> Epoch: 025 Train loss: 0.0000  took : 1.9858965873718262
====> Test loss: -0.0012, Test accuracy: 0.9750
====> Epoch: 026 Train loss: 0.0000  took : 1.974287748336792
====> Test loss: -0.0012, Test accuracy: 0.9750
====> Epoch: 027 Train loss: 0.0000  took : 1.9776971340179443
====> Test loss: -0.0012, Test accuracy: 0.9750
====> Epoch: 028 Train loss: 0.0000  took : 1.9965875148773193
====> Test loss: -0.0012, Test accuracy: 0.9755
====> Epoch: 029 Train loss: 0.0000  took : 2.0253400802612305
====> Test loss: -0.0012, Test accuracy: 0.9750
====> Epoch: 030 Train loss: 0.0000  took : 1.970815658569336
====> Test loss: -0.0012, Test accuracy: 0.9748
====> [MM-VAE] Time:  68.354s or 00:01:08
Arguments (initial):
 Namespace(K=30, batch_size=128, epochs=50, experiment='test/VAE_CMNIST', latent_dim=20, learn_prior=True, llik_scaling=0.0, logp=False, looser=False, model='VAE_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='train', seed=4, use_conditional=False)
Expt: rslt/test/VAE_CMNIST/2023-12-01T23:23:35.3397451g8wd7vk
RunID: 2023-12-01T23:23:35.339745
Arguments (after settings):
 Namespace(K=30, batch_size=128, cuda=True, device='cuda', epochs=50, experiment='test/VAE_CMNIST', latent_dim=20, learn_prior=True, llik_scaling=0.0, logp=False, looser=False, model='VAE_CMNIST', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='train', seed=4, use_conditional=False)
Model runner was initialized.
objectives:  elbo 
t_objectives:  iwae
====> Epoch: 001 Train loss: 1552.5187  took : 3.9541587829589844
====> Test loss: 1531.8803
====> Epoch: 002 Train loss: 1532.9670  took : 3.9445457458496094
====> Test loss: 1526.6407
====> Epoch: 003 Train loss: 1528.6051  took : 3.9483978748321533
====> Test loss: 1524.5271
====> Epoch: 004 Train loss: 1526.7778  took : 3.940157651901245
====> Test loss: 1523.2938
====> Epoch: 005 Train loss: 1525.7485  took : 3.9273407459259033
====> Test loss: 1522.7141
====> Epoch: 006 Train loss: 1525.0318  took : 3.9320030212402344
====> Test loss: 1522.0858
====> Epoch: 007 Train loss: 1524.5036  took : 3.9000566005706787
====> Test loss: 1521.7253
====> Epoch: 008 Train loss: 1524.1532  took : 3.888637065887451
====> Test loss: 1521.3200
====> Epoch: 009 Train loss: 1523.8675  took : 3.9090559482574463
====> Test loss: 1521.0340
====> Epoch: 010 Train loss: 1523.5517  took : 3.950049638748169
====> Test loss: 1520.8163
====> Epoch: 011 Train loss: 1523.3605  took : 3.9572486877441406
====> Test loss: 1520.5691
====> Epoch: 012 Train loss: 1523.1498  took : 3.9014971256256104
====> Test loss: 1520.3178
====> Epoch: 013 Train loss: 1523.0358  took : 3.9576640129089355
====> Test loss: 1520.2239
====> Epoch: 014 Train loss: 1522.8821  took : 3.950277090072632
====> Test loss: 1520.0901
====> Epoch: 015 Train loss: 1522.7292  took : 3.9709982872009277
====> Test loss: 1519.9248
====> Epoch: 016 Train loss: 1522.6118  took : 3.9563968181610107
====> Test loss: 1519.9377
====> Epoch: 017 Train loss: 1522.4812  took : 3.8940041065216064
====> Test loss: 1519.8089
====> Epoch: 018 Train loss: 1522.3818  took : 3.965167760848999
====> Test loss: 1519.6570
====> Epoch: 019 Train loss: 1522.2448  took : 3.9609720706939697
====> Test loss: 1519.5741
====> Epoch: 020 Train loss: 1522.1955  took : 3.9511191844940186
====> Test loss: 1519.5555
====> Epoch: 021 Train loss: 1522.0827  took : 3.94231915473938
====> Test loss: 1519.5557
====> Epoch: 022 Train loss: 1522.0453  took : 3.895932674407959
====> Test loss: 1519.4246
====> Epoch: 023 Train loss: 1521.9616  took : 3.985429286956787
====> Test loss: 1519.4569
====> Epoch: 024 Train loss: 1521.8878  took : 3.8935768604278564
====> Test loss: 1519.3312
====> Epoch: 025 Train loss: 1521.8286  took : 3.951838493347168
====> Test loss: 1519.2250
====> Epoch: 026 Train loss: 1521.7617  took : 3.932722568511963
====> Test loss: 1519.1459
====> Epoch: 027 Train loss: 1521.6733  took : 3.961381435394287
====> Test loss: 1519.1833
====> Epoch: 028 Train loss: 1521.6492  took : 3.90535306930542
====> Test loss: 1518.8714
====> Epoch: 029 Train loss: 1521.6099  took : 3.939817428588867
====> Test loss: 1519.0448
====> Epoch: 030 Train loss: 1521.5508  took : 3.9706525802612305
====> Test loss: 1518.9936
====> Epoch: 031 Train loss: 1521.4722  took : 3.9623115062713623
====> Test loss: 1518.8964
====> Epoch: 032 Train loss: 1521.4659  took : 3.927102565765381
====> Test loss: 1518.9575
====> Epoch: 033 Train loss: 1521.4197  took : 3.9470980167388916
====> Test loss: 1518.8526
====> Epoch: 034 Train loss: 1521.4106  took : 3.9429447650909424
====> Test loss: 1518.8459
====> Epoch: 035 Train loss: 1521.2869  took : 3.9738473892211914
====> Test loss: 1518.7598
====> Epoch: 036 Train loss: 1521.3206  took : 3.9262678623199463
====> Test loss: 1518.7926
====> Epoch: 037 Train loss: 1521.2909  took : 3.929863691329956
====> Test loss: 1518.7414
====> Epoch: 038 Train loss: 1521.2433  took : 3.9332618713378906
====> Test loss: 1518.7543
====> Epoch: 039 Train loss: 1521.2024  took : 3.966218948364258
====> Test loss: 1518.6878
====> Epoch: 040 Train loss: 1521.1678  took : 3.9855880737304688
====> Test loss: 1518.7147
====> Epoch: 041 Train loss: 1521.1435  took : 3.9866480827331543
====> Test loss: 1518.6854
====> Epoch: 042 Train loss: 1521.1380  took : 3.9955809116363525
====> Test loss: 1518.7494
====> Epoch: 043 Train loss: 1521.1074  took : 3.9336137771606445
====> Test loss: 1518.4670
====> Epoch: 044 Train loss: 1521.0487  took : 3.9721527099609375
====> Test loss: 1518.5876
====> Epoch: 045 Train loss: 1521.0464  took : 3.9958736896514893
====> Test loss: 1518.7533
====> Epoch: 046 Train loss: 1521.0278  took : 3.93540358543396
====> Test loss: 1518.5204
====> Epoch: 047 Train loss: 1520.9856  took : 3.9542860984802246
====> Test loss: 1518.5323
====> Epoch: 048 Train loss: 1520.9557  took : 3.99950909614563
====> Test loss: 1518.4904
====> Epoch: 049 Train loss: 1520.9512  took : 3.929187774658203
====> Test loss: 1518.4736
====> Epoch: 050 Train loss: 1520.9237  took : 3.9996278285980225
====> Test loss: 1518.4644
====> [MM-VAE] Time: 346.808s or 00:05:46
Arguments (initial):
 Namespace(K=30, batch_size=128, epochs=50, experiment='test/VAE_OSCN', latent_dim=20, learn_prior=True, llik_scaling=0.0, logp=False, looser=False, model='VAE_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', print_freq=0, run_type='train', seed=4, use_conditional=False)
Expt: rslt/test/VAE_OSCN/2023-12-01T23:29:25.079311e5vor3pu
RunID: 2023-12-01T23:29:25.079311
Arguments (after settings):
 Namespace(K=30, batch_size=128, cuda=True, device='cuda', epochs=50, experiment='test/VAE_OSCN', latent_dim=20, learn_prior=True, llik_scaling=0.0, logp=False, looser=False, model='VAE_OSCN', no_analytics=False, no_cuda=False, num_hidden_layers=1, obj='elbo', output_dir='./', pre_trained='', pretrained_path='', print_freq=0, run_type='train', seed=4, use_conditional=False)
Model runner was initialized.
objectives:  elbo 
t_objectives:  iwae
====> Epoch: 001 Train loss: 1524.9331  took : 6.3040549755096436
====> Test loss: 1485.4076
====> Epoch: 002 Train loss: 1495.0458  took : 6.233850717544556
====> Test loss: 1485.4057
====> Epoch: 003 Train loss: 1495.0451  took : 6.243950366973877
====> Test loss: 1485.4055
====> Epoch: 004 Train loss: 1495.0450  took : 6.213600397109985
====> Test loss: 1485.4056
====> Epoch: 005 Train loss: 1495.0450  took : 6.262757301330566
====> Test loss: 1485.4055
====> Epoch: 006 Train loss: 1495.0450  took : 6.271278381347656
====> Test loss: 1485.4056
====> Epoch: 007 Train loss: 1495.0449  took : 6.222698450088501
====> Test loss: 1485.4055
====> Epoch: 008 Train loss: 1495.0449  took : 6.2579100131988525
====> Test loss: 1485.4056
====> Epoch: 009 Train loss: 1495.0449  took : 6.243956565856934
====> Test loss: 1485.4056
====> Epoch: 010 Train loss: 1495.0449  took : 6.237862825393677
====> Test loss: 1485.4055
====> Epoch: 011 Train loss: 1495.0449  took : 6.284904479980469
====> Test loss: 1485.4056
====> Epoch: 012 Train loss: 1495.0449  took : 6.2591962814331055
====> Test loss: 1485.4055
====> Epoch: 013 Train loss: 1495.0449  took : 6.244665861129761
====> Test loss: 1485.4056
====> Epoch: 014 Train loss: 1495.0449  took : 6.214056968688965
====> Test loss: 1485.4055
====> Epoch: 015 Train loss: 1495.0449  took : 6.260826110839844
====> Test loss: 1485.4055
====> Epoch: 016 Train loss: 1495.0449  took : 6.278381824493408
====> Test loss: 1485.4056
====> Epoch: 017 Train loss: 1495.0449  took : 6.274285793304443
====> Test loss: 1485.4055
====> Epoch: 018 Train loss: 1495.0449  took : 6.257799863815308
====> Test loss: 1485.4056
====> Epoch: 019 Train loss: 1495.0449  took : 6.265653371810913
====> Test loss: 1485.4056
====> Epoch: 020 Train loss: 1495.0449  took : 6.187162160873413
====> Test loss: 1485.4056
====> Epoch: 021 Train loss: 1495.0449  took : 6.270422697067261
====> Test loss: 1485.4055
====> Epoch: 022 Train loss: 1495.0449  took : 6.2375242710113525
====> Test loss: 1485.4055
====> Epoch: 023 Train loss: 1495.0449  took : 6.257220029830933
====> Test loss: 1485.4055
====> Epoch: 024 Train loss: 1495.0449  took : 6.238616943359375
====> Test loss: 1485.4055
====> Epoch: 025 Train loss: 1495.0449  took : 6.258713245391846
====> Test loss: 1485.4056
====> Epoch: 026 Train loss: 1495.0449  took : 6.289507150650024
====> Test loss: 1485.4055
====> Epoch: 027 Train loss: 1495.0449  took : 6.203329086303711
====> Test loss: 1485.4056
====> Epoch: 028 Train loss: 1495.0449  took : 6.261908531188965
====> Test loss: 1485.4056
====> Epoch: 029 Train loss: 1495.0449  took : 6.269170522689819
====> Test loss: 1485.4055
====> Epoch: 030 Train loss: 1495.0449  took : 6.27312707901001
====> Test loss: 1485.4056
====> Epoch: 031 Train loss: 1495.0449  took : 6.2444610595703125
====> Test loss: 1485.4055
====> Epoch: 032 Train loss: 1495.0449  took : 6.267896890640259
====> Test loss: 1485.4055
====> Epoch: 033 Train loss: 1495.0449  took : 6.2475974559783936
====> Test loss: 1485.4056
====> Epoch: 034 Train loss: 1495.0449  took : 6.265350103378296
====> Test loss: 1485.4055
====> Epoch: 035 Train loss: 1495.0448  took : 6.2765491008758545
====> Test loss: 1485.4055
====> Epoch: 036 Train loss: 1495.0449  took : 6.256486177444458
====> Test loss: 1485.4056
====> Epoch: 037 Train loss: 1495.0448  took : 6.26041841506958
====> Test loss: 1485.4056
====> Epoch: 038 Train loss: 1495.0448  took : 6.206454753875732
====> Test loss: 1485.4056
====> Epoch: 039 Train loss: 1495.0449  took : 6.263692617416382
====> Test loss: 1485.4056
====> Epoch: 040 Train loss: 1495.0449  took : 6.231790542602539
====> Test loss: 1485.4056
====> Epoch: 041 Train loss: 1495.0449  took : 6.257396221160889
====> Test loss: 1485.4055
====> Epoch: 042 Train loss: 1495.0448  took : 6.2377729415893555
====> Test loss: 1485.4055
====> Epoch: 043 Train loss: 1495.0448  took : 6.233294248580933
====> Test loss: 1485.4055
====> Epoch: 044 Train loss: 1495.0448  took : 6.229936599731445
====> Test loss: 1485.4055
====> Epoch: 045 Train loss: 1495.0448  took : 6.267846345901489
====> Test loss: 1485.4056
====> Epoch: 046 Train loss: 1495.0448  took : 6.2635602951049805
====> Test loss: 1485.4056
====> Epoch: 047 Train loss: 1495.0448  took : 6.291933059692383
====> Test loss: 1485.4056
====> Epoch: 048 Train loss: 1495.0449  took : 6.271490573883057
====> Test loss: 1485.4056
====> Epoch: 049 Train loss: 1495.0448  took : 6.252908229827881
====> Test loss: 1485.4055
====> Epoch: 050 Train loss: 1495.0448  took : 6.262296915054321
====> Test loss: 1485.4055
====> [MM-VAE] Time: 428.112s or 00:07:08
